[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This is my Statistical Methods website.\nHere you can find collated ‘courses’ on a variety of statistical topics, based largely around generalised linear models.\nThe website was built using Quarto in VSCode."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my statistical methods website.\nThis websites contains materials originally developed as part of my blog. The blog should be considered something of a staging post for new material. Once this material has reached enough maturity, it will be ‘promoted’ to this dedicated statisics site."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-15/index.html",
    "href": "pages/causal-inference/lms-are-glms-part-15/index.html",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "",
    "text": "This is the second post on a short mini-series on causal inference. The previous post provided a non-technical introduction to the core challenge of causal inference, namely that the counterfactual is always unobserved, meaning at least half of the data required to really know the causal effect of something is always missing. In the previous post different historians made different assumptions about what the counterfactual would have looked like - what would have happened if something that did happen, hadn’t happened - and based on this came to very different judgements about the effect that Henry Dundas, an 18th century Scottish politician, had on the transatlantic slave trade.\nThis post is more technical, aiming to show: how awkward phrases like “What would have happened if something that did happen, hadn’t happened” are expressed algebraically; how the core problem of causal inference is expressed in this framework; the technical impossibility of addressing the question of causal inference from the Platinum Standard of estimating causal effects on individuals; and describe the reason why randomised controlled trials (RCTs) provide the Gold Standard for trying to estimate these effects for populations."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-15/index.html#introduction",
    "href": "pages/causal-inference/lms-are-glms-part-15/index.html#introduction",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "",
    "text": "This is the second post on a short mini-series on causal inference. The previous post provided a non-technical introduction to the core challenge of causal inference, namely that the counterfactual is always unobserved, meaning at least half of the data required to really know the causal effect of something is always missing. In the previous post different historians made different assumptions about what the counterfactual would have looked like - what would have happened if something that did happen, hadn’t happened - and based on this came to very different judgements about the effect that Henry Dundas, an 18th century Scottish politician, had on the transatlantic slave trade.\nThis post is more technical, aiming to show: how awkward phrases like “What would have happened if something that did happen, hadn’t happened” are expressed algebraically; how the core problem of causal inference is expressed in this framework; the technical impossibility of addressing the question of causal inference from the Platinum Standard of estimating causal effects on individuals; and describe the reason why randomised controlled trials (RCTs) provide the Gold Standard for trying to estimate these effects for populations."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-15/index.html#models-dont-care-about-causality-but-we-do",
    "href": "pages/causal-inference/lms-are-glms-part-15/index.html#models-dont-care-about-causality-but-we-do",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Models don’t care about causality… but we do",
    "text": "Models don’t care about causality… but we do\nThe first stage when using a statistical model is to take a big rectangle of data, \\(D\\), and split the columns of the data into two types:\n\nPredictor variables, usually denoted \\(X\\)\nResponse variables, usually denoted \\(y\\)\n\nWith the predictor variables and the response variables defined, the challenge of model fitting is then to find some combination of model parameters \\(\\theta\\) that minimises in some way the gap between the observed response values \\(y\\), and the predicted response values from the model \\(Y\\).\nThe first point to note is that, from the perspective of the model, it does not matter which variable or variables from \\(D\\) we choose to put in the predictor side \\(X\\) or the response side \\(y\\). Even if we put a variable from the future in as a predictor of something in the past, the optimisation algorithms will still work in exactly the same way, working to minimise the gap between observed and predicted responses. The only problem is such a model would make no sense from a causal perspective.\nThe model also does not ‘care’ about how we think about and go about defining any of the variables that go into the predictor side of the equation, \\(X\\). But again, we do. In particular, when thinking about causality it can be immensely helpful to imagine splitting the predictor columns up into some conceptually different types. This will be helpful for thinking about causal inference using some algebra."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-15/index.html#the-impossible-platinum-standard",
    "href": "pages/causal-inference/lms-are-glms-part-15/index.html#the-impossible-platinum-standard",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "The (Impossible) Platinum Standard",
    "text": "The (Impossible) Platinum Standard\nIn some previous expressions of the data, \\(D\\), we used the subscript \\(i\\) to indicate the rows of the data which go into the model. Each of these rows is, by convention, a different observation. So, instead of saying the purpose of the model is to predict \\(y\\) on \\(X\\), it’s more precisely to predict \\(y_i\\) on \\(X_i\\), for all \\(i\\) in the data (i.e. all rows in \\(D\\)).\nNow let’s do some predictor variable fission and say, for our purposes, that:\n\\[\nX_i = \\{X_i^*, Z_i\\}\n\\]\nHere \\(Z_i\\) is an assignment variable, and takes either a value of 1, meaning ‘is assigned’, or 0, meaning ‘is not assigned’. The variable \\(X_i^*\\), by contrast, means ‘all other predictor variables’.\nFor individual observations \\(D_i\\) where \\(Z_i = 1\\), the individual is exposed (or treated) to something. And for individual observations \\(D_i\\) where \\(Z_i = 0\\), the individual is not exposed (or not treated) to that thing.\nThe causal effect of assignment, or treatment, for any individual observation is:\n\\[\nTE_i = y_i|(X_i^*, Z = 1) - y_i| (X_i^*, Z = 0)\n\\]\nThe fundamental problem of causal inference, however, is that for any individual observation \\(i\\), one of the two parts of this expression is always missing. If an individual \\(i\\) had been assigned, then \\(y_i|(X_i^*, Z=1)\\) is observed, but \\(y_i|(X_i^*, Z=0)\\) is unobserved. By contrast, if an individual \\(i\\) had not been assigned, then \\(y_i|(X_i^*, Z=0)\\) is observed, but \\(y_i|(X_i^*, Z=1)\\) is unobserved.\nAnother way to think about this is as a table, where the treatment effect for an individual involves comparing the outcomes reported in two columns of the same row, but the cells in one of these two columns is always missing:\n\n\n\n\n\n\n\n\n\nindividual\noutcome if treated\noutcome if not treated\ntreatment effect\n\n\n\n\n1\n4.8\n??\n??\n\n\n2\n3.7\n??\n??\n\n\n3\n??\n2.3\n??\n\n\n4\n3.1\n??\n??\n\n\n5\n??\n3.4\n??\n\n\n6\n??\n2.9\n??\n\n\n\nThe Platinum Standard of causal effect estimation would therefore be if the missing cells in the outcome columns could be accurately filled in, allowing the treatment effect for each individual to be calculated.\nHowever, this isn’t possible. It’s social science fiction, as we can’t split the universe and compare parallel realities: one in which what happened didn’t happen, and the other in which what didn’t happen happened.\nSo, what can be done?"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-15/index.html#the-everyday-fools-gold-standard",
    "href": "pages/causal-inference/lms-are-glms-part-15/index.html#the-everyday-fools-gold-standard",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "The Everyday Fool’s Gold Standard",
    "text": "The Everyday Fool’s Gold Standard\nThere’s one thing you might be tempted to do with the kind of data shown in the table above: compare the average outcome in the treated group with the average outcome in the untreated group, i.e.:\n\\[\nATE = E(y | Z = 1) - E(y | Z = 0)\n\\]\nLet’s do this with the example above:\n\n\nCode\ne_y_z1 &lt;- mean(c(4.8, 3.7, 3.1))\ne_y_z0 &lt;- mean(c(2.3, 3.4, 2.9))\n\n\n# And the difference?\ne_y_z1 - e_y_z0\n\n\n[1] 1\n\n\nIn this example, the difference in the averages between the two groups is 1.0.1 Based on this, we might imagine the first individual, who was treated, would have had a score of 3.8 rather than 4.8, and the third individual, who was not treated, would have received a score of 3.3 rather than 2.3 if they had been treated.\nSo, what’s the problem with just comparing the averages in this way? Potentially, nothing. But potentially, a lot. It depends on the data and the problem. More specifically, it depends on the relationship between the assignment variable, \\(Z\\), and the other characteristics of the individual, which includes but is not usually entirely captured by the known additional characteristics of the individual, \\(X_i^*\\).\nLet’s give a specific example: What if I were to tell you that the outcomes \\(y_i\\) were waiting times at public toilets/bathrooms, and the assignment variable, \\(Z\\), takes the value 1 if the individual has been assigned to a facility containing urinals, and 0 if the individual has been assigned to a facility containing no urinals? Would it be right to infer that the difference in the average is the average causal effect of urinals in public toilets/bathrooms?\nI’d suggest not, because there are characteristics of the individual which govern assignment to bathroom type. What this means is that \\(Z_i\\) and \\(X_i^*\\) are coupled or related to each other in some way. So, any difference in the average outcome between those assigned to (or ‘treated with’) urinals could be due to the urinals themselves; or could be due to other ways that ‘the treated’ and ‘the untreated’ differ from each other systematically. We may be able to observe a difference, and to report that it’s statistically significant. But we don’t know how much, if any, of that difference is due to the exposure or treatment of primary interest to us, and how much is due to other ways in the ‘treated’ and ‘untreated’ groups differ.\nSo, we need some way of breaking the link between \\(Z\\) and \\(X^*\\). How do we do this?"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-15/index.html#why-randomised-controlled-trials-are-the-real-gold-standard",
    "href": "pages/causal-inference/lms-are-glms-part-15/index.html#why-randomised-controlled-trials-are-the-real-gold-standard",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Why Randomised Controlled Trials are the real Gold Standard",
    "text": "Why Randomised Controlled Trials are the real Gold Standard\nThe clue’s in the subheading. Randomised Controlled Trials (RCTs) are known as the Gold Standard for scientific evaluation of effects for a reason, and the reason is this: they’re explicitly designed to break the link between \\(Z\\) and \\(X^*\\). And not just \\(X^*\\), but any unobserved or unincluded characteristics of the individuals, \\(W^*\\), which might also otherwise influence assignment or selection to \\(Z\\) but we either couldn’t measure or didn’t choose to include.\nThe key idea of an RCT is that assignment to either a treated or untreated group, or to any additional arms of the trial, has nothing to do with the characteristics of any individual in the trial. Instead, the allocation is random, determined by a figurature (or historically occasionally literal) coin toss. 2\nWhat this random assignment means is that assignment \\(Z\\) should be unrelated to the known characteristics \\(X^*\\), as well as unknown characteristics \\(W^*\\). The technical term for this (if I remember correctly) is that assignment is orthogonal to other characteristics, represented algebraically as \\(Z \\perp X^*\\) and \\(Z \\perp W^*\\).\nThis doesn’t mean that, for any particular trial, there will be zero correlation between \\(Z\\) and other characteristics. Nor does it mean that the characteristics of participants will be the same across trial arms. Because of random variation there are always going to be differences between arms in any specific RCT. However, we know that, because we are aware of the mechanism used to allocate participants to treated or non-treated groups (or more generally to trial arms), the expected difference in characteristics will be zero across many RCTs. Along with increased observations, this is the reason why, in principle, a meta-analysis of methodologically identical RCTs should offer even greater precision as to the causal effect of a treatment than just relying on a single RCT. 3"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-15/index.html#summing-up-and-coming-up",
    "href": "pages/causal-inference/lms-are-glms-part-15/index.html#summing-up-and-coming-up",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Summing up and coming up",
    "text": "Summing up and coming up\nA key point to note is that, when analysing a properly conducted RCT to estimate a treatment effect, the ATE formula shown above, which is naive and likely to be biased when working with observational data, is likely to produce an unbiased estimate of the treatment effect. Because the trial design is sophisticated in the way it breaks the link between \\(Z\\) and everything else, the statistical analysis does not have to be sophisticated.\nThe flip side of this, however, is that when the data are observational, and it would be naive (as with the urinals and waiting times example) to assume that \\(Z\\) is unlinked to everything else known (\\(X^*\\)) and unknown (\\(W^*\\)), then more careful and bespoke statistical modelling approaches are likely to be required to recover non-biased causal effects. Such modelling approaches need to be mindful of both the platinum and gold standards presented above, and rely on modelling and other assumptions to try to simulate what the treatment effects would be if these unobtainable (platinum) and unobtained (gold) standards had been obtained.\nThe next post will start to delve into some of these approaches."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-15/index.html#footnotes",
    "href": "pages/causal-inference/lms-are-glms-part-15/index.html#footnotes",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is pure fluke. I didn’t choose the values to get a difference of exactly 1, but there we go…↩︎\nIn the gold-plated gold standard of the double-blind RCT, not even the people running the trial and interacting with participants would be aware of which treatment a participant has been assigned. They would simply be given a participant ID, find a pack containing the participant’s treatment, and give this pack to the participant. Only a statistician, who has access to a random number cypher, would know which participants are assigned to which treatment, and they might not know until the trial has concluded. The idea of all of these layers of secrecy in assignment is to reduce the possibility that those running the experiment could intentionally or unintentially inform participants about which treatment they’re receiving, and so create expectations in participants about the effectiveness or otherwise of the treatments, which could have an additional effect on the outcomes.↩︎\nIn practice, issues like methodological variation, and publication bias, mean that meta-analyses of RCTs are unlikely to provide as accurate and unbiased an estimate of treatment effect as we would hope for.↩︎"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-16/index.html",
    "href": "pages/causal-inference/lms-are-glms-part-16/index.html",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "",
    "text": "This is the third post in a short mini-series on causal inference, which extends a much longer series on statistical theory and practice. After introducing the fundamental issue of causal inference, namely that the counterfactual is unobserved, through description alone in part 14, part 15 provided a more technical treatment of the same issues. We described the Platinum 1 Standard of data required for causal inference as involving observing the same individuals in two different scenarios - treated2 and untreated3 - which is not possible; and the Gold Standard as being a randomised controlled trial (RCT), which is sometimes possible, but tends to be time and resource intensive. The RCT is a mechanism for breaking the association between assignment to treatment \\(Z_i\\) and both known/included covariates \\(X^*_i\\) and unknown/unincluded characteristics \\(W_i\\); this link-breaking is described as orthogonality and represented algebraically as \\(Z_i \\perp X_i^*\\) and \\(Z_i \\perp W^*_i\\).\nThe purpose of this post is to introduce some of the statistical approaches used when the only data available are observational, and so do not meet the special properties required for robust causal inference estimation of an RCT."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-16/index.html#introduction",
    "href": "pages/causal-inference/lms-are-glms-part-16/index.html#introduction",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "",
    "text": "This is the third post in a short mini-series on causal inference, which extends a much longer series on statistical theory and practice. After introducing the fundamental issue of causal inference, namely that the counterfactual is unobserved, through description alone in part 14, part 15 provided a more technical treatment of the same issues. We described the Platinum 1 Standard of data required for causal inference as involving observing the same individuals in two different scenarios - treated2 and untreated3 - which is not possible; and the Gold Standard as being a randomised controlled trial (RCT), which is sometimes possible, but tends to be time and resource intensive. The RCT is a mechanism for breaking the association between assignment to treatment \\(Z_i\\) and both known/included covariates \\(X^*_i\\) and unknown/unincluded characteristics \\(W_i\\); this link-breaking is described as orthogonality and represented algebraically as \\(Z_i \\perp X_i^*\\) and \\(Z_i \\perp W^*_i\\).\nThe purpose of this post is to introduce some of the statistical approaches used when the only data available are observational, and so do not meet the special properties required for robust causal inference estimation of an RCT."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-16/index.html#method-one-controlling-for-variables",
    "href": "pages/causal-inference/lms-are-glms-part-16/index.html#method-one-controlling-for-variables",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Method One: ‘Controlling for’ variables",
    "text": "Method One: ‘Controlling for’ variables\nThe most familiar approach for trying to estimate the causal effect of treatment \\(Z\\) on outcome \\(Y\\) is to construct a multivariate 4 regression model. Here we make sure to include those ‘nuisance parameters’ \\(X^*\\) on the predictor side of the model’s equation, along with our treatment parameter of interest \\(Z\\). For each individual \\(i\\) in the dataset \\(D\\) we can therefore use the model, calibrated on the data, to produce a prediction of the outcome \\(Y_i\\) under both the treated scenario \\(Z_i = 1\\) and the untreated scenario \\(Z_i = 0\\). As post four discussed, in the specific case of linear regression, but few other model specifications, this causal effect estimate of treatment \\(Y_i | Z=1 - Y_i | Z = 0\\) can be gleamed directly from the \\(\\beta\\) coefficient for \\(Z\\). As post four also makes clear, for other model specifications, the process for estimating causal effects can be more involved.\nIt is worth pointing out that, when using models in this way, we are really ‘just’ producing estimates of first differences, the quantity of interest which we focused on in posts 11, 12, and 13. The model prediction approach is not fundamentally any different to that discussed previously, except for two things: firstly, that we will usually be averaging across first differences for multiple observations rather a single scenario; and secondly, that we will be interpreting the first differences (or rather their aggregation) as being a causal effect estimate.\nThere are actually two types of causal effect estimate we can produce using this approach, the Average Treatment Effect (ATE), and the Average Treatment Effect on the Treated (ATT). 5 The difference between ATE and ATT is that, for ATE, the counterfactuals are simulated for all observations in the dataset \\(D\\), and that these counterfactuals will be both for individuals which were observed as treated \\(Z=1\\) and untreated \\(Z=0\\). By contrast, for ATT, only those observations in the data which were observed as treated \\(Z=1\\) are included in the causal effect estimation,6 meaning that the counterfactual being modelled will always be of the scenario \\(Z=0\\).\nSo, what are the potential problems with modelling in this way?\n\nUnobserved and unincluded covariates. Remember in the previous part we introduced the term \\(W_i^*\\)? This refers to those factors which could affect assignment \\(Z_i\\) but which are not included in our model. They could either be: i) covariates that exist in the dataset \\(D\\) but we chose not to include in the model \\(M\\); or ii) covariates that are simply not recorded in the dataset \\(D\\), so even if we wanted to, we couldn’t include them. In an RCT, the random allocation mechanism breaks both the \\(X^* \\rightarrow Z\\) and the \\(W \\rightarrow Z\\) causal paths; we don’t have to observe or even know what these factors \\(W\\) might be for an RCT to block their influence. But a regression model can only really operate to attempt to attenuate the \\(X^* \\rightarrow Z\\) pathway.\nInsufficient or improper controls. Returning to our hamster tooth growth example of post 11, recall we looked at a number of different model specifications. Our starter model specification included ‘controls for’ both dosage and supplement, and so did our final model specification. But does this mean either model is equally good at ‘controlling for’ these factors? I’d suggest they aren’t, as though our final model specification included the same covariates \\(X\\) as the initial model specification, it represented the relationship between the predictor and response variables in a qualitatively different way. For the final model specification, the dosage variable was transformed by logging it; additionally, an interaction term was included between (transformed) dosage and supplement. The reasons for this were justified by the observed relationships and by measures of penalised model fit, but we do not know if this represents the ‘best possible’ model specification. And the specification used, and the assumptions contained and represented by the model specification, will affect the predictions the model produces, including the first differences used to produce the ATE and ATT causal effect estimates.\n\nOverall, just remember that, when a researcher states in a paper that they have used a model to ‘control for’ various factors and characteristics, this can often be more a statement of what the researcher aspired to do with the model rather than managed to do. There are often a great many researcher degrees of freedom in terms of how a particular observational dataset can be used to produce modelled estimates of causal effects, and these can markedly affect the effect estimates produced.\nSo, what are some alternatives?"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-16/index.html#method-two-matching-methods",
    "href": "pages/causal-inference/lms-are-glms-part-16/index.html#method-two-matching-methods",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Method Two: Matching methods",
    "text": "Method Two: Matching methods\nRemember the Platinum Standard: For each individual, with their own personal characteristics (\\(X_i^*\\)), we known if they were treated \\(Z_i = 1\\) or untreated \\(Z_i = 0\\). In the sci-fi scenario of the genuine Platinum Standard, we are able to observe a clone of each of these individuals in the parallel universe of the unobserved counterfactual.\nObviously we can’t do that in reality. But maybe was can do something, with the data we have, which allows us to do something like the Platinum Standard, individual level pairwise comparison, \\(Y_i | Z_i = 1 - Y_i | Z_i = 0\\), even though we only precisely observe each individual \\(i\\) in one of the two scenarios \\(Z=1\\) or \\(Z=0\\).\nWe can do this by relaxing the requirement that the counterfactual be of a clone of the observed individual, and so identical in every way except for treatment status, and instead allow them to be compared to someone who’s merely similar to them.\nLet’s think through an example:\n\nBilly is 72 years old, male, overweight but not obese, works part time as a carpenter but is largely retired, married for five years but before that a widower for three, hypertensive; scores in the 85th percentile for conscentiousness, and 40th percentile for openness, in the Big Five Personality scale; owns his own home, worked in a factory in his twenties, likes baked beans with his biweekly fish suppers, enjoys war films but also musicals, liked holidaying in Spain back in the 1990s when his children were still children; owns a thirteen year old dog with advancing arthritis, who when younger used to take him on regular brisk walks, but now has to be cajoled to leave the house, especially when it’s cold and wet outside. He lives in the North East of England, and when that young woman - who seemed friendly but a bit nervous and had that weird piece of metal through the middle of her nose - from the survey company knocked on the door four months ago, and asked him to rate his level of agreement to the statement, “I am satisfied with my life” on a seven point scale, he answered with ‘6 - agree’, but pursed his lips and took five seconds to answer this question.\n\nObviously we have a lot of information about Billy. But that doesn’t mean the survey company, and thus our dataset \\(D\\), knows all that we now know. So, some of the information in the above is contained in \\(X^*_i\\), but others is part of \\(W_i\\).\nAnd what’s our treatment, and what’s our outcome? Let’s say the outcome is the response to the life satisfaction question, and the treatment is UK region, with the South East excluding London as the ‘control’ region.\nSo, how do matching methods work? Well, they can of course only work with the data available to them, \\(D\\). The basic approach is as follows:\n\nFor each person like Billy, who’s in the ‘treatment’ group \\(Z = 1\\) (‘treated’ to living in the North of England), we know various recorded characteristics about them \\(X_j^*\\), and so we want to look for one or more people on the ‘control’ group \\(Z=0\\) who are like the treated individual.\nSo, for Billy, we’re looking for someone in the part of the dataset where \\(Z=0\\) whose characteristics other than treatment assignment, i.e. \\(X^*\\) not \\(Z\\), are similar to Billy’s. Let’s say that, on paper, the person who’s most similar to Billy in the dataset is Mike, who’s 73 (just one year older), also owns his own home, also married, has a BMI of 26.3 (Billy’s is 26.1), and also diagnosed with hypertension. But, whereas Billy lives in the North of England, Mike lives in the South East.\nWe then compare the recorded response for Billy (6 - agree) with the recorded response for Mike (5 - mildly agree), to get an estimated treatment effect for Billy. 7\nWe then repeat the exercise for everyone else who, like Billy, is in the treatment/exposure group, trying to match them up with one or more individuals in the control group pool.\nOnce we’ve done that, we then average up the paired differences in responses - between each treated individual, and each person the’ve been paired up with - to produce an average treatment effect on the treated (ATT) estimate.\n\nHow do we go about about matchmaking Billy and other treated individuals? There are a variety of approaches, and as with using regression to ‘control for’ variables quite a lot of researcher degrees of freedom, different ways of matching, that can lead to different causal effect estimates. These include:\n\nExact matching: Find someone for all available characteristics other than assignment is exactly the same as the individual in the treated group to be matched. Obviously this is seldom possible, so an alternative is:\nCoarsened Exact Matching: Lump the characteristics into broader groups, such as 10 year age groups rather than age in single years, and match on someone who’s exactly roughly the same, i.e. matches the target within the more lumped/aggregated categories rather than exactly the same to the finest level of data resolution agailable.\nPropensity Score Matching: Use the known characteristics of individiduals to predict their probability of being in the treatment group, then use these predicted probabilities to try to balance the known characteristics of the populations in both treatment and control arms.\nSynthetic Controls: Combine and ‘mix’ observed characteristics from multiple untreated/unexposed individuals so that their average/admixed/combined characteristics is closely similar to those of individuals in the treated/exposed population.\n\nThese approaches are neither exhaustive nor mutually exclusive, and there are a great many ways that they could be applied in practice. One of the general aims of matching approaches is to reduce the extent to which ATT or ATE estimates depend on the specific modelling approach adopted, 8 and for Propensity Score Matching, it’s often to try to break the \\(X^* \\rightarrow Z\\) link, and so achieve orthogonality (\\(X^* \\perp Z\\)). However, it can’t necessarily do the same with unobserved characteristics (\\(W \\rightarrow Z\\))."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-16/index.html#method-three-utilise-natural-experiments",
    "href": "pages/causal-inference/lms-are-glms-part-16/index.html#method-three-utilise-natural-experiments",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Method Three: Utilise ‘natural experiments’",
    "text": "Method Three: Utilise ‘natural experiments’\nThe idea with a ‘natural experiment’ is that something happens in the world that just happens to break the links between individual characteristics and assignment to exposure/treatment. The world has therefore created a situation for us where the orthogonality assumptions \\(W \\perp Z\\) and \\(X^* \\perp Z\\) which are safe to assume when working with RCT data can also, probably, possibly, be made with certain types of observational data too. When such factors are proposed and used by economists, they tend to call them instrumental variables. Some examples include:\n\nLottery winnings to estimate the effect of money on happiness: A lottery win is an increase in money available to someone that ‘just happens’ (at least amongst lottery players). Do lottery winners’ subjective wellbeing scores increase following a win? If so for how long? Why is this preferable to just looking at the relationship between income/assets and happiness? Well, the causality could go the other way: perhaps happier people work harder, increasing their income. Or perhaps a common underlying personality factor - something like ‘conscientious stoicism’, which isn’t measured - affects both income and happiness. By utilising the randomness of a big win allocation to just a small minority of players, 9 such alternative explanations for why there are differences between populations being compared can be more safely discounted.\nComparing educational outcomes for pupils who only just got into, and only just got rejected from, selective schools and universities: Say a selective school runs its own standardised entry exam, for which a pass mark of 70 or higher is required to be accepted. An applicant who achieves a mark of 69 isn’t really that different in their aptitude than one who achieves a of 70, but this one point difference sadly appears to make the world of difference for the applicant with a 69, and gladly appears to make the world of difference for the applicant with a 70. For years afterwards, the 70-scoring applicant will have access to a fundemntally different educational environment than the 69-scoring applicant. And presumably both applicants 10 both applied because they thought the selective educational institution really would make a substantial and positive difference for their long-term educational outcomes. But does it really? By following the actual educational outcomes of pupils just north of the selection boundary, and of non-pupils just south of the selection boundaries, we have something like a treatment and control group, whose only main difference is that some are in the selective school and some are not.\n\nNote that neither of these examples are perfect substitutes for an RCT. Perhaps the people who win lotteries, or win big, are different enough from those who don’t that the winner/non-winner group’s aren’t similar in important ways. And perhaps the way people process and feel about money they get through lottery winnings isn’t the same as they they receive through earnings or social security, so the idea of there being a single money-to-happiness pathway isn’t valid. For the second example there are other concerns: of course applicants only one mark apart won’t be very different to each other, but there won’t be many of these, meaning the precision of the estimate will tend to be low. So how about expanding the ‘catchment’ to each arm, either side of the boundary line, to 2 marks, 3 marks, 5 marks? Now there should be more people in both the control and treatment arms, but they’ll also be more different to each other. 11\nAs you might expect, if using instrumental variables, the quality of the instrument matters a lot. But generally the quality of the instrument isn’t something that can be determined through any kind of formal or statistical test. It tends to be, for want of a better term, a matter of story telling. If the story the researcher can tell their audience, about the instrument and why it’s able to break the causal links it needs to break, is convincing to the audience, then the researcher and audience will both be more willing to assume that the estimates produced at the end of the analysis are causal."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-16/index.html#summing-up",
    "href": "pages/causal-inference/lms-are-glms-part-16/index.html#summing-up",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Summing up",
    "text": "Summing up\nSo, three methods for trying to do something technically impossible: using observational data to estimate causal effects. These methods aren’t mutually exclusive, nor are they likely to be exhaustive, and nor are any of them failsafe.\nIn the absence of being able to really know, to peak behind the veil and see the causal chains working their magic, a good pragmatic strategy tends to be to try multiple approaches. At its extreme, this can mean asking multiple teams of researchers the same question, and giving them access to the same dataset, and encouraging each team to not contact any other teams until they’ve finished their analysis, then compare the results they produce. If many different teams, with many different approaches, all tend to produce similar estimates, then maybe the estimates are really tapping into genuine causal effects, and not just reflecting some of the assumptions and biases built into the specific models and methods we’re using?"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-16/index.html#coming-up",
    "href": "pages/causal-inference/lms-are-glms-part-16/index.html#coming-up",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Coming up",
    "text": "Coming up\nThe next post attempts to apply matching methods to a relatively complex dataset on an economic intervention, using the MatchIt package. The post largely follows an introductory example from the package, but at some points goes ‘off piste’. I hope it does so, however, in ways that are interesting, useful, and help bridge the gaps between the theoretical discussions in this and previous posts, with the practical challenges involved in applying such theory."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-16/index.html#footnotes",
    "href": "pages/causal-inference/lms-are-glms-part-16/index.html#footnotes",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Footnotes",
    "text": "Footnotes\n\n\npronounced ‘unobtainium’↩︎\nAKA ‘exposed’↩︎\nAKA ‘unexposed’↩︎\nOr multivariable, if we wish to reserve the term multivariate to models with multiple response columns.↩︎\nLogically, we should assume there is also an Average Treatment Effect on the Untreated (ATU), but this is seldom discussed in practice.↩︎\nThis might be represented as something like \\(D^{(T)} \\subset D \\iff Z_i = 1\\), i.e. the data used are filtered based on the value of \\(Z\\) matching a condition.↩︎\nThis data is really ordinal, meaning we know ‘agree’ is higher than ‘mildly agree’, but don’t know how much higher, so should really be modelled as such, with something like an ordered logit or ordered probit model specification. However it’s often either treated as cardinal - 1, 2, 3, 4, 5, 6, 7 - with something like a linear regression, or collapsed into two categories (agree/ don’t agree) so standard logit or probit regression could be used.↩︎\nEven B-A is a modelling approach, to an extent.↩︎\nIt could be you. But it probably won’t be.↩︎\nOr their pushy parents…↩︎\nAn example of a bias/variance trade-off↩︎"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-17/index.html",
    "href": "pages/causal-inference/lms-are-glms-part-17/index.html",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "",
    "text": "The previous post (re)introduced three ways to try to allow causal effect estimation using observational data: i) ‘controlling for’ variables using multiple regression; ii) matching methods; iii) Identifying possible ‘natural experiments’ in observational datasets. The fundamental challenge of using observational data to estimate causal effects is that we cannot be sure either the observed (\\(X^*\\)) or unobserved (\\(W\\)) characteristics of observations do not influence allocation to exposure/treatment, i.e. cannot rule out \\(X^* \\rightarrow Z\\) or \\(W \\rightarrow Z\\), meaning that statistical estimates of the effect of Z on the outcome \\(Z \\rightarrow y_i\\) may be biased.\nThe first two approaches will, within limits, generally attenuate the link between \\(X^*\\) and \\(Z\\), but can do little to break the link between \\(W\\) and \\(Z\\), as \\(W\\) is by definition those features of observational units that are not contained in the dataset \\(D\\), and so any statistical method will be ‘blind’ to. The last approach, if the instrumental variable possesses the properties we expect and hope it will, should be able to break the \\(W \\rightarrow Z\\) link too. But unfortunately that can be a big if: the instrument may not have the properties we hope it does.\nThis post will go explore some application of the first two approaches: controlling for variables using multiple regression; and using matching methods. A fuller consideration of the issues is provided in Ho et al. (2007), and the main package and dataset used will be that of the associated MatchIt package Ho et al. (2011) and vignette using the lalonde dataset."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-17/index.html#recap-and-aim",
    "href": "pages/causal-inference/lms-are-glms-part-17/index.html#recap-and-aim",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "",
    "text": "The previous post (re)introduced three ways to try to allow causal effect estimation using observational data: i) ‘controlling for’ variables using multiple regression; ii) matching methods; iii) Identifying possible ‘natural experiments’ in observational datasets. The fundamental challenge of using observational data to estimate causal effects is that we cannot be sure either the observed (\\(X^*\\)) or unobserved (\\(W\\)) characteristics of observations do not influence allocation to exposure/treatment, i.e. cannot rule out \\(X^* \\rightarrow Z\\) or \\(W \\rightarrow Z\\), meaning that statistical estimates of the effect of Z on the outcome \\(Z \\rightarrow y_i\\) may be biased.\nThe first two approaches will, within limits, generally attenuate the link between \\(X^*\\) and \\(Z\\), but can do little to break the link between \\(W\\) and \\(Z\\), as \\(W\\) is by definition those features of observational units that are not contained in the dataset \\(D\\), and so any statistical method will be ‘blind’ to. The last approach, if the instrumental variable possesses the properties we expect and hope it will, should be able to break the \\(W \\rightarrow Z\\) link too. But unfortunately that can be a big if: the instrument may not have the properties we hope it does.\nThis post will go explore some application of the first two approaches: controlling for variables using multiple regression; and using matching methods. A fuller consideration of the issues is provided in Ho et al. (2007), and the main package and dataset used will be that of the associated MatchIt package Ho et al. (2011) and vignette using the lalonde dataset."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-17/index.html#getting-started",
    "href": "pages/causal-inference/lms-are-glms-part-17/index.html#getting-started",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Getting started",
    "text": "Getting started\nWe start by loading the Matchit package and exploring the lalonde dataset.\n\n\nCode\nlibrary(tidyverse)\nlibrary(MatchIt)\nunmatched_data &lt;- tibble(lalonde)\n\nunmatched_data\n\n\n# A tibble: 614 × 9\n   treat   age  educ race   married nodegree  re74  re75   re78\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1    37    11 black        1        1     0     0  9930.\n 2     1    22     9 hispan       0        1     0     0  3596.\n 3     1    30    12 black        0        0     0     0 24909.\n 4     1    27    11 black        0        1     0     0  7506.\n 5     1    33     8 black        0        1     0     0   290.\n 6     1    22     9 black        0        1     0     0  4056.\n 7     1    23    12 black        0        0     0     0     0 \n 8     1    32    11 black        0        1     0     0  8472.\n 9     1    22    16 black        0        0     0     0  2164.\n10     1    33    12 white        1        0     0     0 12418.\n# ℹ 604 more rows"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-17/index.html#data",
    "href": "pages/causal-inference/lms-are-glms-part-17/index.html#data",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Data",
    "text": "Data\nThe description of the lalonde dataset is as follows:\n\n\nCode\nhelp(lalonde)\n\n\n\nDescription\nThis is a subsample of the data from the treated group in the National Supported Work Demonstration (NSW) and the comparison sample from the Population Survey of Income Dynamics (PSID). This data was previously analyzed extensively by Lalonde (1986) and Dehejia and Wahba (1999).\nFormat\nA data frame with 614 observations (185 treated, 429 control). There are 9 variables measured for each individual.\n\n“treat” is the treatment assignment (1=treated, 0=control).\n“age” is age in years.\n“educ” is education in number of years of schooling.\n“race” is the individual’s race/ethnicity, (Black, Hispanic, or White). Note previous versions of this dataset used indicator variables black and hispan instead of a single race variable.\n“married” is an indicator for married (1=married, 0=not married).\n“nodegree” is an indicator for whether the individual has a high school degree (1=no degree, 0=degree).\n“re74” is income in 1974, in U.S. dollars.\n“re75” is income in 1975, in U.S. dollars.\n“re78” is income in 1978, in U.S. dollars.\n\n“treat” is the treatment variable, “re78” is the outcome, and the others are pre-treatment covariates.\n\nLet’s look at the data to get a sense of it:\n\n\nCode\nunmatched_data |&gt;\n    mutate(treat = as.factor(treat)) |&gt;\n    filter(re78 &lt; 25000) |&gt;\n    ggplot(aes(y = re78, x = re75, shape = treat, colour = treat)) + \ngeom_point() + \ngeom_abline(intercept = 0, slope = 1) +\ncoord_equal() + \nstat_smooth(se = FALSE, method = \"lm\")\n\n\n\n\n\nClearly this is quite complicated data, where the single implied control, wages in 1975 (re75) is not sufficient. There are also a great many observations where wages in either of both years were 0, hence the horizontal and vertical streaks apparent.\nThe two lines are the linear regression lines for the two treatment groups as a function of earlier wage. The lines are not fixed to have the same slope, so the differences in any crude treatment effect estimate vary by earlier wage, but for most previous wages the wages in 1978 appear to be lower in the treatment group (blue), than the control group (red). This would suggest either that the treatment may be harmful to wages… or that there is severe imbalance between the characteristics of persons in both treatment conditions.\nLet’s now start to use a simple linear regression to estimate an average treatment effect, before adding more covariates to see how these model-derived estimates change\n\n\nCode\n# Model of treatment assignment only\nmod_01 &lt;- lm(re78 ~ treat, unmatched_data)\nsummary(mod_01) \n\n\n\nCall:\nlm(formula = re78 ~ treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6984  -6349  -2048   4100  53959 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6984.2      360.7  19.362   &lt;2e-16 ***\ntreat         -635.0      657.1  -0.966    0.334    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7471 on 612 degrees of freedom\nMultiple R-squared:  0.001524,  Adjusted R-squared:  -0.0001079 \nF-statistic: 0.9338 on 1 and 612 DF,  p-value: 0.3342\n\n\nOn average the treated group had (annual?) wages $635 lower than the control group. However the difference is not statistically significant.\nNow let’s add previous wage from 1975\n\n\nCode\nmod_02 &lt;- lm(re78 ~ re75 + treat, unmatched_data)\nsummary(mod_02)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-15918  -5457  -2025   3824  54103 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5547.63718  412.84637  13.438  &lt; 2e-16 ***\nre75           0.58242    0.08937   6.517  1.5e-10 ***\ntreat        -90.79498  641.40291  -0.142    0.887    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7230 on 611 degrees of freedom\nMultiple R-squared:  0.06642,   Adjusted R-squared:  0.06336 \nF-statistic: 21.73 on 2 and 611 DF,  p-value: 7.611e-10\n\n\nPreviously observed wage is statistically significant and positive. The point estimate on treatment is smaller, and even less ‘starry’.\nNow let’s add all possible control variables and see what the treatment effect estimate produced is:\n\n\nCode\nmod_03 &lt;- lm(re78 ~ re75 + age + educ + race + married + nodegree + re74 + treat, unmatched_data)\nsummary(mod_03)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + age + educ + race + married + nodegree + \n    re74 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13595  -4894  -1662   3929  54570 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.174e+03  2.456e+03  -0.478   0.6328    \nre75         2.315e-01  1.046e-01   2.213   0.0273 *  \nage          1.298e+01  3.249e+01   0.399   0.6897    \neduc         4.039e+02  1.589e+02   2.542   0.0113 *  \nracehispan   1.740e+03  1.019e+03   1.708   0.0882 .  \nracewhite    1.241e+03  7.688e+02   1.614   0.1071    \nmarried      4.066e+02  6.955e+02   0.585   0.5590    \nnodegree     2.598e+02  8.474e+02   0.307   0.7593    \nre74         2.964e-01  5.827e-02   5.086 4.89e-07 ***\ntreat        1.548e+03  7.813e+02   1.982   0.0480 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6948 on 604 degrees of freedom\nMultiple R-squared:  0.1478,    Adjusted R-squared:  0.1351 \nF-statistic: 11.64 on 9 and 604 DF,  p-value: &lt; 2.2e-16\n\n\nWith all of these variables as controls, the effect of treatment is now statistically significant and positive, associated with on average an increase of $155 over the control group.\nHowever, we should probably be concerned about how dependent this estimate is on the specific model specification we used. For example, it is fairly common to try to ‘control for’ nonlinearities in age effects by adding a squared term. If modeller decisions like this don’t make much difference, then its addition shouldn’t affect the treatment effect estimate. Let’s have a look:\n\n\nCode\nmod_04 &lt;- lm(re78 ~ re75 + poly(age, 2) + educ + race + married + nodegree + re74 + treat, unmatched_data)\nsummary(mod_04)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + poly(age, 2) + educ + race + married + \n    nodegree + re74 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13692  -4891  -1514   3884  54313 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -5.395e+02  2.172e+03  -0.248   0.8039    \nre75           2.190e-01  1.057e-01   2.072   0.0387 *  \npoly(age, 2)1  3.895e+03  7.994e+03   0.487   0.6262    \npoly(age, 2)2 -6.787e+03  7.918e+03  -0.857   0.3917    \neduc           3.889e+02  1.599e+02   2.432   0.0153 *  \nracehispan     1.682e+03  1.021e+03   1.648   0.0999 .  \nracewhite      1.257e+03  7.692e+02   1.634   0.1028    \nmarried        2.264e+02  7.267e+02   0.312   0.7555    \nnodegree       3.185e+02  8.504e+02   0.375   0.7081    \nre74           2.948e-01  5.832e-02   5.055 5.73e-07 ***\ntreat          1.369e+03  8.090e+02   1.692   0.0911 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6949 on 603 degrees of freedom\nMultiple R-squared:  0.1488,    Adjusted R-squared:  0.1347 \nF-statistic: 10.54 on 10 and 603 DF,  p-value: &lt; 2.2e-16\n\n\nThe inclusion of the squared term to age has changed the point estimate of treatment from around $1550 to $1370. However it has also changed the statistical significance of the effect from p &lt; 0.05 to p &lt; 0.10, i.e. from ‘statistically significant’ to ‘not statistically significant’. If we were playing the stargazing game, this might be the difference between a publishable finding and an unpublishable finding.\nAnd what if we excluded age, because none of the terms are statistically significant at the standard level?\n\n\nCode\nmod_05 &lt;- lm(re78 ~ re75 + educ + race + married + nodegree + re74 + treat, unmatched_data)\nsummary(mod_05)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + educ + race + married + nodegree + \n    re74 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13681  -4912  -1652   3877  54648 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -676.43048 2115.37702  -0.320   0.7493    \nre75           0.22705    0.10395   2.184   0.0293 *  \neduc         389.00786  154.33865   2.520   0.0120 *  \nracehispan  1710.16654 1015.15590   1.685   0.0926 .  \nracewhite   1241.00510  768.22972   1.615   0.1067    \nmarried      478.55017  671.28910   0.713   0.4762    \nnodegree     201.04497  833.99164   0.241   0.8096    \nre74           0.30209    0.05645   5.351 1.24e-07 ***\ntreat       1564.68896  779.65173   2.007   0.0452 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6943 on 605 degrees of freedom\nMultiple R-squared:  0.1475,    Adjusted R-squared:  0.1363 \nF-statistic: 13.09 on 8 and 605 DF,  p-value: &lt; 2.2e-16\n\n\nNow the exclusion of this term, which the coefficient tables suggested wasn’t statistically significant, but intuitively we recognise as an important determinant of labour market activity, has led to yet another point estimate. It’s switched back to ‘statistically significant’ again, but now the point estimate is about $1565 more. Such estimates aren’t vastly different, but they definitely aren’t the same, and come from just a tiny same of the potentially hundreds of different model specifications we could have considered and decided to present to others."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-17/index.html#matching-with-matchit",
    "href": "pages/causal-inference/lms-are-glms-part-17/index.html#matching-with-matchit",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Matching with MatchIt",
    "text": "Matching with MatchIt\nAs the title of Ho et al. (2007) indicates, matching methods are presented as a way of preprocessing the data to reduce the kind of model dependence we’ve just started to explore. Let’s run the first example they present in the MatchIt vignette then discuss what it means:\n\n\nCode\nm.out0 &lt;- matchit(treat ~ age + educ + race + married + \n                   nodegree + re74 + re75, data = lalonde,\n                 method = NULL, distance = \"glm\")\nsummary(m.out0)\n\n\n\nCall:\nmatchit(formula = treat ~ age + educ + race + married + nodegree + \n    re74 + re75, data = lalonde, method = NULL, distance = \"glm\")\n\nSummary of Balance for All Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.1822          1.7941     0.9211    0.3774\nage              25.8162       28.0303         -0.3094     0.4400    0.0813\neduc             10.3459       10.2354          0.0550     0.4959    0.0347\nraceblack         0.8432        0.2028          1.7615          .    0.6404\nracehispan        0.0595        0.1422         -0.3498          .    0.0827\nracewhite         0.0973        0.6550         -1.8819          .    0.5577\nmarried           0.1892        0.5128         -0.8263          .    0.3236\nnodegree          0.7081        0.5967          0.2450          .    0.1114\nre74           2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75           1532.0553     2466.4844         -0.2903     0.9563    0.1342\n           eCDF Max\ndistance     0.6444\nage          0.1577\neduc         0.1114\nraceblack    0.6404\nracehispan   0.0827\nracewhite    0.5577\nmarried      0.3236\nnodegree     0.1114\nre74         0.4470\nre75         0.2876\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       429     185\nUnmatched       0       0\nDiscarded       0       0\n\n\nWith method = NULL, the matchit function presents some summary estimates of differences in characteristics between the Treatment and Control groups. For example, the treated group has an average age of around 25, compared with 28 in the control group, have a slightly higher education score, are more likely to be Black, less likely to be Hispanic, and much less likely to be White (all important differences in the USA context, especially perhaps of the 1970s). They are also less likely to be married, more likely to have no degree, and have substantially earlier wages in both 1974 and 1975. Clearly a straightforward comparision between average outcomes is far from a like-with-like comparisons between groups. The inclusion of other covariates (\\(X^*\\)) does seem to have made a difference, switching the reported direction of effect and its statistical significance, but if we could find a subsample of the control group whose characteristics better match those of the treatment groups, we would hopefully get a more precise and reliable estimate of the effect of the labour market programme.\nThe next part of the vignette shows MatchIt working with some fairly conventional settings:\n\n\nCode\nm.out1 &lt;- matchit(treat ~ age + educ + race + married + \n                   nodegree + re74 + re75, data = lalonde,\n                 method = \"nearest\", distance = \"glm\")\nm.out1\n\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score\n             - estimated with logistic regression\n - number of obs.: 614 (original), 370 (matched)\n - target estimand: ATT\n - covariates: age, educ, race, married, nodegree, re74, re75\n\n\nThe propensity score, i.e. the probability of being in the treatment group, has been predicted using the other covariates, and using logistic regression. For each individual in the treatment group, a ‘nearest neighbour’ in the control group has been identified with the most similar propensity score, which we hope also will also mean the characteristics of the treatment group, and matched pairs from the control group, will be more similar too.\nWe can start to see what this means in practice by looking at the summary of the above object\n\n\nCode\nsummary(m.out1)\n\n\n\nCall:\nmatchit(formula = treat ~ age + educ + race + married + nodegree + \n    re74 + re75, data = lalonde, method = \"nearest\", distance = \"glm\")\n\nSummary of Balance for All Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.1822          1.7941     0.9211    0.3774\nage              25.8162       28.0303         -0.3094     0.4400    0.0813\neduc             10.3459       10.2354          0.0550     0.4959    0.0347\nraceblack         0.8432        0.2028          1.7615          .    0.6404\nracehispan        0.0595        0.1422         -0.3498          .    0.0827\nracewhite         0.0973        0.6550         -1.8819          .    0.5577\nmarried           0.1892        0.5128         -0.8263          .    0.3236\nnodegree          0.7081        0.5967          0.2450          .    0.1114\nre74           2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75           1532.0553     2466.4844         -0.2903     0.9563    0.1342\n           eCDF Max\ndistance     0.6444\nage          0.1577\neduc         0.1114\nraceblack    0.6404\nracehispan   0.0827\nracewhite    0.5577\nmarried      0.3236\nnodegree     0.1114\nre74         0.4470\nre75         0.2876\n\nSummary of Balance for Matched Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.3629          0.9739     0.7566    0.1321\nage              25.8162       25.3027          0.0718     0.4568    0.0847\neduc             10.3459       10.6054         -0.1290     0.5721    0.0239\nraceblack         0.8432        0.4703          1.0259          .    0.3730\nracehispan        0.0595        0.2162         -0.6629          .    0.1568\nracewhite         0.0973        0.3135         -0.7296          .    0.2162\nmarried           0.1892        0.2108         -0.0552          .    0.0216\nnodegree          0.7081        0.6378          0.1546          .    0.0703\nre74           2095.5737     2342.1076         -0.0505     1.3289    0.0469\nre75           1532.0553     1614.7451         -0.0257     1.4956    0.0452\n           eCDF Max Std. Pair Dist.\ndistance     0.4216          0.9740\nage          0.2541          1.3938\neduc         0.0757          1.2474\nraceblack    0.3730          1.0259\nracehispan   0.1568          1.0743\nracewhite    0.2162          0.8390\nmarried      0.0216          0.8281\nnodegree     0.0703          1.0106\nre74         0.2757          0.7965\nre75         0.2054          0.7381\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       185     185\nUnmatched     244       0\nDiscarded       0       0\n\n\nPreviously, there were 185 people in the treatment group, and 429 people in the control group. After matching there are 185 people in the treatment group… and also 185 people in the control group. So, each of the 185 people in the treatment group has been matched up with a ‘data twin’ in the control group, so the ATT should involve more of a like-with-like comparison.\nThe summary presents covariate-wise differences between the Treatment and Control groups for All Data, then for Matched Data. We would hope that, in the Matched Data, the differences are smaller for each covariate, though this isn’t necessarily the case. After matching, for example, we can see that the Black proportion in the Control group is now 0.47 rather than 0.20, and that the earlier income levels are lower, in both cases bringing the values in the Control group closer to, but not identical to, those in the Treatment group. Another way of seeing how balancing has changed things is to look at density plots:\n\n\nCode\nplot(m.out1, type = \"density\", interactive = FALSE,\n     which.xs = ~age + married + re75+ race + nodegree + re74)\n\n\n\n\n\n\n\n\nIn these density charts, the darker lines indicate the Treatment group and the lighter lines the Control groups. The matched data are on the right hand side, with All data on the left. We are looking to see if, on the right hand side, the two sets of density lines are more similar than they are on the right. Indeed they do appear to be, though we can also tell they are far from identical."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-17/index.html#estimating-treatment-effect-sizes-after-matching",
    "href": "pages/causal-inference/lms-are-glms-part-17/index.html#estimating-treatment-effect-sizes-after-matching",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Estimating Treatment Effect Sizes after matching",
    "text": "Estimating Treatment Effect Sizes after matching\nHistorically, the MatchIt package was designed to work seamlessly with Zelig, which made it much easier to use a single library and framework to produce ‘quantities of interest’ using multiple model structures. However Zelig has since been deprecated, meaning the vignette now recommends using the marginaleffects package. We’ll follow their lead:\nFirst the vignette recommends extracting matched data from the matchit output:\n\n\nCode\nm.data &lt;- match.data(m.out1)\n\nm.data &lt;- as_tibble(m.data)\nm.data\n\n\n# A tibble: 370 × 12\n   treat   age  educ race   married nodegree  re74  re75   re78 distance weights\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1     1    37    11 black        1        1     0     0  9930.   0.639        1\n 2     1    22     9 hispan       0        1     0     0  3596.   0.225        1\n 3     1    30    12 black        0        0     0     0 24909.   0.678        1\n 4     1    27    11 black        0        1     0     0  7506.   0.776        1\n 5     1    33     8 black        0        1     0     0   290.   0.702        1\n 6     1    22     9 black        0        1     0     0  4056.   0.699        1\n 7     1    23    12 black        0        0     0     0     0    0.654        1\n 8     1    32    11 black        0        1     0     0  8472.   0.790        1\n 9     1    22    16 black        0        0     0     0  2164.   0.780        1\n10     1    33    12 white        1        0     0     0 12418.   0.0429       1\n# ℹ 360 more rows\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nWhereas the unmatched data contains 614 observations, the matched data contains 370 observations. Note that the Treatment group contained 185 observations, and that 370 is 185 times two. So, the matched data contains one person in the Control group for each person in the Treatment group.\nWe can also see that, in addition to the metrics originally included, the matched data contains three additional variables: ‘distance’, ‘weights’ and ‘subclass’. The ‘subclass’ field is perhaps especially useful for understanding the intuition of the approach, because it helps show which individual in the Control group has been paired with which individual in the Treatment group. Let’s look at the first three subgroups:\n\n\nCode\nm.data |&gt; filter(subclass == '1')\n\n\n# A tibble: 2 × 12\n  treat   age  educ race  married nodegree   re74  re75  re78 distance weights\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    37    11 black       1        1     0      0 9930.    0.639       1\n2     0    22     8 black       1        1 16961.     0  959.    0.203       1\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nSo, for the first subclass, a 37 year old married Black person with no degree has been matched to a 22 year old Black married person with no degree.\n\n\nCode\nm.data |&gt; filter(subclass == '2')\n\n\n# A tibble: 2 × 12\n  treat   age  educ race  married nodegree  re74  re75   re78 distance weights\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    33    12 white       1        0    0      0 12418.   0.0429       1\n2     0    39    12 white       1        0 1289.     0  1203.   0.0430       1\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nFor the second subclass a 33 year old married White person with a degree has been paired with a 39 year old White person with a degree.\n\n\nCode\nm.data |&gt; filter(subclass == '3')\n\n\n# A tibble: 2 × 12\n  treat   age  educ race   married nodegree  re74  re75   re78 distance weights\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    31     9 hispan       0        1     0    0  26818.    0.250       1\n2     0    16    10 white        0        1     0  190.  2137.    0.105       1\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nFor the third subclass, a 31 year old unmarried Hispanic person with no degree has been paired with a 16 year old White person with no degree.\nIn each case, we can see the pairings are similar in some ways but (as with the last example) quite dissimilar in others. The matching algorithm is trying to do the best it can with the data available, especially with the constraint1 that once a person in the Control group has been paired up once to someone in the Treatment group, they can’t be paired up again with someone else in the Treatment group.\nThe identification of these specific pairings suggests we can used a fairly crude strategy to produce an estimate of the ATT: namely just compare the outcome across each of these pairs. Let’s have a look at this:\n\n\nCode\ntrt_effects &lt;- \n    m.data |&gt;\n        group_by(subclass) |&gt;\n        summarise(\n            ind_treat_effect = re78[treat == 1] - re78[treat == 0]\n        ) |&gt; \n        ungroup()\n\ntrt_effects |&gt;\n    ggplot(aes(ind_treat_effect)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = mean(trt_effects$ind_treat_effect), colour = \"red\") + \n    geom_vline(xintercept = 0, colour = 'lightgray', linetype = 'dashed')\n\n\n\n\n\nThis crude paired comparison suggests an average difference that’s slightly positive, of $894.37.\nThis is not a particularly sophisticated or ‘kosher’ approach however. Instead the vignette suggests calculating the treatment effect estimate as follows:\n\n\nCode\nlibrary(\"marginaleffects\")\n\nfit &lt;- lm(re78 ~ treat * (age + educ + race + married + nodegree + \n             re74 + re75), data = m.data, weights = weights)\n\navg_comparisons(fit,\n                variables = \"treat\",\n                vcov = ~subclass,\n                newdata = subset(m.data, treat == 1),\n                wts = \"weights\")\n\n\n\n  Term          Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n treat mean(1) - mean(0)     1121        837 1.34    0.181 2.5  -520   2763\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\nUsing the recommended approach, the ATT estimate is now $1121. Not statistically significant at the conventional 95% threshold, but also more likely to be positive than negative."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-17/index.html#summary",
    "href": "pages/causal-inference/lms-are-glms-part-17/index.html#summary",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Summary",
    "text": "Summary\nIn this post we have largely followed along with the introductionary vignette from the MatchIt package, in order to go from the fairly cursory theoretical overview in the previous post, to showing how some of the ideas and methods relating to multiple regression and matching methods work in practice. There are a great many ways that both matching, and multiple regression, can be implemented in practice, and both are likely to affect any causal effect estimates we produce. However, the aspiration of using matching methods is to somewhat reduce the dependency that causal effect estimates have on the specific model specifications we used."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-17/index.html#coming-up",
    "href": "pages/causal-inference/lms-are-glms-part-17/index.html#coming-up",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Coming up",
    "text": "Coming up\nThe next post concludes this series on causal inference, by discussing in more detail a topic many users of causal inference will assume I should have started with: the Pearlean school of causal inference. In brief: the approach to causal inference I’m used to interprets the problem, fundamentally, as a missing data problem; whereas the Pearlean approach interprets it more as a modelling problem. I see value in both sides, as well as some points of overlap, but in general I’m both more used to, and more comfortable with, the missing data interpretation."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-17/index.html#footnotes",
    "href": "pages/causal-inference/lms-are-glms-part-17/index.html#footnotes",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI think this is implied by the use of method = \"nearest\", which is the default, meaning ‘greedy nearest neighbour matching’.↩︎"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-18/index.html",
    "href": "pages/causal-inference/lms-are-glms-part-18/index.html",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "",
    "text": "Over posts 14 through to 17 I’ve discussed causal inference. However, readers who’ve been involved and interested in the topic of causal inference over the last few years might be less surprised by what I have covered than by what I’ve not, namely the causal inference framework developed by Judea Pearl, and (somewhat) popularised by his co-authored book, The Book of Why: The New Science of Cause and Effect. (Pearl and Mackenzie (2018))\nThis ‘oversight’ in posts so far has been intentional, but in this post the Pearl framework will finally be discussed. I’ll aim to: i) give an overview of the two primary ways of thinking about causal inference: either as a missing data problem; or as a ‘do-logic’ problem; ii) discuss the concept of the omitted variable vs post treatment effect bias trade-off as offering something of a bridge between the two paradigms; iii) give some brief examples of directed acyclic graphs (DAGs) and do-logic, two important ideas from the Pearl framework, as described in Pearl and Mackenzie (2018); iv) make some suggestions about the benefits and uses of the Pearl framework; and finally v) advocate for epistemic humility when it comes to trying to draw causal inferences from observational data, even where a DAG has been clearly articulated and agreed upon within a research community. 1 Without further ado, let’s begin:"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-18/index.html#introduction-correcting-an-oversight-in-discussing-causality",
    "href": "pages/causal-inference/lms-are-glms-part-18/index.html#introduction-correcting-an-oversight-in-discussing-causality",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "",
    "text": "Over posts 14 through to 17 I’ve discussed causal inference. However, readers who’ve been involved and interested in the topic of causal inference over the last few years might be less surprised by what I have covered than by what I’ve not, namely the causal inference framework developed by Judea Pearl, and (somewhat) popularised by his co-authored book, The Book of Why: The New Science of Cause and Effect. (Pearl and Mackenzie (2018))\nThis ‘oversight’ in posts so far has been intentional, but in this post the Pearl framework will finally be discussed. I’ll aim to: i) give an overview of the two primary ways of thinking about causal inference: either as a missing data problem; or as a ‘do-logic’ problem; ii) discuss the concept of the omitted variable vs post treatment effect bias trade-off as offering something of a bridge between the two paradigms; iii) give some brief examples of directed acyclic graphs (DAGs) and do-logic, two important ideas from the Pearl framework, as described in Pearl and Mackenzie (2018); iv) make some suggestions about the benefits and uses of the Pearl framework; and finally v) advocate for epistemic humility when it comes to trying to draw causal inferences from observational data, even where a DAG has been clearly articulated and agreed upon within a research community. 1 Without further ado, let’s begin:"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-18/index.html#causal-inference-two-paradigms",
    "href": "pages/causal-inference/lms-are-glms-part-18/index.html#causal-inference-two-paradigms",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "Causal Inference: Two paradigms",
    "text": "Causal Inference: Two paradigms\nIn the posts so far, I’ve introduced and kept returning to the idea that the fundamental problem of causal inference is that at least half of the data is always missing. i.e., for each individual observation, who has either been treated or not treated, if they had been treated then we do not observe them in the untreated state, and if they had not been treated we do not observe them in the treated state. It’s this framing of the problem which\nIn introducing causal inference from this perspective, I’ve ‘taken a side’ in an ongoing debate, or battle, or even war, between two clans of applied epistemologists. Let’s call them the Rubinites, and the Pearlites. Put crudely, the Rubinites adopt a data-centred framing of the challenge of causal inference, whereas the Pearlites adopt a model-centred framing of the challenge of causal inference. For the Rubinites, the data-centred framing leads to an intepretation of causal inference as a missing data problem, for which the solution is therefore to perform some kind of data imputation. For the Pearlites, by contrast, the solution is focused on developing, describing and drawing out causal models, which describe how we believe one thing leads to another and the paths of effect and influence that one variable has on each other variable.\nIt is likely no accident that the broader backgrounds and interests of Rubin and Pearl align with type of solution each proposes. Rubin’s other main interests are in data imputation more generally, including methods of multiple imputation which allow ‘missing values’ to be filled in stochastically, rather than deterministically, to allow some representation of uncertainty and variation in the missing values to be indicated by the range of values that are generated for a missing hole in the data. Pearl worked as a computer scientist, whose key contribution to the field was the development of Bayesian networks, which share many similarities with neural networks. For both types of network, there are nodes, and there are directed links. The nodes have values, and these values can be influenced and altered by the values of other nodes that are connected to the node in question. This influence that each node has on other nodes, through the paths indicated in the directed links, is perhaps more likely to be described as updating from the perspective of a Bayesian network, and propagation from the perspective of a neural network. But in either case, it really is correct to say that one node really does cause another node’s value to change through the causal pathway of the directed link. The main graphical tool Pearl proposes for reasoning about causality in obervational data is the directed acyclic graph (DAG), and again it should be unsurprising that DAGs look much like Bayesian networks."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-18/index.html#the-omitted-variable-bias-vs-post-treatment-bias-trade-off-as-a-potential-bridge-between-the-two-paradigms",
    "href": "pages/causal-inference/lms-are-glms-part-18/index.html#the-omitted-variable-bias-vs-post-treatment-bias-trade-off-as-a-potential-bridge-between-the-two-paradigms",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "The Omitted Variable Bias vs Post Treatment Bias Trade-off as a potential bridge between the two paradigms",
    "text": "The Omitted Variable Bias vs Post Treatment Bias Trade-off as a potential bridge between the two paradigms\nThe school of inference I’m most familiar with is that of Gary King, a political scientist, methodologist and (in the hallowed halls of Harvard) populariser of statistical methods in the social sciences. In the crude paradigmatic split I’ve sketched out above, King is a Rubinite, and so I guess - mainly through historical accident but partly through conscious decision - I am too. However, I have read Pearl and Mackenzie (2018) (maybe not recently enough nor enough times to fully digest it), consider it valuable and insightful in many places, and think there’s at least one place where the epistemic gap between the two paradigms can be bridged.\nThe bridge point on the Rubinite side,2 I’d suggest, comes from thinking carefully about the sources of bias enumerated in section 3.2 of King and Zeng (2006), which posits that:\n\\[\nbias = \\Delta_o + \\Delta_p + \\Delta_i + \\Delta_e\n\\]\nThis section states:\n\nThese four terms denote exactly the four sources of bias in using observational data, with the subscripts being mnemonics for the components … . The bias components are due to, respectively, omitted variable bias (\\(\\Delta_o\\)), post-treatment bias (\\(\\Delta_p\\)), interpolation bias (\\(\\Delta_i\\)) and extrapolation bias (\\(\\Delta_e\\)). [Emphases added]\n\nOf the four sources of bias listed, it’s the first two which appear to offer a potential link between the two paradigms, and so suggest to Rubinites why some engagement with the Pearlite approach may be valuable. The section continues:\n\nBriefly, \\(\\Delta_o\\) is the bias due to omitting relevant variables such as common causes of both the treatment and the outcome variables [whereas] \\(\\Delta_p\\) is bias due to controlling for the consequences of the treatment. [Emphases added]\n\nFrom the Rubinite perspective, it seems that omitted variable bias and post-treatment bias are recognised, in combination, as constituting a wicked problem. This is because the inclusion of an specific variable can simultaneously affect both types of bias: reducing omitted variable bias, but also potentially increasing post treatment bias. You’re doomed if you do, but you’re also doomed if you don’t."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-18/index.html#with-apologies-to-economists-and-epidemiologists-alike",
    "href": "pages/causal-inference/lms-are-glms-part-18/index.html#with-apologies-to-economists-and-epidemiologists-alike",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "With apologies to economists and epidemiologists alike…",
    "text": "With apologies to economists and epidemiologists alike…\nOf the two sources of bias, omitted variable bias seems to be the more discussed. And historically, it seems different social and health science disciplines have placed a different weight of addressing these two sources of bias. In particular, at least in the UK context, it’s seemed that economists tend to be more concerned about omitted variable bias, leading to the inclusion of a large number of variables in their statistical models, whereas epidemiologists (though they might not be familiar with and use the term) tend to be more concerned about post-treatment bias, leading a statistical models with fewer variables.\nThe issue of post treatment bias is especially important to consider in the context of root or fundamental causes, which again is often something more of interest to epidemiologists than economists. And the importance of the issue comes into sharp relief if considering factors like sex or race. An economist/econometrician, if asked to estimate the effect of race on (say) the probability of a successful job application to an esteemed organisation, might be very liable to try to include many additional covariates, such as previous work experience and job qualifications, as ‘control variables’ in a statistical model in addition to race. From this, they might find that the covariate associated with race is neither statistically nor substantively, and from this conclude that there is no evidence of (say) racial discrimination in employment, because any disparities in outcomes between racial groups appear to be ‘explained by’ other factors like previous experience and job qualifications.\nTo this, a methodologically minded epidemiologist might counter - very reasonably - that the econometrician’s model is over-controlling, and that the inclusion of factors like educational outcomes and previous work experience in the model risks introducing post treatment bias. If there were discrimination on the basis of race, or sex, it would be unlikely to just affect the specific outcome on the response side of the model. Instead, discrimination (or other race-based factors) would also likely affect the kind of education available to people of different races, and the kinds of educational expectations placed on people of different racial groups. This would then affect the level of educational achievement by group as well. Similarly, both because of prior differences in educational achievement, and because of concurrent effects of discrimination, race might also be expected to affect job history too. Based on this, the epidemiologist might choose to omit both qualifications and job history from the model, because both are presumed to be causallly downstream of the key factor of interest, race.\nSo which type of model is correct? The epidemiologist’s more parsimonious model, which is mindful of post-treatment bias, or the economist’s more complicated model, which is mindful of omitted variable bias? The conclusion from the four-biases position laid out above is that we don’t know, but that all biases potentially exist in observational data, and neither model specification can claim to be free from bias. Perhaps both kinds of model can be run, and perhaps looking at the estimates from both models can give something like a plausible range of possible effects. But fundamentally, we don’t know, and can’t know, and ideally we should seek better quality data, run RCTs and so on.\nPearl and Mackenzie (2018) argues that Rubinites don’t see much (or any) value in causal diagrams, stating “The Rubin causal model treats counterfactuals as abstract mathematical objects that are managed by algebraic machinery but not derived from a model.” [p. 280] Though I think this characterisation is broadly consciously correct, the recognition within the Rubinite community that such things as post-treatment bias and omitted variables exist suggests to me that, unconsciously, even Rubinites employ something like path-diagram reasoning when considering which sources of bias are likely to affect their effect estimates. Put simply: I don’t see how claims of either omitted variable or post treatment bias could be made or believed but for the kind of graphical, path-like thinking at the centre of the Pearlite paradigm.\nLet’s draw the two types of statistical model implied in the discussion above. Firstly the economist’s model:\n\n\n\n\nflowchart LR\n\nrace(race)\nqual(qualifications)\nhist(job history)\naccept(job offer)\n\nrace --&gt;|Z| accept\nqual --&gt;|X*| accept\nhist --&gt;|X*| accept \n\n\n\n\n\n\nAnd now the epidemiologist’s model:\n\n\n\n\nflowchart LR \n\nrace(race)\naccept(job offer)\n\nrace --&gt;|Z| accept\n\n\n\n\n\n\nEmploying a DAG-like causal path diagram would at the very least allow both the economist and epidemiologist to discuss whether or not they agree that the underlying causal pathways are more likely to be something like the follows:\n\n\n\n\nflowchart LR\n\n\nrace(race)\nqual(qualifications)\nhist(job history)\naccept(job offer)\n\nrace --&gt; qual\nqual --&gt; hist\nhist --&gt; accept\n\nrace --&gt; hist\nqual --&gt; accept\nrace --&gt; accept\n\n\n\n\n\n\nIf, having drawn out their presumed causal pathways like this, the economist and epidemiologist end up with the same path diagram, then the Pearlian framework offers plenty of suggestions about how, subject to various assumptions about the types of effect each node has on each downstream node, statistical models based on observational data should be specified, and how the values of various coefficients in the statistical model should be combined in order to produce an overall estimate of the left-most node on the right-most node. Even a Rubinite who does not subscribe to some of these assumptions may still find this kind of graphical, path-based reasoning helpful for thinking through what their concerns are relating to both omitted variable and post-treatment biases are, and whether there’s anything they can do about it. In the path diagram above, for example, the importance of temporal sequence appears important: first there’s education and qualification; then there’s initial labour market experience; and then there’s contemporary labour market experience. This appreciation of the sequence of events might suggest that, perhaps, data employing a longitudinal research design might be preferred to one using only cross-sectional data; and/or that what appeared intially to be only a single research question, investigated through a single statistical model, is actually a series of linked, stepped research questions, each employing a different statistical model, breaking down the cause-effect question into a series of smaller steps."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-18/index.html#summary-thoughts-on-social-complexity-and-the-need-for-epistemic-humility",
    "href": "pages/causal-inference/lms-are-glms-part-18/index.html#summary-thoughts-on-social-complexity-and-the-need-for-epistemic-humility",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "Summary thoughts: on social complexity and the need for epistemic humility",
    "text": "Summary thoughts: on social complexity and the need for epistemic humility\nAs mentioned before, I probably lean somewhat more towards the Rubinite than the Pearlite framework. A lot of this is simply because this is the causal effect framework I was first introduced to, but some of it comes from more fundamental concerns I have about how some users and advocates of the Pearlite framework seem to think, or suggest, it can solve issues of causal inference from observational data that, fundamentally, I don’t think it may be possible to address.\nOne clue about what the Pearlite framework can and cannot do comes from the ‘A’ in DAG: ‘acyclic’. This means that causal pathways of the following form can be specified:\n\n\n\n\nflowchart LR\nA(A)\nB(B)\n\nA --&gt; B\n\n\n\n\n\nBut causal pathways of the following form cannot:\n\n\n\n\nflowchart LR\n\nA(A)\nB(B)\n\nA --&gt; B\nB --&gt; A\n\n\n\n\n\n\nUnfortunately, cyclic relationships between two or more factors, in which the pathways of influence go in both directions, are likely extremely common in social and economic systems, because such systems are complex rather than merely complicated. 3 One approach to trying to fit a representation of a complex coupled system into a DAG-like framework would be to use time to try to break the causal paths:\n\n\n\n\nflowchart LR\n\nc0(Chicken at T0)\ne1(Egg at T1)\nc2(Chicken at T2)\ne3(Egg at T3)\n\nc0 --&gt; e1\ne1 --&gt; c2\nc2 --&gt; e3\n\n\n\n\n\n\nBut another way of reasoning about such localised coupled complexity might be to use something like factor analysis to identify patterns of co-occurence of variables which may be consistent with this kind of localised complex coupling:\n\n\n\n\nflowchart LR\n\nce((ChickenEgg))\ne[egg]\nc[chicken]\n\nce --&gt; e\nce --&gt; c\n\n\n\n\n\n\nWithin the above diagram, based on structural equation modelling, the directed arrows have a different meaning. They’re not claims of causal effects, but instead of membership. The circle is an underlying proposed ‘latent variable’, the ChickenEgg, which is presumed to manifest through the two observed/manifest variables egg and chicken represented by the rectangles. In places with a lot of ChickenEgg, such as a hen house, we would expect to observe a lot of both chickens and eggs. The statistical model in the above case is a measurement model, rather than a causal model, but in this case is one which is informed by an implicit recognition of continual causal influence operating within members of a complex, paired, causal system.\nSo, I guess my first concern relating to DAGs is that, whereas they can be really useful in allowing researchers to express some form of causal thinking and assumptions about paths of influence between factors, their acyclic requirement can also lead researchers to disregard or underplay the role of complexity even when considering inherently complex systems. In summary, they offer the potential both to expand, but also to restrict, our ability to reason effectively about causal influence.\nMy second, related, concern about the potential over-use or over-reach of DAG-like thinking comes from conventional assumptions built into the paths of influence between nodes. We can get to the heart of this latter concern by looking at , and carefully considering the implications of, something called a double pendulum, a video of which is shown below:\n\n\nA double pendulum is not a complicated system, but it is a complex system, and also a chaotic system. The variables at play include two length variables, two mass variables, a gravity variable, and time. The chaotic complexity of the system comes from the way the length and mass of the first arm interact with the length and and mass of the second arm. This complex interaction is what leads to the position of the outer-most part of the second arm (the grey ball) at any given time.\nNow imagine trying to answer a question of the form “what is the effect of the first arm’s mass on the grey ball’s position?” This kind of question is one that it’s simply not meaningful to even ask. It’s the complex interaction between all components of the system that jointly determines the ball’s position, and attempting to decompose the causal effect of any one variable in the system is simply not a fruitful way of trying to understand the system as a whole.\nThis does not mean, however, that we cannot develop a useful understanding of the double pendulum. We know, for example, that the ball cannot be further than the sum of the length of the two arms from the centre of the system. If we were thinking about placing another object near the double pendulum, for example, this would help us work out how far apart from the pendulum we should place it. Also, if one of the arms is much longer or more massive than the other, then maybe we could approximate it with a simple pendulum too. Additionally, all double pendulums tend to behave in similar ways during their initial fall. But the nature of this kind of complex system also means some types of causal question are beyond the realm of being answerable.\nThe double pendulum, for me, is an object lesson on the importance of epistemic humility. My overall concern relating to causal inference applies nearly equally to Rubinites and Pearlites alike, and is that excessive engagement with or enthusiasm for any kind of method or framework can lead to us believing we know more than we really know more about how one thing affects another. This can potentially lead both to errors of judgement - such as not planning sufficiently for eventualities our models suggest cannot happen - and potentially to intolerance towards those who ‘join the dots’ in a different way to ourselves. 4\nIn short: stay methodologically engaged, but also stay epistemically modest."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-18/index.html#footnotes",
    "href": "pages/causal-inference/lms-are-glms-part-18/index.html#footnotes",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI might not cover these areas in the order listed above, and thinking about this further this might be too much territory for a single post. Let’s see how this post develops…↩︎\nThe bridge point on the Pearlite side might be a recognition of the apparent bloody obviousness of the fact that, if an observational unit was treated, we don’t observe untreated, and vice versa. The kind of table with missing cells, as shown in part fifteen, would appear to follow straightforwardly from conceding this point. However, Pearl and Mackenzie (2018) includes an example of this kind of table (table 8.1; p. 273), and argues forcefully against this particular framing.↩︎\nThe economist’s model is more complicated than the epidemiologist’s model, but both are equally complex, i.e. not complex at all, because they don’t involve any pathways going from right to left.↩︎\nA majority of political disagreement, for example, seems to occur when people agree on the facts, but disagree about the primary causal pathway.↩︎"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-14/index.html",
    "href": "pages/causal-inference/lms-are-glms-part-14/index.html",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "",
    "text": "Henry Dundas, as observed\n\n\n\n\n\n\n\nHenry Dundas, the unobserved good counterfactual\n\n\n\n\n\n\n\nHenry Dundas, the unobserved bad counterfactual"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-14/index.html#high-level-notewarning",
    "href": "pages/causal-inference/lms-are-glms-part-14/index.html#high-level-notewarning",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "High level note/warning",
    "text": "High level note/warning\nThere are broadly two schools of thought when it comes to thinking about the problems of causal inference. One which interprets the challenge of causal inference mainly as a missing data problem; and another which interprets it mainly in terms of a modelling problem. The posts in this series are largely drawn from the missing data interpretation. If you want an overview of the two approaches (albeit subject to my own ignorance and biases), please skip briefly to the last post in this series before continuing."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-14/index.html#henry-dundas-hero-or-villain",
    "href": "pages/causal-inference/lms-are-glms-part-14/index.html#henry-dundas-hero-or-villain",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "Henry Dundas: Hero or Villain?",
    "text": "Henry Dundas: Hero or Villain?\nA few minutes’ walk from where I live is St Andrew Square. And in the middle of St Andrew Square is the Melville Monument, a 40 metre tall column, on which stands a statue of Henry Dundas, 1st Viscount Melville.\nThough the Melville Monument was constructed in the 19th century to commemorate and celebrate this 18th century figure, in 2020 the City of Edimburgh Council chose to add more context to Dundas’ legacy by unveiling a plaque with the following message::\n\nAt the top of this neoclassial column stands a statue of Hentry Dundas, 1st Viscount Melville (1742-1811). He was the Scottish Lord Advocate, an MP for Edinburgh and Midlothian, and the First Lord of the Admiralty. Dundas was a contentious figure, provoking controversies that resonate to this day. While Home Secretary in 1792, and first Secretary of State for War in 1796 he was instrumental in deferring the abolition of the Atlantic slave trade. Slave trading by British ships was not abolished until 1807. As a result of this delay, more than half a million enslaved Africans crossed the Atlantic.\n\nSo, the claim of the council plaque was that Dundas caused the enslavement of hundreds of thousands of Africans, by promoting a gradualist policy of abolition.\nThe descendents of Dundas contested these claims, however, instead arguing:\n\nThe claim that Henry Dundas caused the enslavement of more than half a million Africans is patently false. The truth is: Dundas was the first MP to advocate in Parliament for the emancipation of slaves in the British territories along with the abolition of the slave trade. Dundas’s efforts resulted in the House of Commons voting in favour of ending the Atlantic slave trade for the first time in its history.\n\nSo, the claim of the descendents was that Dundas prevented the enslavement of (at least) hundreds of thousands of Africans, by promoting a gradualist policy of abolition.\nHow can the same agreed-upon historical facts lead to such diametrically opposing interpretations of the effects of Dundas and his actions?\nThe answer to this question is at the heart of causal inference, and an example of why, when trying to estimate causal effects, at least half of the data are always missing."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-14/index.html#the-unobserved-counterfactual",
    "href": "pages/causal-inference/lms-are-glms-part-14/index.html#the-unobserved-counterfactual",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "The unobserved counterfactual",
    "text": "The unobserved counterfactual\nBoth parties in the Dundas debate have, as mentioned, access to the same historical facts. They agree on the same observed historical reality. And both are making bold claims about the impact of Dundas in relation to the Transatlantic slave trade. In doing this, they are both comparing this observed historical reality with something else: the unobserved counterfactual.\nThe unobserved counterfactual is the data that would have been observed if what had happened, hadn’t happened 1 However, what happened did happen, so this data isn’t observed. So, as it hasn’t been observed, it doesn’t exist in any historic facts. Instead, the unobserved counterfactual has to be imputed, or inferred… in effect, made up.\nCausal inference always involves some kind of comparison between an observed reality and an unobserved counterfactual. The issue at heart of the Dundas debate is that both parties have compared the observed reality with a different unobserved counterfactual, and from this different Dundas effects have been inferred.\nFor the council, the unobserved counterfactual appears to be something like the following:\n\nDundas doesn’t propose a gradualist amendment to a bill in parliament. The more radical and rapid version of the bill passes, and slavery is abolished earlier, leading to fewer people becoming enslaved.\n\nWhereas for the descendents, the unobserved counterfactual appears to be something like this:\n\nDundas doesn’t propose a gradualist amendment to a bill in parliament. Because of this, the more radical version of the bill doesn’t have enough support in parliament (perhaps because it would be acting too much against the financial interests of some parliamentarians and powerful business interests), and so is defeated. As a result of this, the abolition of slavery is delayed, leading to more people becoming enslaved.\n\nSo, by having the same observed historical facts, the observed Dundas, but radically different counterfactuals, the two parties have used the same methodology to derive near antithetical estimates of the ‘Dundas Effect’."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-14/index.html#coming-up",
    "href": "pages/causal-inference/lms-are-glms-part-14/index.html#coming-up",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "Coming up",
    "text": "Coming up\nThe next post offers more of a technical treatment of the key concept introduced here: namely that causal effect estimation depends on comparing observed with counterfactual data, and as the counterfactual is unobserved, causal effect estimation is fundamentally a missing data problem."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-14/index.html#footnotes",
    "href": "pages/causal-inference/lms-are-glms-part-14/index.html#footnotes",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data that would have been observed if what hadn’t happened, had happened, is the other type of unobserved counterfactual.↩︎"
  },
  {
    "objectID": "pages/intro-to-glms/index.html",
    "href": "pages/intro-to-glms/index.html",
    "title": "Introduction to Generalised Linear Models",
    "section": "",
    "text": "The aims of this web page is to provide an overview of generalised linear models, and ways of thinking about modelling that go beyond ‘star-gazing’."
  },
  {
    "objectID": "pages/intro-to-glms/index.html#fundamentals-of-generalised-linear-models",
    "href": "pages/intro-to-glms/index.html#fundamentals-of-generalised-linear-models",
    "title": "JonStats",
    "section": "",
    "text": "This is some text"
  },
  {
    "objectID": "pages/intro-to-glms/index.html#tldr",
    "href": "pages/intro-to-glms/index.html#tldr",
    "title": "Part One: Model fitting as parameter calibration",
    "section": "tl;dr",
    "text": "tl;dr\nThis is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in @KinTomWit00 ."
  },
  {
    "objectID": "pages/intro-to-glms/index.html#part-1-what-are-statistical-models-and-how-are-they-fit",
    "href": "pages/intro-to-glms/index.html#part-1-what-are-statistical-models-and-how-are-they-fit",
    "title": "Part One: Model fitting as parameter calibration",
    "section": "Part 1: What are statistical models and how are they fit?",
    "text": "Part 1: What are statistical models and how are they fit?\nIt’s common for different statistical methods to be taught as if they’re completely different species or families. In particular, for standard linear regression to be taught first, then additional, more exotic models, like logistic or Poisson regression, to be introduced at a later stage, in an advanced course.\nThe disadvantage with this standard approach to teaching statistics is that it obscures the way that almost all statistical models are, fundamentally, trying to do something very similar, and work in very similar ways.\nSomething I’ve found immensely helpful over the years is the following pair of equations:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nIn words, the above is saying something like:\n\nThe predicted response \\(Y_i\\) for a set of predictors \\(X_i\\) is assumed to be drawn from (the \\(\\sim\\) symbol) a stochastic distribution (\\(f(.,.)\\))\nThe stochastic distribution contains both parameters we’re interested in, and which are determined by the data \\(\\theta_i\\), and parameters we’re not interested in and might just have to assume, \\(\\alpha\\).\nThe parameters we’re interested in determining from the data \\(\\theta_i\\) are themselves determined by a systematic component \\(g(.,.)\\) which take and transform two inputs: The observed predictor data \\(X_i\\), and a set of coefficients \\(\\beta\\)\n\nAnd graphically this looks something like:\n\n\n\n\nflowchart LR\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  \n  X --&gt; g\n  beta --&gt; g\n  g --&gt; theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt; Y\n\n\n\n\n\n\n\nTo understand how this fits into the ‘whole game’ of modelling, it’s worth introducing another term, \\(D\\), for the data we’re using, and to say that \\(D\\) is partitioned into observed predictors \\(X_i\\), and observed responses, \\(y_i\\).\nFor each observation, \\(i\\), we therefore have a predicted response, \\(Y_i\\), and an observed response, \\(y_i\\). We can compare \\(Y_i\\) with \\(y_i\\) to get the difference between the two, \\(\\delta_i\\).\nNow, obviously can’t change the data to make it fit our model better. But what we can do is calibrate the model a little better. How do we do this? Through adjusting the \\(\\beta\\) parameters that feed into the systematic component \\(g\\). Graphically, this process of comparison, adjustment, and calibration looks as follows:\n\n\n\n\nflowchart LR\n  D\n  y\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  diff\n  \n  D --&gt;|partition| X\n  D --&gt;|partition| y\n  X --&gt; g\n  beta --&gt;|rerun| g\n  g --&gt;|transform| theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt;|predict| Y\n  \n  Y --&gt;|compare| diff\n  y --&gt;|compare| diff\n  \n  diff --&gt;|adjust| beta\n  \n  \n  \n  linkStyle default stroke:blue, stroke-width:1px\n\n\n\n\n\n\nPretty much all statistical model fitting involves iterating along this \\(g \\to \\beta\\) and \\(\\beta \\to g\\) feedback loop until some kind of condition is met involving minimising \\(\\delta\\).\nI’ll expand on this idea further in part 2."
  },
  {
    "objectID": "pages/intro-to-glms/index.html#footnotes",
    "href": "pages/intro-to-glms/index.html#footnotes",
    "title": "Introduction to Generalised Linear Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote here I’m using \\(x_j\\), not \\(x_i\\), and that \\(X\\beta\\) is shorthand for \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) and so on. In using the \\(j\\) suffix, I’m referring to just one of the specific \\(x\\) values, \\(x_1\\), \\(x_2\\), \\(x_3\\), which is equivalent to selecting one of the columns in \\(X\\). By contrast \\(i\\) should be considered shorthand for selection of one of the rows of \\(X\\), i.e. one of the series of observations that goes into the dataset \\(D\\).↩︎\nNote here I’m using \\(x_j\\), not \\(x_i\\), and that \\(X\\beta\\) is shorthand for \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) and so on. In using the \\(j\\) suffix, I’m referring to just one of the specific \\(x\\) values, \\(x_1\\), \\(x_2\\), \\(x_3\\), which is equivalent to selecting one of the columns in \\(X\\). By contrast \\(i\\) should be considered shorthand for selection of one of the rows of \\(X\\), i.e. one of the series of observations that goes into the dataset \\(D\\).↩︎\n\\(E(.)\\) is the expectation operator, and \\(|\\) indicates a condition. So, the two terms mean, respectively, what is the expected value of the outcome if the variable of interest is ‘switched on’?, and what is the expected value of the outcome if the variable of interest is ‘switched off’?↩︎\nThe logistic function maps any real number z onto the value range 0 to 1. z is \\(X\\beta\\), which in non-matrix notation is equivalent to a sum of products \\(\\sum_{k=0}^{K}x_k\\beta_k\\) (where, usually, \\(x_0\\) is 1, i.e. the intercept term). Another way of expressing this would be something like \\(\\sum_{k \\in S}x_k\\beta_k\\) where by default \\(S = \\{0, 1, 2, ..., K\\}\\). We can instead imagine partitioning out \\(S = \\{S^{-J}, S^{J}\\}\\) where the superscript \\(J\\) indicates the Jth variable, and \\(-J\\) indicates everything in \\(S\\) apart from the Jth variable. Where J is a discrete variable, the effect of J on \\(P(Y=1)\\) is \\(logistic({\\sum_{k \\in S^{-J}}x_k\\beta_k + \\beta_J}) - logistic({\\sum_{k \\in S^{-J}}x_k\\beta_k})\\), where \\(logistic(z) = \\frac{1}{1 + e^{-z}}\\). The marginal effect of the \\(\\beta_J\\) coefficient thus depends on the other term \\(\\sum_{k \\in S^{-J}}x_k\\beta_k\\). Where this other term is set to 0 the marginal effect of \\(\\beta_J\\) becomes \\(logistic(\\beta_J) - logistic(0)\\). According to p.82 of this chapter by Gelman we can equivalently ask the question ‘what is the first derivative of the logistic regression with respect to \\(\\beta\\)?’. Asking more about this to Wolfram Alpha we get this page of information, and scrolling down to the section on the global minimum we indeed get an absolute value of \\(\\frac{1}{4}\\), so the maximum change in \\(P(Y=1)\\) given a unit change in \\(\\beta\\) is indeed one quarter of the value of \\(\\beta\\), hence why the ‘divide-by-four’ heuristic ‘works’. This isn’t quite a full derivation, but more explanation than I was planning for a footnote! In general, it’s better just to remember ‘divide-by-four’ than go down the rabbit warren of derivation each time! (As I’ve just learned, to my cost, writing this footnote!)↩︎\nWe should always expect the absolute value of a coefficient for a discrete variable to be less than four, for this reason.↩︎\nThe lower bound for the marginal effect of a discrete variable, or any variable, is zero. This is when the absolute value of the sum of the product of the other variables is infinite.↩︎\nOr the base R expand.grid function↩︎"
  },
  {
    "objectID": "pages/intro-to-glms/index.html#what-are-statistical-models-and-how-are-they-fit",
    "href": "pages/intro-to-glms/index.html#what-are-statistical-models-and-how-are-they-fit",
    "title": "Introduction to Generalised Linear Models",
    "section": "What are statistical models and how are they fit?",
    "text": "What are statistical models and how are they fit?\nIt’s common for different statistical methods to be taught as if they’re completely different species or families. In particular, for standard linear regression to be taught first, then additional, more exotic models, like logistic or Poisson regression, to be introduced at a later stage, in an advanced course.\nThe disadvantage with this standard approach to teaching statistics is that it obscures the way that almost all statistical models are, fundamentally, trying to do something very similar, and work in very similar ways.\nSomething I’ve found immensely helpful over the years is the following pair of equations:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nIn words, the above is saying something like:\n\nThe predicted response \\(Y_i\\) for a set of predictors \\(X_i\\) is assumed to be drawn from (the \\(\\sim\\) symbol) a stochastic distribution (\\(f(.,.)\\))\nThe stochastic distribution contains both parameters we’re interested in, and which are determined by the data \\(\\theta_i\\), and parameters we’re not interested in and might just have to assume, \\(\\alpha\\).\nThe parameters we’re interested in determining from the data \\(\\theta_i\\) are themselves determined by a systematic component \\(g(.,.)\\) which take and transform two inputs: The observed predictor data \\(X_i\\), and a set of coefficients \\(\\beta\\)\n\nAnd graphically this looks something like:\n\n\n\n\nflowchart LR\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  \n  X --&gt; g\n  beta --&gt; g\n  g --&gt; theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt; Y\n\n\n\n\n\n\nTo understand how this fits into the ‘whole game’ of modelling, it’s worth introducing another term, \\(D\\), for the data we’re using, and to say that \\(D\\) is partitioned into observed predictors \\(X_i\\), and observed responses, \\(y_i\\).\nFor each observation, \\(i\\), we therefore have a predicted response, \\(Y_i\\), and an observed response, \\(y_i\\). We can compare \\(Y_i\\) with \\(y_i\\) to get the difference between the two, \\(\\delta_i\\).\nNow, obviously can’t change the data to make it fit our model better. But what we can do is calibrate the model a little better. How do we do this? Through adjusting the \\(\\beta\\) parameters that feed into the systematic component \\(g\\). Graphically, this process of comparison, adjustment, and calibration looks as follows:\n\n\n\n\nflowchart LR\n  D\n  y\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  diff\n  \n  D --&gt;|partition| X\n  D --&gt;|partition| y\n  X --&gt; g\n  beta --&gt;|rerun| g\n  g --&gt;|transform| theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt;|predict| Y\n  \n  Y --&gt;|compare| diff\n  y --&gt;|compare| diff\n  \n  diff --&gt;|adjust| beta\n  \n  \n  \n  linkStyle default stroke:blue, stroke-width:1px\n\n\n\n\n\n\nPretty much all statistical model fitting involves iterating along this \\(g \\to \\beta\\) and \\(\\beta \\to g\\) feedback loop until some kind of condition is met involving minimising \\(\\delta\\)."
  },
  {
    "objectID": "pages/intro-to-glms/index.html#systematic-components-and-link-functions",
    "href": "pages/intro-to-glms/index.html#systematic-components-and-link-functions",
    "title": "Introduction to Generalised Linear Models",
    "section": "Systematic components and link functions",
    "text": "Systematic components and link functions\nThe two part equation shown above is too general and abstract to be implemented directly. Instead, specific choices about the \\(f(.)\\) and \\(g(.)\\) need to be made. King, Tomz, and Wittenberg (2000) gives the following examples:\nLogistic Regression\n\\[\nY_i \\sim Bernoulli(\\pi_i)\n\\]\n\\[\n\\pi_i = \\frac{1}{1 + e^{-X_i\\beta}}\n\\]\nLinear Regression\n\\[\nY_i \\sim N(\\mu_i, \\sigma^2)\n\\] \\[\n\\mu_i = X_i\\beta\n\\]\nSo, what’s so special about linear regression, in this framework?\nIn one sense, not so much. It’s got a systematic component, and it’s got a stochastic component. But so do other models. But in another sense, quite a lot. It’s a rare case where the systematic component, \\(g(.)\\), doesn’t transform its inputs in some weird and wonderful way. We can say that \\(g(.)\\) is the identity transform, \\(I(.)\\), which in words means take what you’re given, do nothing to it, and pass it on.\nBy contrast, the systematic component for logistic regression is known as the logistic function. \\(logistic(x) := \\frac{1}{1 + e^{-x}}\\) It transforms inputs that could be anywhere on the real number line to values that lay somewhere between 0 and 1. Why 0 to 1? Because what logistic regression models produce aren’t predicted values, but predicted probabilities, and nothing can be more probable than certain (1) or less probable than impossible (0).\nWe can compare the transformations used in linear and logistic regression as follows:1\n\n\nCode\n# Define transformations\nident &lt;- function(x) {x}\nlgt &lt;- function(x) {1 / (1 + exp(-x))}\n\n\n# Draw the associations\ncurve(ident, -6, 6,\n      xlab = \"x (before transform)\",\n      ylab = \"z (after transform)\",\n      main = \"The Identity 'Transformation'\"\n      )\ncurve(lgt, -6, 6, \n      xlab = \"x (before transform)\", \n      ylab = \"z (after transform)\",\n      main = \"The Logistic Transformation\"\n      )\n\n\n\n\n\n\n\nIdentity Transformation\n\n\n\n\n\n\n\nLogistic Transformation\n\n\n\n\n\n\nThe usual input to the transformation function \\(g(.)\\) is a sum of products. For three variables, for example, this could be \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\). In matrix algebra this generalises to \\(\\boldsymbol{X\\beta}\\) , where \\(\\boldsymbol{X}\\) is the predictor data whose rows are observations, columns are variables, and whose first column is a vector of 1s (for the intercept term). The \\(\\boldsymbol{\\beta}\\) term is a row-wise vector comprising each specific \\(\\beta\\) term, such as \\(\\boldsymbol{\\beta} = \\{ \\beta_0, \\beta_1, \\beta_2 \\}\\) in the three variable example above.\nWhat’s special about the identity transformation, and so linear regression, is that there is a fairly clear correspondence between a \\(\\beta_j\\) term and the estimated influence of changing a predictor variable \\(x_j\\) on the predicted outcome \\(Y\\), i.e. the ‘effect of \\(x_j\\) on \\(Y\\)’. For other transformations this tends to not be the case."
  },
  {
    "objectID": "pages/intro-to-glms/index.html#how-to-express-a-linear-model-as-a-generalised-linear-model",
    "href": "pages/intro-to-glms/index.html#how-to-express-a-linear-model-as-a-generalised-linear-model",
    "title": "Introduction to Generalised Linear Models",
    "section": "How to express a linear model as a generalised linear model",
    "text": "How to express a linear model as a generalised linear model\nIn R, there’s the lm function for linear models, and the glm function for generalised linear models.\nI’ve argued previously that the standard linear regression is just a specific type of generalised linear model, one that makes use of an identity transformation I(.) for its systematic component g(.). Let’s now demonstrate that by producing the same model specification using both lm and glm.\nWe can start by being painfully unimaginative and picking using one of R’s standard datasets\n\n\nCode\nlibrary(tidyverse)\n\niris |&gt; \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n\n\n\n\n\nIt looks like, where the petal length is over 2.5, the relationship with sepal length is fairly linear\n\n\nCode\niris |&gt; \n  filter(Petal.Length &gt; 2.5) |&gt; \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n\n\n\n\n\nSo, let’s make a linear regression just of this subset\n\n\nCode\niris_ss &lt;- \n  iris |&gt; \n  filter(Petal.Length &gt; 2.5) \n\n\nWe can produce the regression using lm as follows:\n\n\nCode\nmod_lm &lt;- lm(Sepal.Length ~ Petal.Length, data = iris_ss)\n\n\nAnd we can use the summary function (which checks the type of mod_lm and evokes summary.lm implicitly) to get the following:\n\n\nCode\nsummary(mod_lm)\n\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = iris_ss)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.09194 -0.26570  0.00761  0.21902  0.87502 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3731 on 98 degrees of freedom\nMultiple R-squared:  0.6864,    Adjusted R-squared:  0.6832 \nF-statistic: 214.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nWoohoo! Three stars next to the Petal.Length coefficient! Definitely publishable!\nTo do the same using glm.\n\n\nCode\nmod_glm &lt;- glm(Sepal.Length ~ Petal.Length, data = iris_ss)\n\n\nAnd we can use the summary function for this data too. In this case, summary evokes summary.glm because it knows the class of mod_glm contains glm.\n\n\nCode\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = Sepal.Length ~ Petal.Length, data = iris_ss)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1391962)\n\n    Null deviance: 43.496  on 99  degrees of freedom\nResidual deviance: 13.641  on 98  degrees of freedom\nAIC: 90.58\n\nNumber of Fisher Scoring iterations: 2\n\n\nSo, the coefficients are exactly the same. But there’s also some additional information in the summary, including on the type of ‘family’ used. Why is this?\nIf we look at the help for glm we can see that, by default, the family argument is set to gaussian.\nAnd if we delve a bit further into the help file, in the details about the family argument, it links to the family help page. The usage statement of the family help file is as follows:\nfamily(object, ...)\n\nbinomial(link = \"logit\")\ngaussian(link = \"identity\")\nGamma(link = \"inverse\")\ninverse.gaussian(link = \"1/mu^2\")\npoisson(link = \"log\")\nquasi(link = \"identity\", variance = \"constant\")\nquasibinomial(link = \"logit\")\nquasipoisson(link = \"log\")\nEach family has a default link argument, and for this gaussian family, this link is the identity function.\nWe can also see that, for both the binomial and quasibinomial family, the default link is logit, which transforms all predictors onto a 0-1 scale, as shown in the last post.\nSo, by using the default family, the Gaussian family is selected, and by using the default Gaussian family member, the identity link is selected.\nWe can confirm this by setting the family and link explicitly, showing that we get the same results\n\n\nCode\nmod_glm2 &lt;- glm(Sepal.Length ~ Petal.Length, family = gaussian(link = \"identity\"), data = iris_ss)\nsummary(mod_glm2)\n\n\n\nCall:\nglm(formula = Sepal.Length ~ Petal.Length, family = gaussian(link = \"identity\"), \n    data = iris_ss)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1391962)\n\n    Null deviance: 43.496  on 99  degrees of freedom\nResidual deviance: 13.641  on 98  degrees of freedom\nAIC: 90.58\n\nNumber of Fisher Scoring iterations: 2\n\n\nIt’s the same!\nHow do these terms used in the glm function, family and link, relate to the general framework in King, Tomz, and Wittenberg (2000)?\n\nfamily is the stochastic component, f(.)\nlink is the systematic component, g(.)\n\nThey’re different terms, but it’s the same broad framework.\nLinear models are just one type of general linear model!"
  },
  {
    "objectID": "pages/intro-to-glms/index.html#why-only-betas-look-at-betas",
    "href": "pages/intro-to-glms/index.html#why-only-betas-look-at-betas",
    "title": "Introduction to Generalised Linear Models",
    "section": "Why only betas look at betas",
    "text": "Why only betas look at betas\n\nWhy overuse of linear regression leads people to look at models in the wrong way\nThough it’s not always phrased this way, a motivating question behind the construction of most statistical models is, “What influence does a single input to the model, \\(x_j\\), have on the output, \\(Y\\)?”2 For a single variable \\(x_j\\) which is either present (1) or absent (0), this is in effect asking what is \\(E(Y | x_j = 1) - E(Y | x_j = 0)\\) ?3\nLet’s look at a linear regression case, then a logistic regression case.\n\n\nLinear Regression example\nUsing the iris dataset, let’s try to predict Sepal Width (a continuous variable) on Sepal Length (a continuous variable) and whether the species is setosa or not (a discrete variable). As a reminder, the data relating these three variables look as follows:\n\n\nCode\nlibrary(ggplot2)\n\niris |&gt;\n    ggplot(aes(Sepal.Length, Sepal.Width, group = Species, colour = Species, shape = Species)) + \n    geom_point()\n\n\n\n\n\nLet’s now build the model:\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- iris |&gt; mutate(is_setosa = Species == 'setosa')\n\nmod_lm &lt;- lm(Sepal.Width ~ Sepal.Length + is_setosa, data = df)\n\nmod_lm\n\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length + is_setosa, data = df)\n\nCoefficients:\n  (Intercept)   Sepal.Length  is_setosaTRUE  \n       0.7307         0.3420         0.9855  \n\n\nThe coefficients \\(\\boldsymbol{\\beta} = \\{\\beta_0, \\beta_1, \\beta_2\\}\\) are \\(\\{0.73, 0.34, 0.99\\}\\), and refer to the intercept, Sepal Length and is_setosa respectively.\nIf we assume a Sepel Length of 6, for example, then the expected Sepal Width (the thing we are predicting) is 0.73 + 6 * 0.34 + 0.99 or about 3.77 in the case where is_setosa is true, and 0.73 + 6 * 0.34 or about 2.78 where is_setosa is false.\nThe difference between these two values, 3.77 and 2.78, i.e. the ‘influence of setosa’ on the outcome, is 0.99, i.e. the \\(\\beta_2\\) coefficient shown before. In fact, for any conceivable (and non-conceivable, i.e. negative) value of Sepal Length, the difference is still 0.99.\nThis is the \\(\\beta_2\\) coefficient, and the reason why, for linear regression, and almost exclusively linear regression, looking at the coefficients themselves provides substantively meaningful information (something King, Tomz, and Wittenberg (2000) calls a ‘quantity of interest’) about the size of influence that a predictor has on a response.\n\n\nLogistic Regression example\nNow let’s look at an example using logistic regression. We will use another tiresomely familiar dataset, mtcars. We are interested in estimating the effect that having a straight engine (vs=1) has on the probability of the car having a manual transmission (am=1). Our model also tries to control for the miles-per-gallon (mpg). The model specification is shown, the model is run, and the coefficeints are all shown below:\n\n\nCode\nmod_logistic &lt;- glm(\n    am ~ mpg + vs,\n    data = mtcars, \n    family = binomial()\n    )\n\nmod_logistic\n\n\n\nCall:  glm(formula = am ~ mpg + vs, family = binomial(), data = mtcars)\n\nCoefficients:\n(Intercept)          mpg           vs  \n    -9.9183       0.5359      -2.7957  \n\nDegrees of Freedom: 31 Total (i.e. Null);  29 Residual\nNull Deviance:      43.23 \nResidual Deviance: 24.94    AIC: 30.94\n\n\nHere the coefficients \\(\\boldsymbol{\\beta} = \\{\\beta_0, \\beta_1, \\beta_2\\}\\) are \\(\\{-9.92, 0.54, -2.80\\}\\), and refer to the intercept, mpg, and vs respectively.\nBut what does this actually mean, substantively?\n\n\n(Don’t) Stargaze\nA very common approach to trying to answer this question is to look at the statistical significance of the coefficients, which we can do with the summary() function\n\n\nCode\nsummary(mod_logistic)\n\n\n\nCall:\nglm(formula = am ~ mpg + vs, family = binomial(), data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -9.9183     3.4942  -2.839  0.00453 **\nmpg           0.5359     0.1967   2.724  0.00644 **\nvs           -2.7957     1.4723  -1.899  0.05758 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 24.944  on 29  degrees of freedom\nAIC: 30.944\n\nNumber of Fisher Scoring iterations: 6\n\n\nA common practice in many social and health sciences is to offer something like a narrative summary of the above, something like:\n\nOur logistic regression model indicates that manualness is positively and significantly associated with our measure of fuel efficiency (p &lt; 0.01). There is also an indication of a negative association with straight engine, but this effect does not quite meet conventional thresholds for statistical significance (p &lt; 0.10).\n\nThis above practice is known as ‘star-gazing’, because summary tables like those above tend to have one or more * symbols in the final row, if the value of the Pr(&gt;|z|) is below 0.05, and narrative summaries like those just above tend to involve looking at the number of stars in each row, alongside whether the Estimate values have a minus sign in front of them.\nStar gazing is a very common practice. It’s also a terrible practice, which - ironically - turns the final presented output of a quantitative model into the crudest of qualitative summaries (positive, negative; significant, not significant). Star gazing is what researchers tend to default to when presented with model outputs from the above because, unlike in the linear regression example, the extent to which the \\(\\beta\\) coefficients answer substantive ‘how-much’-ness questions, like “How much does having a straight engine change the probability of manual transmission?, is not easily apparent from the coefficients themselves.\n\n\nStandardisation\nSo, how can we do better?\nOne approach is to standardise the data that goes into the model before passing them to the model. Standardisation means attempting to make the distribution and range of different variables more similar, and is especially useful when comparing between different continuous variables.\nTo give an example of this, let’s look at a specification with weight (wt) and horsepower (hp) in place of mpg, but keeping engine-type indicator (vs):\n\n\nCode\nmod_logistic_2 &lt;- glm(\n    am ~ vs + wt + hp,\n    data = mtcars, \n    family = binomial()\n    )\n\nsummary(mod_logistic_2)\n\n\n\nCall:\nglm(formula = am ~ vs + wt + hp, family = binomial(), data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) 25.35510   11.24613   2.255   0.0242 *\nvs          -3.12906    2.92958  -1.068   0.2855  \nwt          -9.64982    4.05528  -2.380   0.0173 *\nhp           0.03242    0.01959   1.655   0.0979 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.2297  on 31  degrees of freedom\nResidual deviance:  8.5012  on 28  degrees of freedom\nAIC: 16.501\n\nNumber of Fisher Scoring iterations: 8\n\n\nHere both wt and hp are continuous variables.\nA star gazing zombie might say something like\n\nmanualness is negatively and significantly associated with weight (p &lt; 0.05); there is a positive association with horsepower but this does not meet standard thresholds of statistical significance (0.05 &lt; p &lt; 0.10).\n\nA slightly better approach would be to standardise the variables wt and hp before passing to the model. Standardising means trying to set the variables to a common scale, and giving the variables more similar statistical characteristics.\n\n\nCode\nstandardise &lt;- function(x){\n  (x - mean(x)) / sd(x)\n}\n\nmtcars_z &lt;- mtcars\nmtcars_z$wt_z = standardise(mtcars$wt)\nmtcars_z$hp_z = standardise(mtcars$hp)\n\nmod_logistic_2_z &lt;- glm(\n    am ~ vs + wt_z + hp_z,\n    data = mtcars_z, \n    family = binomial()\n    )\n\nsummary(mod_logistic_2_z)\n\n\n\nCall:\nglm(formula = am ~ vs + wt_z + hp_z, family = binomial(), data = mtcars_z)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -0.9348     1.4500  -0.645   0.5191  \nvs           -3.1291     2.9296  -1.068   0.2855  \nwt_z         -9.4419     3.9679  -2.380   0.0173 *\nhp_z          2.2230     1.3431   1.655   0.0979 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.2297  on 31  degrees of freedom\nResidual deviance:  8.5012  on 28  degrees of freedom\nAIC: 16.501\n\nNumber of Fisher Scoring iterations: 8\n\n\nwt_z is the standardised version of wt, and hp_z is the standardised version of hp. By convention, whereas unstandardised coefficients are usually referred to as \\(\\beta\\) (‘beta’) coefficients, standardised coefficients are instead referred to as \\(b\\) coefficients. But really, it’s the same model.\nNote the p value of wt_z is the same as for wt, and the p value of hp_z is the same as that for hp. Note also the directions of effect are the same: the coefficients on wt and wt_z are both negative, and the coefficients of hp and hp_z are both positive.\nThis isn’t a coincidence. Of course standardising can’t really add any new information, can’t really change the relationship between a predictor and a response. It’s not really a new variable, it’s the same old variable, so the relationship between predictor and response that there used to be is still there now.\nSo why bother standardising?\nOne reason is it gives, subject to some assumptions and caveats, a way of gauging the relative importance of the two different continuous variables, by allowing a slightly more meaningful comparison between the two coefficients.\nIn this case, we have a standardised \\(b\\) coefficient of -9.44 for wt_z, and of 2.22 for hp_z. As with the unstandardised coefficients we can still assert that manualness is negatively associated with weight, and positively associated with horsepower. But now we can also compare the two numbers -9.44 and 2.22. The ratio of these two numbers is around 4.3. So, we might hazard to suggest something like:\n\na given increase in weight is around four times as important in negatively predicting manual transmission (i.e. in predicting an automatic transmission) as an equivalent increase in horsepower is in positively predicting manual transmission.\n\nThis isn’t a statement that’s easy to parse, but does at least allow slightly more information to be gleamed from the model. For example, it implies that, if a proposed change to a vehicle leads to similar relative (standardised) increases in both weight and horsepower then, as the weight effect is greater than the horsepower effect, the model will predict a decreased probability of manualness as a result.\nBut what about the motivating question, “What’s the effect of a straight engine (vs=1) on the probability of manual transmission (am=1)?”\nThe problem, unlike with the linear regression, is this is now a badly formulated question, based on an incorrect premise. The problem is with the word ‘the’, which implies there should be a single answer to this question, i.e. that the effect of vs on the probability of am=1 should always be the same. But, at least when it comes to absolute changes in the probability of am=1, this is no longer the case, as it depends on the values of the other variables in the model.\nInstead of assuming vs=1 has a single effect on P(am=1), we instead need to think about predictions of the marginal effects of vs on am in the context of other plausible values of the other predictors in the model, wt and hp. This involves asking the model a series of well formulated and specific questions.\n\n\nMaximum marginal effects: Divide-by-four\nBefore we do that, however, there’s a useful heuristic that can be employed when looking at discrete variables and using a logistic regression specification. The heuristic, which is based on the properties of the logistic function,4 is called divide-by-four. What this means is that, if we take the coefficient on vs of -3.13, and divide this value by four, we get a value of -0.78. Notice that the absolute value of -0.78 is between 0 and 1.5 What this value gives is the maximum possible effect that the discrete variable (the presence rather than absence of a straight engine) has on the probability of being a manual transmission. We can say, “a straight engine reduces the probability of a manual transmission by up to 78%”\nBut, as mentioned, this doesn’t quite answer the motivating question, it gives an upper bound to the answer, not the answer itself.6 We can instead start to get a sense of ‘the’ effect of the variable vs on P(am=1) by asking the model a series of questions.\n\n\nPredictions on a matrix\nWe can start by getting the range of observed values for the two continuous variables, hp and mpg:\n\n\nCode\nmin(mtcars$hp)\n\n\n[1] 52\n\n\nCode\nmax(mtcars$hp)\n\n\n[1] 335\n\n\nCode\nmin(mtcars$wt)\n\n\n[1] 1.513\n\n\nCode\nmax(mtcars$wt)\n\n\n[1] 5.424\n\n\nWe can then ask the model to make predictions of \\(P(am=1)\\) for a large number of values of hp and wt within the observed range, both in the condition in which vs=0 and in the condition in which vs=1. The expand_grid function7 can help us do this:\n\n\nCode\npredictors &lt;- expand_grid(\n  hp = seq(min(mtcars$hp), max(mtcars$hp), length.out = 100),\n  wt = seq(min(mtcars$wt), max(mtcars$wt), length.out = 100)\n)\n\npredictors_straight &lt;- predictors |&gt; \n  mutate(vs = 1)\n\npredictors_vshaped &lt;- predictors |&gt; \n  mutate(vs = 0)\n\n\nFor each of these permutations of inputs, we can use the model to get a conditional prediction. For convenience, we can also attach this as an additional column to the predictor data frame:\n\n\nCode\npredictions_predictors_straight &lt;- predictors_straight |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictors_straight)\n  )\n\npredictions_predictors_vshaped &lt;- predictors_vshaped |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictors_vshaped)\n  )\n\n\nWe can see how the predictions vary over hp and wt using a heat map or contour map:\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  ggplot(aes(x = hp, y = wt, z = p_manual)) + \n  geom_contour_filled() + \n  facet_wrap(~vs) +\n  labs(\n    title = \"Predicted probability of manual transmission by wt, hp, and vs\"\n  )\n\n\n\n\n\nWe can also produce a contour map of the differences between these two contour maps, i.e. the effect of a straight (vs=1) compared with v-shaped (vs=0) engine, which gets us a bit closer to the answer:\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  group_by(hp, wt) |&gt; \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |&gt; \n  ungroup() |&gt; \n  ggplot(\n    aes(x = hp, y = wt, z = diff_p_manual)\n  ) + \n  geom_contour_filled() + \n  labs(\n    title = \"Marginal effect of vs=1 given wt and hp on P(am=1)\"\n  )\n\n\n\n\n\nWe can see here that, for large ranges of wt and hp, the marginal effect of vs=1 is small. However, for particular combinations of hp and wt, such as where hp is around 200 and wt is slightly below 3, then the marginal effect of vs=1 becomes large, up to around a -70% reduction in the probability of manual transmission. (i.e. similar to the theoretical maximum marginal effect of around -78%).\nSo, what’s the effect of vs=1 on P(am=1)? i.e. how should we boil down all these 10,000 predicted effect sizes into a single effect size?\nI guess, if we have to try to answer this silly question, then we could take the average effect size…\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  group_by(hp, wt) |&gt; \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |&gt; \n  ungroup() |&gt; \n  summarise(\n    mean_diff_p_manual = mean(diff_p_manual)\n  )\n\n\n# A tibble: 1 × 1\n  mean_diff_p_manual\n               &lt;dbl&gt;\n1            -0.0821\n\n\nSo, we get an average difference of around -0.08, i.e. about an 8% reduction in probability of manual transmission.\n\n\nMarginal effects on observed data\nIs this a reasonable answer? Probably not, because although the permutations of wt and hp we looked at come from the observed range, most of these combinations are likely very ‘theoretical’. We can get a sense of this by plotting the observed values of wt and hp onto the above contour map:\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  group_by(hp, wt) |&gt; \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |&gt; \n  ungroup() |&gt; \n  ggplot(\n    aes(x = hp, y = wt, z = diff_p_manual)\n  ) + \n  geom_contour_filled(alpha = 0.2, show.legend = FALSE) + \n  labs(\n    title = \"Observations from mtcars on the predicted probability surface\"\n  ) +\n  geom_point(\n    aes(x = hp, y = wt), inherit.aes = FALSE,\n    data = mtcars\n  )\n\n\n\n\n\nPerhaps a better option, then, would be to calculate an average marginal effect using the observed values, but switching the observations for vs to 1 in one scenario, and 0 in another scenario:\n\n\nCode\npredictions_predictors_observed_straight &lt;- mtcars |&gt; \n  select(hp, wt) |&gt; \n  mutate(vs = 1)\n\npredictions_predictors_observed_straight &lt;- predictions_predictors_observed_straight |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictions_predictors_observed_straight)\n  )\n\npredictions_predictors_observed_vshaped &lt;- mtcars |&gt; \n  select(hp, wt) |&gt; \n  mutate(vs = 0) \n\npredictions_predictors_observed_vshaped &lt;- predictions_predictors_observed_vshaped |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictions_predictors_observed_vshaped)\n  )\n  \n\npredictions_predictors_observed &lt;- \n  bind_rows(\n    predictions_predictors_observed_straight,\n    predictions_predictors_observed_vshaped\n  )\n\npredictions_marginal &lt;- \n  predictions_predictors_observed |&gt; \n    group_by(hp, wt) |&gt; \n    summarise(\n      diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n    )\n\npredictions_marginal |&gt; \n  ggplot(aes(x = diff_p_manual)) + \n  geom_histogram() +\n  geom_vline(aes(xintercept = mean(diff_p_manual)), colour = \"red\") + \n  geom_vline(aes(xintercept = median(diff_p_manual)), colour = \"green\")\n\n\n\n\n\nIn the above the red line indicates the mean value of these marginal differences, which is -0.12, and the green line the median value of these differences, which is around -0.02. So, even with just these two measures of central tendency, there’s around a six-fold difference in the estimate of ‘the effect’. We can also see there’s a lot of variation, from around nothing (right hand side), to around a 65% reduction (left hand side).\nIf forced to give a simple answer (to this overly simplistic question), we might plump for the mean for theoretical reasons, and say something like “The effect of a straight engine is to reduce the probability of a manual transmission by around an eighth”. But I’m sure, having seen how much variation there is in these marginal effects, we can agree this ‘around an eighth’ answer, or any single number answer, is likely to be overly reductive.\nHopefully, however, it is more informative than ‘statistically significant and negative’, (the stargazing approach) or ‘up to around 78%’ (the divide-by-four approach).\n\n\nConclusion\nLinear regression tends to give a false impression about how straightforward it is to use a model to answer questions of the form “What is the effect of x on y?”. This is because, for linear regression, but few other model specifications, the answer to this question is in the \\(\\beta\\) coefficients themselves. For other model specifications, like the logistic regression example above, the correct-but-uninformative answer tends to be “it depends”, and potentially more informative answers tend to require a bit more work to derive and interpret."
  },
  {
    "objectID": "pages/intro-to-glms/index.html#conclusion",
    "href": "pages/intro-to-glms/index.html#conclusion",
    "title": "Part Four: why only betas just look at betas",
    "section": "Conclusion",
    "text": "Conclusion\nLinear regression tends to give a false impression about how straightforward it is to use a model to answer questions of the form “What is the effect of x on y?”. This is because, for linear regression, but few other model specifications, the answer to this question is in the \\(\\beta\\) coefficients themselves. For other model specifications, like the logistic regression example above, the correct-but-uninformative answer tends to be “it depends”, and potentially more informative answers tend to require a bit more work to derive and interpret."
  },
  {
    "objectID": "pages/intro-to-glms/index.html#coming-up",
    "href": "pages/intro-to-glms/index.html#coming-up",
    "title": "Part Four: why only betas just look at betas",
    "section": "Coming up",
    "text": "Coming up\nThis post concludes the first section of this blog series. We showed the importance of producing predictions from models, rather than just staring at tables of coefficients and producing qualitative ‘stargazing’ summaries of their statistical significance and direction of effect. Statistical significance of individual coefficients almost never answers questions of substantive significance, which instead come from model predictions.\nHowever in the predictions so far, we’ve accidentially pretended to know more than we do. For each prediction, despite imperfect knowledge, we’ve presented point estimates, a single prediction, implying our estimates are sometimes perfectly precise. To be more honest to the user, we should instead present a range of estimates that takes into account all the sources of uncertainty in our modelling which lead to uncertainty in our predictions.\nSection two of this blog series, starting with part five, takes us through the material necessary to go from presenting the kind of dishonest certainty of point estimates in predictions, to honest uncertainty in predictive intervals. This involves covering a lot of theoretical and methodological territory, and is fairly challenging. However we do this in order to make it easier for the end user of statistical models, decision makers, to get the kind of information they need and value most from models."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "",
    "text": "The aim of this this post is to show how the best parameter combinations tend to be estimated from a model’s log likelihood in practice, using an optimisation algorithm that iteratively tries out new parameter values, and keeps trying and trying until some kind of condition is met. This is what the last figure in the first post is trying to illustrate."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#optimisation-algorithms-getting-there-faster",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#optimisation-algorithms-getting-there-faster",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Optimisation algorithms: getting there faster",
    "text": "Optimisation algorithms: getting there faster\nIn the previous post, we ‘cheated’ a bit when using the log likelihood function, fixing the value for one of the parameters \\(\\sigma^2\\) to the value we used when we generated the data, so we could instead look at how the log likelihood surface varied as different combinations of \\(\\beta_0\\) and \\(\\beta_1\\) were plugged into the formula. \\(\\beta_0\\) and \\(\\beta_1\\) values ranging from -5 to 5, and at steps of 0.1, were considered: 101 values of \\(\\beta_0\\), 101 values of \\(\\beta_1\\), and so over 10,0001 unique \\(\\{\\beta_0, \\beta_1\\}\\) combinations were stepped through. This approach is known as grid search, and seldom used in practice (except for illustration purposes) because the number of calculations involved can very easily get out of hand. For example, if we were to use it to explore as many distinct values of \\(\\sigma^2\\) as we considered for \\(\\beta_0\\) and \\(\\beta_1\\), the total number of \\(\\{\\beta_0, \\beta_1, \\sigma^2 \\}\\) combinations we would crawl through would be over 100,000 2 rather than over 10,000.\nOne feature we noticed with the likelihood surface over \\(\\beta_0\\) and \\(\\beta_1\\) in the previous post is that it appears to look like a hill, with a clearly defined highest point (the region of maximum likelihood) and descent in all directions from this highest point. Where likelihood surfaces have this feature of being single-peaked in this way (known as ‘unimodal’), then a class of algorithms known as ‘hill climbing algorithms’ can be applied to find the top of such peaks in a way that tends to be both quicker (fewer steps) and more precise than the grid search approach used for illustration in the previous post."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#code-recap",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#code-recap",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Code recap",
    "text": "Code recap\nLet’s copy over the code we used in the previous post for:\n\n\nCalculating log likelihood\n\n\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\nAnd\n\n\nGenerating our tame toy dataset of 10 data points\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nTo recap, the toy dataset looks as follows:\n\n\nCode\nlibrary(tidyverse)\ntibble(x=x, y=y) |&gt;\n    ggplot(aes(x, y)) + \n    geom_point()"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#optim-our-robo-chauffeur",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#optim-our-robo-chauffeur",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "optim: our Robo-Chauffeur",
    "text": "optim: our Robo-Chauffeur\nNote how the llNormal function takes a single argument, pars, which packages up all the specific candidate parameter values we want to try out. In our previous post, we also had a ‘feeder function’, feed_to_ll, which takes the various \\(\\beta\\) candidate values from the grid and packages them into pars. In our previous post, we had to specify the candidate values to try to feed to llNormal packages inside pars.\nBut we don’t have to do this. We can instead use an algorithm to take candidate parameters, try them out, then make new candidate parameters and try them out, for us. Much as a taxi driver needs to know where to meet a passenger, but doesn’t want the passenger to tell them exactly which route to take, we just need to specify a starting set of values for the parameters to optimise. R’s standard way of doing this is with the optim function. Here’s it in action:\n\n\nCode\noptim_results &lt;-  optim(\n    # par contains our initial guesses for the three parameters to estimate\n    par = c(0, 0, 0), \n\n    # by default, most optim algorithms prefer to search for a minima (lowest point) rather than maxima \n    # (highest point). So, I'm making a function to call which simply inverts the log likelihood by multiplying \n    # what it returns by -1\n    fn = function(par, y, X) {-llNormal(par, y, X)}, \n\n    # in addition to the par vector, our function also needs the observed output (y)\n    # and the observed predictors (X). These have to be specified as additional arguments.\n    y = y, X = X\n    )\n\noptim_results\n\n\n$par\n[1]  2.460571  1.375421 -1.336209\n\n$value\n[1] -1.51397\n\n$counts\nfunction gradient \n     216       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nThe optim function returns a fairly complex output structure, with the following components:\n\npar: the values for the parameters (in our case \\(\\{\\beta_0, \\beta_1, \\eta \\}\\)) which the optimisation algorithm ended up with.\nvalue: the value returned by the function fn when the optim routine was stopped.\ncounts: the number of times the function fn was repeatedly called by optim before optim decided it had had enough\nconvergence: whether the algorithm used by optim completed successfully (i.e. reached what it considers a good set of parameter estimates in par), or not.\n\nIn this case, convergence is 0, which (perhaps counterintuitively) indicates a successful completion. counts indicates that optim called the log likelihood function 216 times before stopping, and par indicates values of \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\eta = -1.34\\}\\) were arrived at. As \\(\\sigma^2 = e^\\eta\\), this means \\(\\theta = \\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\sigma^2 = 0.26 \\}\\). As a reminder, the ‘true’ values are \\(\\{\\beta_0 = 2.50, \\beta_1 = 1.40, \\sigma^2 = 0.25\\}\\).\nSo, the optim algorithm has arrived at pretty much the correct answers for all three parameters, in 216 calls to the log likelihood function, whereas for the grid search approach in the last post we made over 10,000 calls to the log likelihood function for just two of the three parameters.\nLet’s see if we can get more information on exactly what kind of path optim took to get to this set of parameter estimates. We should be able to do this by specifying a value in the trace component in the control argument slot…"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#comparisons",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#comparisons",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Comparisons",
    "text": "Comparisons\nFor comparison let’s see what lm and glm produce.\nFirst lm:\n\n\nCode\ntoy_df &lt;- tibble(\n    x = x, \n    y = y\n)\n\n\nmod_lm &lt;- lm(y ~ x, data = toy_df)\nsummary(mod_lm)\n\n\n\nCall:\nlm(formula = y ~ x, data = toy_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6082 -0.3852 -0.1668  0.2385  1.1092 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5813 on 7 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9767 \nF-statistic:   336 on 1 and 7 DF,  p-value: 3.564e-07\n\n\n\\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\), i.e. the same to 2 decimal places.\nAnd now with glm:\n\n\nCode\nmod_glm &lt;- glm(y ~ x, data = toy_df, family = gaussian(link = \"identity\"))\n\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = toy_df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nOnce again, \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\)"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#discussion",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#discussion",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Discussion",
    "text": "Discussion\nIn the above, we’ve successfully used optim, our Robo-Chauffeur, to arrive very quickly at some good estimates for our parameters of interest, \\(\\beta_0\\) and \\(\\beta_1\\), which are in effect identical to those produced by the lm and glm functions.\nThis isn’t a coincidence. What we’ve done the hard way is what the glm function (in particular) largely does ‘under the hood’."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#coming-up",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#coming-up",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Coming up",
    "text": "Coming up\nIn the next part of this series, we’ll see how other outputs available from optim can be used to estimate uncertainty in the parameters of interest, how this information can be used to produce the kinds of estimates of standard errors around coefficients which are summarised in glm and lm summary() functions, and which many (ab)users of statistical models obsess about when star-gazing, and how information about uncertainty in parameter estimates allows for more honest model-based predictions."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#footnotes",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#footnotes",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(101^2 = 10201\\)↩︎\n\\(101^3 = 1030301\\)↩︎"
  },
  {
    "objectID": "pages/intro-to-glms/index.html#aim",
    "href": "pages/intro-to-glms/index.html#aim",
    "title": "Introduction to Generalised Linear Models",
    "section": "",
    "text": "The aims of this web page is to provide an overview of generalised linear models, and ways of thinking about modelling that go beyond ‘star-gazing’."
  },
  {
    "objectID": "pages/intro-to-glms/index.html#page-discussion",
    "href": "pages/intro-to-glms/index.html#page-discussion",
    "title": "Introduction to Generalised Linear Models",
    "section": "Page discussion",
    "text": "Page discussion\nThis section of the course has aimed to reintroduce statistics from the perspective of generalised linear models (GLMs), in order to make the following clearer:\n\nThat linear regression is just one member of a broader ‘family’ of regression models\nThat all regression models can be thought of as just ‘types’ of GLM, with more in common than divides them\nThat we can and should aim for substantive significance when using the outputs of GLMs, i.e. use them for prediction and simulation rather than focus on whether individual coefficients are ‘statistically significant’ or not.\n\nThe next section of the course delves further into the fundamentals of model fitting and statistical inference, including likelihood theory."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html",
    "href": "pages/likelihood-and-simulation-theory/index.html",
    "title": "Likelihood and Simulation Theory",
    "section": "",
    "text": "In the first part of the course, I stated that statistical model fitting, within the generalised model framework presented in King, Tomz, and Wittenberg (2000), involves adjusting candidate values for elements of \\(\\beta = \\{\\beta_0, \\beta_1, ..., \\beta_K \\}\\) such that the difference between what the model predicts given some predictor values, \\(Y_i | X_i\\), and what has been observed alongside the predictors, \\(y_i\\), is minimised on average1 in some way.\nThe aim of this post is to show how this process is typically implemented in GLMs, using likelihood theory."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#aim",
    "href": "pages/likelihood-and-simulation-theory/index.html#aim",
    "title": "Likelihood and Simulation Theory",
    "section": "Aim",
    "text": "Aim\nIn this post, we’ll now, finally, show how this knowledge can be applied to do something with statistical models that ought to be done far more often: report on what King, Tomz, and Wittenberg (2000) calls quantities of interest, including predicted values, expected values, and first differences. Quantities of interest are not the direction and statistical significance (P-values) that many users of statistical models convince themselves matter, leading to the kind of mindless stargazing summaries of model outputs described in post four. Instead, they’re the kind of questions that someone, not trained to think that stargazing is satisfactory, might reasonably want answers to. These might include:\n\nWhat is the expected income of someone who completes course X in the five years after graduation? (Expected values)\nWhat is the expected range of incomes of someone who completes course X in the five years after graduation? (Predicted values)\nWhat is the expected difference in incomes between someone who completes course X, compared to course Y, in the five years after graduation? (First Differences)\n\nIn post four, we showed how to answer some of the questions of this form, for both standard linear regression and logistic regression. We showed that for linear regression such answers tend to come directly from the summary of coefficients, but that for logistic regression such answers tend to be both more ambiguous and dependent on other factors (such as gender of graduate, degree, ethnicity, age and so on), and require more processing in order to produce estimates for.\nHowever, we previously produced only point estimates for these questions, and so in a sense misled the questioner with the apparent certainty of our estimates. We now know, from post eight, that we can use information about parameter uncertainty to produce parameter estimates \\(\\tilde{\\theta}\\) that do convey parameter uncertainty, and so we can do better than the point estimates alone to answer such questions in way that takes into account such uncertainty, with a range of values rather than a single value."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#bayes-rule-and-likelihood",
    "href": "pages/likelihood-and-simulation-theory/index.html#bayes-rule-and-likelihood",
    "title": "Likelihood and Simulation Theory",
    "section": "Bayes’ Rule and Likelihood",
    "text": "Bayes’ Rule and Likelihood\nStatisticians and more advanced users of statistical models often divide themselves into ‘frequentists’ and ‘Bayesians’. To some extent the distinction is really between ‘improper Bayesians’ and ‘proper Bayesians’, however, as Bayes’ Rule is at the root of both approaches. Bayes’ Rule is:\n\\[\nP(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n\\]\nNote in the above the left hand side of the equation is \\(P(A|B)\\) and the right hand side of the equation includes \\(P(B|A)\\). To write it out as awkward prose, therefore, Bayes’ Rule is a way of expressing that given this in terms of this given that.\nAs with much of algebra, \\(A\\) and \\(B\\) are just placeholders. We could instead use different symbols instead, such as:\n\\[\nP(\\tilde{\\theta} | y) = \\frac{P(y | \\tilde{\\theta})P(\\tilde{\\theta})}{P(y)}\n\\]\nLikelihood theory offers a way of thinking about how good a model is in terms of its relationship to the data. According to King (1998) (p. 59), it can be expressed as:\n\\[\nL(\\tilde{\\theta}| y) = k(y) P(y | \\tilde{\\theta})\n\\]\nOr\n\\[\nL(\\tilde{\\theta} | y) \\propto P(y | \\tilde{\\theta})\n\\]\nWhere \\(\\tilde{\\theta}\\) is a proposed parameter or parameter combination for the model, and \\(y\\) is the observed outcome.2\nThe important thing to note is that both Bayes’ Rule and Likelihood Theory are ways of expressing this given that as a function of that given this. Specifically, the model given the data, as a function of the data given the model. 3"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#likelihood-for-linear-regression",
    "href": "pages/likelihood-and-simulation-theory/index.html#likelihood-for-linear-regression",
    "title": "Likelihood and Simulation Theory",
    "section": "Likelihood for linear regression",
    "text": "Likelihood for linear regression\nWhen, many years ago, I completed the course from this modelling framework is most associated, a hazing ritual employed near the start of the course was to require participants to derive the likelihood of different model specifications. However, I don’t feel like hazing myself right now, so instead we can use the derivation shown on slide 8 of these slides:\n\\[\nL(\\beta, \\sigma^2 | y) = \\prod{L(y_i | \\mu_i, \\sigma^2)}\n\\]\nWhere \\(\\mu = X \\beta\\), \\(i\\) indicates an observation in the data (a row of \\(X\\) when \\(X\\) is in matrix form), and \\(\\prod\\) indicates the likelihoods from each observation should be multiplied with each other to derive the overall likelihood for all observed data.\nIn practice the log Likelihood, rather than the likelihood itself, is used, because this allows calculation of a sum of terms (\\(\\sum\\)) rather than product of terms (\\(\\prod\\)), and the latter tends to be computationally easier to calculate.\nAs we are interested only in how likelihood varies as a function of those model parameters we wish to estimate, \\(\\theta = \\{\\beta, \\sigma^2\\}\\), some of the terms in the log likelihood expression can be omitted, leaving us with:\n\\[\n\\log{L(\\beta, \\sigma^2 | y)} \\doteq \\sum{-\\frac{1}{2}[\\log{\\sigma^2} + \\frac{(y_i - X_i\\beta)^2}{\\sigma^2}]}\n\\]\nFor all the complexity of the above expression, at heart it takes three inputs:\n\n\\(\\theta = \\{\\beta, \\sigma^2\\}\\) : The candidate parameters for the model.\n\\(y\\) : the observed response value from the dataset \\(D\\)\n\\(X\\) : the observed predictor values from the dataset \\(D\\)\n\nAnd returns one value, the log likelihood \\(\\log{L(.)}\\).\nTo reiterate, we can’t change the data, but we can keep changing the candidate parameters \\(\\theta\\). Each time we do so, \\(\\log{L(.)}\\) will change too.\nThe aim of model calibration, in the Likelihood framework, is to maximise the Likelihood. The parameter set that maximises the likelihood is also the parameter set that maximises the log likelihood.\nTo continue the example from the slides, we can write out a function for calculating the log likelihood of standard linear regression as follows:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\nIn the above, pars is (almost but not quite) \\(\\theta\\), the parameters to estimate. For standard linear regression \\(\\theta = \\{\\beta, \\sigma^2\\}\\), where \\(\\beta = \\{\\beta_0, \\beta_1, ..., \\beta_k\\}\\), i.e. a vector of beta parameters, one for each column (variable) in \\(X\\), the predictor matrix of observations; this is why \\(beta\\) is selected from the first K values in pars where K is the number of columns in \\(X\\).\nThe last value in pars is used to derive the proposed \\(\\sigma^2\\). If we call this last value eta (\\(\\eta\\)), then we can say \\(\\sigma^2 = e^{\\eta}\\). So, whereas \\(\\theta\\) is a vector that ‘packs’ \\(\\beta\\) and \\(\\sigma^2\\) into a single ordered series of values, pars packs eta in place of \\(\\sigma^2\\). This substitution of eta for \\(\\sigma^2\\) is done to make it easier for standard parameter fitting algorithms to work, as they tend to operate over the full real number range, rather than just over positive values.\nIn order to illustrate how the log likelihood function llNormal works in practice, let’s construct a simple toy dataset \\(D\\), and decompose \\(D = \\{y, X\\}\\), the two types of data input that go into the llNormal function.\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nIn the code above we have created \\(y\\), a vector of nine observed responses; and \\(X\\), a matrix of predictors with two columns (the number of variables for which \\(beta\\) terms need to be estimated) and nine rows (the number of observations).\nGraphically, the relationship between x and y looks as follows:\n\n\nCode\nlibrary(tidyverse)\ntibble(x=x, y=y) |&gt;\n    ggplot(aes(x, y)) + \n    geom_point()\n\n\n\n\n\nIn this toy example, but almost never in reality, we know the correct parameters for the model. These are \\({\\beta_0 = 2.5, \\beta_1 = 1.4}\\) and \\(\\sigma^2 = 0.25\\). 4 Soon, we will see how effectively we can use optimisation algorithms to recover these true model parameters. But first, let’s see how the log likelihood varies as a function jointly of different candidate values of \\(\\beta_0\\) (the intercept) and \\(\\beta_1\\) (the slope parameter), if we already set \\(\\sigma^2\\) to 0.25.\n\n\nCode\ncandidate_param_values &lt;- expand_grid(\n    beta_0 = seq(-5, 5, by = 0.1),\n    beta_1 = seq(-5, 5, by = 0.1)\n)\n\nfeed_to_ll &lt;- function(b0, b1){\n    pars &lt;- c(b0, b1, log(0.25))\n    llNormal(pars, y, X)\n}\n\ncandidate_param_values &lt;- candidate_param_values |&gt;\n    mutate(\n        ll = map2_dbl(beta_0, beta_1, feed_to_ll)\n    )\n\n\n\n\nCode\ncandidate_param_values |&gt;\n    ggplot(aes(beta_0, beta_1, z = ll)) + \n    geom_contour_filled() + \n    geom_vline(xintercept = 0) +\n    geom_hline(yintercept = 0) +\n    labs(\n        title = \"Log likelihood as a function of possible values of beta_0 and beta_1\",\n        x = \"beta0 (the intercept)\",\n        y = \"beta1 (the slope)\"\n    )\n\n\n\n\n\nLooking at this joint surface of values, we can see a ‘hotspot’ where \\(\\beta_0\\) is around 2.5, and \\(\\beta_1\\) is around 1.4, just as we should expect. We can check this further by filtering candidate_param_values on the highest observed values of ll.\n\n\nCode\ncandidate_param_values |&gt; \n    filter(ll == max(ll))\n\n\n# A tibble: 1 × 3\n  beta_0 beta_1    ll\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    2.4    1.4  1.41"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#summary",
    "href": "pages/likelihood-and-simulation-theory/index.html#summary",
    "title": "Likelihood and Simulation Theory",
    "section": "Summary",
    "text": "Summary\nThis post has shown how optim(), which in its vanilla state only returns point estimates, can be configured to also calculater and report the Hessian, a record of instantaneous curvature around the point estimates. Even without a fine-grained and exhausive search throughout the likelihood surface, this measure of curvature can be used to produce similar measures of uncertainty to the more exhausive approach, in a fraction of the number of computations.\nMore importantly, it can be used to generate draws of plausible combinations of parameter values, something denoted as \\(\\tilde{\\theta}\\) earlier. This is something especially useful for producing honest quantities of interest, which both tell users of models something they want to know, while also representing how uncertain we are in this knowledge."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#coming-up",
    "href": "pages/likelihood-and-simulation-theory/index.html#coming-up",
    "title": "Likelihood and Simulation Theory",
    "section": "Coming up",
    "text": "Coming up\nThe next post covers the same kind of exercise we’ve performed for standard linear regression - specifying the likelihood function, and fitting it using optim() - but for logistic regression instead. This same kind of exercise could be repeated for all kinds of other model types. But hopefully this one additional example is sufficient."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#footnotes",
    "href": "pages/likelihood-and-simulation-theory/index.html#footnotes",
    "title": "Likelihood and Simulation Theory",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf \\(Y_i\\) is what the model predicts given observations \\(X_i\\), and \\(y_i\\) is the outcome observed to have occurred alongside \\(X_i\\), then we can call \\(\\delta_i = h(y_i, Y_i)\\) the difference, or error, between predicted and observed value. The function \\(h(.,.)\\) is typically the squared difference between predicted and observed values, \\((Y_i - y_i)^2\\), but could also in principle be the absolute difference \\(|Y_i - y_i|\\). Term-fitting algorithms usually compare not any individual \\(\\delta_i\\), but a sum of these error terms \\(\\delta\\). The aim of the algorithm is to find the set of \\(\\beta\\) terms that is least wrong for the whole dataset \\(D\\), rather than any specific row in the dataset \\(D_i\\).↩︎\nAs King (1998) (p. 59) describes it, “\\(k(y)\\) is an unknown fuction of the data. Whereas traditional probability is a measure of absolute uncertainty … the constant \\(k(y)\\) means that likelihood is only a relative measure of uncertainty”↩︎\nFrequentist approaches can thus be considered a kind of ‘improper Bayesian’ approach by considering \\(k(y)\\) in the Likelihood formula as a stand-in for \\(\\frac{P(\\tilde{\\theta})}{P(y)}\\) in Bayes’ Rule. Roughly speaking, it’s because of the improperness of treating the two terms as equivalent, and the relativeness of \\(k(y)\\), that mean frequentist probability statements can’t be interpreted as Bayesian probability statements. But thinking of the two terms as equivalent can be helpful for spotting the similarity between the two formulae.↩︎\ni.e. the square of the sd passed to rnorm() of 0.5↩︎\n\\(101^2 = 10201\\)↩︎\n\\(101^3 = 1030301\\)↩︎\nThough I had assumed Hessian matrices are called Hessian matrices because they sort-of resemble the criss-crossing grids of Hessian bags, they’re actually named after Otto Hesse, who proposed them.↩︎\nI’ve narrowed the space between values slightly, and increased the range of permutations of values to search through, for an even more precise recovery of the likelihood landscape.↩︎\nIn practice, the algorithm seeks to minimise the value returned by the function, not maximise it, hence the negative being applied through the argument fnscale = -1 in the control argument. But the principle is identical.↩︎\nThis means that, whereas the standard Normal returns a single output, the Multivariate Normal returns a vector of outputs, one for each parameter in \\(\\theta\\), which should also be the length of the diagonal (or alternatively either the number of rows or columns) of \\(\\Sigma\\).↩︎\nThe values will not be identical because the values for \\(\\eta\\), and so \\(\\sigma^2\\), have not been fixed at the true value in this example.↩︎\nWhere \\(\\sigma^2\\) is from \\(\\eta\\) and we defined \\(e^{\\eta} = \\sigma^2\\), a transformation which allowed optim() to search over an unbounded rather than bounded real number line↩︎\nIt can be easier to see this by using the more conventional way of expressing Normal linear regression: \\(Y_i = x_i \\beta + \\epsilon\\), where \\(\\epsilon \\sim Normal(0, \\sigma^2)\\). The expectation is therefore \\(E(Y_i) = E( x_i \\beta + \\epsilon ) = E(x_i \\beta) + E(\\epsilon)\\). For the first part of this equation, \\(E(x_i \\beta) = x_i \\beta\\), because the systematic component is always the same value, no matter how many times a draw is taken from the model. And for the second part, \\(E(\\epsilon) = 0\\), because Normal distributions are symmetrical around their central value over the long term: on average, every large positive value drawn from this distribution will become cancelled out by an equally large negative value, meaning the expected value returned by the distribution is zero. Hence, \\(E(Y) = x_i \\beta\\).↩︎\nBecause these estimates depend on random variation, these intervals may be slightly different to two decimal places than the values I’m quoting here.↩︎\nThanks to this post. My calculus is a bit rusty these days.↩︎\nAn important point to note is that, though bill_size is derived from other variables, it’s its own variable, and so has another distinct ‘slot’ in the vector of \\(\\beta\\) parameters. It’s just another dimension in the search space for optim to search through.↩︎\nThis is fancy-speak for when two terms aren’t independent, or both adding unique information. For example, length in mm, length in cm, and length in inches would all be perfectly collinear, so shouldn’t all be included in the model.↩︎"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#optimisation-algorithms-getting-there-faster",
    "href": "pages/likelihood-and-simulation-theory/index.html#optimisation-algorithms-getting-there-faster",
    "title": "Likelihood and Simulation Theory",
    "section": "Optimisation algorithms: getting there faster",
    "text": "Optimisation algorithms: getting there faster\nPreviously, we ‘cheated’ a bit when using the log likelihood function, fixing the value for one of the parameters \\(\\sigma^2\\) to the value we used when we generated the data, so we could instead look at how the log likelihood surface varied as different combinations of \\(\\beta_0\\) and \\(\\beta_1\\) were plugged into the formula. \\(\\beta_0\\) and \\(\\beta_1\\) values ranging from -5 to 5, and at steps of 0.1, were considered: 101 values of \\(\\beta_0\\), 101 values of \\(\\beta_1\\), and so over 10,0005 unique \\(\\{\\beta_0, \\beta_1\\}\\) combinations were stepped through. This approach is known as grid search, and seldom used in practice (except for illustration purposes) because the number of calculations involved can very easily get out of hand. For example, if we were to use it to explore as many distinct values of \\(\\sigma^2\\) as we considered for \\(\\beta_0\\) and \\(\\beta_1\\), the total number of \\(\\{\\beta_0, \\beta_1, \\sigma^2 \\}\\) combinations we would crawl through would be over 100,000 6 rather than over 10,000.\nOne feature we noticed with the likelihood surface over \\(\\beta_0\\) and \\(\\beta_1\\) in the previous post is that it appears to look like a hill, with a clearly defined highest point (the region of maximum likelihood) and descent in all directions from this highest point. Where likelihood surfaces have this feature of being single-peaked in this way (known as ‘unimodal’), then a class of algorithms known as ‘hill climbing algorithms’ can be applied to find the top of such peaks in a way that tends to be both quicker (fewer steps) and more precise than the grid search approach used for illustration in the previous post."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#code-recap",
    "href": "pages/likelihood-and-simulation-theory/index.html#code-recap",
    "title": "Likelihood and Simulation Theory",
    "section": "Code recap",
    "text": "Code recap\nLet’s copy over the code we used in the previous post for:\n\n\nCalculating log likelihood\n\n\n\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\nAnd\n\n\nGenerating our tame toy dataset of 10 data points\n\n\n\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\nTo recap, the toy dataset looks as follows:\n\nlibrary(tidyverse)\ntibble(x=x, y=y) |&gt;\n    ggplot(aes(x, y)) + \n    geom_point()"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#optim-our-robo-chauffeur",
    "href": "pages/likelihood-and-simulation-theory/index.html#optim-our-robo-chauffeur",
    "title": "Likelihood and Simulation Theory",
    "section": "optim: our Robo-Chauffeur",
    "text": "optim: our Robo-Chauffeur\nNote how the llNormal function takes a single argument, pars, which packages up all the specific candidate parameter values we want to try out. In our previous post, we also had a ‘feeder function’, feed_to_ll, which takes the various \\(\\beta\\) candidate values from the grid and packages them into pars. In our previous post, we had to specify the candidate values to try to feed to llNormal packages inside pars.\nBut we don’t have to do this. We can instead use an algorithm to take candidate parameters, try them out, then make new candidate parameters and try them out, for us. Much as a taxi driver needs to know where to meet a passenger, but doesn’t want the passenger to tell them exactly which route to take, we just need to specify a starting set of values for the parameters to optimise. R’s standard way of doing this is with the optim function. Here’s it in action:\n\noptim_results &lt;-  optim(\n    # par contains our initial guesses for the three parameters to estimate\n    par = c(0, 0, 0), \n\n    # by default, most optim algorithms prefer to search for a minima (lowest point) rather than maxima \n    # (highest point). So, I'm making a function to call which simply inverts the log likelihood by multiplying \n    # what it returns by -1\n    fn = function(par, y, X) {-llNormal(par, y, X)}, \n\n    # in addition to the par vector, our function also needs the observed output (y)\n    # and the observed predictors (X). These have to be specified as additional arguments.\n    y = y, X = X\n    )\n\noptim_results\n\n$par\n[1]  2.460571  1.375421 -1.336209\n\n$value\n[1] -1.51397\n\n$counts\nfunction gradient \n     216       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nThe optim function returns a fairly complex output structure, with the following components:\n\npar: the values for the parameters (in our case \\(\\{\\beta_0, \\beta_1, \\eta \\}\\)) which the optimisation algorithm ended up with.\nvalue: the value returned by the function fn when the optim routine was stopped.\ncounts: the number of times the function fn was repeatedly called by optim before optim decided it had had enough\nconvergence: whether the algorithm used by optim completed successfully (i.e. reached what it considers a good set of parameter estimates in par), or not.\n\nIn this case, convergence is 0, which (perhaps counterintuitively) indicates a successful completion. counts indicates that optim called the log likelihood function 216 times before stopping, and par indicates values of \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\eta = -1.34\\}\\) were arrived at. As \\(\\sigma^2 = e^\\eta\\), this means \\(\\theta = \\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\sigma^2 = 0.26 \\}\\). As a reminder, the ‘true’ values are \\(\\{\\beta_0 = 2.50, \\beta_1 = 1.40, \\sigma^2 = 0.25\\}\\).\nSo, the optim algorithm has arrived at pretty much the correct answers for all three parameters, in 216 calls to the log likelihood function, whereas for the grid search approach in the last post we made over 10,000 calls to the log likelihood function for just two of the three parameters.\nLet’s see if we can get more information on exactly what kind of path optim took to get to this set of parameter estimates. We should be able to do this by specifying a value in the trace component in the control argument slot…\n\nComparisons\nFor comparison let’s see what lm and glm produce.\nFirst lm:\n\ntoy_df &lt;- tibble(\n    x = x, \n    y = y\n)\n\n\nmod_lm &lt;- lm(y ~ x, data = toy_df)\nsummary(mod_lm)\n\n\nCall:\nlm(formula = y ~ x, data = toy_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6082 -0.3852 -0.1668  0.2385  1.1092 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5813 on 7 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9767 \nF-statistic:   336 on 1 and 7 DF,  p-value: 3.564e-07\n\n\n\\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\), i.e. the same to 2 decimal places.\nAnd now with glm:\n\nmod_glm &lt;- glm(y ~ x, data = toy_df, family = gaussian(link = \"identity\"))\n\nsummary(mod_glm)\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = toy_df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nOnce again, \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\)\n\n\nOptim discussion\nIn the above, we’ve successfully used optim, our Robo-Chauffeur, to arrive very quickly at some good estimates for our parameters of interest, \\(\\beta_0\\) and \\(\\beta_1\\), which are in effect identical to those produced by the lm and glm functions.\nThis isn’t a coincidence. What we’ve done the hard way is what the glm function (in particular) largely does ‘under the hood’."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#comparisons",
    "href": "pages/likelihood-and-simulation-theory/index.html#comparisons",
    "title": "Likelihood and Simulation Theory",
    "section": "Comparisons",
    "text": "Comparisons\nFor comparison let’s see what lm and glm produce.\nFirst lm:\n\ntoy_df &lt;- tibble(\n    x = x, \n    y = y\n)\n\n\nmod_lm &lt;- lm(y ~ x, data = toy_df)\nsummary(mod_lm)\n\n\nCall:\nlm(formula = y ~ x, data = toy_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6082 -0.3852 -0.1668  0.2385  1.1092 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5813 on 7 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9767 \nF-statistic:   336 on 1 and 7 DF,  p-value: 3.564e-07\n\n\n\\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\), i.e. the same to 2 decimal places.\nAnd now with glm:\n\nmod_glm &lt;- glm(y ~ x, data = toy_df, family = gaussian(link = \"identity\"))\n\nsummary(mod_glm)\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = toy_df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nOnce again, \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\)\n\nOptim discussion\nIn the above, we’ve successfully used optim, our Robo-Chauffeur, to arrive very quickly at some good estimates for our parameters of interest, \\(\\beta_0\\) and \\(\\beta_1\\), which are in effect identical to those produced by the lm and glm functions.\nThis isn’t a coincidence. What we’ve done the hard way is what the glm function (in particular) largely does ‘under the hood’."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#discussion",
    "href": "pages/likelihood-and-simulation-theory/index.html#discussion",
    "title": "Likelihood and Simulation Theory",
    "section": "Discussion",
    "text": "Discussion\nIn the above, we’ve successfully used optim, our Robo-Chauffeur, to arrive very quickly at some good estimates for our parameters of interest, \\(\\beta_0\\) and \\(\\beta_1\\), which are in effect identical to those produced by the lm and glm functions.\nThis isn’t a coincidence. What we’ve done the hard way is what the glm function (in particular) largely does ‘under the hood’."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#barefoot-and-blind-a-weird-analogy-for-a-complicated-idea",
    "href": "pages/likelihood-and-simulation-theory/index.html#barefoot-and-blind-a-weird-analogy-for-a-complicated-idea",
    "title": "Likelihood and Simulation Theory",
    "section": "Barefoot and Blind: A weird analogy for a complicated idea",
    "text": "Barefoot and Blind: A weird analogy for a complicated idea\nImagine optim, your hill-finding robo-chauffeur, has taken you to the top of a likelihood surface. Then it leaves you there…\n… and you’re blind, and have no shoes. (You also have an uncanny sense of your orientation, whether north-south, east-west, or some other angle.)\nSo, you know you’re at the top of the hill, but you can’t see what the landscape around you looks like. However, you still want to get a sense of this landscape, and how it varies around the spot you’re standing on.\nWhat do you do?\nIf you’re playing along with this weird thought experiment, one approach would be to use your feet as depth sensors. You make sure you never stray from where you started, and to always keep one foot planted on this initial spot (which you understand to be the highest point on the landscape). Then you use your other foot to work out how much further down the surface is from the highest point as you venture away from the highest point in different directions.\nSay you keep your left foot planted on the highest point, and make sure your right foot is always positioned (say) 10 cm horizontally from your left foot. Initially your two feet are arranged east-west; let’s call this 0 degrees. When you put your right foot down, you notice it needs to travel 2 cm further down to reach terra ferma relative to your left foot.\n2cm at 0 degrees. You’ll remember that.\nNow you rotate yourself 45 degrees, and repeat the same right foot drop. This time it needs to travel 3cm down relative to your left foot.\n3cm at 45 degrees. You remember that too.\nNow you rotate another 45 degrees, north-south orientation, place your right foot down; now it falls 5cm down relative to your left foot.\n2cm at 0 degrees; 3cm at 45 degrees; 5cm at 90 degrees.\nNow with this information, you try to construct the landscape you’re on top of with your mind’s eye, making the assumption that the way it has to have curved from the peak you’re on to lead to the drops you’ve observed is consistent all around you; i.e. that there’s only one hill, you’re on top of it, and it’s smoothly curved in all directions.\nIf you could further entertain the idea that your feet are infinitely small, and the gap between feet is also infinitely small (rather than the 10cm above), then you have the intuition behind this scary-looking but very important formula from (King98?) (p. 89):\n\\[\n\\widehat{V(\\hat{\\theta})} = - \\frac{1}{n}[\\frac{\\delta^2lnL(\\tilde{\\theta}|y)}{\\delta \\tilde{\\theta} \\delta \\tilde{\\theta}^{'}}]^{-1}_{\\tilde{\\theta} = \\hat{\\theta}}\n\\]\nWhat this is saying, in something closer to humanese, is something like:\n\nOur best estimate of the amount of uncertainty we have in our estimates is a function of how much the likelihood surface curves at the highest point on the surface. (It also gets less uncertain, the more observations we have)."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#information-and-uncertainty",
    "href": "pages/likelihood-and-simulation-theory/index.html#information-and-uncertainty",
    "title": "Likelihood and Simulation Theory",
    "section": "Information and uncertainty",
    "text": "Information and uncertainty\nAmongst the various bells, whistles and decals in the previous formula is the superscript \\((.)^{-1}\\). This means invert, which for a single value means \\(\\frac{1}{.}\\) but for a matrix means something conceptually the same but technically not.\nAnd what’s being inverted in the last formula? A horrible-looking expression, \\([\\frac{\\delta^2lnL(\\tilde{\\theta}|y)}{\\delta \\tilde{\\theta} \\delta \\tilde{\\theta}^{'}}]_{\\tilde{\\theta} = \\hat{\\theta}}\\), that’s basically an answer to the question of how curvy is the log likelihood surface at its peak position?\nWithin (King98?) (p.89, eq. 4.18), this expression (or rather the negative of the term) is defined as \\(I(\\hat{\\theta} | y)\\), where \\(I(.)\\) stands for information.\nSo, the algebra are saying\n\nUncertainty is inversely related to information\n\nOr perhaps even more intuitively\n\nThe more information we have, the less uncertain we are\n\nOf course this makes sense. If you ask someone “How long will this task take?”, and they say “Between one hour and one month”, they likely have less information about how long the task will actually than if they had said “Between two and a half and three hours”. More generally:\n\nShallow gradients mean wide uncertainty intervals mean low information\nSharp gradients mean narrow uncertaintly intervals mean high information\n\nThis is, fundamentally, what the blind and barefoot person in the previous analogy is trying to achieve: by feeling out the local curvature around the highest point, they are trying to work out how much information they have about different pieces of the model. The curvature along any one dimension of the surface (equivalent to the 0 and 90 degree explorations) indicates how much information there is about any single coefficient, and the curvature along the equivalent of a 45 degree plane gives a measure of how associated any two coefficients tend to be.\nWith these many analogies and equations spinning in our heads, let’s now see how these concepts can be applied in practice."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#optimal-uncertainty",
    "href": "pages/likelihood-and-simulation-theory/index.html#optimal-uncertainty",
    "title": "Likelihood and Simulation Theory",
    "section": "optimal uncertainty",
    "text": "optimal uncertainty\nWhen using optim() above, we managed to get it to return a set of parameter values for our model that it thought was ‘best’, i.e. minimised the loss function specified by the log likelihood. These are known as point estimates, and are effectively the coefficients presented by lm or glm or equivalent statistical functions and packages. However optim() just returned these point estimates, without any indication of how uncertain we should be about these point estimates. A standard statistical model summary will tend to also report measures of uncertainty around the point estimates, in the form of standard errors. When these are implicitly combined with a Null hypothesis, namely that the ‘true’ value of a parameter may be zero, the point estimate together with its standard error allows the calculation of z values and p values.\nHow can we use optim() to return measures of uncertainty, which will allow the standard errors to be estimated as well as the point values?\nWe’ll start with a weird analogy to get an intuition for how this can be done with optim().\n\nBarefoot and Blind: A weird analogy for a complicated idea\nImagine optim, your hill-finding robo-chauffeur, has taken you to the top of a likelihood surface. Then it leaves you there…\n… and you’re blind, and have no shoes. (You also have an uncanny sense of your orientation, whether north-south, east-west, or some other angle.)\nSo, you know you’re at the top of the hill, but you can’t see what the landscape around you looks like. However, you still want to get a sense of this landscape, and how it varies around the spot you’re standing on.\nWhat do you do?\nIf you’re playing along with this weird thought experiment, one approach would be to use your feet as depth sensors. You make sure you never stray from where you started, and to always keep one foot planted on this initial spot (which you understand to be the highest point on the landscape). Then you use your other foot to work out how much further down the surface is from the highest point as you venture away from the highest point in different directions.\nSay you keep your left foot planted on the highest point, and make sure your right foot is always positioned (say) 10 cm horizontally from your left foot. Initially your two feet are arranged east-west; let’s call this 0 degrees. When you put your right foot down, you notice it needs to travel 2 cm further down to reach terra ferma relative to your left foot.\n2cm at 0 degrees. You’ll remember that.\nNow you rotate yourself 45 degrees, and repeat the same right foot drop. This time it needs to travel 3cm down relative to your left foot.\n3cm at 45 degrees. You remember that too.\nNow you rotate another 45 degrees, north-south orientation, place your right foot down; now it falls 5cm down relative to your left foot.\n2cm at 0 degrees; 3cm at 45 degrees; 5cm at 90 degrees.\nNow with this information, you try to construct the landscape you’re on top of with your mind’s eye, making the assumption that the way it has to have curved from the peak you’re on to lead to the drops you’ve observed is consistent all around you; i.e. that there’s only one hill, you’re on top of it, and it’s smoothly curved in all directions.\n\n\nInformation and uncertainty\nIf you could further entertain the idea that your feet are infinitely small, and the gap between feet is also infinitely small (rather than the 10cm above), then you have the intuition behind this scary-looking but very important formula from King (1998) (p. 89):\n\\[\n\\widehat{V(\\hat{\\theta})} = - \\frac{1}{n}[\\frac{\\delta^2lnL(\\tilde{\\theta}|y)}{\\delta \\tilde{\\theta} \\delta \\tilde{\\theta}^{'}}]^{-1}_{\\tilde{\\theta} = \\hat{\\theta}}\n\\]\nWhat this is saying, in something closer to humanese, is something like:\n\nOur best estimate of the amount of uncertainty we have in our estimates is a function of how much the likelihood surface curves at the highest point on the surface. (It also gets less uncertain, the more observations we have).\n\nAmongst the various bells, whistles and decals in the previous formula is the superscript \\((.)^{-1}\\). This means invert, which for a single value means \\(\\frac{1}{.}\\) but for a matrix means something conceptually the same but technically not.\nAnd what’s being inverted in the last formula? A horrible-looking expression, \\([\\frac{\\delta^2lnL(\\tilde{\\theta}|y)}{\\delta \\tilde{\\theta} \\delta \\tilde{\\theta}^{'}}]_{\\tilde{\\theta} = \\hat{\\theta}}\\), that’s basically an answer to the question of how curvy is the log likelihood surface at its peak position?\nWithin King (1998) (p.89, eq. 4.18), this expression (or rather the negative of the term) is defined as \\(I(\\hat{\\theta} | y)\\), where \\(I(.)\\) stands for information.\nSo, the algebra are saying\n\nUncertainty is inversely related to information\n\nOr perhaps even more intuitively\n\nThe more information we have, the less uncertain we are\n\nOf course this makes sense. If you ask someone “How long will this task take?”, and they say “Between one hour and one month”, they likely have less information about how long the task will actually than if they had said “Between two and a half and three hours”. More generally:\n\nShallow gradients mean wide uncertainty intervals mean low information\nSharp gradients mean narrow uncertaintly intervals mean high information\n\nThis is, fundamentally, what the blind and barefoot person in the previous analogy is trying to achieve: by feeling out the local curvature around the highest point, they are trying to work out how much information they have about different pieces of the model. The curvature along any one dimension of the surface (equivalent to the 0 and 90 degree explorations) indicates how much information there is about any single coefficient, and the curvature along the equivalent of a 45 degree plane gives a measure of how associated any two coefficients tend to be.\nWith these many analogies and equations spinning in our heads, let’s now see how these concepts can be applied in practice.\n\n\nHow to get optim() to return this information\nHaving reminded myself of the particular options for optim that are typically used to report parameter uncertainty, let’s run the follows:\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nWe have used a slightly different algorithm (‘BFGS’), and a different way of specifying the function to search over (using fnscale = -1 to invert the likelihood), but we have the same par estimates as before: \\(\\beta = \\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\). So the changes we’ve made to the optim arguments haven’t changed what it estimates.\nOne new argument we’ve set in optim is hessian = TRUE. Hessian is a kind of coarse fabric made from vegetable waste, typically woven in a criss-crossing, grid-like pattern. Hessian matrices are matrices of second derivatives, as described in the wikipedia article. 7 If you can bear to recall the really complex expression above, for calculating the curvature around a point on a surface, you’ll recall it’s also about second derivatives.\nNone of this is a coincidence. The hessian component of the optim output above contains what we need.\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\nhess\n\n\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nYou might notice that the Hessian matrix is square, with as many columns as rows. And, that the number of columns (or rows) is equal to the number of parameters we have estimated, i.e. three in this case.\nYou might also notice that the values are symmetrical about the diagonal running from the top left to the bottom right.\nAgain, this is no accident.\nRemember that variation is inversely related to information, and that \\((.)^{-1}\\) is the inversion operator on \\(I(.)\\), the Information Matrix. Well, this Hessian is (pretty much) \\(I(.)\\). So let’s see what happens when we invert it (using the solve operator):\n\n\nCode\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\nAs with hess, inv_hess is symmetric around the top-left to bottom-right diagonal. For example, the value on row 2 and column 1 is the same as on row 1, column 2.\nWe’re mainly interested in the first two columns and rows, as these contain the values most comparable with the glm summary reports\n\n\nCode\ninv_hess_betas &lt;- inv_hess[1:2, 1:2]\n\ninv_hess_betas\n\n\n             [,1]         [,2]\n[1,]  0.033577455 -0.004379668\n[2,] -0.004379668  0.004379668\n\n\nWhat the elements of the above matrix provide are estimates of the variances of a single parameter \\(\\beta_j\\), and/or the covariances between any two parameters \\(\\{\\beta_0, \\beta_1\\}\\). In this example:\n\\[\n\\begin{bmatrix}\nvar(\\beta_0) & cov(\\beta_0, \\beta_1) \\\\\ncov(\\beta_1, \\beta_0) & var(\\beta_1)\n\\end{bmatrix}\n\\]\nIt’s because the on-diagonal terms are variances of uncertaintly for a single term, that it can be useful to take the square root of these terms to get estimates of the standard errors:\n\n\nCode\nsqrt(diag(inv_hess_betas))\n\n\n[1] 0.18324152 0.06617906\n\n\nCompare with the Std Err term in the following:\n\n\nCode\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = toy_df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nThe estimates from the Hessian in optim, of \\(\\{0.18, 0.07\\}\\), are not exactly the same as the \\(\\{0.21, 0.08\\}\\) reported for mod_glm; the methods employed are not identical. But they are hopefully similar enough to demonstrate they provide similar information about similar quantities of uncertainty.\nBack in part five, we used this same dataset to show how the log likelihood varies for various, equally spaced, candidate values for \\(\\beta_0\\) and \\(\\beta_1\\) (having fixed \\(\\eta = \\exp({\\sigma^2})\\) at its true value). This led to the followng map of the landscape8\n\n\nCode\nlibrary(tidyverse)\ncandidate_param_values &lt;- expand_grid(\n    beta_0 = seq(-15, 15, by = 0.05),\n    beta_1 = seq(-15, 15, by = 0.05)\n)\n\nfeed_to_ll &lt;- function(b0, b1){\n    pars &lt;- c(b0, b1, log(0.25))\n    llNormal(pars, y, X)\n}\n\ncandidate_param_values &lt;- candidate_param_values |&gt;\n    mutate(\n        ll = map2_dbl(beta_0, beta_1, feed_to_ll)\n    )\n\ncandidate_param_values |&gt;\n    ggplot(aes(beta_0, beta_1, z = ll)) + \n    geom_contour_filled() + \n    geom_vline(xintercept = 0) +\n    geom_hline(yintercept = 0) +\n    labs(\n        title = \"Log likelihood as a function of possible values of beta_0 and beta_1\",\n        x = \"beta0 (the intercept)\",\n        y = \"beta1 (the slope)\"\n    )\n\n\n\n\n\nWithin the above we can see that the log likelihood landscape for these two parameters looks like a bivariate normal distribution, we can also see a bit of a slant in this normal distribution. This implies a correlation between the two candidate values. The direction of the slant is downwards from left to right, implying the correlation is negative.\nFirstly let’s check that the correlation between \\(\\beta_0\\) and \\(\\beta_1\\) implied by the Hessian is negative. These are the off-diagonal elements, either first row, second column, or second row, first column:\n\n\nCode\ninv_hess_betas[1,2]\n\n\n[1] -0.004379668\n\n\nCode\ninv_hess_betas[2,1]\n\n\n[1] -0.004379668\n\n\nYes they are!\nAs mentioned previously, the likelihood surface produced by the gridsearch method involves a lot of computations, so a lot of steps, and likely a lot of trial and error, if it were to be used to try to find the maximum likelihood value for the parameters. By contrast, the optim() algorithm typically involves far fewer steps, ‘feeling’ its way up the hill until it reaches a point where there’s nowhere higher. 9 When it then reaches this highest point, it then ‘feels’ the curvature around this point in multiple directions, producing the Hessian. The algorithm doesn’t see the likelihood surface, because it hasn’t travelled along most of it. But the Hessian can be used to infer the likelihood surface, subject to subject (usually) reasonable assumptions.\nWhat are these (usually) reasonable assumptions? Well, that the likelihood surface can be approximated by a multivariate normal distribution, which is a generalisation of the standard Normal distribution over more than one dimensions.10\nWe can use the mvrnorm function from the MASS package, alongside the point estimates and Hessian from optim, in order to produce estimates of \\(\\theta = \\{ \\beta_0, \\beta_1, \\eta \\}\\) which represent reasonable uncertainty about the true values of each of these parameters. Algebraically, this can be expressed as something like the following:\n\\[\n\\tilde{\\theta} \\sim Multivariate Normal(\\mu = \\dot{\\theta}, \\sigma^2 = \\Sigma)\n\\]\nWhere \\(\\dot{\\theta}\\) are the point estimates from optim() and \\(\\Sigma\\) is the implied variance-covariance matrix recovered from the Hessian.\nLet’s create this MVN model and see what kinds of outputs it produces.\n\n\nCode\nlibrary(MASS)\n\npoint_estimates &lt;- fuller_optim_output$par\n\nvcov &lt;- -solve(fuller_optim_output$hessian)\nparam_draws &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = point_estimates, \n    Sigma = vcov\n)\n\ncolnames(param_draws) &lt;- c(\n    \"beta0\", \"beta1\", \"eta\"\n)\n\nhead(param_draws)\n\n\n        beta0    beta1         eta\n[1,] 2.564978 1.375636 -0.30407255\n[2,] 2.440111 1.367774 -1.16815288\n[3,] 2.775332 1.338583 -0.05574937\n[4,] 2.283011 1.481799 -0.26095101\n[5,] 2.695635 1.228565 -1.18369341\n[6,] 2.686818 1.483601 -0.44262363\n\n\nWe can see that mvrnorm(), with these inputs from optim() produces three columns: one for each parameter being estimated \\(\\{ \\beta_0, \\beta_1, \\eta \\}\\). The n argumment indicates the number of draws to take; in this case, 10000. This number of draws makes it easier to see how much variation there is in each of the estimates.\n\n\nCode\ndf_param_draws &lt;- \nparam_draws |&gt;\n    as_tibble(\n        rownames = 'draw'\n    ) |&gt;\n    mutate(\n        sig2 = exp(eta)\n    ) |&gt;\n    pivot_longer(\n        -draw, \n        names_to = \"param\",\n        values_to = \"value\"\n    ) \n    \ndf_param_draws |&gt;\n    ggplot(aes(x = value)) + \n    geom_density() + \n    facet_grid(param ~ .) + \n    geom_vline(xintercept=0)\n\n\n\n\n\nThere are a number of things to note here: firstly, that the average of the \\(\\beta_0\\) and \\(\\beta_1\\) values appear close to their known ‘true’ values of 2.5 and 1.4 respectively. Secondly, that whereas the \\(\\eta\\) values are normally distributed, the \\(\\sigma^2\\) values derived from them are not, and are never below zero; this is the effect of the exponential link between quantities. Thirdly, that the implied values of \\(\\sigma^2\\) do appear to be centred around 0.25, as they should be as \\(\\sigma\\) was set to 0.50 in the model.\nAnd forthly, that the density around \\(\\beta_1\\) is more peaked than around \\(\\beta_0\\). This concords with what we saw previously in the filled contour map: both the horizontal beta0 axis and vertical beta1 axis are on the same scale, but the oval is broader along the horizontal axis than the vertical axis. This in effect implies that we have more information about the true value of \\(\\beta_1\\), the slope, than about the true value of \\(\\beta_0\\), the intercept.\nWe can also use these draws to reproduce something similar to, but not identical to, 11 the previous filled contour map:\n\n\nCode\n# param_draws |&gt;\n#     as_tibble(\n#         rownames = 'draw'\n#     ) |&gt;\n#     ggplot(aes(x = beta0, y = beta1)) + \n#     geom_point(alpha = 0.1) + \n#     coord_cartesian(xlim = c(-10, 10), ylim = c(-10, 10))\n\nparam_draws |&gt;\n    as_tibble(\n        rownames = 'draw'\n    ) |&gt;\n    ggplot(aes(x = beta0, y = beta1)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nOnce again, we see the same qualities as the contour map produced by interrogating the likelihood surface exhaustively: the distribution appears bivariate normal; there is a greater range in the distribution along the beta0 than the beta1 axis; and there is evidence of some negative correlation between the two parameters.\n\n\nSummary\nThis post has shown how optim(), which in its vanilla state only returns point estimates, can be configured to also calculater and report the Hessian, a record of instantaneous curvature around the point estimates. Even without a fine-grained and exhausive search throughout the likelihood surface, this measure of curvature can be used to produce similar measures of uncertainty to the more exhausive approach, in a fraction of the number of computations.\nMore importantly, it can be used to generate draws of plausible combinations of parameter values, something denoted as \\(\\tilde{\\theta}\\) earlier. This is something especially useful for producing honest quantities of interest, which both tell users of models something they want to know, while also representing how uncertain we are in this knowledge."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#optim-for-parameter-point-estimation-our-robo-chauffeur",
    "href": "pages/likelihood-and-simulation-theory/index.html#optim-for-parameter-point-estimation-our-robo-chauffeur",
    "title": "Likelihood and Simulation Theory",
    "section": "optim for parameter point estimation: our Robo-Chauffeur",
    "text": "optim for parameter point estimation: our Robo-Chauffeur\nNote how the llNormal function takes a single argument, pars, which packages up all the specific candidate parameter values we want to try out. In our previous post, we also had a ‘feeder function’, feed_to_ll, which takes the various \\(\\beta\\) candidate values from the grid and packages them into pars. In our previous post, we had to specify the candidate values to try to feed to llNormal packages inside pars.\nBut we don’t have to do this. We can instead use an algorithm to take candidate parameters, try them out, then make new candidate parameters and try them out, for us. Much as a taxi driver needs to know where to meet a passenger, but doesn’t want the passenger to tell them exactly which route to take, we just need to specify a starting set of values for the parameters to optimise. R’s standard way of doing this is with the optim function. Here’s it in action:\n\n\nCode\noptim_results &lt;-  optim(\n    # par contains our initial guesses for the three parameters to estimate\n    par = c(0, 0, 0), \n\n    # by default, most optim algorithms prefer to search for a minima (lowest point) rather than maxima \n    # (highest point). So, I'm making a function to call which simply inverts the log likelihood by multiplying \n    # what it returns by -1\n    fn = function(par, y, X) {-llNormal(par, y, X)}, \n\n    # in addition to the par vector, our function also needs the observed output (y)\n    # and the observed predictors (X). These have to be specified as additional arguments.\n    y = y, X = X\n    )\n\noptim_results\n\n\n$par\n[1]  2.460571  1.375421 -1.336209\n\n$value\n[1] -1.51397\n\n$counts\nfunction gradient \n     216       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nThe optim function returns a fairly complex output structure, with the following components:\n\npar: the values for the parameters (in our case \\(\\{\\beta_0, \\beta_1, \\eta \\}\\)) which the optimisation algorithm ended up with.\nvalue: the value returned by the function fn when the optim routine was stopped.\ncounts: the number of times the function fn was repeatedly called by optim before optim decided it had had enough\nconvergence: whether the algorithm used by optim completed successfully (i.e. reached what it considers a good set of parameter estimates in par), or not.\n\nIn this case, convergence is 0, which (perhaps counterintuitively) indicates a successful completion. counts indicates that optim called the log likelihood function 216 times before stopping, and par indicates values of \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\eta = -1.34\\}\\) were arrived at. As \\(\\sigma^2 = e^\\eta\\), this means \\(\\theta = \\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\sigma^2 = 0.26 \\}\\). As a reminder, the ‘true’ values are \\(\\{\\beta_0 = 2.50, \\beta_1 = 1.40, \\sigma^2 = 0.25\\}\\).\nSo, the optim algorithm has arrived at pretty much the correct answers for all three parameters, in 216 calls to the log likelihood function, whereas for the grid search approach in the last post we made over 10,000 calls to the log likelihood function for just two of the three parameters.\nLet’s see if we can get more information on exactly what kind of path optim took to get to this set of parameter estimates. We should be able to do this by specifying a value in the trace component in the control argument slot…\n\nComparisons with ‘canned’ functions\nFor comparison let’s see what lm and glm produce.\nFirst lm:\n\n\nCode\ntoy_df &lt;- tibble(\n    x = x, \n    y = y\n)\n\n\nmod_lm &lt;- lm(y ~ x, data = toy_df)\nsummary(mod_lm)\n\n\n\nCall:\nlm(formula = y ~ x, data = toy_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6082 -0.3852 -0.1668  0.2385  1.1092 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5813 on 7 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9767 \nF-statistic:   336 on 1 and 7 DF,  p-value: 3.564e-07\n\n\n\\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\), i.e. the same to 2 decimal places.\nAnd now with glm:\n\n\nCode\nmod_glm &lt;- glm(y ~ x, data = toy_df, family = gaussian(link = \"identity\"))\n\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = toy_df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nOnce again, \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\)\n\n\nDiscussion\nIn the above, we’ve successfully used optim, our Robo-Chauffeur, to arrive very quickly at some good estimates for our parameters of interest, \\(\\beta_0\\) and \\(\\beta_1\\), which are in effect identical to those produced by the lm and glm functions.\nThis isn’t a coincidence. What we’ve done the hard way is what the glm function (in particular) largely does ‘under the hood’."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#method",
    "href": "pages/likelihood-and-simulation-theory/index.html#method",
    "title": "Likelihood and Simulation Theory",
    "section": "Method",
    "text": "Method\nLet’s make use of our toy dataset one last time, and go through the motions to produce the \\(\\tilde{\\theta}\\) draws we ended with on the last post:\n\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\n\npoint_estimates &lt;- fuller_optim_output$par\n\nvcov &lt;- -solve(fuller_optim_output$hessian)\nparam_draws &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = point_estimates, \n    Sigma = vcov\n)\n\ncolnames(param_draws) &lt;- c(\n    \"beta0\", \"beta1\", \"eta\"\n)\n\nLet’s now look at our toy data again, and decide on some specific questions to answer:\n\nlibrary(tidyverse)\ntoy_df &lt;- tibble(x = x, y = y)\n\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() \n\n\n\n\nWithin the data itself, we have only supplied x and y values for whole numbers of x between -3 and 5. But we can use the model to produce estimates for non-integer values of x. Let’s try 2.5. For this single value of x, we can produce both predicted values and expected values, by passing the same value of x to each of the plausible estimates of \\(\\theta\\) returned by the multivariate normal function above.\n\ncandidate_x &lt;- 2.5"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#expected-values",
    "href": "pages/likelihood-and-simulation-theory/index.html#expected-values",
    "title": "Likelihood and Simulation Theory",
    "section": "Expected values",
    "text": "Expected values\nHere’s an example of estimating the expected value of y for x = 2.5 using loops and standard algebra:\n\n# Using standard algebra and loops\nN &lt;- nrow(param_draws)\nexpected_y_simpler &lt;- vector(\"numeric\", N)\nfor (i in 1:N){\n    expected_y_simpler[i] &lt;- param_draws[i, \"beta0\"] + candidate_x * param_draws[i, \"beta1\"]\n}\n\nhead(expected_y_simpler)\n\n[1] 6.004068 5.859547 6.121791 5.987509 5.767047 6.395820\n\n\nWe can see just from the first few values that each estimate is slightly different. Let’s order the values from lowest to highest, and find the range where 95% of values sit:\n\nev_range &lt;- quantile(expected_y_simpler,  probs = c(0.025, 0.500, 0.975)) \n\nev_range\n\n    2.5%      50%    97.5% \n5.505104 5.898148 6.291150 \n\n\nThe 95% interval is therefore between 5.51 and 6.29, with the median (similar but not quite the point estimate) being 5.90. Let’s plot this against the data:\n\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() + \n    annotate(\"point\", x = candidate_x, y =  median(expected_y_simpler), size = 1.2, shape = 2, colour = \"blue\") + \n    annotate(\"segment\", x = candidate_x, xend=candidate_x, y = ev_range[1], yend = ev_range[3], colour = \"blue\")\n\n\n\n\nThe vertical blue line therefore shows the range of estimates for \\(Y|x=2.5\\) that contain 95% of the expected values given the draws of \\(\\beta = \\{\\beta_0, \\beta_1\\}\\) which we produced from the Multivariate Normal given the point estimates and Hessian from optim(). This is our estimated range for the expected value, not predicted value. What’s the difference?"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#predicted-values",
    "href": "pages/likelihood-and-simulation-theory/index.html#predicted-values",
    "title": "Likelihood and Simulation Theory",
    "section": "Predicted values",
    "text": "Predicted values\nOne clue about the difference between expected value lies in the parameters from optim() we did and did not use: Whereas we have both point estimates and uncertainty estimates for the parameters \\(\\{\\beta_0, \\beta_1, \\sigma^2\\}\\),12 we only made use of the the two \\(\\beta\\) parameters when producing this estimate.\nNow let’s recall the general model formula, from the start of King, Tomz, and Wittenberg (2000), which we repeated for the first few posts in the series:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nThe manual for Zelig, the (now defunct) R package that used to support analysis using this approach, states that for Normal Linear Regression these two components are resolved as follows:\nStochastic Component\n\\[\nY_i \\sim Normal(\\mu_i, \\sigma^2)\n\\]\nSystematic Component\n\\[\n\\mu_i = x_i \\beta\n\\]\nThe page then goes onto state that the expected value, \\(E(Y)\\), is :\n\\[\nE(Y) = \\mu_i = x_i \\beta\n\\]\nSo, in this case, the expected value is the systematic component only, and does not involve the dispersion parameter in the stochastic component, which for normal linear regression is the \\(\\sigma^2\\) term. That’s why we didn’t use estimates of \\(\\sigma^2\\) when simulating the expected values.\nBut why is this? Well, it comes from the expectation operator, \\(E(.)\\). This operator means something like, return to me the value that would be expected if this experiment were performed an infinite number of times.\nThere are two types of uncertainty which give rise to variation in the predicted estimate: sampling uncertainty, and stochastic variation. In the expected value condition, this second source of variation falls to zero,13 leaving only the influence of sampling uncertainty, as in uncertainty about the true value of the \\(\\beta\\) parameters, remaining on uncertainty on the predicted outputs.\nFor predicted values, we therefore need to reintroduce stochastic variation as a source of variation in the range of estimates produced. Each \\(\\eta\\) value we have implies a different \\(\\sigma^2\\) value in the stochastic part of the equation, which we can then add onto the variation caused by parameter uncertainty alone:\n\nN &lt;- nrow(param_draws)\npredicted_y_simpler &lt;- vector(\"numeric\", N)\nfor (i in 1:N){\n    predicted_y_simpler[i] &lt;- param_draws[i, \"beta0\"] + candidate_x * param_draws[i, \"beta1\"] + \n        rnorm(\n            1, mean = 0, \n            sd = sqrt(exp(param_draws[i, \"eta\"]))\n        )\n}\n\nhead(predicted_y_simpler)\n\n[1] 4.802092 6.706397 7.073450 6.118750 6.757717 7.461254\n\n\nLet’s now get the 95% prediction interval for the predicted values, and compare them with the expected values predicted interval earlier\n\npv_range &lt;- \n    quantile(\n        predicted_y_simpler, \n        probs = c(0.025, 0.500, 0.975)\n    )\n\npv_range\n\n    2.5%      50%    97.5% \n4.766300 5.895763 7.055408 \n\n\nSo, whereas the median is similar to before, 5.90, the 95% interval is now from 4.77 to 7.0614. This compares with the 5.51 to 6.29 range for the expected values. Let’s now plot this predicted value range just as we did with the expected values:\n\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() + \n    annotate(\"point\", x = candidate_x, y =  pv_range[2], size = 1.2, shape = 2, colour = \"blue\") + \n    annotate(\"segment\", x = candidate_x, xend=candidate_x, y = pv_range[1], yend = pv_range[3], colour = \"red\")\n\n\n\n\nClearly considerably wider."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#summary-1",
    "href": "pages/likelihood-and-simulation-theory/index.html#summary-1",
    "title": "Likelihood and Simulation Theory",
    "section": "Summary",
    "text": "Summary\nIn the exercise above we did for logistic regression what the previous few posts in section two did for standard regression: i.e. we derived the log likelihood, applied it using optim, and compared with results from the glm() package. We saw in this case that fitting models isn’t always straightforward. We were - well, I was - overly ambitious in building and applying an overly parameterised model specification. But we eventually got to similar parameter values using both approaches.\nThough this wasn’t as straightforward as I was hoping for, I’m presenting it warts-and-all. In principle, the log-likelihood maximisation approach generalises to a great many model specifications, even if in practice some model structures aren’t as straightforward to fit as others."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#recap",
    "href": "pages/likelihood-and-simulation-theory/index.html#recap",
    "title": "Likelihood and Simulation Theory",
    "section": "Recap",
    "text": "Recap\nWithin this series, parts 1-4 formed what we might call ‘section one’, and part 5-9 ‘section two’.\nSection one (re)introduced statistical models as siblings, children of a mother model which combines a systematic component (an equation with a \\(=\\) symbol in it) and a stochastic component (an equation with a \\(\\sim\\) in it, which can largely be read as ‘drawn from’). Part one provided a graphical representation of the challenge of model fitting from an algorithmic perspective, in which the parameters that go into the two component are tweaked and tweaked until some condition is met: usually that the discrepency between model predictions and observed outcomes are minimised some way. The two component mother model is largely equivalent to the concept of the generalised linear model: parts two and three explored this association a bit more. Part four demonstrated how, for statistical models other than standard linear regression, the kinds of answer one usually wants from a model are not readily apparent from the model coefficients themselves, and so careful use of model predictions, and calibration of the questions, are required to use models to answer substantivelly meaningful questions.\nSection two aimed to show how likelihood theory is used in practice in order to justify a loss function that algorithms can be used to try to ‘solve’.15 These loss functions and optimisation algorithms are usually called implicitly by statistical model functions, but we did things the hard way by building the loss function from scratch, and evoking the algorithms more directly, using R’s optim() function. As well as the pedagogical value (and bragging rights) of being able to create and fit statistical models directly, an additional benefit of using optim() (with some of its algorithms) is that it returns something called the Hessian. The Hessian is what allows us to be honest when making model predictions and projections, showing how our uncertainty about the true value of the model parameters (the multiple inputs that optim() algorithms try to tweak until they’re good enough) leads to uncertainty in what we’re predicting and projecting.\nUnfortunately, we’re still in section two. The material below aims to repeat the same kind of exercise performed for standard linear regression, but using logistic regression instead."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#log-likelihood-for-logistic-regression",
    "href": "pages/likelihood-and-simulation-theory/index.html#log-likelihood-for-logistic-regression",
    "title": "Likelihood and Simulation Theory",
    "section": "Log likelihood for logistic regression",
    "text": "Log likelihood for logistic regression\nPreviously we derived the log likelihood for Normal (Gaussian) regression and did some cool things with it. Let’s now do the same with logistic regression. We need to start with definition, then calculate log likelihood, then write it as a function in R that optim() can work its magic with.\nAccording to the relevant section of the Zelig website:\nStochastic component \\[\nY_i \\sim Bernoulli(y_i | \\pi_i )\n\\]\n\\[\nY_i = \\pi_i^{y_i}(1 - \\pi_i)^{1-y_i}\n\\]\nwhere \\(\\pi_i = P(Y_i = 1)\\)\nAnd\nSystematic Component\n\\[\n\\pi_i = \\frac{1}{1 + \\exp{(-x_i \\beta)}}\n\\]\nThe likelihood is the product of the above for all observations in the dataset \\(i \\in N\\)\n\\[\nL(.) = \\prod{\\pi_i^{y_i}(1 - \\pi_i)^{1-y_i}}\n\\]\nThe effect of logging the above15:\n\\[\n\\log{L(.)} = \\sum{[y_i \\log{\\pi_i} + (1-y_i)\\log{(1-y_i)}]}\n\\]\nThis can now be implemented as a function:\n\n\nCode\nllogit &lt;- function(par, y, X){\n    xform &lt;- function(z) {1 / (1 + exp(-z))}\n    p &lt;- xform(X%*%par)\n    sum(y * log(p) + (1-y) * log(1 - p))\n}\n\n\nLet’s pick an appropriate dataset. How about… picking a Palmer Penguin!?\n\n\nCode\nlibrary(tidyverse)\npalmerpenguins::penguins\n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nLet’s say we want to predict whether a penguin is of the Chinstrap species\n\n\nCode\npalmerpenguins::penguins %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(is_chinstrap = species == \"Chinstrap\") |&gt;\n    ggplot(aes(x = bill_length_mm, y = bill_depth_mm, colour = is_chinstrap, shape = sex)) + \n    geom_point()\n\n\n\n\n\nNeither bill length nor bill depth alone appears to distinguish between chinstrap and other species. But perhaps the interaction (product) of the two terms would do:\n\n\nCode\npalmerpenguins::penguins %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(is_chinstrap = species == \"Chinstrap\") |&gt;\n    mutate(bill_size = bill_length_mm * bill_depth_mm) |&gt;\n    ggplot(aes(x = bill_size, fill = is_chinstrap)) + \n    facet_wrap(~sex) + \n    geom_histogram()\n\n\n\n\n\nThe interaction term isn’t great at separating the two classes, but seems to be better than either length or size alone. So I’ll include it in the model.\n\n\nCode\ndf &lt;- palmerpenguins::penguins %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(is_chinstrap = species == \"Chinstrap\") |&gt;\n    mutate(bill_size = bill_length_mm * bill_depth_mm) |&gt;\n    mutate(is_male = as.numeric(sex == \"male\"))\n\ny &lt;- df$is_chinstrap\n\nX &lt;- cbind(1, df[,c(\"bill_length_mm\", \"bill_depth_mm\", \"bill_size\", \"is_male\")]) |&gt;\nas.matrix()\n\n\nSo, including the intercept term, our predictor matrix \\(X\\) contains 5 columns, including the interaction term bill_size. 16\nLet’s try now to use the above in optim()\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = rep(0, 5), \n    fn = llogit,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1] 82.9075239 -2.4368673 -6.4311531  0.1787047 -6.4900678\n\n$value\n[1] -33.31473\n\n$counts\nfunction gradient \n     137       45 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n             [,1]         [,2]          [,3]         [,4]         [,5]\n[1,]   -12.103063    -550.0621    -209.30944    -9674.925    -3.700623\n[2,]  -550.062097  -25256.3082   -9500.55848  -443670.225  -184.360139\n[3,]  -209.309443   -9500.5585   -3650.65107  -168517.417   -68.158844\n[4,] -9674.924703 -443670.2251 -168517.41718 -7846293.352 -3464.964868\n[5,]    -3.700623    -184.3601     -68.15884    -3464.965    -3.700623\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n            [,1]         [,2]         [,3]          [,4]         [,5]\n[1,] 41.95816335 -0.156192235 -0.309892876 -4.036895e-02  9.329019450\n[2,] -0.15619224 -0.005017392 -0.024806420  1.070652e-03 -0.139430425\n[3,] -0.30989288 -0.024806420 -0.042869947  2.854565e-03 -0.337480429\n[4,] -0.04036895  0.001070652  0.002854565 -7.331214e-05  0.003098092\n[5,]  9.32901945 -0.139430425 -0.337480429  3.098092e-03  1.202424836\n\n\nNow let’s compare with glm()\n\n\nCode\nmod_glm &lt;- glm(is_chinstrap ~ bill_length_mm * bill_depth_mm +is_male, data = df, \nfamily = binomial())\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = is_chinstrap ~ bill_length_mm * bill_depth_mm + \n    is_male, family = binomial(), data = df)\n\nCoefficients:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                  365.2924    88.3341   4.135 3.54e-05 ***\nbill_length_mm                -8.9312     2.0713  -4.312 1.62e-05 ***\nbill_depth_mm                -23.6184     5.5003  -4.294 1.75e-05 ***\nis_male                      -11.8725     2.6121  -4.545 5.49e-06 ***\nbill_length_mm:bill_depth_mm   0.5752     0.1292   4.452 8.53e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 337.113  on 332  degrees of freedom\nResidual deviance:  49.746  on 328  degrees of freedom\nAIC: 59.746\n\nNumber of Fisher Scoring iterations: 9\n\n\nUh oh! On this occasion it appears one or both approaches have become confused. A five dimensional search space might be too much for the algorithms to cope with, especially with collinearity 17 between some of the terms. Let’s simplify the task a bit, and just use intercept, bill size, and is_male as covariates. First with the standard package:\n\n\nCode\nmod_glm_simpler &lt;- glm(is_chinstrap ~ bill_size +is_male,   data = df, \nfamily = binomial())\nsummary(mod_glm_simpler)\n\n\n\nCall:\nglm(formula = is_chinstrap ~ bill_size + is_male, family = binomial(), \n    data = df)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -32.815339   4.325143  -7.587 3.27e-14 ***\nbill_size     0.043433   0.005869   7.400 1.36e-13 ***\nis_male      -7.038215   1.207740  -5.828 5.62e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 337.11  on 332  degrees of freedom\nResidual deviance:  90.60  on 330  degrees of freedom\nAIC: 96.6\n\nNumber of Fisher Scoring iterations: 7\n\n\nAnd now with the bespoke function and optim\n\n\nCode\nX &lt;- cbind(1, df[,c(\"bill_size\", \"is_male\")]) |&gt;\nas.matrix()\n\nfuller_optim_output &lt;- optim(\n    par = rep(0, 3), \n    fn = llogit,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1] -32.60343219   0.04314546  -6.98585077\n\n$value\n[1] -45.30114\n\n$counts\nfunction gradient \n      73       18 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]         [,2]         [,3]\n[1,]    -13.008605   -10662.078    -5.201308\n[2,] -10662.078251 -8846787.584 -4846.390833\n[3,]     -5.201308    -4846.391    -5.201308\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n             [,1]          [,2]         [,3]\n[1,] -536.7022079  0.7206703142 -134.7923170\n[2,]    0.7206703 -0.0009674672    0.1807806\n[3,] -134.7923170  0.1807806218  -33.4602664\n\n\nThe estimates from the two approaches are now much closer, even if they aren’t as close to each other as in the earlier examples. Using optim(), we have parameter estimates \\(\\beta = \\{\\beta_0 = -32.60, \\beta_1 = 0.04, \\beta_2 = -6.99\\}\\), and using glm(), we have estimates \\(\\beta = \\{\\beta_0 = -32.82, \\beta_1 = 0.04, \\beta_2 = -7.04 \\}\\)\nIf we cheat a bit, and give the five dimensional version starting values closer to the estimates from glm(), we can probably get similar estimates too.\n\n\nCode\nX &lt;- cbind(1, df[,c(\"bill_length_mm\", \"bill_depth_mm\", \"bill_size\", \"is_male\")]) |&gt;\nas.matrix()\n\nfuller_optim_output &lt;- optim(\n    par = c(300, -10, -29, 0.5, -10), \n    fn = llogit,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1] 299.5512512  -7.3684567 -19.3951742   0.4747209  -9.7521255\n\n$value\n[1] -25.33208\n\n$counts\nfunction gradient \n     153       22 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n             [,1]          [,2]          [,3]         [,4]         [,5]\n[1,]    -8.378918    -370.41592    -140.86865    -6342.301    -1.800406\n[2,]  -370.415921  -16580.87909   -6238.75358  -284403.350   -91.239716\n[3,]  -140.868648   -6238.75358   -2387.19776  -107598.410   -33.018551\n[4,] -6342.300809 -284403.34960 -107598.40987 -4906697.476 -1685.235507\n[5,]    -1.800406     -91.23972     -33.01855    -1685.236    -1.800406\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n            [,1]         [,2]        [,3]          [,4]         [,5]\n[1,] -59.5448267  2.316365876  5.14842594 -0.1737609491 10.383684649\n[2,]   2.3163659 -0.064512887 -0.16844980  0.0044962968 -0.166413655\n[3,]   5.1484259 -0.168449797 -0.33888931  0.0106735535 -0.387558164\n[4,]  -0.1737609  0.004496297  0.01067355 -0.0002712683  0.004068597\n[5,]  10.3836846 -0.166413655 -0.38755816  0.0040685965  1.904433768\n\n\nWell, they are closer, but they aren’t very close. As mentioned, the glm() model produced warnings, and some of the variables are likely to be collinear, so this initial specification may have been especially difficult to fit. Both approaches found an answer, but neither seem happy about it!\n\nSummary\nIn the exercise above we did for logistic regression what the previous few posts in section two did for standard regression: i.e. we derived the log likelihood, applied it using optim, and compared with results from the glm() package. We saw in this case that fitting models isn’t always straightforward. We were - well, I was - overly ambitious in building and applying an overly parameterised model specification. But we eventually got to similar parameter values using both approaches.\nThough this wasn’t as straightforward as I was hoping for, I’m presenting it warts-and-all. In principle, the log-likelihood maximisation approach generalises to a great many model specifications, even if in practice some model structures aren’t as straightforward to fit as others."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#summary-2",
    "href": "pages/likelihood-and-simulation-theory/index.html#summary-2",
    "title": "Likelihood and Simulation Theory",
    "section": "Summary",
    "text": "Summary\nIn the exercise above we did for logistic regression what the previous few posts in section two did for standard regression: i.e. we derived the log likelihood, applied it using optim, and compared with results from the glm() package. We saw in this case that fitting models isn’t always straightforward. We were - well, I was - overly ambitious in building and applying an overly parameterised model specification. But we eventually got to similar parameter values using both approaches.\nThough this wasn’t as straightforward as I was hoping for, I’m presenting it warts-and-all. In principle, the log-likelihood maximisation approach generalises to a great many model specifications, even if in practice some model structures aren’t as straightforward to fit as others."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#coming-up-1",
    "href": "pages/likelihood-and-simulation-theory/index.html#coming-up-1",
    "title": "Likelihood and Simulation Theory",
    "section": "Coming up",
    "text": "Coming up\nIn the next post, I’ll finally be moving off ‘section two’, with its algebra and algorithms, and showing some tools that can be used to make honest prediction and projections with models, but without all the efforts undertaken here and in the last few posts."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#quantities-of-interest",
    "href": "pages/likelihood-and-simulation-theory/index.html#quantities-of-interest",
    "title": "Likelihood and Simulation Theory",
    "section": "Quantities of interest",
    "text": "Quantities of interest\nWe’ll now, finally, show how this knowledge can be applied to do something with statistical models that ought to be done far more often: report on what King, Tomz, and Wittenberg (2000) calls quantities of interest, including predicted values, expected values, and first differences. Quantities of interest are not the direction and statistical significance (P-values) that many users of statistical models convince themselves matter, leading to the kind of mindless stargazing summaries of model outputs described in section one. Instead, they’re the kind of questions that someone, not trained to think that stargazing is satisfactory, might reasonably want answers to. These might include:\n\nWhat is the expected income of someone who completes course X in the five years after graduation? (Expected values)\nWhat is the expected range of incomes of someone who completes course X in the five years after graduation? (Predicted values)\nWhat is the expected difference in incomes between someone who completes course X, compared to course Y, in the five years after graduation? (First Differences)\n\nIn section one, we showed how to answer some of the questions of this form, for both standard linear regression and logistic regression. We showed that for linear regression such answers tend to come directly from the summary of coefficients, but that for logistic regression such answers tend to be both more ambiguous and dependent on other factors (such as gender of graduate, degree, ethnicity, age and so on), and require more processing in order to produce estimates for.\nHowever, we previously produced only point estimates for these questions, and so in a sense misled the questioner with the apparent certainty of our estimates. We now know, from earlier in this section, that we can use information about parameter uncertainty to produce parameter estimates \\(\\tilde{\\theta}\\) that do convey parameter uncertainty, and so we can do better than the point estimates alone to answer such questions in way that takes into account such uncertainty, with a range of values rather than a single value.\n\nMethod\nLet’s make use of our toy dataset one last time, and go through the motions to produce the \\(\\tilde{\\theta}\\) draws we ended with on the last post:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\n\n\nCode\npoint_estimates &lt;- fuller_optim_output$par\n\nvcov &lt;- -solve(fuller_optim_output$hessian)\nparam_draws &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = point_estimates, \n    Sigma = vcov\n)\n\ncolnames(param_draws) &lt;- c(\n    \"beta0\", \"beta1\", \"eta\"\n)\n\n\nLet’s now look at our toy data again, and decide on some specific questions to answer:\n\n\nCode\nlibrary(tidyverse)\ntoy_df &lt;- tibble(x = x, y = y)\n\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() \n\n\n\n\n\nWithin the data itself, we have only supplied x and y values for whole numbers of x between -3 and 5. But we can use the model to produce estimates for non-integer values of x. Let’s try 2.5. For this single value of x, we can produce both predicted values and expected values, by passing the same value of x to each of the plausible estimates of \\(\\theta\\) returned by the multivariate normal function above.\n\n\nCode\ncandidate_x &lt;- 2.5\n\n\n\n\nExpected values\nHere’s an example of estimating the expected value of y for x = 2.5 using loops and standard algebra:\n\n\nCode\n# Using standard algebra and loops\nN &lt;- nrow(param_draws)\nexpected_y_simpler &lt;- vector(\"numeric\", N)\nfor (i in 1:N){\n    expected_y_simpler[i] &lt;- param_draws[i, \"beta0\"] + candidate_x * param_draws[i, \"beta1\"]\n}\n\nhead(expected_y_simpler)\n\n\n[1] 6.004068 5.859547 6.121791 5.987509 5.767047 6.395820\n\n\nWe can see just from the first few values that each estimate is slightly different. Let’s order the values from lowest to highest, and find the range where 95% of values sit:\n\n\nCode\nev_range &lt;- quantile(expected_y_simpler,  probs = c(0.025, 0.500, 0.975)) \n\nev_range\n\n\n    2.5%      50%    97.5% \n5.505104 5.898148 6.291150 \n\n\nThe 95% interval is therefore between 5.51 and 6.29, with the median (similar but not quite the point estimate) being 5.90. Let’s plot this against the data:\n\n\nCode\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() + \n    annotate(\"point\", x = candidate_x, y =  median(expected_y_simpler), size = 1.2, shape = 2, colour = \"blue\") + \n    annotate(\"segment\", x = candidate_x, xend=candidate_x, y = ev_range[1], yend = ev_range[3], colour = \"blue\")\n\n\n\n\n\nThe vertical blue line therefore shows the range of estimates for \\(Y|x=2.5\\) that contain 95% of the expected values given the draws of \\(\\beta = \\{\\beta_0, \\beta_1\\}\\) which we produced from the Multivariate Normal given the point estimates and Hessian from optim(). This is our estimated range for the expected value, not predicted value. What’s the difference?\n\n\nPredicted values\nOne clue about the difference between expected value lies in the parameters from optim() we did and did not use: Whereas we have both point estimates and uncertainty estimates for the parameters \\(\\{\\beta_0, \\beta_1, \\sigma^2\\}\\),12 we only made use of the the two \\(\\beta\\) parameters when producing this estimate.\nNow let’s recall the general model formula, from the start of King, Tomz, and Wittenberg (2000), which we repeated for the first few posts in the series:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nThe manual for Zelig, the (now defunct) R package that used to support analysis using this approach, states that for Normal Linear Regression these two components are resolved as follows:\nStochastic Component\n\\[\nY_i \\sim Normal(\\mu_i, \\sigma^2)\n\\]\nSystematic Component\n\\[\n\\mu_i = x_i \\beta\n\\]\nThe page then goes onto state that the expected value, \\(E(Y)\\), is :\n\\[\nE(Y) = \\mu_i = x_i \\beta\n\\]\nSo, in this case, the expected value is the systematic component only, and does not involve the dispersion parameter in the stochastic component, which for normal linear regression is the \\(\\sigma^2\\) term. That’s why we didn’t use estimates of \\(\\sigma^2\\) when simulating the expected values.\nBut why is this? Well, it comes from the expectation operator, \\(E(.)\\). This operator means something like, return to me the value that would be expected if this experiment were performed an infinite number of times.\nThere are two types of uncertainty which give rise to variation in the predicted estimate: sampling uncertainty, and stochastic variation. In the expected value condition, this second source of variation falls to zero,13 leaving only the influence of sampling uncertainty, as in uncertainty about the true value of the \\(\\beta\\) parameters, remaining on uncertainty on the predicted outputs.\nFor predicted values, we therefore need to reintroduce stochastic variation as a source of variation in the range of estimates produced. Each \\(\\eta\\) value we have implies a different \\(\\sigma^2\\) value in the stochastic part of the equation, which we can then add onto the variation caused by parameter uncertainty alone:\n\n\nCode\nN &lt;- nrow(param_draws)\npredicted_y_simpler &lt;- vector(\"numeric\", N)\nfor (i in 1:N){\n    predicted_y_simpler[i] &lt;- param_draws[i, \"beta0\"] + candidate_x * param_draws[i, \"beta1\"] + \n        rnorm(\n            1, mean = 0, \n            sd = sqrt(exp(param_draws[i, \"eta\"]))\n        )\n}\n\nhead(predicted_y_simpler)\n\n\n[1] 4.802092 6.706397 7.073450 6.118750 6.757717 7.461254\n\n\nLet’s now get the 95% prediction interval for the predicted values, and compare them with the expected values predicted interval earlier\n\n\nCode\npv_range &lt;- \n    quantile(\n        predicted_y_simpler, \n        probs = c(0.025, 0.500, 0.975)\n    )\n\npv_range\n\n\n    2.5%      50%    97.5% \n4.766300 5.895763 7.055408 \n\n\nSo, whereas the median is similar to before, 5.90, the 95% interval is now from 4.77 to 7.0614. This compares with the 5.51 to 6.29 range for the expected values. Let’s now plot this predicted value range just as we did with the expected values:\n\n\nCode\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() + \n    annotate(\"point\", x = candidate_x, y =  pv_range[2], size = 1.2, shape = 2, colour = \"blue\") + \n    annotate(\"segment\", x = candidate_x, xend=candidate_x, y = pv_range[1], yend = pv_range[3], colour = \"red\")\n\n\n\n\n\nClearly considerably wider."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html",
    "href": "pages/complete-simulation-example/index.html",
    "title": "Statistical Simulation: A Complete Example",
    "section": "",
    "text": "Section One of this course introduced generalised linear models (GLMs) and statistical simuation. Section Two then delved more into the underlying theory and technicalities involved in implementing GLMs and using them for simulation.\nThis section gives a complete example of the methodology developed in these two sections, from start to finish. It also shows how the methodology is similar enough to Bayesian methods of statistical inference that applying a fully Bayesian modelling framework is just a small hop and jump from where we already are."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#recap",
    "href": "pages/complete-simulation-example/index.html#recap",
    "title": "Statistical Simulation: A Complete Example",
    "section": "",
    "text": "Section One of this course introduced generalised linear models (GLMs) and statistical simuation. Section Two then delved more into the underlying theory and technicalities involved in implementing GLMs and using them for simulation.\nThis section gives a complete example of the methodology developed in these two sections, from start to finish. It also shows how the methodology is similar enough to Bayesian methods of statistical inference that applying a fully Bayesian modelling framework is just a small hop and jump from where we already are."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#aim",
    "href": "pages/complete-simulation-example/index.html#aim",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Aim",
    "text": "Aim\nIn the last post we reached the end of a winding journey. This post will show how Bayesian approaches to model fitting, rather than the frequentist approaches more commonly used, can reach the intended destination of this journey more quickly, despite being a bit more conceptually challenging to start with."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#linear-regression-example",
    "href": "pages/complete-simulation-example/index.html#linear-regression-example",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Linear regression example",
    "text": "Linear regression example\nLet’s start with one of the built-in datasets, ToothGrowth, which is described as follows:\n\nThe response is the length of odontoblasts (cells responsible for tooth growth) in 60 guinea pigs. Each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods, orange juice or ascorbic acid (a form of vitamin C and coded as VC).\n\nLet’s load the dataset and visualise\n\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nWhat does it look like?\n\ndf |&gt;\n    ggplot(aes(y = len, x = dose, shape = supp, colour = supp)) + \n    geom_point() + \n    expand_limits(x = 0, y = 0)\n\n\n\n\nSo, although this has just three variables, there is some complexity involved in thinking about how the two predictor variables, supp and dose, relate to the response variable len. These include:\n\nWhether the relationship between len and dose is linear in a straightforward sense, or associated in a more complicated wway\nWhether supp has the same effect on len regardless of dose, or whether there is an interaction between dose and supp.\n\n\nStage One: model fitting\nWe can address each of these questions in turn, but should probably start with a model which includes both predictors:\n\nmod_01 &lt;- lm(len ~ dose + supp, data = df)\n\nsummary(mod_01)\n\n\nCall:\nlm(formula = len ~ dose + supp, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.600 -3.700  0.373  2.116  8.800 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.2725     1.2824   7.231 1.31e-09 ***\ndose          9.7636     0.8768  11.135 6.31e-16 ***\nsuppVC       -3.7000     1.0936  -3.383   0.0013 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.236 on 57 degrees of freedom\nMultiple R-squared:  0.7038,    Adjusted R-squared:  0.6934 \nF-statistic: 67.72 on 2 and 57 DF,  p-value: 8.716e-16\n\n\nEach term is statistically significant at the conventional thresholds (P &lt; 0.05), with higher doses associated with greater lengths. Compared to OJ, the reference category, a vitamin C (VC) supplement is associated with lower lengths.\nTurning to the first question, about the type of relationship between len and dose, one possibility is that greater doses lead to greater lengths, but there are diminishing marginal returns: the first mg has the biggest marginal effect, then the second mg has a lower marginal effect. An easy way to model this would be to include the log of dose in the regression model, rather than the dose itself.1 We can get a sense of whether this log dose specification might be preferred by plotting the data with a log scale on the x axis, and seeing if the points look like they ‘line up’ better:\n\ndf |&gt;\n    ggplot(aes(y = len, x = dose, shape = supp, colour = supp)) + \n    geom_point() + \n    scale_x_log10() + \n    expand_limits(x = 0.250, y = 0)\n\n\n\n\nYes, with this scaling, the points associated with the three dosage regimes look like they line up better. Let’s now build this model specification:\n\nmod_02 &lt;- lm(len ~ log(dose) + supp, data = df)\n\nsummary(mod_02)\n\n\nCall:\nlm(formula = len ~ log(dose) + supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2108 -2.9896 -0.5633  2.2842  9.1892 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.6633     0.7033   29.38  &lt; 2e-16 ***\nlog(dose)    11.1773     0.8788   12.72  &lt; 2e-16 ***\nsuppVC       -3.7000     0.9947   -3.72 0.000457 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.852 on 57 degrees of freedom\nMultiple R-squared:  0.755, Adjusted R-squared:  0.7464 \nF-statistic: 87.81 on 2 and 57 DF,  p-value: &lt; 2.2e-16\n\n\nAgain, the same kind of relationship between variables is observed: higher log dose: greater length; and VC rather than OJ is associated with lower growth. But is this model actually any better? The model summary for the linear dose model gives an adjusted \\(R^2\\) of 0.69, whereas for the log dose model the adjusted \\(R^2\\) is 0.75. So, as the data are fundamentally the same,2 this suggests it is. However, as we know that linear regression models are really just another kind of generalised linear models, and that model fitting tends to involve trying to maximise the log likelihood, we can also compare the log likelihoods of the two models, using the logLik() function, and so which is higher:\n\nlogLik(mod_01)\n\n'log Lik.' -170.2078 (df=4)\n\nlogLik(mod_02)\n\n'log Lik.' -164.5183 (df=4)\n\n\nBoth report the same number of degrees of freedom (‘df’), which shouldn’t be suprising as they involve the same number of parameters. But the log likelihood for mod_02 is higher, which like the Adjusted R-squared metric suggests a better fit.\nAnother approach, which generalises better to other types of model, is to compare the AICs, which are metrics that try to show the trade off between model complexity (based on number of parameters), and model fit (based on the log likelihood). By this criterion, the lower the score, the better the model:\n\nAIC(mod_01, mod_02)\n\n       df      AIC\nmod_01  4 348.4155\nmod_02  4 337.0367\n\n\nAs both models have exactly the same number of parameters, it should be of no surprise that mod_02 is still preferred.\nLet’s now address the second question: is there an interaction between dose and supp. This interaction term can be specified in one of two ways:\n\n# add interaction term explicitly, using the : symbol\nmod_03a &lt;- lm(len ~ log(dose) + supp + log(dose) : supp, data = df)\n\n# add interaction term implicitly, using the * symbol \nmod_03b &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(mod_03a)\n\n\nCall:\nlm(formula = len ~ log(dose) + supp + log(dose):supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\nsummary(mod_03b)\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nWe can see from the summaries that both ways of specifying the models lead to exactly the same model, with exactly the same estimates, standared errors, adjusted \\(R^2\\)s, and so on. The adjusted \\(R^2\\) is now 0.76, a slight improvement on the 0.75 value for the model without the interaction term. As before, we can also compare the trade-off between additional complexity and improved fit using AIC\n\nAIC(mod_02, mod_03a)\n\n        df      AIC\nmod_02   4 337.0367\nmod_03a  5 333.7750\n\n\nSo, the AIC of the more complex model is lower, suggesting a better model, but the additional improvement in fit is small.\nWe can also compare the fit, and answer the question of whether the two models can be compared, in a couple of other ways. Firstly, we can use BIC, AIC’s (usually) stricter cousin, which tends to penalise model complexity more harshly:\n\nBIC(mod_02, mod_03a)\n\n        df      BIC\nmod_02   4 345.4140\nmod_03a  5 344.2467\n\n\nEven using BIC, the more complex model is still preferred, though the difference in values is now much smaller.\nThe other way we can compare the models is using an F-test using the anova (analysis of variance) function:\n\nanova(mod_02, mod_03a)\n\nAnalysis of Variance Table\n\nModel 1: len ~ log(dose) + supp\nModel 2: len ~ log(dose) + supp + log(dose):supp\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     57 845.91                              \n2     56 774.89  1    71.022 5.1327 0.02737 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere anova compares the two models, notes that the first model can be understood as a restricted variant of the second model,3 and compares the change in model fit between the two models against the change in number of parameters used to fit the model. The key parts of the summary to look at are the F test value, 5.13, and the associated P value, which is between 0.01 and 0.05. This, again, suggests the interaction term is worth keeping.\nSo, after all that, we finally have a fitted model. Let’s look now at making some predictions from it.\n\n\nStage Two: Model predictions\nThe simplest approach to getting model predictions is to use the predict function, passing it a dataframe of values for which we want predictions:\n\npredictor_df &lt;- expand_grid(\n    supp = c('VC', 'OJ'), \n    dose = seq(0.25, 2.25, by = 0.01)\n)\npreds_predictors_df &lt;- predictor_df |&gt;\n    mutate(pred_len = predict(mod_03a, predictor_df))\n\npreds_predictors_df\n\n# A tibble: 402 × 3\n   supp   dose pred_len\n   &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 VC     0.25   -1.20 \n 2 VC     0.26   -0.683\n 3 VC     0.27   -0.189\n 4 VC     0.28    0.288\n 5 VC     0.29    0.748\n 6 VC     0.3     1.19 \n 7 VC     0.31    1.62 \n 8 VC     0.32    2.04 \n 9 VC     0.33    2.44 \n10 VC     0.34    2.83 \n# ℹ 392 more rows\n\n\nWe can visualise these predictions as follows, with the predicted values as lines, and the observed values as points:\n\npreds_predictors_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, y = pred_len, colour = supp, linetype = interextrap)) + \n    geom_line() + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\"\n    )\n\n\n\n\nIn the above, I’ve shown the lines as solid when they represent interpolations of the data, i.e. are in the range of measured doses, and as dashed when they represent extrapolations from the data, meaning they are are predictions made outside the range of observed values. We can see an obvious issue when we extrapolate too far to the left: for low doses, and for the VC supplement, the model predicts negative tooth lengths. Extrapolation is dangerous! And gets more dangerous the further we extrapolate from available observations.\nWe can also use the predict function to produce uncertainty intervals, either of expected values, or predicted values. By default these are 95% intervals, meaning they are expected to contain 95% of the range of expected or predicted values from the model.\nLet’s first look at expected values, which include uncertainty about parameter estimates, but not observed variation in outcomes:\n\ndf_pred_intvl &lt;- predict(mod_03a, newdata = predictor_df, interval = \"confidence\")\n\npreds_predictors_intervals_df &lt;- \n    bind_cols(predictor_df, df_pred_intvl)\n\npreds_predictors_intervals_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, linetype = interextrap)) + \n    geom_line(aes(y = fit, colour = supp)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = supp), alpha = 0.2) + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\",\n        subtitle = \"Range of expected values\"\n    )\n\n\n\n\nAnd the following shows the equivalent prediction intervals, which also incorporate known variance, as well as parameter uncertainty:\n\ndf_pred_intvl &lt;- predict(mod_03a, newdata = predictor_df, interval = \"prediction\")\n\npreds_predictors_intervals_df &lt;- \n    bind_cols(predictor_df, df_pred_intvl)\n\npreds_predictors_intervals_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, linetype = interextrap)) + \n    geom_line(aes(y = fit, colour = supp)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = supp), alpha = 0.2) + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\",\n        subtitle = \"Range of predicted values\"\n    )\n\n\n\n\nAs should be clear from the above, and discussion of the difference between expected and predicted values in previous posts, predicted values and expected values are very different, and it is important to be aware of the difference between these two quantities of interest. Regardless, we can see once again how dangerous it is to use this particular model specification to extrapolate beyond the range of observations, expecially for lower doses."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#stage-two-model-predictions",
    "href": "pages/complete-simulation-example/index.html#stage-two-model-predictions",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Stage Two: Model predictions",
    "text": "Stage Two: Model predictions\nThe simplest approach to getting model predictions is to use the predict function, passing it a dataframe of values for which we want predictions:\n\n\nCode\npredictor_df &lt;- expand_grid(\n    supp = c('VC', 'OJ'), \n    dose = seq(0.25, 2.25, by = 0.01)\n)\npreds_predictors_df &lt;- predictor_df |&gt;\n    mutate(pred_len = predict(mod_03a, predictor_df))\n\npreds_predictors_df\n\n\n# A tibble: 402 × 3\n   supp   dose pred_len\n   &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 VC     0.25   -1.20 \n 2 VC     0.26   -0.683\n 3 VC     0.27   -0.189\n 4 VC     0.28    0.288\n 5 VC     0.29    0.748\n 6 VC     0.3     1.19 \n 7 VC     0.31    1.62 \n 8 VC     0.32    2.04 \n 9 VC     0.33    2.44 \n10 VC     0.34    2.83 \n# ℹ 392 more rows\n\n\nWe can visualise these predictions as follows, with the predicted values as lines, and the observed values as points:\n\n\nCode\npreds_predictors_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, y = pred_len, colour = supp, linetype = interextrap)) + \n    geom_line() + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\"\n    )\n\n\n\n\n\nIn the above, I’ve shown the lines as solid when they represent interpolations of the data, i.e. are in the range of measured doses, and as dashed when they represent extrapolations from the data, meaning they are are predictions made outside the range of observed values. We can see an obvious issue when we extrapolate too far to the left: for low doses, and for the VC supplement, the model predicts negative tooth lengths. Extrapolation is dangerous! And gets more dangerous the further we extrapolate from available observations.\nWe can also use the predict function to produce uncertainty intervals, either of expected values, or predicted values. By default these are 95% intervals, meaning they are expected to contain 95% of the range of expected or predicted values from the model.\nLet’s first look at expected values, which include uncertainty about parameter estimates, but not observed variation in outcomes:\n\n\nCode\ndf_pred_intvl &lt;- predict(mod_03a, newdata = predictor_df, interval = \"confidence\")\n\npreds_predictors_intervals_df &lt;- \n    bind_cols(predictor_df, df_pred_intvl)\n\npreds_predictors_intervals_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, linetype = interextrap)) + \n    geom_line(aes(y = fit, colour = supp)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = supp), alpha = 0.2) + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\",\n        subtitle = \"Range of expected values\"\n    )\n\n\n\n\n\nAnd the following shows the equivalent prediction intervals, which also incorporate known variance, as well as parameter uncertainty:\n\n\nCode\ndf_pred_intvl &lt;- predict(mod_03a, newdata = predictor_df, interval = \"prediction\")\n\npreds_predictors_intervals_df &lt;- \n    bind_cols(predictor_df, df_pred_intvl)\n\npreds_predictors_intervals_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, linetype = interextrap)) + \n    geom_line(aes(y = fit, colour = supp)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = supp), alpha = 0.2) + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\",\n        subtitle = \"Range of predicted values\"\n    )\n\n\n\n\n\nAs should be clear from the above, and discussion of the difference between expected and predicted values in previous posts, predicted values and expected values are very different, and it is important to be aware of the difference between these two quantities of interest. Regardless, we can see once again how dangerous it is to use this particular model specification to extrapolate beyond the range of observations, expecially for lower doses."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#summary-and-coming-up",
    "href": "pages/complete-simulation-example/index.html#summary-and-coming-up",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Summary and coming up",
    "text": "Summary and coming up\nIn this post, we’ve started and finished building the model, and started but not quite finished using the model to generate expected and predicted values. We’ve discussed some approaches to deciding on a model specification, by incrementally comparing a series of different specifications which test different ideas we have about how the predictor variables might be related to each other, and to the response variable.\nAs we’ve done quite a lot of work on building the model, we’ve not covered everything that I was planning to in terms of model prediction, and what we can do with a linear regression model (and generalised linear regression model), beyond (yawn) stargazing, for using models to get expected values, predicted values, and especially first differences. So, I guess that’s coming up in the next post!"
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#aim-1",
    "href": "pages/complete-simulation-example/index.html#aim-1",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Aim",
    "text": "Aim\nThe last post ended by showing how the predict function can be used to show point estimates and uncertainty intervals for expected values and predicted values for a model based on a toothsome dataset. In this post we will start with that model and look at other information that can be recovered from it, information that will allow the effects of joint parameter uncertainty to be propagated through to prediction."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#recap-of-core-concepts",
    "href": "pages/complete-simulation-example/index.html#recap-of-core-concepts",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Recap of core concepts",
    "text": "Recap of core concepts\nBack in part 8 we stated that estimates of the cloud of uncertainty in model parameters, that results from having limited numbers of observations in the data, can be represented as:\n\\[\n\\tilde{\\theta} \\sim MVN(\\mu = \\dot{\\theta}, \\sigma^2 = \\Sigma)\n\\]\nWhere MVN means multivariate normal, and needs the two quantities \\(\\dot{\\theta}\\) and \\(\\Sigma\\) as parameters.\nPreviously we showed how to extract (estimates of) these two quantities from optim(), where the first quantity, \\(\\dot{\\theta}\\), was taken from the converged parameter point estimate slot par, and the second quantity, \\(\\Sigma\\), was derived from the hessian slot.\nBut we don’t need to use optim() directly in order to recover these quantities. Instead we can get them from the standard model objects produced by either lm() or glm(). Let’s check this out…"
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#building-our-model",
    "href": "pages/complete-simulation-example/index.html#building-our-model",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Building our model",
    "text": "Building our model\nLet’s load the data and model we arrived at previously\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nCode\nbest_model &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(best_model)\n\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s now look at some convenience functions, other than just summary, that work with lm() and glm() objects, and recover the quantities required from MVN to represent the uncertainty cloud."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#extracting-quantities-for-modelling-uncertainty",
    "href": "pages/complete-simulation-example/index.html#extracting-quantities-for-modelling-uncertainty",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Extracting quantities for modelling uncertainty",
    "text": "Extracting quantities for modelling uncertainty\nFirstly, for the point estimates \\(\\dot{\\theta}\\), we can use the coefficients() function\n\n\nCode\ncoef &lt;- coefficients(best_model)\n\ncoef\n\n\n     (Intercept)        log(dose)           suppVC log(dose):suppVC \n       20.663333         9.254889        -3.700000         3.844782 \n\n\nAnd for the variance-covariance matrix, for representing joint uncertainty about the above estimates, we can use the vcov function\n\n\nCode\nSig &lt;- vcov(best_model)\n\nSig\n\n\n                   (Intercept)     log(dose)        suppVC log(dose):suppVC\n(Intercept)       4.612422e-01 -8.768056e-17 -4.612422e-01    -7.224251e-17\nlog(dose)        -8.768056e-17  1.440023e+00  1.753611e-16    -1.440023e+00\nsuppVC           -4.612422e-01  1.753611e-16  9.224843e-01     1.748938e-16\nlog(dose):suppVC -7.224251e-17 -1.440023e+00  1.748938e-16     2.880045e+00\n\n\nFinally, we can extract the point estimate for stochastic variation in the model, i.e. variation assumed by the model even if parameter uncertainty were minimised, using the sigma function:\n\n\nCode\nsig &lt;- sigma(best_model)\n\nsig\n\n\n[1] 3.719847\n\n\nWe now have three quantities, coef, Sig and sig (note the upper and lower case s in the above). These provide something almost but not exactly equivalent to the contents of par and that derived from hessian when using optim() previously. The section below explains this distinction in more detail.\n\nBack to the weeds (potentially skippable)\nRecall the ‘grandmother formulae’, from King, Tomz, and Wittenberg (2000), which the first few posts in this series started with:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nFor standard linear regression this becomes:\nStochastic Component\n\\[\nY_i \\sim Norm(\\theta_i, \\sigma^2)\n\\]\nSystematic Component\n\\[\n\\theta_i =X_i \\beta\n\\]\nOur main parameters are \\(\\theta\\), which combined our predictors \\(X_i\\) and our model parameter estimates \\(\\beta\\). Of these two components we know the data - they are what they are - but are merely estimating our model parameters \\(\\beta\\). So, any estimation uncertainty in this part of the equation results from \\(\\beta\\) alone.\nOur ancillary parameter is \\(\\sigma^2\\). This is our estimate of how much fundamental variation there is in how the data (the response variables \\(Y\\)) is drawn from the stochastic data generating process.\nWhen we used optim() directly, we estimated \\(\\sigma^2\\) along with the other \\(\\beta\\) parameters, via the \\(\\eta\\) parameter eta, defined as \\(\\sigma^2 = e^{\\eta}\\) to allow optim() to search over an unbounded real number range. If there are k \\(\\beta\\) parameters, therefore, optim()’s par vector contained k + 1 values, with this last value being the point estimate for the eta parameter. Similarly, the number of rows, columns, and length of diagonal elements in the variance-covariance matrix recoverable through optim’s hessian slot was also k + 1 rather than k, with the last row, last column, and last diagonal element being measures of covariance between \\(\\eta\\) and the \\(\\beta\\) elements, and variance in \\(\\eta\\) itself.\nBy contrast, the length of coefficients returned by coefficients(best_model) is k, the number of \\(\\beta\\) parameters being estimated, and the dimensions of vcov(best_model) returned are also k by k.\nThis means there is one fewer piece/type of information about model parameters returned by coefficients(model), vcov(model) and sigma(model) than was potentially recoverable by optim()’s par and hessian parameter slots: namely, uncertainty about the true value of the ancillary parameter \\(\\sigma^2\\). The following table summarises this difference:\n\n\n\n\n\n\n\n\nInformation type\nvia optim\nvia lm and glm\n\n\n\n\nMain parameters: point\nfirst k elements of par\ncoefficients() function\n\n\nMain parameters: uncertainty\nfirst k rows and columns of hessian\nvcov() function\n\n\nAncillary parameters: point\nk+1th through to last element of par\nsigma() function or equivalent for glm()\n\n\nAncillary parameters: uncertainty\nlast columns and rows of hessian (after rows and columns k)\n—\n\n\n\nSo long as capturing uncertainty about the fundamental variability in the stochastic part of the model isn’t critical to our predictions then omission of a measure of uncertainty in the ancillary parameters \\(\\alpha\\) is likely a price worth paying for the additional convenience of being able to use the model objects directly. However we should be aware that, whereas with optim we potentially have both \\(\\tilde{\\beta}\\) and \\(\\tilde{\\alpha}\\) to represent model uncertainty, when using the three convenience functions coefficients(), vcov() and sigma() we technically ‘only’ have \\(\\tilde{\\beta}\\) and \\(\\dot{\\alpha}\\) (i.e. point estimates alone for the ancillary parameters).\nWith the above caveat in mind, let’s now look at using the results of coefficients(), vcov() and sigma() to generate (mostly) honest representations of expected values, predicted values, and first differences"
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#model-predictions",
    "href": "pages/complete-simulation-example/index.html#model-predictions",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Model predictions",
    "text": "Model predictions\nAs covered in section two, we can use the mvrnorm function from the MASS package to create \\(\\tilde{\\beta}\\), our parameter estimates with uncertainty:\n\nParameter simulation\n\n\nCode\nbeta_tilde &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = coef, \n    Sigma = Sig\n)\n\nhead(beta_tilde)\n\n\n     (Intercept) log(dose)    suppVC log(dose):suppVC\n[1,]    20.24094 10.215063 -3.746876         2.387160\n[2,]    20.82397  8.443172 -3.589557         2.883642\n[3,]    21.39830 11.584301 -3.812187         1.835707\n[4,]    19.46575 11.311284 -3.376451         2.455034\n[5,]    21.11652  7.487017 -3.352625         6.903544\n[6,]    21.85636  8.954243 -3.947732         5.630424\n\n\nLet’s first look at each of these parameters individually:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    pivot_longer(everything(), names_to = \"coefficient\", values_to = \"value\") |&gt; \n    ggplot(aes(x = value)) + \n    facet_grid(coefficient ~ .) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nNow let’s look at a couple of coefficients jointly, to see how they’re correlated. Firstly the association between the intercept and the log dosage:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `(Intercept)`, y = `log(dose)`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nHere the covariance between the two parameters appears very low. Now let’s look at how log dosage and Vitamin C supplement factor are associated:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `log(dose)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nAgain, the covariance appears low. Finally, the association between log dose and the interaction term\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `log(dose)`, y = `log(dose):suppVC`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nHere we have a much stronger negative covariance between the two coefficients. Let’s look at the variance-covariance extracted from the model previously to confirm this:\n\n\nCode\nknitr::kable(Sig)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\nlog(dose)\nsuppVC\nlog(dose):suppVC\n\n\n\n\n(Intercept)\n0.4612422\n0.000000\n-0.4612422\n0.000000\n\n\nlog(dose)\n0.0000000\n1.440023\n0.0000000\n-1.440023\n\n\nsuppVC\n-0.4612422\n0.000000\n0.9224843\n0.000000\n\n\nlog(dose):suppVC\n0.0000000\n-1.440023\n0.0000000\n2.880045\n\n\n\n\n\nHere we can see that the covariance between intercept and log dose is effectively zero, as is the covariance between the intercept and the interaction term, and the covariance between the log(dose) and suppVC factor. However, there is a negative covariance between log dose and the interaction term, i.e. what we have plotted above, and also between the intercept and the VC factor. For completeness, let’s look at this last assocation, which we expect to show negative association:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `(Intercept)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nYes it is! The parameter estimates follow the covariance provided by Sigma, as we would expect.\n\n\nExpected values\nLet’s stay we are initially interested in the expected values for a dosage of 1.25mg, with the OJ (rather than VC) supplement:\n\n\nCode\n# first element is 1 due to intercept\npredictor &lt;- c(1, log(1.25), 0, 0) \n\npredictions_ev &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor %*% this_beta\n)\n\nhead(predictions_ev)\n\n\n[1] 22.52037 22.70801 23.98326 21.98979 22.78720 23.85444\n\n\nLet’s now get a 95% credible interval:\n\n\nCode\nquantile(predictions_ev, probs = c(0.025, 0.500, 0.975))\n\n\n    2.5%      50%    97.5% \n21.25316 22.71753 24.14249 \n\n\nSo, the 95% interval for the expected value is between 21.31 and 24.14, with a middle (median) estimate of 22.73.4 Let’s check this against estimates from the predict() function:\n\n\nCode\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'confidence')\n\n\n      fit      lwr      upr\n1 22.7285 21.26607 24.19093\n\n\nThe expected values using the predict function give a 95% confidence interval of 21.27 to 24.19, with a point estimate of 22.73. These are not identical, as the methods employed are not identical,5 but they are hopefully similar enough to demonstrate they are attempts at getting at the same quantities of interest.\n\n\nPredicted values\nPredicted values also include inherent stochastic variation from the ancillary parameters \\(\\alpha\\), which for linear regression is \\(\\sigma^2\\). We can simply add these only the expected values above to produce predicted values:\n\n\nCode\nn &lt;- length(predictions_ev)\n\nshoogliness &lt;- rnorm(n=n, mean = 0, sd = sig)\n\npredictions_pv &lt;- predictions_ev + shoogliness\n\n\nhead(predictions_pv)\n\n\n[1] 17.32760 24.95960 23.66540 20.11720 26.22552 27.89927\n\n\nLet’s get the 95% interval from the above using quantile\n\n\nCode\nquantile(predictions_pv, probs = c(0.025, 0.5000, 0.975))\n\n\n    2.5%      50%    97.5% \n15.12090 22.82004 30.00322 \n\n\nAs expected, the interval is now much wider, with a 95% interval from 15.34 to 30.11. The central estimate should in theory, with an infinite number of runs, be the same, however because of random variation it will never be exactly the same to an arbitrary number of decimal places. In this case, the middle estimate is 22.75, not identical to the central estimate from the expected values distribution of 22.72. The number of simulations can always be increased to produce greater precision if needed.\nLet’s now compare this with the prediction interval produce by the predict function:\n\n\nCode\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'prediction')\n\n\n      fit      lwr     upr\n1 22.7285 15.13461 30.3224\n\n\nAgain, the interval estimates are not exactly the same, but they are very similar.\n\n\nFirst differences\nIt’s in the production of estimates of first differences - this, compared to that, holding all else constant - that the simulation approach shines for producing estimates with credible uncertainty. In our case, let’s say we are interested in asking:\n\nWhat is the expected effect of using the VC supplement, rather than the OJ supplement, where the dose is 1.25mg?\n\nSo, the first difference is from switching from OJ to VC, holding the other factor constant.\nWe can answer this question by using the same selection of \\(\\tilde{\\beta}\\) draws, but passing two different scenarios:\n\n\nCode\n#scenario 0: supplement is OJ\npredictor_x0 &lt;- c(1, log(1.25), 0, 0) \n\n#scenario 1: supplement is VC\npredictor_x1 &lt;- c(1, log(1.25), 1, 1 * log(1.25)) \n\n\npredictions_ev_x0 &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x0 %*% this_beta\n)\n\npredictions_ev_x1 &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x1 %*% this_beta\n)\n\npredictions_df &lt;- \n    tibble(\n        x0 = predictions_ev_x0,\n        x1 = predictions_ev_x1\n    ) |&gt;\n    mutate(\n        fd = x1 - x0\n    )\n\npredictions_df\n\n\n# A tibble: 10,000 × 3\n      x0    x1    fd\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  22.5  19.3 -3.21\n 2  22.7  19.8 -2.95\n 3  24.0  20.6 -3.40\n 4  22.0  19.2 -2.83\n 5  22.8  21.0 -1.81\n 6  23.9  21.2 -2.69\n 7  23.4  20.4 -2.98\n 8  23.4  20.2 -3.26\n 9  22.6  20.7 -1.94\n10  22.5  18.7 -3.83\n# ℹ 9,990 more rows\n\n\nLet’s look at the distribution of both scenarios individually:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario != \"fd\") |&gt;\n    ggplot(aes(x = estimate)) + \n    facet_wrap(~scenario, ncol = 1) + \n    geom_histogram(bins = 100)\n\n\n\n\n\nAnd the distribution of the pairwise differences between them:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario == \"fd\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nIt’s this last distribution which shows our first differences, i.e. our answer, hedged with an appropriate dose of uncertainty, to the specific question shown above. We can get a 95% interval of the first difference as follows:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario == \"fd\") |&gt; \n    pull('estimate') |&gt;\n    quantile(probs = c(0.025, 0.500, 0.975))\n\n\n      2.5%        50%      97.5% \n-4.8366800 -2.8406462 -0.8197071 \n\n\nSo, 95% of estimates of the first difference are between -4.85 and -0.81, with the middle of this distribution (on this occasion) being -2.83.\nUnlike with the expected values and predicted values, the predict() function does not return first differences with honest uncertainty in this way. What we have above is something new."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#summary",
    "href": "pages/complete-simulation-example/index.html#summary",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Summary",
    "text": "Summary\nIn this post we’ve finally combined all the learning we’ve developed over the 11 previous posts to answer three specific ‘what if?’ questions: one on expected values, one on predicted values, and one on first differences. These are what King, Tomz, and Wittenberg (2000) refer to as quantities of interest, and I hope you agree these are more organic and reasonable types of question to ask of data and statistical models than simply looking at coefficients and p-values and reporting which ones are ‘statistically significant’.\nIf you’ve been able to follow everything in these posts, and can generalise the approach shown above to other types of statistical model, then congratulations! You’ve learned the framework for answering meaningful questions using statistical models which is at the heart of one of the toughest methods courses for social scientists offered by one of the most prestigious universities in the world."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#coming-up",
    "href": "pages/complete-simulation-example/index.html#coming-up",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Coming up",
    "text": "Coming up\nThe next post uses the same dataset and model we’ve developed and applied, but shows how it can be implemented using a Bayesian rather than Frequentist modelling approach. In some ways it’s very familar, but in others it introduces a completely new paradigm to how models are fit and run."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#aim-2",
    "href": "pages/complete-simulation-example/index.html#aim-2",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Aim",
    "text": "Aim\nIn the last post we reached the end of a winding journey. This post will show how Bayesian approaches to model fitting, rather than the frequentist approaches more commonly used, can reach the intended destination of this journey more quickly, despite being a bit more conceptually challenging to start with."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#recap-1",
    "href": "pages/complete-simulation-example/index.html#recap-1",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Recap",
    "text": "Recap\nThe start of this blog series aimed to do two things:\n\nReintroduce statistical models via a generalised model formulae, comprising a systematic component and a stochastic component.\nReintroduce the fitting of statistical models from the perspective of algorithmic optimisation, in which the gap between what the model predicts and what’s observed is minimised in some way.\n\nThe rest of the first section of the series - posts two, three and four - added more context to the first post, and introduced the concept of using models for prediction - and the types of quantities of interest they can predict. The first section ended with post four, which illustrated some of the complexities of getting meaningful effect estimates - the overall effect of one specific predictor variable on the outcome being predicted - for model structures under than standard linear regression.\nThe second section - covering posts five to ten - delved into a lot more detail about how statistical models are fit. It introduced the concept of likelihood as a means of deciding what the target of a statistical optimisation algorithm should be. And it also showed - in sometimes excruciating detail - how to perform numeric optimisation based on likelihood in order to extract not just the best set of model parameters, but estimates of joint uncertainty in the best estimated set of model parameters. It’s this joint uncertainty in parameter estimates, estimated via the Hessian from the optim() function, which allowed uncertainty in model parameter estimates to be propagated and percolated through specific ‘what-if?’ questions - i.e. specific configurations of predictor variables passed through to the model - in order to produce honest answers to these ‘what-if?’ questions, which provide a range of answers, rather than a single answer, in order to show how model parameter estimation uncertainty leads to uncertainty in the answers the model provides.\nThe third section - posts 10-12 - completed the journey, showing how many of the concepts and ideas learned through considerable effort in sections one and (especially) two allow more intelligent and effective use of standard statistical model outputs - produced using R’s lm() and glm() functions - for honest prediction.\nThis post will extend the third section to show why the kind of honest prediction which we managed to produce using the kind of frequentist modelling framework used by lm() and glm() are, in fact, easier to produce using Bayesian models."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#on-marbles-and-jumping-beans",
    "href": "pages/complete-simulation-example/index.html#on-marbles-and-jumping-beans",
    "title": "Statistical Simulation: A Complete Example",
    "section": "On marbles and jumping beans",
    "text": "On marbles and jumping beans\nSection Two introduced Bayes’ Rule and the Likelihood axiom. It pointed out that, at heart, Bayes’ Rule is a way of expressing that given this in terms of this given that; and that Likelihood is also a claim about how that given this relates to this given that. More specifically, the claim of Likelihood is:\n\nThe likelihood of the model given the data is proportional to the probability of the data given the model.\n\nThere are two aspects to the model: firstly its structure; secondly its parameters. The structure includes the type of statistical model - whether it is a standard linear regression, negative binomial regression, logistic regression, Poisson regression model and so on - and also the specific types of columns from the dataset selected as either predictor variables (\\(X\\)) or response variables (\\(Y\\)). It is only after both the higher level structure of the model family, and the lower level structure of the data inputs (what’s being regressed on what?) have been decided that the Likelihood theory is used.\nAnd how is Likelihood theory used? Well, it defines a landscape over which an algorithm searches. This landscape has as many dimensions as there are parameters to fit. Where there are just two parameters, \\(\\beta_0\\) and \\(\\beta_1\\) to fit, we can visualise this landscape using something like a contour plot, with \\(\\beta_0\\) as latitude, \\(\\beta_1\\) as longitude, and the likelihood at this position its elevation or depth. Each possible joint value \\(\\beta = \\{\\beta_0, \\beta_1\\}\\) which the algorithm might wish to propose leads to a different long-lat coordinate over the surface, and each coordinate has a different elevation or depth. Although we can’t see beyond three dimensions (latitude, longitude, and elevation/depth), mathematics has no problem extending the concept of multidimensional space into far more dimensions than we can see or meaningfully comprehenend. If a model has ten parameters to fit, for example, the likelihood search space really is ten dimensional, and so on.\nNoticed I used elevation and depth interchangably in the description above. Well, this is because it really doesn’t matter whether an optimisation algorithm is trying to find the greatest elevation over a surface, or the greatest depth over the surface. The aim of maximum likelihood estimation is to find the configuration of parameters that maximises the likelihood, i.e. finds the top of the surface. However we saw that when passing the likelihood function to optim() we often inverted the function by multiplying it by -1. This is because the optimisation algorithms themselves seek to minimise the objective function they’re passed, not maximise it. By multiplying the likelihood function by -1 we made what we were trying to seek compatible with what the optimisation algorithms seek to do: find the greatest depth over a surface, rather than the highest elevation over the surface.\nTo make this all a bit less abstract let’s develop the intuition of an algorithm that seeks to minimise a function by way of a(nother) weird little story:\n\nImagine there is a landscape made out of transparent perspex. It’s not just transparent, it’s invisible to the naked eye. And you want to know where the lowest point of this surface is. All you have to do this is a magical leaking marble. The marble is just like any other marble, except every few moments, at regular intervals (say every tenth of a second), it dribbles out a white dye that you can see. And this dye sticks on and stains the otherwise invisible landscape whose lowest point you wish to find.\n\n\nNow, you drop the marble somewhere on the surface. You see the first point it hits on the surface - a white blob appears. The second blob appears some distance away from the first blob; and the third blob slightly less far away from the second blob as the second was to the second. After a few seconds, a trail of white spots is visible, the first few of which form something like a straight line, each consecutive point slightly less closer to the previous one. A second or two later, and the rumbling sounds of the marble rolling over the surface cease; the marble has clearly run out of momentum. And as you look at the trail of dots it’s generated, and is still generating, and you see it keeps highlighting the same point on the otherwise invisible surface, again and again.\n\nPreviously I used the analogy of a magical robo-chauffer, taking you to the top of a landscape. But the falling marble is probably a closer analogy to how many of optim()’s algorithms actually work. Using gravity and its shape alone, it finds the lowest point on the surface, and with its magical leaking dye, it tells you where this lowest point is.\nNow let’s extend the story to convert the analogy of the barefoot-and-blind person from part seven as well:\n\nThe marble has now ‘told’ you where the lowest point on the invisible surface is. However you also want to know more about the shape of the depression it’s in. You want to know if it’s a steep depression, or a shallow depression. And you want to know if it’s as steep or shallow in every direction, or if it’s steeper in some ways than the other.\n\n\nSo you now have to do a bit more work. You move your hand to just above the marble, and with your forefinger ‘flick’ it in a particular direction (say east-west): you see it move in the direction you flick it briefly, before rolling back towards (and beyond, and then towards) the depression point. As it does so, it leaks dye onto the surface, revealing a bit more about the landscape’s steepness or shallowness in this dimension. Then you do the same, but along a different dimension (say, north-south). After you’ve done this enough times, you are left with a collection of dyed points on the part of the surface closest to its deepest depression. The spacing and shape of these points tells you something about the nature of the depression and the part of the landscape it’s surrounding.\n\nNotice in this analogy you had to do extra work to get the marble to reveal more information about the surface. By default, the marble tells you the specific location of the depression, but not what the surface is like around this point. Instead, you need to intervene twice: firstly by dropping the marble onto the surface; secondly by flicking it around once it’s reached the lowest point on the surface.\nNow, let’s imagine swapping out our magical leaking marble for something even weirder: a magical leaking jumping bean.\n\nThe magical jumping bean does two things: it leaks and it jumps. (Okay, it does three things: when it leaks it also sticks to the surface it’s dying). When the bean is first dropped onto the surface, it marks the location it lands on. Then, it jumps up and across in a random direction. After jumping, it drops onto another part of the surface, marks it, and the process starts again. Jumping, sticking, marking; jumping, sticking, marking; jumping, sticking, marking… potentially forever.\n\n\nBecause of the effect of gravity, though the jumping bean jumps in a random direction, after a few jump-stick-mark steps it’s still, like the marble, very likely to move towards the depression. However, unlike the marble, even when it gets towards the lowest point in the depression, it’s not going to just rest there. The magical jumping bean is never at rest. It’s forever jump-stick-marking, jump-stick-marking.\n\n\nHowever, once the magical bean has moved towards the depression, though it keeps moving, it’s likely never to move too far from the depression. Instead, it’s likely to bounce around the depression. And as it does so, it drops ever more marks on the surface, which keep showing what the surface looks like around the depression in ever more detail.\n\nSo, because of the behaviour of the jumping bean, you only have to act on it once, by choosing where to drop it, rather than twice as with the marble: first choosing where to drop it, then flicking it around once it’s reached the lowest point on the surface."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#so-what",
    "href": "pages/complete-simulation-example/index.html#so-what",
    "title": "Statistical Simulation: A Complete Example",
    "section": "So what?",
    "text": "So what?\nIn the analogies above, the marble is to frequentist statistics as the jumping bean is to Bayesian statistics. A technical distinction between the marble and the jumping bean is that the marble converges towards a point (meaning it reaches a point of rest on the surface) whereas the jumping bean converges towards a distribution (meaning it never rests).\nIt’s Bayesian statistics’ 7 property of converging to a distribution rather than a point that makes the converged posterior distribution of parameter estimates Bayesian models produce ideal for the kind of honest prediction so much of this blog series has been focused on.\nLet’s now do some Bayesian modelling to compare…"
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#bayesian-modelling-now-significantly-less-terrifying-than-it-used-to-be",
    "href": "pages/complete-simulation-example/index.html#bayesian-modelling-now-significantly-less-terrifying-than-it-used-to-be",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Bayesian modelling: now significantly less terrifying than it used to be",
    "text": "Bayesian modelling: now significantly less terrifying than it used to be\nThere are a lot of packages and approaches for building Bayesian models. In fact there are whole statistical programming languages - like JAGS, BUGS 8 and Stan - dedicated to precisely describing every assumption the statistician wants to make about how a Bayesian model should be built. For more complicated and bespoke models these are ideal.\nHowever there are also an increasingly large number of Bayesian modelling packages that abstract away some of the assumptions and complexity apparent in the above specialised Bayesian modelling languages, and allow Bayesian versions of the kinds of model we’re already familiar with to be specified using formulae interfaces almost identical to what we’ve already worked with. Let’s look at one of them, rstanarm, which allows us to use stan, a full Bayesian statistical programming language, without quite as much thinking and set-up being required on our part.\nLet’s try to use this to build a Bayesian equivalent of the hamster tooth model we worked on in the last couple of posts.\n\nData Preparation and Frequentist modelling\nLet’s start by getting the dataset and building the frequentist version of the model we’re already familiar with:\n\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\nbest_model_frequentist &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(best_model_frequentist)\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nBuilding the Bayesian equivalent\nNow how would we build a Bayesian equivalent of this? Firstly let’s load (and if necessary install9) rstanarm.\n\nlibrary(rstanarm)\n\nWhereas for the frequentist model we used the function lm(), rstanarm has what looks like a broadly equivalent function stan_lm(). However, as I’ve just discovered, it’s actually more straightforward with stan_glm instead:\n\nbest_model_bayesian &lt;- stan_glm(len ~ log(dose) * supp, data = df)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000816 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 8.16 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.03 seconds (Warm-up)\nChain 1:                0.031 seconds (Sampling)\nChain 1:                0.061 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 8e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.034 seconds (Warm-up)\nChain 2:                0.032 seconds (Sampling)\nChain 2:                0.066 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.1e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.034 seconds (Warm-up)\nChain 3:                0.032 seconds (Sampling)\nChain 3:                0.066 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 5e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.031 seconds (Warm-up)\nChain 4:                0.034 seconds (Sampling)\nChain 4:                0.065 seconds (Total)\nChain 4: \n\nsummary(best_model_bayesian)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      len ~ log(dose) * supp\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 60\n predictors:   4\n\nEstimates:\n                   mean   sd   10%   50%   90%\n(Intercept)      20.7    0.7 19.7  20.6  21.6 \nlog(dose)         9.2    1.2  7.6   9.2  10.8 \nsuppVC           -3.7    1.0 -4.9  -3.7  -2.4 \nlog(dose):suppVC  3.9    1.8  1.6   3.9   6.2 \nsigma             3.8    0.4  3.3   3.8   4.3 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 18.8    0.7 17.9  18.8  19.7 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                 mcse Rhat n_eff\n(Intercept)      0.0  1.0  3507 \nlog(dose)        0.0  1.0  2194 \nsuppVC           0.0  1.0  3221 \nlog(dose):suppVC 0.0  1.0  2274 \nsigma            0.0  1.0  3084 \nmean_PPD         0.0  1.0  3721 \nlog-posterior    0.0  1.0  1655 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nSome parts of the summary for the Bayesian model look fairly familiar compared with the frequentist model summary; other bits a lot more exotic. We’ll skip over a detailed discussion of these outputs for now, though it is worth comparing the estimates section of the summary directly above, from the Bayesian approach, with the frequentist model produced earlier.\nThe frequentist model had point estimates of \\(\\{20.7, 9.3, -3.7, 3.8\\}\\). The analogous section of the Bayesian model summary is the mean column of the estimates section. These are reported to fewer decimal places by default - Bayesians are often more mindful of spurious precision - but are also \\(\\{20.7, 9.3, -3.7, 3.8\\}\\), so the same to this number of decimal places.\nNote also the Bayesian model reports an estimate for an additional parameter, sigma. This should be expected if we followed along with some of the examples using optim() for linear regression: the likelihood function required the ancillary parameters (referred to as \\(\\alpha\\) in the ‘mother model’ which this series started with, and part of the stochastic component \\(f(.)\\)) be estimated as well as the primary model parameters (referred to as \\(\\beta\\) in the ‘mother model’, and part of the systematic component \\(g(.)\\)). The Bayesian model’s coefficients (Intercept), log(dose), suppVC and the interaction term log(dose):suppVC are all part of \\(\\beta\\), whereas the sigma parameter is part of \\(\\alpha\\). The Bayesian model has just been more explicit about exactly which parameters it’s estimated from the data.\nFor the \\(\\beta\\) parameters, the Std. Error column in the Frequentist model summary is broadly comparable with the sd column in the Bayesian model summary. For the \\(\\beta\\) parameters these values are \\(\\{0.7, 1.2, 1.0, 1.7\\}\\) in the Frequentist model, and \\(\\{0.7, 1.2, 1.0, 1.7\\}\\) in the Bayesian model the summary. i.e. they’re the same to the degree of precision offered in the Bayesian model summary.\nBut let’s get to the crux of the argument: with Bayesian models honest predictions are easier.\nAnd they are, with the posterior_predict() function, passing what we want to predict on through the newdata argument, much as we did with the predict() function with frequentist models.\n\n\nScenario modelling\nLet’s recall the scenarios we looked at previously:\n\npredicted and expected values: length when dosage is 1.25mg and supplement is OJ\nfirst difference difference between OJ and VC supplement when dosage is 1.25mg\n\nLet’s start with the first question:\n\npredictors &lt;- data.frame(supp = \"OJ\", dose = 1.25)\n\npredictions &lt;- rstanarm::posterior_predict(\n    best_model_bayesian,\n    newdata = predictors\n)\n\nhead(predictions)\n\n            1\n[1,] 19.53831\n[2,] 21.74326\n[3,] 27.11863\n[4,] 25.79660\n[5,] 20.25985\n[6,] 21.96020\n\ndim(predictions)\n\n[1] 4000    1\n\n\nBy default posterior_predict() returns a matrix, which in this case has 4000 rows and just a single column. Let’s do a little work on this and visualise the distribution of estimates it produces:\n\npreds_df &lt;- tibble(estimate = predictions[,1])\n\n# lower, median, upper\nlmu &lt;- quantile(preds_df$estimate, c(0.025, 0.500, 0.975))\n\nlwr &lt;- lmu[1]\nmed &lt;- lmu[2]\nupr &lt;- lmu[3]\n\npreds_df |&gt;\n    mutate(\n        in_range = between(estimate, lwr, upr)\n    ) |&gt;\n    ggplot(aes(x = estimate, fill = in_range)) + \n    geom_histogram(bins = 100) + \n    scale_fill_manual(\n        values = c(`FALSE` = 'lightgray', `TRUE` = 'darkgray')\n    ) +\n    theme(legend.position = \"none\") + \n    geom_vline(xintercept = med, linewidth = 1.2, colour = \"steelblue\")\n\n\n\n\nThe darker-shaded parts of the histogram show the 95% uncertainty interval, and the blue vertical line the median estimate. This 95% interval range is 15.15 to 30.34.\nRemember we previously estimated both the expected values and the predicted values for this condition. Our 95% range for the expected values were 20.27 to 24.19 (or thereabouts), whereas our 95% range for the predicted values were (by design) wider, at 15.34 to 30.11. The 95% uncertainty interval above is therefore of predicted values, which include fundamental variation due to the ancillary parameters \\(\\sigma\\), rather than expected values, which result from parameter uncertainty alone.\nThere are a couple of other functions in rstanarm we can look at: predictive_error() and predictive_interval()\nFirst here’s predictive_interval. It is a convenience function that the posterior distribution generated previously, predictions, and returns an uncertainty interval:\n\npredictive_interval(\n    predictions\n)\n\n       5%      95%\n1 16.2481 29.01502\n\n\nWe can see by default the intervals returned are from 5% to 95%, i.e. are the 90% intervals rather than the 95% intervals considered previously. We can change the intervals requested with the prob argument:\n\npredictive_interval(\n    predictions, \n    prob = 0.95\n)\n\n      2.5%    97.5%\n1 15.14936 30.33882\n\n\nAs expected, this requested interval returns an interval closer to (but not identical to) the interval estimated using the quantile function.\nLet’s see if we can also use the model directly, specifying newdata directly to predictive_interval:\n\npredictive_interval(\n    best_model_bayesian,\n    newdata = predictors, \n    prob = 0.95\n)\n\n      2.5%    97.5%\n1 15.22315 30.33166\n\n\nYes. This approach works too. The values aren’t identical as, no doubt, a more sophisticated approach is used by predictive_interval to estimate the interval than simply arranging the posterior estimates in order using quantile.\nFor producing expected values we can use the function posterior_epred:\n\nepreds &lt;- posterior_epred(\n    best_model_bayesian,\n    newdata = predictors\n)\n\nexp_values &lt;- epreds[,1]\n\nquantile(exp_values, probs = c(0.025, 0.500, 0.975))\n\n    2.5%      50%    97.5% \n21.22628 22.70983 24.18352 \n\n\nFor comparison, the expected value 95% interval we obtained from the Frequentist model was 21.3 to 24.2 when drawing from the quasi-posterior distribution, and 22.7 to 24.2 when using the predict() function with the interval argument set to \"confidence\".\nNow, finally, let’s see if we can produce first differences: the estimated effect of using VC rather than OJ as a supplement when the dose is 1.25mg\n\npredictors_x0 &lt;- data.frame(supp = \"OJ\", dose = 1.25)\npredictors_x1 &lt;- data.frame(supp = \"VC\", dose = 1.25)\n\npredictors_fd &lt;- rbind(predictors_x0, predictors_x1)\n\npredictions_fd &lt;- rstanarm::posterior_predict(\n    best_model_bayesian,\n    newdata = predictors_fd\n)\n\nhead(predictions_fd)\n\n            1        2\n[1,] 19.58169 17.93272\n[2,] 24.06914 13.56129\n[3,] 21.66818 24.05286\n[4,] 28.27754 21.81103\n[5,] 27.50420 21.87387\n[6,] 26.59873 14.80452\n\n\nThe newdata argument to posterior_predict now has two rows, one for the OJ supplement and the other for the VC supplement scenario. And the predictions matrix returned by posterior_predict now has two columns: one for each scenario (row) in predictors_fd. We can look at the distribution of both of these columns, as well as the rowwise comparisions between columns, which will give our distribution of first differences for the predicted values:\n\npreds_fd_df &lt;- \n    predictions_fd |&gt;\n        as_tibble(rownames = \"draw\") |&gt;\n        rename(x0 = `1`, x1 = `2`) |&gt;\n        mutate(fd = x1 - x0)\n\npreds_fd_df |&gt; \n    select(-fd) |&gt;\n    pivot_longer(cols = c(\"x0\", \"x1\"), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    facet_wrap(~ scenario, nrow = 2)\n\n\n\n\nTo reiterate, these are predicted values for the two scenarios, not the expected values shown in the first differences section of post 12. This explains why there is greater overlap between the two distributions. Let’s visualise and calculate the first differences in predicted values:\n\npreds_fd_df |&gt;\n    select(fd) |&gt;\n    ggplot(aes(x = fd)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\nWe can see that the average of the distribution is below 0, but as we are looking at predicted values the range of distributions is much higher. Let’s get 95% intervals:\n\nquantile(preds_fd_df$fd, probs = c(0.025, 0.500, 0.975))\n\n      2.5%        50%      97.5% \n-13.425527  -2.987180   7.656461 \n\n\nThe 95% intervals for first differences in predicted values is from -13.6 to +7.9, with the median estimate at -3.0. As expected, the median is similar to the equivalent value from using expected values (-2.9) but the range is wider.\nNow let’s use posterior_epred to produce estimates of first differences in expected values, which will be more directly comparable to our first differences estimates in part 12:\n\npredictions_fd_ev &lt;- posterior_epred(\n    best_model_bayesian,\n    newdata = predictors_fd\n)\n\nhead(predictions_fd_ev)\n\n          \niterations        1        2\n      [1,] 22.87146 19.45426\n      [2,] 22.13428 19.98284\n      [3,] 22.03512 20.71388\n      [4,] 24.11545 20.26789\n      [5,] 23.27148 19.55058\n      [6,] 22.82728 20.14719\n\n\n\npreds_fd_df_ev &lt;- \n    predictions_fd_ev |&gt;\n        as_tibble(rownames = \"draw\") |&gt;\n        rename(x0 = `1`, x1 = `2`) |&gt;\n        mutate(fd = x1 - x0)\n\npreds_fd_df_ev |&gt; \n    select(-fd) |&gt;\n    pivot_longer(cols = c(\"x0\", \"x1\"), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    facet_wrap(~ scenario, nrow = 2)\n\n\n\n\nThis time, as the stochastic variation related to the \\(\\sigma\\) term has been removed, the distributions of the expected values are more distinct, with less overlap. Let’s visualise and compare the first differences of the expected values:\n\npreds_fd_df_ev |&gt;\n    select(fd) |&gt;\n    ggplot(aes(x = fd)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nquantile(preds_fd_df_ev$fd, probs = c(0.025, 0.500, 0.975))\n\n      2.5%        50%      97.5% \n-4.8901131 -2.8312784 -0.6689083 \n\n\nWe now have a 95% interval for the first difference in expected values of -4.9 to -0.7. By contrast, the equivalent range estimated using the Frequentist model in part 12 was -4.8 to -0.8. So, although they’re not identical, they do seem to be very similar."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#summing-up",
    "href": "pages/complete-simulation-example/index.html#summing-up",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Summing up",
    "text": "Summing up\nUp until now we’ve been using Frequentist approaches to modelling. However the simulation approach required to produce honest uncertainty depends on ‘tricking’ Frequentist models into producing something like the converged posterior distributions which, in Bayesian modelling approaches, come ‘for free’ from the way in which Bayesian frameworks estimate model parameters.\nAlthough Bayesian models are generally more technically and computationally demanding than Frequentist models, we have shown the folllowing:\n\nThat packages like rstanarm abstract away some of the challenges of building Bayesian models from scratch;\nThat the posterior distributions produced by Bayesian models produce estimates of expected values, predicted values, and first differences - our substantive quantities of interest - that are similar to those produced previously from Frequentist models\nThat for the estimation of these quantities of interest, the posterior distributions Bayesian models generate make it more straightforward, not less, to produce using Bayesian methods than using Frequentist methods.\n\nThanks for reading, and congratulations on getting this far through the series."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#footnotes",
    "href": "pages/complete-simulation-example/index.html#footnotes",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr perhaps more accurately Bayesian statistical model estimation rather than Bayesian statistics more generally? Bayes’ Rule can be usefully applied to interpret results derived from frequentist models. But the term Bayesian Modelling generally implies that Bayes’ Rule is used as part of the model parameter estimation process, in which a prior distribution is updated according to some algorithm, and then crucially the posterior distribution produced then forms the prior distribution at the next step in the estimation. The specific algorithm that works as the ‘jumping bean’ is usually something like Hamiltonian Monte Carlo, HMC, and the general simulation framework in which a posterior distribution generated from applying Bayes’ Rule is repeatedly fed back into the Bayes’ Rule equation as the prior distribution is known as Markov Chain Monte Carlo, MCMC.↩︎\nBecause the simulation approach relies on random numbers, the draws will never be the same unless the same random number seed is using using set.seed(). However with more simulations, using the n parameter from mvrnorm, the distributions of estimates should become ever closer to each other.↩︎\nIn this example, our more complex model has coefficients fit from the data for the intercept, log(dose), supp and the interaction term log(dose):supp, whereas the less complex model has coefficients fit from the data for the intercept, log(dose), and supp. This means the less complex model can be specified as a restricted version of the more complex model, where the value of the coefficient on the interaction term log(dose):supp is set to be equal to zero, rather than determined from the data. An equivalent way of phrasing and thinking about this is that the two model specifications are nested, with the restricted model nested inside the unrestricted model, which includes the interaction term. It’s this requirement for models to be nested in this way which meant that mod_01 and mod_02 could not be compared using an F-test, as neither model could be described strictly as restricted variants of the other model: they’re siblings, not mothers and daughters. However, both mod_01 and mod_02 could be compared against a common ancestor model which only includes the intercept term.↩︎\nOr perhaps more accurately Bayesian statistical model estimation rather than Bayesian statistics more generally? Bayes’ Rule can be usefully applied to interpret results derived from frequentist models. But the term Bayesian Modelling generally implies that Bayes’ Rule is used as part of the model parameter estimation process, in which a prior distribution is updated according to some algorithm, and then crucially the posterior distribution produced then forms the prior distribution at the next step in the estimation. The specific algorithm that works as the ‘jumping bean’ is usually something like Hamiltonian Monte Carlo, HMC, and the general simulation framework in which a posterior distribution generated from applying Bayes’ Rule is repeatedly fed back into the Bayes’ Rule equation as the prior distribution is known as Markov Chain Monte Carlo, MCMC.↩︎\nBecause the simulation approach relies on random numbers, the draws will never be the same unless the same random number seed is using using set.seed(). However with more simulations, using the n parameter from mvrnorm, the distributions of estimates should become ever closer to each other.↩︎\nOr perhaps more accurately Bayesian statistical model estimation rather than Bayesian statistics more generally? Bayes’ Rule can be usefully applied to interpret results derived from frequentist models. But the term Bayesian Modelling generally implies that Bayes’ Rule is used as part of the model parameter estimation process, in which a prior distribution is updated according to some algorithm, and then crucially the posterior distribution produced then forms the prior distribution at the next step in the estimation. The specific algorithm that works as the ‘jumping bean’ is usually something like Hamiltonian Monte Carlo, HMC, and the general simulation framework in which a posterior distribution generated from applying Bayes’ Rule is repeatedly fed back into the Bayes’ Rule equation as the prior distribution is known as Markov Chain Monte Carlo, MCMC.↩︎\nOminously named.↩︎\nrstanarm has a lot of dependencies. It’s the friendly, cuddly face of a beast!↩︎"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html",
    "title": "Part Seven: Feeling Uncertain",
    "section": "",
    "text": "In the previous post we managed to use numerical optimisation, with the optim() function, to good \\(\\beta\\) estimates for linear regression model fit to some toy data. In this post, we will explore how the optim() function can be used to produce estimates of uncertainty about these \\(\\beta\\) coefficients, and how these relates to measures of uncertainty presented in the standard lm and glm summary functions."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#aim",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#aim",
    "title": "Part Seven: Feeling Uncertain",
    "section": "",
    "text": "In the previous post we managed to use numerical optimisation, with the optim() function, to good \\(\\beta\\) estimates for linear regression model fit to some toy data. In this post, we will explore how the optim() function can be used to produce estimates of uncertainty about these \\(\\beta\\) coefficients, and how these relates to measures of uncertainty presented in the standard lm and glm summary functions."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#prereqs",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#prereqs",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Prereqs",
    "text": "Prereqs\nAs before, we’ll be using the same toy dataset, and same log likelihood function, as in the last two posts in this series. Let’s create these again:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nLet’s also run and save our parameter estimates produced both ‘the hard way’ (using optim), and ‘the easier way’ (using ‘glm’)\n\n\nCode\noptim_results &lt;-  optim(\n    # par contains our initial guesses for the three parameters to estimate\n    par = c(0, 0, 0), \n\n    # by default, most optim algorithms prefer to search for a minima (lowest point) rather than maxima \n    # (highest point). So, I'm making a function to call which simply inverts the log likelihood by multiplying \n    # what it returns by -1\n    fn = function(par, y, X) {-llNormal(par, y, X)}, \n\n    # in addition to the par vector, our function also needs the observed output (y)\n    # and the observed predictors (X). These have to be specified as additional arguments.\n    y = y, X = X\n    )\n\noptim_results\n\n\n$par\n[1]  2.460571  1.375421 -1.336209\n\n$value\n[1] -1.51397\n\n$counts\nfunction gradient \n     216       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nCode\npars_optim &lt;- optim_results$par\n\nnames(pars_optim) &lt;- c(\"beta0\", \"beta1\", \"eta\")\n\npars_optim\n\n\n    beta0     beta1       eta \n 2.460571  1.375421 -1.336209 \n\n\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- tibble(x = x, y = y)\nmod_glm &lt;- glm(y ~ x, data = df, family = gaussian(link=\"identity\"))\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nSo, both optim and the summary to mod_glm report \\(\\{\\beta_0 = 2.36, \\beta_1 = 1.38\\}\\), so both approaches appear to arrive at the same point on the log likelihood surface.\nHowever, note that the glm summary reports not just the estimates themselves (in the Estimate column of coefficients), but also standard errors (the Std. Error column) and derived quantities (t value, Pr(&gt;|t|), and the damnable stars at the very right of the table). How can these measures of uncertainty about the true value of the \\(\\beta\\) coefficients be derived from optim?"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#summary",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#summary",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Summary",
    "text": "Summary\nThis is probably the most difficult single section so far. Don’t worry: it’s likely to get easier from here on in."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#coming-up",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#coming-up",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Coming up",
    "text": "Coming up\nThe next part of the series goes into more detail about how numerical optimisation works."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#propagating-uncertainty",
    "href": "pages/complete-simulation-example/index.html#propagating-uncertainty",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Propagating uncertainty",
    "text": "Propagating uncertainty\nIn this section we will start with the model developed above and look at other information that can be recovered from it, information that will allow the effects of joint parameter uncertainty to be propagated through to prediction.\n\nRecap of core concepts\nBack in Section Two we stated that estimates of the cloud of uncertainty in model parameters, that results from having limited numbers of observations in the data, can be represented as:\n\\[\n\\tilde{\\theta} \\sim MVN(\\mu = \\dot{\\theta}, \\sigma^2 = \\Sigma)\n\\]\nWhere MVN means multivariate normal, and needs the two quantities \\(\\dot{\\theta}\\) and \\(\\Sigma\\) as parameters.\nPreviously we showed how to extract (estimates of) these two quantities from optim(), where the first quantity, \\(\\dot{\\theta}\\), was taken from the converged parameter point estimate slot par, and the second quantity, \\(\\Sigma\\), was derived from the hessian slot.\nBut we don’t need to use optim() directly in order to recover these quantities. Instead we can get them from the standard model objects produced by either lm() or glm(). Let’s check this out…\nWith the model developed previously, let’s now look at some convenience functions, other than just summary, that work with lm() and glm() objects, and recover the quantities required from MVN to represent the uncertainty cloud.\n\n\nExtracting quantities for modelling uncertainty\nFirstly, for the point estimates \\(\\dot{\\theta}\\), we can use the coefficients() function\n\n## Building our model \n\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\nbest_model &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(best_model)\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\ncoef &lt;- coefficients(best_model)\n\ncoef\n\n     (Intercept)        log(dose)           suppVC log(dose):suppVC \n       20.663333         9.254889        -3.700000         3.844782 \n\n\nAnd for the variance-covariance matrix, for representing joint uncertainty about the above estimates, we can use the vcov function\n\nSig &lt;- vcov(best_model)\n\nSig\n\n                   (Intercept)     log(dose)        suppVC log(dose):suppVC\n(Intercept)       4.612422e-01 -8.768056e-17 -4.612422e-01    -7.224251e-17\nlog(dose)        -8.768056e-17  1.440023e+00  1.753611e-16    -1.440023e+00\nsuppVC           -4.612422e-01  1.753611e-16  9.224843e-01     1.748938e-16\nlog(dose):suppVC -7.224251e-17 -1.440023e+00  1.748938e-16     2.880045e+00\n\n\nFinally, we can extract the point estimate for stochastic variation in the model, i.e. variation assumed by the model even if parameter uncertainty were minimised, using the sigma function:\n\nsig &lt;- sigma(best_model)\n\nsig\n\n[1] 3.719847\n\n\nWe now have three quantities, coef, Sig and sig (note the upper and lower case s in the above). These provide something almost but not exactly equivalent to the contents of par and that derived from hessian when using optim() previously. The section below explains this distinction in more detail.\n\nBack to the weeds (potentially skippable)\nRecall the ‘grandmother formulae’, from King, Tomz, and Wittenberg (2000), which the first few posts in this series started with:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nFor standard linear regression this becomes:\nStochastic Component\n\\[\nY_i \\sim Norm(\\theta_i, \\sigma^2)\n\\]\nSystematic Component\n\\[\n\\theta_i =X_i \\beta\n\\]\nOur main parameters are \\(\\theta\\), which combined our predictors \\(X_i\\) and our model parameter estimates \\(\\beta\\). Of these two components we know the data - they are what they are - but are merely estimating our model parameters \\(\\beta\\). So, any estimation uncertainty in this part of the equation results from \\(\\beta\\) alone.\nOur ancillary parameter is \\(\\sigma^2\\). This is our estimate of how much fundamental variation there is in how the data (the response variables \\(Y\\)) is drawn from the stochastic data generating process.\nWhen we used optim() directly, we estimated \\(\\sigma^2\\) along with the other \\(\\beta\\) parameters, via the \\(\\eta\\) parameter eta, defined as \\(\\sigma^2 = e^{\\eta}\\) to allow optim() to search over an unbounded real number range. If there are k \\(\\beta\\) parameters, therefore, optim()’s par vector contained k + 1 values, with this last value being the point estimate for the eta parameter. Similarly, the number of rows, columns, and length of diagonal elements in the variance-covariance matrix recoverable through optim’s hessian slot was also k + 1 rather than k, with the last row, last column, and last diagonal element being measures of covariance between \\(\\eta\\) and the \\(\\beta\\) elements, and variance in \\(\\eta\\) itself.\nBy contrast, the length of coefficients returned by coefficients(best_model) is k, the number of \\(\\beta\\) parameters being estimated, and the dimensions of vcov(best_model) returned are also k by k.\nThis means there is one fewer piece/type of information about model parameters returned by coefficients(model), vcov(model) and sigma(model) than was potentially recoverable by optim()’s par and hessian parameter slots: namely, uncertainty about the true value of the ancillary parameter \\(\\sigma^2\\). The following table summarises this difference:\n\n\n\n\n\n\n\n\nInformation type\nvia optim\nvia lm and glm\n\n\n\n\nMain parameters: point\nfirst k elements of par\ncoefficients() function\n\n\nMain parameters: uncertainty\nfirst k rows and columns of hessian\nvcov() function\n\n\nAncillary parameters: point\nk+1th through to last element of par\nsigma() function or equivalent for glm()\n\n\nAncillary parameters: uncertainty\nlast columns and rows of hessian (after rows and columns k)\n—\n\n\n\nSo long as capturing uncertainty about the fundamental variability in the stochastic part of the model isn’t critical to our predictions then omission of a measure of uncertainty in the ancillary parameters \\(\\alpha\\) is likely a price worth paying for the additional convenience of being able to use the model objects directly. However we should be aware that, whereas with optim we potentially have both \\(\\tilde{\\beta}\\) and \\(\\tilde{\\alpha}\\) to represent model uncertainty, when using the three convenience functions coefficients(), vcov() and sigma() we technically ‘only’ have \\(\\tilde{\\beta}\\) and \\(\\dot{\\alpha}\\) (i.e. point estimates alone for the ancillary parameters).\nWith the above caveat in mind, let’s now look at using the results of coefficients(), vcov() and sigma() to generate (mostly) honest representations of expected values, predicted values, and first differences\n\n\n\nModel predictions\nAs covered in section two, we can use the mvrnorm function from the MASS package to create \\(\\tilde{\\beta}\\), our parameter estimates with uncertainty:\n\nParameter simulation\n\nbeta_tilde &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = coef, \n    Sigma = Sig\n)\n\nhead(beta_tilde)\n\n     (Intercept) log(dose)    suppVC log(dose):suppVC\n[1,]    19.28142 11.350126 -2.476069         2.156141\n[2,]    20.63281 11.331101 -3.554217         2.296231\n[3,]    20.29771  9.046539 -2.878383         5.427045\n[4,]    19.65064  7.575844 -3.125506         6.400589\n[5,]    20.74269  9.280805 -2.857679         5.190796\n[6,]    21.20619  8.907086 -5.193592         4.511823\n\n\nLet’s first look at each of these parameters individually:\n\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    pivot_longer(everything(), names_to = \"coefficient\", values_to = \"value\") |&gt; \n    ggplot(aes(x = value)) + \n    facet_grid(coefficient ~ .) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\nNow let’s look at a couple of coefficients jointly, to see how they’re correlated. Firstly the association between the intercept and the log dosage:\n\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `(Intercept)`, y = `log(dose)`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\nHere the covariance between the two parameters appears very low. Now let’s look at how log dosage and Vitamin C supplement factor are associated:\n\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `log(dose)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\nAgain, the covariance appears low. Finally, the association between log dose and the interaction term\n\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `log(dose)`, y = `log(dose):suppVC`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\nHere we have a much stronger negative covariance between the two coefficients. Let’s look at the variance-covariance extracted from the model previously to confirm this:\n\nknitr::kable(Sig)\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\nlog(dose)\nsuppVC\nlog(dose):suppVC\n\n\n\n\n(Intercept)\n0.4612422\n0.000000\n-0.4612422\n0.000000\n\n\nlog(dose)\n0.0000000\n1.440023\n0.0000000\n-1.440023\n\n\nsuppVC\n-0.4612422\n0.000000\n0.9224843\n0.000000\n\n\nlog(dose):suppVC\n0.0000000\n-1.440023\n0.0000000\n2.880045\n\n\n\n\n\nHere we can see that the covariance between intercept and log dose is effectively zero, as is the covariance between the intercept and the interaction term, and the covariance between the log(dose) and suppVC factor. However, there is a negative covariance between log dose and the interaction term, i.e. what we have plotted above, and also between the intercept and the VC factor. For completeness, let’s look at this last assocation, which we expect to show negative association:\n\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `(Intercept)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\nYes it is! The parameter estimates follow the covariance provided by Sigma, as we would expect.\n\n\n\nExpected values\nLet’s stay we are initially interested in the expected values for a dosage of 1.25mg, with the OJ (rather than VC) supplement:\n\n# first element is 1 due to intercept\npredictor &lt;- c(1, log(1.25), 0, 0) \n\npredictions_ev &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor %*% this_beta\n)\n\nhead(predictions_ev)\n\n[1] 21.81413 23.16128 22.31638 21.34114 22.81364 23.19374\n\n\nLet’s now get a 95% credible interval:\n\nquantile(predictions_ev, probs = c(0.025, 0.500, 0.975))\n\n    2.5%      50%    97.5% \n21.30076 22.72980 24.13689 \n\n\nSo, the 95% interval for the expected value is between 21.31 and 24.14, with a middle (median) estimate of 22.73.4 Let’s check this against estimates from the predict() function:\n\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'confidence')\n\n      fit      lwr      upr\n1 22.7285 21.26607 24.19093\n\n\nThe expected values using the predict function give a 95% confidence interval of 21.27 to 24.19, with a point estimate of 22.73. These are not identical, as the methods employed are not identical,5 but they are hopefully similar enough to demonstrate they are attempts at getting at the same quantities of interest.\n\n\nPredicted values\nPredicted values also include inherent stochastic variation from the ancillary parameters \\(\\alpha\\), which for linear regression is \\(\\sigma^2\\). We can simply add these only the expected values above to produce predicted values:\n\nn &lt;- length(predictions_ev)\n\nshoogliness &lt;- rnorm(n=n, mean = 0, sd = sig)\n\npredictions_pv &lt;- predictions_ev + shoogliness\n\n\nhead(predictions_pv)\n\n[1] 23.90780 21.48269 18.18728 20.95734 27.29535 20.88399\n\n\nLet’s get the 95% interval from the above using quantile\n\nquantile(predictions_pv, probs = c(0.025, 0.5000, 0.975))\n\n    2.5%      50%    97.5% \n15.49003 22.76191 30.00879 \n\n\nAs expected, the interval is now much wider, with a 95% interval from 15.34 to 30.11. The central estimate should in theory, with an infinite number of runs, be the same, however because of random variation it will never be exactly the same to an arbitrary number of decimal places. In this case, the middle estimate is 22.75, not identical to the central estimate from the expected values distribution of 22.72. The number of simulations can always be increased to produce greater precision if needed.\nLet’s now compare this with the prediction interval produce by the predict function:\n\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'prediction')\n\n      fit      lwr     upr\n1 22.7285 15.13461 30.3224\n\n\nAgain, the interval estimates are not exactly the same, but they are very similar.\n\n\nFirst differences\nIt’s in the production of estimates of first differences - this, compared to that, holding all else constant - that the simulation approach shines for producing estimates with credible uncertainty. In our case, let’s say we are interested in asking:\n\nWhat is the expected effect of using the VC supplement, rather than the OJ supplement, where the dose is 1.25mg?\n\nSo, the first difference is from switching from OJ to VC, holding the other factor constant.\nWe can answer this question by using the same selection of \\(\\tilde{\\beta}\\) draws, but passing two different scenarios:\n\n#scenario 0: supplement is OJ\npredictor_x0 &lt;- c(1, log(1.25), 0, 0) \n\n#scenario 1: supplement is VC\npredictor_x1 &lt;- c(1, log(1.25), 1, 1 * log(1.25)) \n\n\npredictions_ev_x0 &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x0 %*% this_beta\n)\n\npredictions_ev_x1 &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x1 %*% this_beta\n)\n\npredictions_df &lt;- \n    tibble(\n        x0 = predictions_ev_x0,\n        x1 = predictions_ev_x1\n    ) |&gt;\n    mutate(\n        fd = x1 - x0\n    )\n\npredictions_df\n\n# A tibble: 10,000 × 3\n      x0    x1    fd\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  21.8  19.8 -1.99\n 2  23.2  20.1 -3.04\n 3  22.3  20.6 -1.67\n 4  21.3  19.6 -1.70\n 5  22.8  21.1 -1.70\n 6  23.2  19.0 -4.19\n 7  23.4  18.7 -4.66\n 8  23.8  19.6 -4.22\n 9  23.5  19.0 -4.45\n10  22.3  21.0 -1.25\n# ℹ 9,990 more rows\n\n\nLet’s look at the distribution of both scenarios individually:\n\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario != \"fd\") |&gt;\n    ggplot(aes(x = estimate)) + \n    facet_wrap(~scenario, ncol = 1) + \n    geom_histogram(bins = 100)\n\n\n\n\nAnd the distribution of the pairwise differences between them:\n\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario == \"fd\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\nIt’s this last distribution which shows our first differences, i.e. our answer, hedged with an appropriate dose of uncertainty, to the specific question shown above. We can get a 95% interval of the first difference as follows:\n\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario == \"fd\") |&gt; \n    pull('estimate') |&gt;\n    quantile(probs = c(0.025, 0.500, 0.975))\n\n      2.5%        50%      97.5% \n-4.8877866 -2.8385155 -0.7893288 \n\n\nSo, 95% of estimates of the first difference are between -4.85 and -0.81, with the middle of this distribution (on this occasion) being -2.83.\nUnlike with the expected values and predicted values, the predict() function does not return first differences with honest uncertainty in this way. What we have above is something new."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "",
    "text": "The previous post, perhaps the toughest of the series, showed how some special settings within R’s numerical optimisation optim() function can be used to estimate how much uncertainty there is in our estimates of the the model parameters \\(\\beta\\). We covered the concept that information and uncertainty are inversely related: the more information we have, the less uncertain we are, and vice versa. We estimated parameter uncertainty around the point that maximised (log) likelihood by asking the algorithm to take small steps from this highest point in different directions (dimensions, in effect variables), and report how steep the fall is in different directions. Steeper falls along a dimension imply less uncertainty and so more more information and narrower confidence intervals; as usual, the converse is also true. The component returned by optim() which reports the results of this ‘stepping out’ is a square matrix called the Hessian, which can be inverted to produce estimates of the variances and covarainces of each of the parameters being estimated in our model."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#recap",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#recap",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "",
    "text": "The previous post, perhaps the toughest of the series, showed how some special settings within R’s numerical optimisation optim() function can be used to estimate how much uncertainty there is in our estimates of the the model parameters \\(\\beta\\). We covered the concept that information and uncertainty are inversely related: the more information we have, the less uncertain we are, and vice versa. We estimated parameter uncertainty around the point that maximised (log) likelihood by asking the algorithm to take small steps from this highest point in different directions (dimensions, in effect variables), and report how steep the fall is in different directions. Steeper falls along a dimension imply less uncertainty and so more more information and narrower confidence intervals; as usual, the converse is also true. The component returned by optim() which reports the results of this ‘stepping out’ is a square matrix called the Hessian, which can be inverted to produce estimates of the variances and covarainces of each of the parameters being estimated in our model."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#aim",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#aim",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Aim",
    "text": "Aim\nThe aims of this post are to show how estimates of uncertainty around the point estimates produced from the Hessian, based around the curvature measured around the point of maximum likelihood, are similar to those produced using a much more extensive (and computationally intensive) interrogation of the likelihood surface using a grid-search approach. It will also show how representations of joint uncertainty for parameter values can be generated using the multivariate normal distribution."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#comparing-inferred-and-observed-likelihood-surfaces",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#comparing-inferred-and-observed-likelihood-surfaces",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Comparing inferred and observed likelihood surfaces",
    "text": "Comparing inferred and observed likelihood surfaces\nLet’s return once again to the toy dataset used in the last two posts, whose true parameters we know because we made them up; and also the log likelihood function:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nTo extract estimates of uncertainty about the uncertainty of each of these parameters, we used optim() with the options shown below, and then inverted the matrix to go from information to uncertainty.\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\nWe were especially interested in the first two rows and columns of this matrix, as they correspond to uncertainty in \\(\\beta = \\{ \\beta_0, \\beta_1 \\}\\).\n\n\nCode\ninv_hess_betas &lt;- inv_hess[1:2, 1:2]\n\ninv_hess_betas\n\n\n             [,1]         [,2]\n[1,]  0.033577455 -0.004379668\n[2,] -0.004379668  0.004379668\n\n\nBack in part five, we used this same dataset to show how the log likelihood varies for various, equally spaced, candidate values for \\(\\beta_0\\) and \\(\\beta_1\\) (having fixed \\(\\eta = \\exp({\\sigma^2})\\) at its true value). This led to the followng map of the landscape1\n\n\nCode\nlibrary(tidyverse)\ncandidate_param_values &lt;- expand_grid(\n    beta_0 = seq(-15, 15, by = 0.05),\n    beta_1 = seq(-15, 15, by = 0.05)\n)\n\nfeed_to_ll &lt;- function(b0, b1){\n    pars &lt;- c(b0, b1, log(0.25))\n    llNormal(pars, y, X)\n}\n\ncandidate_param_values &lt;- candidate_param_values |&gt;\n    mutate(\n        ll = map2_dbl(beta_0, beta_1, feed_to_ll)\n    )\n\ncandidate_param_values |&gt;\n    ggplot(aes(beta_0, beta_1, z = ll)) + \n    geom_contour_filled() + \n    geom_vline(xintercept = 0) +\n    geom_hline(yintercept = 0) +\n    labs(\n        title = \"Log likelihood as a function of possible values of beta_0 and beta_1\",\n        x = \"beta0 (the intercept)\",\n        y = \"beta1 (the slope)\"\n    )\n\n\n\n\n\nWithin the above we can see that the log likelihood landscape for these two parameters looks like a bivariate normal distribution, we can also see a bit of a slant in this normal distribution. This implies a correlation between the two candidate values. The direction of the slant is downwards from left to right, implying the correlation is negative.\nFirstly let’s check that the correlation between \\(\\beta_0\\) and \\(\\beta_1\\) implied by the Hessian is negative. These are the off-diagonal elements, either first row, second column, or second row, first column:\n\n\nCode\ninv_hess_betas[1,2]\n\n\n[1] -0.004379668\n\n\nCode\ninv_hess_betas[2,1]\n\n\n[1] -0.004379668\n\n\nYes they are!\nAs mentioned previously, the likelihood surface produced by the gridsearch method involves a lot of computations, so a lot of steps, and likely a lot of trial and error, if it were to be used to try to find the maximum likelihood value for the parameters. By contrast, the optim() algorithm typically involves far fewer steps, ‘feeling’ its way up the hill until it reaches a point where there’s nowhere higher. 2 When it then reaches this highest point, it then ‘feels’ the curvature around this point in multiple directions, producing the Hessian. The algorithm doesn’t see the likelihood surface, because it hasn’t travelled along most of it. But the Hessian can be used to infer the likelihood surface, subject to subject (usually) reasonable assumptions.\nWhat are these (usually) reasonable assumptions? Well, that the likelihood surface can be approximated by a multivariate normal distribution, which is a generalisation of the standard Normal distribution over more than one dimensions.3\nWe can use the mvrnorm function from the MASS package, alongside the point estimates and Hessian from optim, in order to produce estimates of \\(\\theta = \\{ \\beta_0, \\beta_1, \\eta \\}\\) which represent reasonable uncertainty about the true values of each of these parameters. Algebraically, this can be expressed as something like the following:\n\\[\n\\tilde{\\theta} \\sim Multivariate Normal(\\mu = \\dot{\\theta}, \\sigma^2 = \\Sigma)\n\\]\nWhere \\(\\dot{\\theta}\\) are the point estimates from optim() and \\(\\Sigma\\) is the implied variance-covariance matrix recovered from the Hessian.\nLet’s create this MVN model and see what kinds of outputs it produces.\n\n\nCode\nlibrary(MASS)\n\npoint_estimates &lt;- fuller_optim_output$par\n\nvcov &lt;- -solve(fuller_optim_output$hessian)\nparam_draws &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = point_estimates, \n    Sigma = vcov\n)\n\ncolnames(param_draws) &lt;- c(\n    \"beta0\", \"beta1\", \"eta\"\n)\n\nhead(param_draws)\n\n\n        beta0    beta1         eta\n[1,] 2.564978 1.375636 -0.30407255\n[2,] 2.440111 1.367774 -1.16815288\n[3,] 2.775332 1.338583 -0.05574937\n[4,] 2.283011 1.481799 -0.26095101\n[5,] 2.695635 1.228565 -1.18369341\n[6,] 2.686818 1.483601 -0.44262363\n\n\nWe can see that mvrnorm(), with these inputs from optim() produces three columns: one for each parameter being estimated \\(\\{ \\beta_0, \\beta_1, \\eta \\}\\). The n argumment indicates the number of draws to take; in this case, 10000. This number of draws makes it easier to see how much variation there is in each of the estimates.\n\n\nCode\ndf_param_draws &lt;- \nparam_draws |&gt;\n    as_tibble(\n        rownames = 'draw'\n    ) |&gt;\n    mutate(\n        sig2 = exp(eta)\n    ) |&gt;\n    pivot_longer(\n        -draw, \n        names_to = \"param\",\n        values_to = \"value\"\n    ) \n    \ndf_param_draws |&gt;\n    ggplot(aes(x = value)) + \n    geom_density() + \n    facet_grid(param ~ .) + \n    geom_vline(xintercept=0)\n\n\n\n\n\nThere are a number of things to note here: firstly, that the average of the \\(\\beta_0\\) and \\(\\beta_1\\) values appear close to their known ‘true’ values of 2.5 and 1.4 respectively. Secondly, that whereas the \\(\\eta\\) values are normally distributed, the \\(\\sigma^2\\) values derived from them are not, and are never below zero; this is the effect of the exponential link between quantities. Thirdly, that the implied values of \\(\\sigma^2\\) do appear to be centred around 0.25, as they should be as \\(\\sigma\\) was set to 0.50 in the model.\nAnd forthly, that the density around \\(\\beta_1\\) is more peaked than around \\(\\beta_0\\). This concords with what we saw previously in the filled contour map: both the horizontal beta0 axis and vertical beta1 axis are on the same scale, but the oval is broader along the horizontal axis than the vertical axis. This in effect implies that we have more information about the true value of \\(\\beta_1\\), the slope, than about the true value of \\(\\beta_0\\), the intercept.\nWe can also use these draws to reproduce something similar to, but not identical to, 4 the previous filled contour map:\n\n\nCode\n# param_draws |&gt;\n#     as_tibble(\n#         rownames = 'draw'\n#     ) |&gt;\n#     ggplot(aes(x = beta0, y = beta1)) + \n#     geom_point(alpha = 0.1) + \n#     coord_cartesian(xlim = c(-10, 10), ylim = c(-10, 10))\n\nparam_draws |&gt;\n    as_tibble(\n        rownames = 'draw'\n    ) |&gt;\n    ggplot(aes(x = beta0, y = beta1)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nOnce again, we see the same qualities as the contour map produced by interrogating the likelihood surface exhaustively: the distribution appears bivariate normal; there is a greater range in the distribution along the beta0 than the beta1 axis; and there is evidence of some negative correlation between the two parameters."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#summary",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#summary",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Summary",
    "text": "Summary\nThis post has shown how optim(), which in its vanilla state only returns point estimates, can be configured to also calculater and report the Hessian, a record of instantaneous curvature around the point estimates. Even without a fine-grained and exhausive search throughout the likelihood surface, this measure of curvature can be used to produce similar measures of uncertainty to the more exhausive approach, in a fraction of the number of computations.\nMore importantly, it can be used to generate draws of plausible combinations of parameter values, something denoted as \\(\\tilde{\\theta}\\) earlier. This is something especially useful for producing honest quantities of interest, which both tell users of models something they want to know, while also representing how uncertain we are in this knowledge.\nWe’ll cover that in the next post… 5"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#footnotes",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#footnotes",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’ve narrowed the space between values slightly, and increased the range of permutations of values to search through, for an even more precise recovery of the likelihood landscape.↩︎\nIn practice, the algorithm seeks to minimise the value returned by the function, not maximise it, hence the negative being applied through the argument fnscale = -1 in the control argument. But the principle is identical.↩︎\nThis means that, whereas the standard Normal returns a single output, the Multivariate Normal returns a vector of outputs, one for each parameter in \\(\\theta\\), which should also be the length of the diagonal (or alternatively either the number of rows or columns) of \\(\\Sigma\\).↩︎\nThe values will not be identical because the values for \\(\\eta\\), and so \\(\\sigma^2\\), have not been fixed at the true value in this example.↩︎\nI was expecting to cover it in the current post, but this is probably enough content for now!↩︎"
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#modelling-hamster-tooth-growth",
    "href": "pages/complete-simulation-example/index.html#modelling-hamster-tooth-growth",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Modelling Hamster Tooth Growth",
    "text": "Modelling Hamster Tooth Growth\nLet’s start with one of the built-in datasets, ToothGrowth, which is described as follows:\n\nThe response is the length of odontoblasts (cells responsible for tooth growth) in 60 guinea pigs. Each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods, orange juice or ascorbic acid (a form of vitamin C and coded as VC).\n\nLet’s load the dataset and visualise\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nWhat does it look like?\n\n\nCode\ndf |&gt;\n    ggplot(aes(y = len, x = dose, shape = supp, colour = supp)) + \n    geom_point() + \n    expand_limits(x = 0, y = 0)\n\n\n\n\n\nSo, although this has just three variables, there is some complexity involved in thinking about how the two predictor variables, supp and dose, relate to the response variable len. These include:\n\nWhether the relationship between len and dose is linear in a straightforward sense, or associated in a more complicated wway\nWhether supp has the same effect on len regardless of dose, or whether there is an interaction between dose and supp.\n\n\nStage One: model fitting\nWe can address each of these questions in turn, but should probably start with a model which includes both predictors:\n\n\nCode\nmod_01 &lt;- lm(len ~ dose + supp, data = df)\n\nsummary(mod_01)\n\n\n\nCall:\nlm(formula = len ~ dose + supp, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.600 -3.700  0.373  2.116  8.800 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.2725     1.2824   7.231 1.31e-09 ***\ndose          9.7636     0.8768  11.135 6.31e-16 ***\nsuppVC       -3.7000     1.0936  -3.383   0.0013 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.236 on 57 degrees of freedom\nMultiple R-squared:  0.7038,    Adjusted R-squared:  0.6934 \nF-statistic: 67.72 on 2 and 57 DF,  p-value: 8.716e-16\n\n\nEach term is statistically significant at the conventional thresholds (P &lt; 0.05), with higher doses associated with greater lengths. Compared to OJ, the reference category, a vitamin C (VC) supplement is associated with lower lengths.\nTurning to the first question, about the type of relationship between len and dose, one possibility is that greater doses lead to greater lengths, but there are diminishing marginal returns: the first mg has the biggest marginal effect, then the second mg has a lower marginal effect. An easy way to model this would be to include the log of dose in the regression model, rather than the dose itself.1 We can get a sense of whether this log dose specification might be preferred by plotting the data with a log scale on the x axis, and seeing if the points look like they ‘line up’ better:\n\n\nCode\ndf |&gt;\n    ggplot(aes(y = len, x = dose, shape = supp, colour = supp)) + \n    geom_point() + \n    scale_x_log10() + \n    expand_limits(x = 0.250, y = 0)\n\n\n\n\n\nYes, with this scaling, the points associated with the three dosage regimes look like they line up better. Let’s now build this model specification:\n\n\nCode\nmod_02 &lt;- lm(len ~ log(dose) + supp, data = df)\n\nsummary(mod_02)\n\n\n\nCall:\nlm(formula = len ~ log(dose) + supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2108 -2.9896 -0.5633  2.2842  9.1892 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.6633     0.7033   29.38  &lt; 2e-16 ***\nlog(dose)    11.1773     0.8788   12.72  &lt; 2e-16 ***\nsuppVC       -3.7000     0.9947   -3.72 0.000457 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.852 on 57 degrees of freedom\nMultiple R-squared:  0.755, Adjusted R-squared:  0.7464 \nF-statistic: 87.81 on 2 and 57 DF,  p-value: &lt; 2.2e-16\n\n\nAgain, the same kind of relationship between variables is observed: higher log dose: greater length; and VC rather than OJ is associated with lower growth. But is this model actually any better? The model summary for the linear dose model gives an adjusted \\(R^2\\) of 0.69, whereas for the log dose model the adjusted \\(R^2\\) is 0.75. So, as the data are fundamentally the same,2 this suggests it is. However, as we know that linear regression models are really just another kind of generalised linear models, and that model fitting tends to involve trying to maximise the log likelihood, we can also compare the log likelihoods of the two models, using the logLik() function, and so which is higher:\n\n\nCode\nlogLik(mod_01)\n\n\n'log Lik.' -170.2078 (df=4)\n\n\nCode\nlogLik(mod_02)\n\n\n'log Lik.' -164.5183 (df=4)\n\n\nBoth report the same number of degrees of freedom (‘df’), which shouldn’t be suprising as they involve the same number of parameters. But the log likelihood for mod_02 is higher, which like the Adjusted R-squared metric suggests a better fit.\nAnother approach, which generalises better to other types of model, is to compare the AICs, which are metrics that try to show the trade off between model complexity (based on number of parameters), and model fit (based on the log likelihood). By this criterion, the lower the score, the better the model:\n\n\nCode\nAIC(mod_01, mod_02)\n\n\n       df      AIC\nmod_01  4 348.4155\nmod_02  4 337.0367\n\n\nAs both models have exactly the same number of parameters, it should be of no surprise that mod_02 is still preferred.\nLet’s now address the second question: is there an interaction between dose and supp. This interaction term can be specified in one of two ways:\n\n\nCode\n# add interaction term explicitly, using the : symbol\nmod_03a &lt;- lm(len ~ log(dose) + supp + log(dose) : supp, data = df)\n\n# add interaction term implicitly, using the * symbol \nmod_03b &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(mod_03a)\n\n\n\nCall:\nlm(formula = len ~ log(dose) + supp + log(dose):supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(mod_03b)\n\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nWe can see from the summaries that both ways of specifying the models lead to exactly the same model, with exactly the same estimates, standared errors, adjusted \\(R^2\\)s, and so on. The adjusted \\(R^2\\) is now 0.76, a slight improvement on the 0.75 value for the model without the interaction term. As before, we can also compare the trade-off between additional complexity and improved fit using AIC\n\n\nCode\nAIC(mod_02, mod_03a)\n\n\n        df      AIC\nmod_02   4 337.0367\nmod_03a  5 333.7750\n\n\nSo, the AIC of the more complex model is lower, suggesting a better model, but the additional improvement in fit is small.\nWe can also compare the fit, and answer the question of whether the two models can be compared, in a couple of other ways. Firstly, we can use BIC, AIC’s (usually) stricter cousin, which tends to penalise model complexity more harshly:\n\n\nCode\nBIC(mod_02, mod_03a)\n\n\n        df      BIC\nmod_02   4 345.4140\nmod_03a  5 344.2467\n\n\nEven using BIC, the more complex model is still preferred, though the difference in values is now much smaller.\nThe other way we can compare the models is using an F-test using the anova (analysis of variance) function:\n\n\nCode\nanova(mod_02, mod_03a)\n\n\nAnalysis of Variance Table\n\nModel 1: len ~ log(dose) + supp\nModel 2: len ~ log(dose) + supp + log(dose):supp\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     57 845.91                              \n2     56 774.89  1    71.022 5.1327 0.02737 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere anova compares the two models, notes that the first model can be understood as a restricted variant of the second model,3 and compares the change in model fit between the two models against the change in number of parameters used to fit the model. The key parts of the summary to look at are the F test value, 5.13, and the associated P value, which is between 0.01 and 0.05. This, again, suggests the interaction term is worth keeping.\nSo, after all that, we finally have a fitted model. Let’s look now at making some predictions from it.\n\n\nStage Two: Model predictions\nThe simplest approach to getting model predictions is to use the predict function, passing it a dataframe of values for which we want predictions:\n\n\nCode\npredictor_df &lt;- expand_grid(\n    supp = c('VC', 'OJ'), \n    dose = seq(0.25, 2.25, by = 0.01)\n)\npreds_predictors_df &lt;- predictor_df |&gt;\n    mutate(pred_len = predict(mod_03a, predictor_df))\n\npreds_predictors_df\n\n\n# A tibble: 402 × 3\n   supp   dose pred_len\n   &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 VC     0.25   -1.20 \n 2 VC     0.26   -0.683\n 3 VC     0.27   -0.189\n 4 VC     0.28    0.288\n 5 VC     0.29    0.748\n 6 VC     0.3     1.19 \n 7 VC     0.31    1.62 \n 8 VC     0.32    2.04 \n 9 VC     0.33    2.44 \n10 VC     0.34    2.83 \n# ℹ 392 more rows\n\n\nWe can visualise these predictions as follows, with the predicted values as lines, and the observed values as points:\n\n\nCode\npreds_predictors_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, y = pred_len, colour = supp, linetype = interextrap)) + \n    geom_line() + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\"\n    )\n\n\n\n\n\nIn the above, I’ve shown the lines as solid when they represent interpolations of the data, i.e. are in the range of measured doses, and as dashed when they represent extrapolations from the data, meaning they are are predictions made outside the range of observed values. We can see an obvious issue when we extrapolate too far to the left: for low doses, and for the VC supplement, the model predicts negative tooth lengths. Extrapolation is dangerous! And gets more dangerous the further we extrapolate from available observations.\nWe can also use the predict function to produce uncertainty intervals, either of expected values, or predicted values. By default these are 95% intervals, meaning they are expected to contain 95% of the range of expected or predicted values from the model.\nLet’s first look at expected values, which include uncertainty about parameter estimates, but not observed variation in outcomes:\n\n\nCode\ndf_pred_intvl &lt;- predict(mod_03a, newdata = predictor_df, interval = \"confidence\")\n\npreds_predictors_intervals_df &lt;- \n    bind_cols(predictor_df, df_pred_intvl)\n\npreds_predictors_intervals_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, linetype = interextrap)) + \n    geom_line(aes(y = fit, colour = supp)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = supp), alpha = 0.2) + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\",\n        subtitle = \"Range of expected values\"\n    )\n\n\n\n\n\nAnd the following shows the equivalent prediction intervals, which also incorporate known variance, as well as parameter uncertainty:\n\n\nCode\ndf_pred_intvl &lt;- predict(mod_03a, newdata = predictor_df, interval = \"prediction\")\n\npreds_predictors_intervals_df &lt;- \n    bind_cols(predictor_df, df_pred_intvl)\n\npreds_predictors_intervals_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, linetype = interextrap)) + \n    geom_line(aes(y = fit, colour = supp)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = supp), alpha = 0.2) + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\",\n        subtitle = \"Range of predicted values\"\n    )\n\n\n\n\n\nAs should be clear from the above, and discussion of the difference between expected and predicted values in previous posts, predicted values and expected values are very different, and it is important to be aware of the difference between these two quantities of interest. Regardless, we can see once again how dangerous it is to use this particular model specification to extrapolate beyond the range of observations, expecially for lower doses."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#a-lower-level-example",
    "href": "pages/complete-simulation-example/index.html#a-lower-level-example",
    "title": "Statistical Simulation: A Complete Example",
    "section": "A lower level example",
    "text": "A lower level example\nWe’re now going to use the same hamster tooth growth model we used previously, but do some slightly more complicated things with it, relying a bit less on convenience functions and a bit more on our understanding of some of the fundamentals from Section Two.\n\nRecap of core concepts\nBack in Section Two we stated that estimates of the cloud of uncertainty in model parameters, that results from having limited numbers of observations in the data, can be represented as:\n\\[\n\\tilde{\\theta} \\sim MVN(\\mu = \\dot{\\theta}, \\sigma^2 = \\Sigma)\n\\]\nWhere MVN means multivariate normal, and needs the two quantities \\(\\dot{\\theta}\\) and \\(\\Sigma\\) as parameters.\nPreviously we showed how to extract (estimates of) these two quantities from optim(), where the first quantity, \\(\\dot{\\theta}\\), was taken from the converged parameter point estimate slot par, and the second quantity, \\(\\Sigma\\), was derived from the hessian slot.\nBut we don’t need to use optim() directly in order to recover these quantities. Instead we can get them from the standard model objects produced by either lm() or glm(). Let’s check this out…\nWith the model developed previously, let’s now look at some convenience functions, other than just summary, that work with lm() and glm() objects, and recover the quantities required from MVN to represent the uncertainty cloud.\n\n\nExtracting quantities for modelling uncertainty\nFirstly, for the point estimates \\(\\dot{\\theta}\\), we can use the coefficients() function\n\n\nCode\n## Building our model \n\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nCode\nbest_model &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(best_model)\n\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nCode\ncoef &lt;- coefficients(best_model)\n\ncoef\n\n\n     (Intercept)        log(dose)           suppVC log(dose):suppVC \n       20.663333         9.254889        -3.700000         3.844782 \n\n\nAnd for the variance-covariance matrix, for representing joint uncertainty about the above estimates, we can use the vcov function\n\n\nCode\nSig &lt;- vcov(best_model)\n\nSig\n\n\n                   (Intercept)     log(dose)        suppVC log(dose):suppVC\n(Intercept)       4.612422e-01 -8.768056e-17 -4.612422e-01    -7.224251e-17\nlog(dose)        -8.768056e-17  1.440023e+00  1.753611e-16    -1.440023e+00\nsuppVC           -4.612422e-01  1.753611e-16  9.224843e-01     1.748938e-16\nlog(dose):suppVC -7.224251e-17 -1.440023e+00  1.748938e-16     2.880045e+00\n\n\nFinally, we can extract the point estimate for stochastic variation in the model, i.e. variation assumed by the model even if parameter uncertainty were minimised, using the sigma function:\n\n\nCode\nsig &lt;- sigma(best_model)\n\nsig\n\n\n[1] 3.719847\n\n\nWe now have three quantities, coef, Sig and sig (note the upper and lower case s in the above). These provide something almost but not exactly equivalent to the contents of par and that derived from hessian when using optim() previously. The section below explains this distinction in more detail.\n\nBack to the weeds (potentially skippable)\nRecall the ‘grandmother formulae’, from King, Tomz, and Wittenberg (2000), which the first few posts in this series started with:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nFor standard linear regression this becomes:\nStochastic Component\n\\[\nY_i \\sim Norm(\\theta_i, \\sigma^2)\n\\]\nSystematic Component\n\\[\n\\theta_i =X_i \\beta\n\\]\nOur main parameters are \\(\\theta\\), which combined our predictors \\(X_i\\) and our model parameter estimates \\(\\beta\\). Of these two components we know the data - they are what they are - but are merely estimating our model parameters \\(\\beta\\). So, any estimation uncertainty in this part of the equation results from \\(\\beta\\) alone.\nOur ancillary parameter is \\(\\sigma^2\\). This is our estimate of how much fundamental variation there is in how the data (the response variables \\(Y\\)) is drawn from the stochastic data generating process.\nWhen we used optim() directly, we estimated \\(\\sigma^2\\) along with the other \\(\\beta\\) parameters, via the \\(\\eta\\) parameter eta, defined as \\(\\sigma^2 = e^{\\eta}\\) to allow optim() to search over an unbounded real number range. If there are k \\(\\beta\\) parameters, therefore, optim()’s par vector contained k + 1 values, with this last value being the point estimate for the eta parameter. Similarly, the number of rows, columns, and length of diagonal elements in the variance-covariance matrix recoverable through optim’s hessian slot was also k + 1 rather than k, with the last row, last column, and last diagonal element being measures of covariance between \\(\\eta\\) and the \\(\\beta\\) elements, and variance in \\(\\eta\\) itself.\nBy contrast, the length of coefficients returned by coefficients(best_model) is k, the number of \\(\\beta\\) parameters being estimated, and the dimensions of vcov(best_model) returned are also k by k.\nThis means there is one fewer piece/type of information about model parameters returned by coefficients(model), vcov(model) and sigma(model) than was potentially recoverable by optim()’s par and hessian parameter slots: namely, uncertainty about the true value of the ancillary parameter \\(\\sigma^2\\). The following table summarises this difference:\n\n\n\n\n\n\n\n\nInformation type\nvia optim\nvia lm and glm\n\n\n\n\nMain parameters: point\nfirst k elements of par\ncoefficients() function\n\n\nMain parameters: uncertainty\nfirst k rows and columns of hessian\nvcov() function\n\n\nAncillary parameters: point\nk+1th through to last element of par\nsigma() function or equivalent for glm()\n\n\nAncillary parameters: uncertainty\nlast columns and rows of hessian (after rows and columns k)\n—\n\n\n\nSo long as capturing uncertainty about the fundamental variability in the stochastic part of the model isn’t critical to our predictions then omission of a measure of uncertainty in the ancillary parameters \\(\\alpha\\) is likely a price worth paying for the additional convenience of being able to use the model objects directly. However we should be aware that, whereas with optim we potentially have both \\(\\tilde{\\beta}\\) and \\(\\tilde{\\alpha}\\) to represent model uncertainty, when using the three convenience functions coefficients(), vcov() and sigma() we technically ‘only’ have \\(\\tilde{\\beta}\\) and \\(\\dot{\\alpha}\\) (i.e. point estimates alone for the ancillary parameters).\nWith the above caveat in mind, let’s now look at using the results of coefficients(), vcov() and sigma() to generate (mostly) honest representations of expected values, predicted values, and first differences\n\n\n\nModel predictions\nAs covered in section two, we can use the mvrnorm function from the MASS package to create \\(\\tilde{\\beta}\\), our parameter estimates with uncertainty:\n\nParameter simulation\n\n\nCode\nbeta_tilde &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = coef, \n    Sigma = Sig\n)\n\nhead(beta_tilde)\n\n\n     (Intercept) log(dose)    suppVC log(dose):suppVC\n[1,]    21.61503  9.809988 -3.433280         4.228822\n[2,]    20.42343 10.143190 -2.631949         2.273298\n[3,]    20.95245  9.141178 -2.161788         5.508275\n[4,]    21.63064  9.986450 -4.718762         3.082064\n[5,]    20.64959 10.056879 -3.779049         1.106520\n[6,]    21.60778  9.693264 -4.229600         2.862196\n\n\nLet’s first look at each of these parameters individually:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    pivot_longer(everything(), names_to = \"coefficient\", values_to = \"value\") |&gt; \n    ggplot(aes(x = value)) + \n    facet_grid(coefficient ~ .) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nNow let’s look at a couple of coefficients jointly, to see how they’re correlated. Firstly the association between the intercept and the log dosage:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `(Intercept)`, y = `log(dose)`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nHere the covariance between the two parameters appears very low. Now let’s look at how log dosage and Vitamin C supplement factor are associated:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `log(dose)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nAgain, the covariance appears low. Finally, the association between log dose and the interaction term\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `log(dose)`, y = `log(dose):suppVC`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nHere we have a much stronger negative covariance between the two coefficients. Let’s look at the variance-covariance extracted from the model previously to confirm this:\n\n\nCode\nknitr::kable(Sig)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\nlog(dose)\nsuppVC\nlog(dose):suppVC\n\n\n\n\n(Intercept)\n0.4612422\n0.000000\n-0.4612422\n0.000000\n\n\nlog(dose)\n0.0000000\n1.440023\n0.0000000\n-1.440023\n\n\nsuppVC\n-0.4612422\n0.000000\n0.9224843\n0.000000\n\n\nlog(dose):suppVC\n0.0000000\n-1.440023\n0.0000000\n2.880045\n\n\n\n\n\nHere we can see that the covariance between intercept and log dose is effectively zero, as is the covariance between the intercept and the interaction term, and the covariance between the log(dose) and suppVC factor. However, there is a negative covariance between log dose and the interaction term, i.e. what we have plotted above, and also between the intercept and the VC factor. For completeness, let’s look at this last assocation, which we expect to show negative association:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `(Intercept)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nYes it is! The parameter estimates follow the covariance provided by Sigma, as we would expect.\n\n\n\nExpected values\nLet’s stay we are initially interested in the expected values for a dosage of 1.25mg, with the OJ (rather than VC) supplement:\n\n\nCode\n# first element is 1 due to intercept\npredictor &lt;- c(1, log(1.25), 0, 0) \n\npredictions_ev &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor %*% this_beta\n)\n\nhead(predictions_ev)\n\n\n[1] 23.80406 22.68681 22.99225 23.85905 22.89371 23.77077\n\n\nLet’s now get a 95% credible interval:\n\n\nCode\nquantile(predictions_ev, probs = c(0.025, 0.500, 0.975))\n\n\n    2.5%      50%    97.5% \n21.27584 22.71071 24.17225 \n\n\nSo, the 95% interval for the expected value is between 21.31 and 24.14, with a middle (median) estimate of 22.73.4 Let’s check this against estimates from the predict() function:\n\n\nCode\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'confidence')\n\n\n      fit      lwr      upr\n1 22.7285 21.26607 24.19093\n\n\nThe expected values using the predict function give a 95% confidence interval of 21.27 to 24.19, with a point estimate of 22.73. These are not identical, as the methods employed are not identical,5 but they are hopefully similar enough to demonstrate they are attempts at getting at the same quantities of interest.\n\n\nPredicted values\nPredicted values also include inherent stochastic variation from the ancillary parameters \\(\\alpha\\), which for linear regression is \\(\\sigma^2\\). We can simply add these only the expected values above to produce predicted values:\n\n\nCode\nn &lt;- length(predictions_ev)\n\nshoogliness &lt;- rnorm(n=n, mean = 0, sd = sig)\n\npredictions_pv &lt;- predictions_ev + shoogliness\n\n\nhead(predictions_pv)\n\n\n[1] 21.83280 24.55774 19.38543 23.78526 26.18489 21.07653\n\n\nLet’s get the 95% interval from the above using quantile\n\n\nCode\nquantile(predictions_pv, probs = c(0.025, 0.5000, 0.975))\n\n\n    2.5%      50%    97.5% \n15.23557 22.69762 30.06805 \n\n\nAs expected, the interval is now much wider, with a 95% interval from 15.34 to 30.11. The central estimate should in theory, with an infinite number of runs, be the same, however because of random variation it will never be exactly the same to an arbitrary number of decimal places. In this case, the middle estimate is 22.75, not identical to the central estimate from the expected values distribution of 22.72. The number of simulations can always be increased to produce greater precision if needed.\nLet’s now compare this with the prediction interval produce by the predict function:\n\n\nCode\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'prediction')\n\n\n      fit      lwr     upr\n1 22.7285 15.13461 30.3224\n\n\nAgain, the interval estimates are not exactly the same, but they are very similar.\n\n\nFirst differences\nIt’s in the production of estimates of first differences - this, compared to that, holding all else constant - that the simulation approach shines for producing estimates with credible uncertainty. In our case, let’s say we are interested in asking:\n\nWhat is the expected effect of using the VC supplement, rather than the OJ supplement, where the dose is 1.25mg?\n\nSo, the first difference is from switching from OJ to VC, holding the other factor constant.\nWe can answer this question by using the same selection of \\(\\tilde{\\beta}\\) draws, but passing two different scenarios:\n\n\nCode\n#scenario 0: supplement is OJ\npredictor_x0 &lt;- c(1, log(1.25), 0, 0) \n\n#scenario 1: supplement is VC\npredictor_x1 &lt;- c(1, log(1.25), 1, 1 * log(1.25)) \n\n\npredictions_ev_x0 &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x0 %*% this_beta\n)\n\npredictions_ev_x1 &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x1 %*% this_beta\n)\n\npredictions_df &lt;- \n    tibble(\n        x0 = predictions_ev_x0,\n        x1 = predictions_ev_x1\n    ) |&gt;\n    mutate(\n        fd = x1 - x0\n    )\n\npredictions_df\n\n\n# A tibble: 10,000 × 3\n      x0    x1     fd\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  23.8  21.3 -2.49 \n 2  22.7  20.6 -2.12 \n 3  23.0  22.1 -0.933\n 4  23.9  19.8 -4.03 \n 5  22.9  19.4 -3.53 \n 6  23.8  20.2 -3.59 \n 7  23.5  20.1 -3.42 \n 8  22.7  20.7 -2.01 \n 9  23.5  20.4 -3.09 \n10  23.5  20.0 -3.47 \n# ℹ 9,990 more rows\n\n\nLet’s look at the distribution of both scenarios individually:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario != \"fd\") |&gt;\n    ggplot(aes(x = estimate)) + \n    facet_wrap(~scenario, ncol = 1) + \n    geom_histogram(bins = 100)\n\n\n\n\n\nAnd the distribution of the pairwise differences between them:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario == \"fd\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nIt’s this last distribution which shows our first differences, i.e. our answer, hedged with an appropriate dose of uncertainty, to the specific question shown above. We can get a 95% interval of the first difference as follows:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario == \"fd\") |&gt; \n    pull('estimate') |&gt;\n    quantile(probs = c(0.025, 0.500, 0.975))\n\n\n      2.5%        50%      97.5% \n-4.8337957 -2.8331646 -0.8278675 \n\n\nSo, 95% of estimates of the first difference are between -4.85 and -0.81, with the middle of this distribution (on this occasion) being -2.83.\nUnlike with the expected values and predicted values, the predict() function does not return first differences with honest uncertainty in this way. What we have above is something new.\n\n\nSummary\nIn this subsection we’ve finally combined all the learning we’ve developed over the two previous sectionsto answer three specific ‘what if?’ questions: one on expected values, one on predicted values, and one on first differences. These are what King, Tomz, and Wittenberg (2000) refer to as quantities of interest, and I hope you agree these are more organic and reasonable types of question to ask of data and statistical models than simply looking at coefficients and p-values and reporting which ones are ‘statistically significant’.\nIf you’ve been able to follow everything in these posts, and can generalise the approach shown above to other types of statistical model, then congratulations! You’ve learned the framework for answering meaningful questions using statistical models which is at the heart of one of the toughest methods courses for social scientists offered by one of the most prestigious universities in the world."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#introducing-bayesian-statistics-on-marbles-and-jumping-beans",
    "href": "pages/complete-simulation-example/index.html#introducing-bayesian-statistics-on-marbles-and-jumping-beans",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Introducing Bayesian Statistics: On marbles and jumping beans",
    "text": "Introducing Bayesian Statistics: On marbles and jumping beans\nSection Two introduced Bayes’ Rule and the Likelihood axiom. It pointed out that, at heart, Bayes’ Rule is a way of expressing that given this in terms of this given that; and that Likelihood is also a claim about how that given this relates to this given that. More specifically, the claim of Likelihood is:\n\nThe likelihood of the model given the data is proportional to the probability of the data given the model.\n\nThere are two aspects to the model: firstly its structure; secondly its parameters. The structure includes the type of statistical model - whether it is a standard linear regression, negative binomial regression, logistic regression, Poisson regression model and so on - and also the specific types of columns from the dataset selected as either predictor variables (\\(X\\)) or response variables (\\(Y\\)). It is only after both the higher level structure of the model family, and the lower level structure of the data inputs (what’s being regressed on what?) have been decided that the Likelihood theory is used.\nAnd how is Likelihood theory used? Well, it defines a landscape over which an algorithm searches. This landscape has as many dimensions as there are parameters to fit. Where there are just two parameters, \\(\\beta_0\\) and \\(\\beta_1\\) to fit, we can visualise this landscape using something like a contour plot, with \\(\\beta_0\\) as latitude, \\(\\beta_1\\) as longitude, and the likelihood at this position its elevation or depth. Each possible joint value \\(\\beta = \\{\\beta_0, \\beta_1\\}\\) which the algorithm might wish to propose leads to a different long-lat coordinate over the surface, and each coordinate has a different elevation or depth. Although we can’t see beyond three dimensions (latitude, longitude, and elevation/depth), mathematics has no problem extending the concept of multidimensional space into far more dimensions than we can see or meaningfully comprehenend. If a model has ten parameters to fit, for example, the likelihood search space really is ten dimensional, and so on.\nNoticed I used elevation and depth interchangably in the description above. Well, this is because it really doesn’t matter whether an optimisation algorithm is trying to find the greatest elevation over a surface, or the greatest depth over the surface. The aim of maximum likelihood estimation is to find the configuration of parameters that maximises the likelihood, i.e. finds the top of the surface. However we saw that when passing the likelihood function to optim() we often inverted the function by multiplying it by -1. This is because the optimisation algorithms themselves seek to minimise the objective function they’re passed, not maximise it. By multiplying the likelihood function by -1 we made what we were trying to seek compatible with what the optimisation algorithms seek to do: find the greatest depth over a surface, rather than the highest elevation over the surface.\nTo make this all a bit less abstract let’s develop the intuition of an algorithm that seeks to minimise a function by way of a(nother) weird little story:\n\nImagine there is a landscape made out of transparent perspex. It’s not just transparent, it’s invisible to the naked eye. And you want to know where the lowest point of this surface is. All you have to do this is a magical leaking marble. The marble is just like any other marble, except every few moments, at regular intervals (say every tenth of a second), it dribbles out a white dye that you can see. And this dye sticks on and stains the otherwise invisible landscape whose lowest point you wish to find.\n\n\nNow, you drop the marble somewhere on the surface. You see the first point it hits on the surface - a white blob appears. The second blob appears some distance away from the first blob; and the third blob slightly less far away from the second blob as the second was to the second. After a few seconds, a trail of white spots is visible, the first few of which form something like a straight line, each consecutive point slightly less closer to the previous one. A second or two later, and the rumbling sounds of the marble rolling over the surface cease; the marble has clearly run out of momentum. And as you look at the trail of dots it’s generated, and is still generating, and you see it keeps highlighting the same point on the otherwise invisible surface, again and again.\n\nPreviously I used the analogy of a magical robo-chauffer, taking you to the top of a landscape. But the falling marble is probably a closer analogy to how many of optim()’s algorithms actually work. Using gravity and its shape alone, it finds the lowest point on the surface, and with its magical leaking dye, it tells you where this lowest point is.\nNow let’s extend the story to convert the analogy of the barefoot-and-blind person from section two as well:\n\nThe marble has now ‘told’ you where the lowest point on the invisible surface is. However you also want to know more about the shape of the depression it’s in. You want to know if it’s a steep depression, or a shallow depression. And you want to know if it’s as steep or shallow in every direction, or if it’s steeper in some ways than the other.\n\n\nSo you now have to do a bit more work. You move your hand to just above the marble, and with your forefinger ‘flick’ it in a particular direction (say east-west): you see it move in the direction you flick it briefly, before rolling back towards (and beyond, and then towards) the depression point. As it does so, it leaks dye onto the surface, revealing a bit more about the landscape’s steepness or shallowness in this dimension. Then you do the same, but along a different dimension (say, north-south). After you’ve done this enough times, you are left with a collection of dyed points on the part of the surface closest to its deepest depression. The spacing and shape of these points tells you something about the nature of the depression and the part of the landscape it’s surrounding.\n\nNotice in this analogy you had to do extra work to get the marble to reveal more information about the surface. By default, the marble tells you the specific location of the depression, but not what the surface is like around this point. Instead, you need to intervene twice: firstly by dropping the marble onto the surface; secondly by flicking it around once it’s reached the lowest point on the surface.\nNow, let’s imagine swapping out our magical leaking marble for something even weirder: a magical leaking jumping bean.\n\nThe magical jumping bean does two things: it leaks and it jumps. (Okay, it does three things: when it leaks it also sticks to the surface it’s dying). When the bean is first dropped onto the surface, it marks the location it lands on. Then, it jumps up and across in a random direction. After jumping, it drops onto another part of the surface, marks it, and the process starts again. Jumping, sticking, marking; jumping, sticking, marking; jumping, sticking, marking… potentially forever.\n\n\nBecause of the effect of gravity, though the jumping bean jumps in a random direction, after a few jump-stick-mark steps it’s still, like the marble, very likely to move towards the depression. However, unlike the marble, even when it gets towards the lowest point in the depression, it’s not going to just rest there. The magical jumping bean is never at rest. It’s forever jump-stick-marking, jump-stick-marking.\n\n\nHowever, once the magical bean has moved towards the depression, though it keeps moving, it’s likely never to move too far from the depression. Instead, it’s likely to bounce around the depression. And as it does so, it drops ever more marks on the surface, which keep showing what the surface looks like around the depression in ever more detail.\n\nSo, because of the behaviour of the jumping bean, you only have to act on it once, by choosing where to drop it, rather than twice as with the marble: first choosing where to drop it, then flicking it around once it’s reached the lowest point on the surface.\n\nSo what?\nIn the analogies above, the marble is to frequentist statistics as the jumping bean is to Bayesian statistics. A technical distinction between the marble and the jumping bean is that the marble converges towards a point (meaning it reaches a point of rest on the surface) whereas the jumping bean converges towards a distribution (meaning it never rests).\nIt’s Bayesian statistics’ 6 property of converging to a distribution rather than a point that makes the converged posterior distribution of parameter estimates Bayesian models produce ideal for the kind of honest prediction so much of this blog series has been focused on.\nLet’s now do some Bayesian modelling to compare…\n\n\nBayesian modelling: now significantly less terrifying than it used to be\nThere are a lot of packages and approaches for building Bayesian models. In fact there are whole statistical programming languages - like JAGS, BUGS 7 and Stan - dedicated to precisely describing every assumption the statistician wants to make about how a Bayesian model should be built. For more complicated and bespoke models these are ideal.\nHowever there are also an increasingly large number of Bayesian modelling packages that abstract away some of the assumptions and complexity apparent in the above specialised Bayesian modelling languages, and allow Bayesian versions of the kinds of model we’re already familiar with to be specified using formulae interfaces almost identical to what we’ve already worked with. Let’s look at one of them, rstanarm, which allows us to use stan, a full Bayesian statistical programming language, without quite as much thinking and set-up being required on our part.\nLet’s try to use this to build a Bayesian equivalent of the hamster tooth model we worked on in the last couple of posts.\n\n\nData Preparation and Frequentist modelling\nLet’s start by getting the dataset and building the frequentist version of the model we’re already familiar with:\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nCode\nbest_model_frequentist &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(best_model_frequentist)\n\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nBuilding the Bayesian equivalent\nNow how would we build a Bayesian equivalent of this? Firstly let’s load (and if necessary install8) rstanarm.\n\n\nCode\nlibrary(rstanarm)\n\n\nWhereas for the frequentist model we used the function lm(), rstanarm has what looks like a broadly equivalent function stan_lm(). However, as I’ve just discovered, it’s actually more straightforward with stan_glm instead:\n\n\nCode\nbest_model_bayesian &lt;- stan_glm(len ~ log(dose) * supp, data = df)\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.35 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.032 seconds (Warm-up)\nChain 1:                0.033 seconds (Sampling)\nChain 1:                0.065 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.031 seconds (Warm-up)\nChain 2:                0.026 seconds (Sampling)\nChain 2:                0.057 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 5e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.032 seconds (Warm-up)\nChain 3:                0.031 seconds (Sampling)\nChain 3:                0.063 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 4e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.029 seconds (Warm-up)\nChain 4:                0.029 seconds (Sampling)\nChain 4:                0.058 seconds (Total)\nChain 4: \n\n\nCode\nsummary(best_model_bayesian)\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      len ~ log(dose) * supp\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 60\n predictors:   4\n\nEstimates:\n                   mean   sd   10%   50%   90%\n(Intercept)      20.6    0.7 19.8  20.7  21.5 \nlog(dose)         9.3    1.2  7.7   9.3  10.8 \nsuppVC           -3.7    1.0 -4.9  -3.7  -2.4 \nlog(dose):suppVC  3.8    1.7  1.7   3.8   6.0 \nsigma             3.8    0.4  3.3   3.7   4.2 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 18.8    0.7 18.0  18.8  19.6 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                 mcse Rhat n_eff\n(Intercept)      0.0  1.0  3815 \nlog(dose)        0.0  1.0  2299 \nsuppVC           0.0  1.0  3644 \nlog(dose):suppVC 0.0  1.0  2636 \nsigma            0.0  1.0  3249 \nmean_PPD         0.0  1.0  3821 \nlog-posterior    0.0  1.0  1908 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nSome parts of the summary for the Bayesian model look fairly familiar compared with the frequentist model summary; other bits a lot more exotic. We’ll skip over a detailed discussion of these outputs for now, though it is worth comparing the estimates section of the summary directly above, from the Bayesian approach, with the frequentist model produced earlier.\nThe frequentist model had point estimates of \\(\\{20.7, 9.3, -3.7, 3.8\\}\\). The analogous section of the Bayesian model summary is the mean column of the estimates section. These are reported to fewer decimal places by default - Bayesians are often more mindful of spurious precision - but are also \\(\\{20.7, 9.3, -3.7, 3.8\\}\\), so the same to this number of decimal places.\nNote also the Bayesian model reports an estimate for an additional parameter, sigma. This should be expected if we followed along with some of the examples using optim() for linear regression: the likelihood function required the ancillary parameters (referred to as \\(\\alpha\\) in the ‘mother model’ which this series started with, and part of the stochastic component \\(f(.)\\)) be estimated as well as the primary model parameters (referred to as \\(\\beta\\) in the ‘mother model’, and part of the systematic component \\(g(.)\\)). The Bayesian model’s coefficients (Intercept), log(dose), suppVC and the interaction term log(dose):suppVC are all part of \\(\\beta\\), whereas the sigma parameter is part of \\(\\alpha\\). The Bayesian model has just been more explicit about exactly which parameters it’s estimated from the data.\nFor the \\(\\beta\\) parameters, the Std. Error column in the Frequentist model summary is broadly comparable with the sd column in the Bayesian model summary. For the \\(\\beta\\) parameters these values are \\(\\{0.7, 1.2, 1.0, 1.7\\}\\) in the Frequentist model, and \\(\\{0.7, 1.2, 1.0, 1.7\\}\\) in the Bayesian model the summary. i.e. they’re the same to the degree of precision offered in the Bayesian model summary.\nBut let’s get to the crux of the argument: with Bayesian models honest predictions are easier.\nAnd they are, with the posterior_predict() function, passing what we want to predict on through the newdata argument, much as we did with the predict() function with frequentist models.\n\n\nScenario modelling\nLet’s recall the scenarios we looked at previously:\n\npredicted and expected values: length when dosage is 1.25mg and supplement is OJ\nfirst difference difference between OJ and VC supplement when dosage is 1.25mg\n\nLet’s start with the first question:\n\n\nCode\npredictors &lt;- data.frame(supp = \"OJ\", dose = 1.25)\n\npredictions &lt;- rstanarm::posterior_predict(\n    best_model_bayesian,\n    newdata = predictors\n)\n\nhead(predictions)\n\n\n            1\n[1,] 19.61122\n[2,] 29.06774\n[3,] 25.37852\n[4,] 24.62945\n[5,] 28.64383\n[6,] 24.33114\n\n\nCode\ndim(predictions)\n\n\n[1] 4000    1\n\n\nBy default posterior_predict() returns a matrix, which in this case has 4000 rows and just a single column. Let’s do a little work on this and visualise the distribution of estimates it produces:\n\n\nCode\npreds_df &lt;- tibble(estimate = predictions[,1])\n\n# lower, median, upper\nlmu &lt;- quantile(preds_df$estimate, c(0.025, 0.500, 0.975))\n\nlwr &lt;- lmu[1]\nmed &lt;- lmu[2]\nupr &lt;- lmu[3]\n\npreds_df |&gt;\n    mutate(\n        in_range = between(estimate, lwr, upr)\n    ) |&gt;\n    ggplot(aes(x = estimate, fill = in_range)) + \n    geom_histogram(bins = 100) + \n    scale_fill_manual(\n        values = c(`FALSE` = 'lightgray', `TRUE` = 'darkgray')\n    ) +\n    theme(legend.position = \"none\") + \n    geom_vline(xintercept = med, linewidth = 1.2, colour = \"steelblue\")\n\n\n\n\n\nThe darker-shaded parts of the histogram show the 95% uncertainty interval, and the blue vertical line the median estimate. This 95% interval range is 14.93 to 30.14.\nRemember we previously estimated both the expected values and the predicted values for this condition. Our 95% range for the expected values were 20.27 to 24.19 (or thereabouts), whereas our 95% range for the predicted values were (by design) wider, at 15.34 to 30.11. The 95% uncertainty interval above is therefore of predicted values, which include fundamental variation due to the ancillary parameters \\(\\sigma\\), rather than expected values, which result from parameter uncertainty alone.\nThere are a couple of other functions in rstanarm we can look at: predictive_error() and predictive_interval()\nFirst here’s predictive_interval. It is a convenience function that the posterior distribution generated previously, predictions, and returns an uncertainty interval:\n\n\nCode\npredictive_interval(\n    predictions\n)\n\n\n        5%      95%\n1 16.18055 28.97743\n\n\nWe can see by default the intervals returned are from 5% to 95%, i.e. are the 90% intervals rather than the 95% intervals considered previously. We can change the intervals requested with the prob argument:\n\n\nCode\npredictive_interval(\n    predictions, \n    prob = 0.95\n)\n\n\n     2.5%   97.5%\n1 14.9348 30.1403\n\n\nAs expected, this requested interval returns an interval closer to (but not identical to) the interval estimated using the quantile function.\nLet’s see if we can also use the model directly, specifying newdata directly to predictive_interval:\n\n\nCode\npredictive_interval(\n    best_model_bayesian,\n    newdata = predictors, \n    prob = 0.95\n)\n\n\n      2.5%    97.5%\n1 14.92205 30.28054\n\n\nYes. This approach works too. The values aren’t identical as, no doubt, a more sophisticated approach is used by predictive_interval to estimate the interval than simply arranging the posterior estimates in order using quantile.\nFor producing expected values we can use the function posterior_epred:\n\n\nCode\nepreds &lt;- posterior_epred(\n    best_model_bayesian,\n    newdata = predictors\n)\n\nexp_values &lt;- epreds[,1]\n\nquantile(exp_values, probs = c(0.025, 0.500, 0.975))\n\n\n    2.5%      50%    97.5% \n21.19042 22.71725 24.12859 \n\n\nFor comparison, the expected value 95% interval we obtained from the Frequentist model was 21.3 to 24.2 when drawing from the quasi-posterior distribution, and 22.7 to 24.2 when using the predict() function with the interval argument set to \"confidence\".\nNow, finally, let’s see if we can produce first differences: the estimated effect of using VC rather than OJ as a supplement when the dose is 1.25mg\n\n\nCode\npredictors_x0 &lt;- data.frame(supp = \"OJ\", dose = 1.25)\npredictors_x1 &lt;- data.frame(supp = \"VC\", dose = 1.25)\n\npredictors_fd &lt;- rbind(predictors_x0, predictors_x1)\n\npredictions_fd &lt;- rstanarm::posterior_predict(\n    best_model_bayesian,\n    newdata = predictors_fd\n)\n\nhead(predictions_fd)\n\n\n            1        2\n[1,] 23.46942 24.60202\n[2,] 22.06149 20.35834\n[3,] 24.07686 19.06467\n[4,] 24.63437 27.01567\n[5,] 15.64022 17.77962\n[6,] 11.45183 34.21425\n\n\nThe newdata argument to posterior_predict now has two rows, one for the OJ supplement and the other for the VC supplement scenario. And the predictions matrix returned by posterior_predict now has two columns: one for each scenario (row) in predictors_fd. We can look at the distribution of both of these columns, as well as the rowwise comparisions between columns, which will give our distribution of first differences for the predicted values:\n\n\nCode\npreds_fd_df &lt;- \n    predictions_fd |&gt;\n        as_tibble(rownames = \"draw\") |&gt;\n        rename(x0 = `1`, x1 = `2`) |&gt;\n        mutate(fd = x1 - x0)\n\npreds_fd_df |&gt; \n    select(-fd) |&gt;\n    pivot_longer(cols = c(\"x0\", \"x1\"), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    facet_wrap(~ scenario, nrow = 2)\n\n\n\n\n\nTo reiterate, these are predicted values for the two scenarios, not the expected values shown in the first differences section of post 12. This explains why there is greater overlap between the two distributions. Let’s visualise and calculate the first differences in predicted values:\n\n\nCode\npreds_fd_df |&gt;\n    select(fd) |&gt;\n    ggplot(aes(x = fd)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nWe can see that the average of the distribution is below 0, but as we are looking at predicted values the range of distributions is much higher. Let’s get 95% intervals:\n\n\nCode\nquantile(preds_fd_df$fd, probs = c(0.025, 0.500, 0.975))\n\n\n      2.5%        50%      97.5% \n-13.838512  -2.860489   7.733071 \n\n\nThe 95% intervals for first differences in predicted values is from -13.6 to +7.9, with the median estimate at -3.0. As expected, the median is similar to the equivalent value from using expected values (-2.9) but the range is wider.\nNow let’s use posterior_epred to produce estimates of first differences in expected values, which will be more directly comparable to our first differences estimates in section two:\n\n\nCode\npredictions_fd_ev &lt;- posterior_epred(\n    best_model_bayesian,\n    newdata = predictors_fd\n)\n\nhead(predictions_fd_ev)\n\n\n          \niterations        1        2\n      [1,] 22.11832 21.00645\n      [2,] 23.24191 19.50229\n      [3,] 22.54839 19.81125\n      [4,] 22.66001 20.25431\n      [5,] 23.12972 19.85558\n      [6,] 21.16934 20.18880\n\n\n\n\nCode\npreds_fd_df_ev &lt;- \n    predictions_fd_ev |&gt;\n        as_tibble(rownames = \"draw\") |&gt;\n        rename(x0 = `1`, x1 = `2`) |&gt;\n        mutate(fd = x1 - x0)\n\npreds_fd_df_ev |&gt; \n    select(-fd) |&gt;\n    pivot_longer(cols = c(\"x0\", \"x1\"), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    facet_wrap(~ scenario, nrow = 2)\n\n\n\n\n\nThis time, as the stochastic variation related to the \\(\\sigma\\) term has been removed, the distributions of the expected values are more distinct, with less overlap. Let’s visualise and compare the first differences of the expected values:\n\n\nCode\npreds_fd_df_ev |&gt;\n    select(fd) |&gt;\n    ggplot(aes(x = fd)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\n\n\nCode\nquantile(preds_fd_df_ev$fd, probs = c(0.025, 0.500, 0.975))\n\n\n      2.5%        50%      97.5% \n-4.9299560 -2.8195143 -0.6882904 \n\n\nWe now have a 95% interval for the first difference in expected values of -4.9 to -0.7. By contrast, the equivalent range estimated using the Frequentist model in part 12 was -4.8 to -0.8. So, although they’re not identical, they do seem to be very similar.\n\n\nBayesian Statistics Summary\nUp until now we’ve been using Frequentist approaches to modelling. However the simulation approach required to produce honest uncertainty depends on ‘tricking’ Frequentist models into producing something like the converged posterior distributions which, in Bayesian modelling approaches, come ‘for free’ from the way in which Bayesian frameworks estimate model parameters.\nAlthough Bayesian models are generally more technically and computationally demanding than Frequentist models, we have shown the folllowing:\n\nThat packages like rstanarm abstract away some of the challenges of building Bayesian models from scratch;\nThat the posterior distributions produced by Bayesian models produce estimates of expected values, predicted values, and first differences - our substantive quantities of interest - that are similar to those produced previously from Frequentist models\nThat for the estimation of these quantities of interest, the posterior distributions Bayesian models generate make it more straightforward, not less, to produce using Bayesian methods than using Frequentist methods.\n\nThanks for reading, and congratulations on getting this far through the series."
  },
  {
    "objectID": "pages/main-course/likelihood-and-simulation-theory/index.html",
    "href": "pages/main-course/likelihood-and-simulation-theory/index.html",
    "title": "Likelihood and Simulation Theory",
    "section": "",
    "text": "In the first part of the course, I stated that statistical model fitting, within the generalised model framework presented in King, Tomz, and Wittenberg (2000), involves adjusting candidate values for elements of \\(\\beta = \\{\\beta_0, \\beta_1, ..., \\beta_K \\}\\) such that the difference between what the model predicts given some predictor values, \\(Y_i | X_i\\), and what has been observed alongside the predictors, \\(y_i\\), is minimised on average1 in some way.\nThe aim of this post is to show how this process is typically implemented in GLMs, using likelihood theory."
  },
  {
    "objectID": "pages/main-course/likelihood-and-simulation-theory/index.html#bayes-rule-and-likelihood",
    "href": "pages/main-course/likelihood-and-simulation-theory/index.html#bayes-rule-and-likelihood",
    "title": "Likelihood and Simulation Theory",
    "section": "Bayes’ Rule and Likelihood",
    "text": "Bayes’ Rule and Likelihood\nStatisticians and more advanced users of statistical models often divide themselves into ‘frequentists’ and ‘Bayesians’. To some extent the distinction is really between ‘improper Bayesians’ and ‘proper Bayesians’, however, as Bayes’ Rule is at the root of both approaches. Bayes’ Rule is:\n\\[\nP(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n\\]\nNote in the above the left hand side of the equation is \\(P(A|B)\\) and the right hand side of the equation includes \\(P(B|A)\\). To write it out as awkward prose, therefore, Bayes’ Rule is a way of expressing that given this in terms of this given that.\nAs with much of algebra, \\(A\\) and \\(B\\) are just placeholders. We could instead use different symbols instead, such as:\n\\[\nP(\\tilde{\\theta} | y) = \\frac{P(y | \\tilde{\\theta})P(\\tilde{\\theta})}{P(y)}\n\\]\nLikelihood theory offers a way of thinking about how good a model is in terms of its relationship to the data. According to King (1998) (p. 59), it can be expressed as:\n\\[\nL(\\tilde{\\theta}| y) = k(y) P(y | \\tilde{\\theta})\n\\]\nOr\n\\[\nL(\\tilde{\\theta} | y) \\propto P(y | \\tilde{\\theta})\n\\]\nWhere \\(\\tilde{\\theta}\\) is a proposed parameter or parameter combination for the model, and \\(y\\) is the observed outcome.2\nThe important thing to note is that both Bayes’ Rule and Likelihood Theory are ways of expressing this given that as a function of that given this. Specifically, the model given the data, as a function of the data given the model. 3"
  },
  {
    "objectID": "pages/main-course/likelihood-and-simulation-theory/index.html#likelihood-for-linear-regression",
    "href": "pages/main-course/likelihood-and-simulation-theory/index.html#likelihood-for-linear-regression",
    "title": "Likelihood and Simulation Theory",
    "section": "Likelihood for linear regression",
    "text": "Likelihood for linear regression\nWhen, many years ago, I completed the course from this modelling framework is most associated, a hazing ritual employed near the start of the course was to require participants to derive the likelihood of different model specifications. However, I don’t feel like hazing myself right now, so instead we can use the derivation shown on slide 8 of these slides:\n\\[\nL(\\beta, \\sigma^2 | y) = \\prod{L(y_i | \\mu_i, \\sigma^2)}\n\\]\nWhere \\(\\mu = X \\beta\\), \\(i\\) indicates an observation in the data (a row of \\(X\\) when \\(X\\) is in matrix form), and \\(\\prod\\) indicates the likelihoods from each observation should be multiplied with each other to derive the overall likelihood for all observed data.\nIn practice the log Likelihood, rather than the likelihood itself, is used, because this allows calculation of a sum of terms (\\(\\sum\\)) rather than product of terms (\\(\\prod\\)), and the latter tends to be computationally easier to calculate.\nAs we are interested only in how likelihood varies as a function of those model parameters we wish to estimate, \\(\\theta = \\{\\beta, \\sigma^2\\}\\), some of the terms in the log likelihood expression can be omitted, leaving us with:\n\\[\n\\log{L(\\beta, \\sigma^2 | y)} \\doteq \\sum{-\\frac{1}{2}[\\log{\\sigma^2} + \\frac{(y_i - X_i\\beta)^2}{\\sigma^2}]}\n\\]\nFor all the complexity of the above expression, at heart it takes three inputs:\n\n\\(\\theta = \\{\\beta, \\sigma^2\\}\\) : The candidate parameters for the model.\n\\(y\\) : the observed response value from the dataset \\(D\\)\n\\(X\\) : the observed predictor values from the dataset \\(D\\)\n\nAnd returns one value, the log likelihood \\(\\log{L(.)}\\).\nTo reiterate, we can’t change the data, but we can keep changing the candidate parameters \\(\\theta\\). Each time we do so, \\(\\log{L(.)}\\) will change too.\nThe aim of model calibration, in the Likelihood framework, is to maximise the Likelihood. The parameter set that maximises the likelihood is also the parameter set that maximises the log likelihood.\nTo continue the example from the slides, we can write out a function for calculating the log likelihood of standard linear regression as follows:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\nIn the above, pars is (almost but not quite) \\(\\theta\\), the parameters to estimate. For standard linear regression \\(\\theta = \\{\\beta, \\sigma^2\\}\\), where \\(\\beta = \\{\\beta_0, \\beta_1, ..., \\beta_k\\}\\), i.e. a vector of beta parameters, one for each column (variable) in \\(X\\), the predictor matrix of observations; this is why \\(beta\\) is selected from the first K values in pars where K is the number of columns in \\(X\\).\nThe last value in pars is used to derive the proposed \\(\\sigma^2\\). If we call this last value eta (\\(\\eta\\)), then we can say \\(\\sigma^2 = e^{\\eta}\\). So, whereas \\(\\theta\\) is a vector that ‘packs’ \\(\\beta\\) and \\(\\sigma^2\\) into a single ordered series of values, pars packs eta in place of \\(\\sigma^2\\). This substitution of eta for \\(\\sigma^2\\) is done to make it easier for standard parameter fitting algorithms to work, as they tend to operate over the full real number range, rather than just over positive values.\nIn order to illustrate how the log likelihood function llNormal works in practice, let’s construct a simple toy dataset \\(D\\), and decompose \\(D = \\{y, X\\}\\), the two types of data input that go into the llNormal function.\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nIn the code above we have created \\(y\\), a vector of nine observed responses; and \\(X\\), a matrix of predictors with two columns (the number of variables for which \\(beta\\) terms need to be estimated) and nine rows (the number of observations).\nGraphically, the relationship between x and y looks as follows:\n\n\nCode\nlibrary(tidyverse)\ntibble(x=x, y=y) |&gt;\n    ggplot(aes(x, y)) + \n    geom_point()\n\n\n\n\n\nIn this toy example, but almost never in reality, we know the correct parameters for the model. These are \\({\\beta_0 = 2.5, \\beta_1 = 1.4}\\) and \\(\\sigma^2 = 0.25\\). 4 Soon, we will see how effectively we can use optimisation algorithms to recover these true model parameters. But first, let’s see how the log likelihood varies as a function jointly of different candidate values of \\(\\beta_0\\) (the intercept) and \\(\\beta_1\\) (the slope parameter), if we already set \\(\\sigma^2\\) to 0.25.\n\n\nCode\ncandidate_param_values &lt;- expand_grid(\n    beta_0 = seq(-5, 5, by = 0.1),\n    beta_1 = seq(-5, 5, by = 0.1)\n)\n\nfeed_to_ll &lt;- function(b0, b1){\n    pars &lt;- c(b0, b1, log(0.25))\n    llNormal(pars, y, X)\n}\n\ncandidate_param_values &lt;- candidate_param_values |&gt;\n    mutate(\n        ll = map2_dbl(beta_0, beta_1, feed_to_ll)\n    )\n\n\n\n\nCode\ncandidate_param_values |&gt;\n    ggplot(aes(beta_0, beta_1, z = ll)) + \n    geom_contour_filled() + \n    geom_vline(xintercept = 0) +\n    geom_hline(yintercept = 0) +\n    labs(\n        title = \"Log likelihood as a function of possible values of beta_0 and beta_1\",\n        x = \"beta0 (the intercept)\",\n        y = \"beta1 (the slope)\"\n    )\n\n\n\n\n\nLooking at this joint surface of values, we can see a ‘hotspot’ where \\(\\beta_0\\) is around 2.5, and \\(\\beta_1\\) is around 1.4, just as we should expect. We can check this further by filtering candidate_param_values on the highest observed values of ll.\n\n\nCode\ncandidate_param_values |&gt; \n    filter(ll == max(ll))\n\n\n# A tibble: 1 × 3\n  beta_0 beta_1    ll\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    2.4    1.4  1.41"
  },
  {
    "objectID": "pages/main-course/likelihood-and-simulation-theory/index.html#optimisation-algorithms-getting-there-faster",
    "href": "pages/main-course/likelihood-and-simulation-theory/index.html#optimisation-algorithms-getting-there-faster",
    "title": "Likelihood and Simulation Theory",
    "section": "Optimisation algorithms: getting there faster",
    "text": "Optimisation algorithms: getting there faster\nPreviously, we ‘cheated’ a bit when using the log likelihood function, fixing the value for one of the parameters \\(\\sigma^2\\) to the value we used when we generated the data, so we could instead look at how the log likelihood surface varied as different combinations of \\(\\beta_0\\) and \\(\\beta_1\\) were plugged into the formula. \\(\\beta_0\\) and \\(\\beta_1\\) values ranging from -5 to 5, and at steps of 0.1, were considered: 101 values of \\(\\beta_0\\), 101 values of \\(\\beta_1\\), and so over 10,0005 unique \\(\\{\\beta_0, \\beta_1\\}\\) combinations were stepped through. This approach is known as grid search, and seldom used in practice (except for illustration purposes) because the number of calculations involved can very easily get out of hand. For example, if we were to use it to explore as many distinct values of \\(\\sigma^2\\) as we considered for \\(\\beta_0\\) and \\(\\beta_1\\), the total number of \\(\\{\\beta_0, \\beta_1, \\sigma^2 \\}\\) combinations we would crawl through would be over 100,000 6 rather than over 10,000.\nOne feature we noticed with the likelihood surface over \\(\\beta_0\\) and \\(\\beta_1\\) in the previous post is that it appears to look like a hill, with a clearly defined highest point (the region of maximum likelihood) and descent in all directions from this highest point. Where likelihood surfaces have this feature of being single-peaked in this way (known as ‘unimodal’), then a class of algorithms known as ‘hill climbing algorithms’ can be applied to find the top of such peaks in a way that tends to be both quicker (fewer steps) and more precise than the grid search approach used for illustration in the previous post."
  },
  {
    "objectID": "pages/main-course/likelihood-and-simulation-theory/index.html#optim-for-parameter-point-estimation-our-robo-chauffeur",
    "href": "pages/main-course/likelihood-and-simulation-theory/index.html#optim-for-parameter-point-estimation-our-robo-chauffeur",
    "title": "Likelihood and Simulation Theory",
    "section": "optim for parameter point estimation: our Robo-Chauffeur",
    "text": "optim for parameter point estimation: our Robo-Chauffeur\nNote how the llNormal function takes a single argument, pars, which packages up all the specific candidate parameter values we want to try out. In our previous post, we also had a ‘feeder function’, feed_to_ll, which takes the various \\(\\beta\\) candidate values from the grid and packages them into pars. In our previous post, we had to specify the candidate values to try to feed to llNormal packages inside pars.\nBut we don’t have to do this. We can instead use an algorithm to take candidate parameters, try them out, then make new candidate parameters and try them out, for us. Much as a taxi driver needs to know where to meet a passenger, but doesn’t want the passenger to tell them exactly which route to take, we just need to specify a starting set of values for the parameters to optimise. R’s standard way of doing this is with the optim function. Here’s it in action:\n\n\nCode\noptim_results &lt;-  optim(\n    # par contains our initial guesses for the three parameters to estimate\n    par = c(0, 0, 0), \n\n    # by default, most optim algorithms prefer to search for a minima (lowest point) rather than maxima \n    # (highest point). So, I'm making a function to call which simply inverts the log likelihood by multiplying \n    # what it returns by -1\n    fn = function(par, y, X) {-llNormal(par, y, X)}, \n\n    # in addition to the par vector, our function also needs the observed output (y)\n    # and the observed predictors (X). These have to be specified as additional arguments.\n    y = y, X = X\n    )\n\noptim_results\n\n\n$par\n[1]  2.460571  1.375421 -1.336209\n\n$value\n[1] -1.51397\n\n$counts\nfunction gradient \n     216       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nThe optim function returns a fairly complex output structure, with the following components:\n\npar: the values for the parameters (in our case \\(\\{\\beta_0, \\beta_1, \\eta \\}\\)) which the optimisation algorithm ended up with.\nvalue: the value returned by the function fn when the optim routine was stopped.\ncounts: the number of times the function fn was repeatedly called by optim before optim decided it had had enough\nconvergence: whether the algorithm used by optim completed successfully (i.e. reached what it considers a good set of parameter estimates in par), or not.\n\nIn this case, convergence is 0, which (perhaps counterintuitively) indicates a successful completion. counts indicates that optim called the log likelihood function 216 times before stopping, and par indicates values of \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\eta = -1.34\\}\\) were arrived at. As \\(\\sigma^2 = e^\\eta\\), this means \\(\\theta = \\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\sigma^2 = 0.26 \\}\\). As a reminder, the ‘true’ values are \\(\\{\\beta_0 = 2.50, \\beta_1 = 1.40, \\sigma^2 = 0.25\\}\\).\nSo, the optim algorithm has arrived at pretty much the correct answers for all three parameters, in 216 calls to the log likelihood function, whereas for the grid search approach in the last post we made over 10,000 calls to the log likelihood function for just two of the three parameters.\nLet’s see if we can get more information on exactly what kind of path optim took to get to this set of parameter estimates. We should be able to do this by specifying a value in the trace component in the control argument slot…\n\nComparisons with ‘canned’ functions\nFor comparison let’s see what lm and glm produce.\nFirst lm:\n\n\nCode\ntoy_df &lt;- tibble(\n    x = x, \n    y = y\n)\n\n\nmod_lm &lt;- lm(y ~ x, data = toy_df)\nsummary(mod_lm)\n\n\n\nCall:\nlm(formula = y ~ x, data = toy_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6082 -0.3852 -0.1668  0.2385  1.1092 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5813 on 7 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9767 \nF-statistic:   336 on 1 and 7 DF,  p-value: 3.564e-07\n\n\n\\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\), i.e. the same to 2 decimal places.\nAnd now with glm:\n\n\nCode\nmod_glm &lt;- glm(y ~ x, data = toy_df, family = gaussian(link = \"identity\"))\n\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = toy_df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nOnce again, \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\)\n\n\nDiscussion\nIn the above, we’ve successfully used optim, our Robo-Chauffeur, to arrive very quickly at some good estimates for our parameters of interest, \\(\\beta_0\\) and \\(\\beta_1\\), which are in effect identical to those produced by the lm and glm functions.\nThis isn’t a coincidence. What we’ve done the hard way is what the glm function (in particular) largely does ‘under the hood’."
  },
  {
    "objectID": "pages/main-course/likelihood-and-simulation-theory/index.html#optimal-uncertainty",
    "href": "pages/main-course/likelihood-and-simulation-theory/index.html#optimal-uncertainty",
    "title": "Likelihood and Simulation Theory",
    "section": "optimal uncertainty",
    "text": "optimal uncertainty\nWhen using optim() above, we managed to get it to return a set of parameter values for our model that it thought was ‘best’, i.e. minimised the loss function specified by the log likelihood. These are known as point estimates, and are effectively the coefficients presented by lm or glm or equivalent statistical functions and packages. However optim() just returned these point estimates, without any indication of how uncertain we should be about these point estimates. A standard statistical model summary will tend to also report measures of uncertainty around the point estimates, in the form of standard errors. When these are implicitly combined with a Null hypothesis, namely that the ‘true’ value of a parameter may be zero, the point estimate together with its standard error allows the calculation of z values and p values.\nHow can we use optim() to return measures of uncertainty, which will allow the standard errors to be estimated as well as the point values?\nWe’ll start with a weird analogy to get an intuition for how this can be done with optim().\n\nBarefoot and Blind: A weird analogy for a complicated idea\nImagine optim, your hill-finding robo-chauffeur, has taken you to the top of a likelihood surface. Then it leaves you there…\n… and you’re blind, and have no shoes. (You also have an uncanny sense of your orientation, whether north-south, east-west, or some other angle.)\nSo, you know you’re at the top of the hill, but you can’t see what the landscape around you looks like. However, you still want to get a sense of this landscape, and how it varies around the spot you’re standing on.\nWhat do you do?\nIf you’re playing along with this weird thought experiment, one approach would be to use your feet as depth sensors. You make sure you never stray from where you started, and to always keep one foot planted on this initial spot (which you understand to be the highest point on the landscape). Then you use your other foot to work out how much further down the surface is from the highest point as you venture away from the highest point in different directions.\nSay you keep your left foot planted on the highest point, and make sure your right foot is always positioned (say) 10 cm horizontally from your left foot. Initially your two feet are arranged east-west; let’s call this 0 degrees. When you put your right foot down, you notice it needs to travel 2 cm further down to reach terra ferma relative to your left foot.\n2cm at 0 degrees. You’ll remember that.\nNow you rotate yourself 45 degrees, and repeat the same right foot drop. This time it needs to travel 3cm down relative to your left foot.\n3cm at 45 degrees. You remember that too.\nNow you rotate another 45 degrees, north-south orientation, place your right foot down; now it falls 5cm down relative to your left foot.\n2cm at 0 degrees; 3cm at 45 degrees; 5cm at 90 degrees.\nNow with this information, you try to construct the landscape you’re on top of with your mind’s eye, making the assumption that the way it has to have curved from the peak you’re on to lead to the drops you’ve observed is consistent all around you; i.e. that there’s only one hill, you’re on top of it, and it’s smoothly curved in all directions.\n\n\nInformation and uncertainty\nIf you could further entertain the idea that your feet are infinitely small, and the gap between feet is also infinitely small (rather than the 10cm above), then you have the intuition behind this scary-looking but very important formula from King (1998) (p. 89):\n\\[\n\\widehat{V(\\hat{\\theta})} = - \\frac{1}{n}[\\frac{\\delta^2lnL(\\tilde{\\theta}|y)}{\\delta \\tilde{\\theta} \\delta \\tilde{\\theta}^{'}}]^{-1}_{\\tilde{\\theta} = \\hat{\\theta}}\n\\]\nWhat this is saying, in something closer to humanese, is something like:\n\nOur best estimate of the amount of uncertainty we have in our estimates is a function of how much the likelihood surface curves at the highest point on the surface. (It also gets less uncertain, the more observations we have).\n\nAmongst the various bells, whistles and decals in the previous formula is the superscript \\((.)^{-1}\\). This means invert, which for a single value means \\(\\frac{1}{.}\\) but for a matrix means something conceptually the same but technically not.\nAnd what’s being inverted in the last formula? A horrible-looking expression, \\([\\frac{\\delta^2lnL(\\tilde{\\theta}|y)}{\\delta \\tilde{\\theta} \\delta \\tilde{\\theta}^{'}}]_{\\tilde{\\theta} = \\hat{\\theta}}\\), that’s basically an answer to the question of how curvy is the log likelihood surface at its peak position?\nWithin King (1998) (p.89, eq. 4.18), this expression (or rather the negative of the term) is defined as \\(I(\\hat{\\theta} | y)\\), where \\(I(.)\\) stands for information.\nSo, the algebra are saying\n\nUncertainty is inversely related to information\n\nOr perhaps even more intuitively\n\nThe more information we have, the less uncertain we are\n\nOf course this makes sense. If you ask someone “How long will this task take?”, and they say “Between one hour and one month”, they likely have less information about how long the task will actually than if they had said “Between two and a half and three hours”. More generally:\n\nShallow gradients mean wide uncertainty intervals mean low information\nSharp gradients mean narrow uncertaintly intervals mean high information\n\nThis is, fundamentally, what the blind and barefoot person in the previous analogy is trying to achieve: by feeling out the local curvature around the highest point, they are trying to work out how much information they have about different pieces of the model. The curvature along any one dimension of the surface (equivalent to the 0 and 90 degree explorations) indicates how much information there is about any single coefficient, and the curvature along the equivalent of a 45 degree plane gives a measure of how associated any two coefficients tend to be.\nWith these many analogies and equations spinning in our heads, let’s now see how these concepts can be applied in practice.\n\n\nHow to get optim() to return this information\nHaving reminded myself of the particular options for optim that are typically used to report parameter uncertainty, let’s run the follows:\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nWe have used a slightly different algorithm (‘BFGS’), and a different way of specifying the function to search over (using fnscale = -1 to invert the likelihood), but we have the same par estimates as before: \\(\\beta = \\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\). So the changes we’ve made to the optim arguments haven’t changed what it estimates.\nOne new argument we’ve set in optim is hessian = TRUE. Hessian is a kind of coarse fabric made from vegetable waste, typically woven in a criss-crossing, grid-like pattern. Hessian matrices are matrices of second derivatives, as described in the wikipedia article. 7 If you can bear to recall the really complex expression above, for calculating the curvature around a point on a surface, you’ll recall it’s also about second derivatives.\nNone of this is a coincidence. The hessian component of the optim output above contains what we need.\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\nhess\n\n\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nYou might notice that the Hessian matrix is square, with as many columns as rows. And, that the number of columns (or rows) is equal to the number of parameters we have estimated, i.e. three in this case.\nYou might also notice that the values are symmetrical about the diagonal running from the top left to the bottom right.\nAgain, this is no accident.\nRemember that variation is inversely related to information, and that \\((.)^{-1}\\) is the inversion operator on \\(I(.)\\), the Information Matrix. Well, this Hessian is (pretty much) \\(I(.)\\). So let’s see what happens when we invert it (using the solve operator):\n\n\nCode\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\nAs with hess, inv_hess is symmetric around the top-left to bottom-right diagonal. For example, the value on row 2 and column 1 is the same as on row 1, column 2.\nWe’re mainly interested in the first two columns and rows, as these contain the values most comparable with the glm summary reports\n\n\nCode\ninv_hess_betas &lt;- inv_hess[1:2, 1:2]\n\ninv_hess_betas\n\n\n             [,1]         [,2]\n[1,]  0.033577455 -0.004379668\n[2,] -0.004379668  0.004379668\n\n\nWhat the elements of the above matrix provide are estimates of the variances of a single parameter \\(\\beta_j\\), and/or the covariances between any two parameters \\(\\{\\beta_0, \\beta_1\\}\\). In this example:\n\\[\n\\begin{bmatrix}\nvar(\\beta_0) & cov(\\beta_0, \\beta_1) \\\\\ncov(\\beta_1, \\beta_0) & var(\\beta_1)\n\\end{bmatrix}\n\\]\nIt’s because the on-diagonal terms are variances of uncertaintly for a single term, that it can be useful to take the square root of these terms to get estimates of the standard errors:\n\n\nCode\nsqrt(diag(inv_hess_betas))\n\n\n[1] 0.18324152 0.06617906\n\n\nCompare with the Std Err term in the following:\n\n\nCode\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = toy_df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nThe estimates from the Hessian in optim, of \\(\\{0.18, 0.07\\}\\), are not exactly the same as the \\(\\{0.21, 0.08\\}\\) reported for mod_glm; the methods employed are not identical. But they are hopefully similar enough to demonstrate they provide similar information about similar quantities of uncertainty.\nBack in part five, we used this same dataset to show how the log likelihood varies for various, equally spaced, candidate values for \\(\\beta_0\\) and \\(\\beta_1\\) (having fixed \\(\\eta = \\exp({\\sigma^2})\\) at its true value). This led to the followng map of the landscape8\n\n\nCode\nlibrary(tidyverse)\ncandidate_param_values &lt;- expand_grid(\n    beta_0 = seq(-15, 15, by = 0.05),\n    beta_1 = seq(-15, 15, by = 0.05)\n)\n\nfeed_to_ll &lt;- function(b0, b1){\n    pars &lt;- c(b0, b1, log(0.25))\n    llNormal(pars, y, X)\n}\n\ncandidate_param_values &lt;- candidate_param_values |&gt;\n    mutate(\n        ll = map2_dbl(beta_0, beta_1, feed_to_ll)\n    )\n\ncandidate_param_values |&gt;\n    ggplot(aes(beta_0, beta_1, z = ll)) + \n    geom_contour_filled() + \n    geom_vline(xintercept = 0) +\n    geom_hline(yintercept = 0) +\n    labs(\n        title = \"Log likelihood as a function of possible values of beta_0 and beta_1\",\n        x = \"beta0 (the intercept)\",\n        y = \"beta1 (the slope)\"\n    )\n\n\n\n\n\nWithin the above we can see that the log likelihood landscape for these two parameters looks like a bivariate normal distribution, we can also see a bit of a slant in this normal distribution. This implies a correlation between the two candidate values. The direction of the slant is downwards from left to right, implying the correlation is negative.\nFirstly let’s check that the correlation between \\(\\beta_0\\) and \\(\\beta_1\\) implied by the Hessian is negative. These are the off-diagonal elements, either first row, second column, or second row, first column:\n\n\nCode\ninv_hess_betas[1,2]\n\n\n[1] -0.004379668\n\n\nCode\ninv_hess_betas[2,1]\n\n\n[1] -0.004379668\n\n\nYes they are!\nAs mentioned previously, the likelihood surface produced by the gridsearch method involves a lot of computations, so a lot of steps, and likely a lot of trial and error, if it were to be used to try to find the maximum likelihood value for the parameters. By contrast, the optim() algorithm typically involves far fewer steps, ‘feeling’ its way up the hill until it reaches a point where there’s nowhere higher. 9 When it then reaches this highest point, it then ‘feels’ the curvature around this point in multiple directions, producing the Hessian. The algorithm doesn’t see the likelihood surface, because it hasn’t travelled along most of it. But the Hessian can be used to infer the likelihood surface, subject to subject (usually) reasonable assumptions.\nWhat are these (usually) reasonable assumptions? Well, that the likelihood surface can be approximated by a multivariate normal distribution, which is a generalisation of the standard Normal distribution over more than one dimensions.10\nWe can use the mvrnorm function from the MASS package, alongside the point estimates and Hessian from optim, in order to produce estimates of \\(\\theta = \\{ \\beta_0, \\beta_1, \\eta \\}\\) which represent reasonable uncertainty about the true values of each of these parameters. Algebraically, this can be expressed as something like the following:\n\\[\n\\tilde{\\theta} \\sim Multivariate Normal(\\mu = \\dot{\\theta}, \\sigma^2 = \\Sigma)\n\\]\nWhere \\(\\dot{\\theta}\\) are the point estimates from optim() and \\(\\Sigma\\) is the implied variance-covariance matrix recovered from the Hessian.\nLet’s create this MVN model and see what kinds of outputs it produces.\n\n\nCode\nlibrary(MASS)\n\npoint_estimates &lt;- fuller_optim_output$par\n\nvcov &lt;- -solve(fuller_optim_output$hessian)\nparam_draws &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = point_estimates, \n    Sigma = vcov\n)\n\ncolnames(param_draws) &lt;- c(\n    \"beta0\", \"beta1\", \"eta\"\n)\n\nhead(param_draws)\n\n\n        beta0    beta1         eta\n[1,] 2.564978 1.375636 -0.30407255\n[2,] 2.440111 1.367774 -1.16815288\n[3,] 2.775332 1.338583 -0.05574937\n[4,] 2.283011 1.481799 -0.26095101\n[5,] 2.695635 1.228565 -1.18369341\n[6,] 2.686818 1.483601 -0.44262363\n\n\nWe can see that mvrnorm(), with these inputs from optim() produces three columns: one for each parameter being estimated \\(\\{ \\beta_0, \\beta_1, \\eta \\}\\). The n argumment indicates the number of draws to take; in this case, 10000. This number of draws makes it easier to see how much variation there is in each of the estimates.\n\n\nCode\ndf_param_draws &lt;- \nparam_draws |&gt;\n    as_tibble(\n        rownames = 'draw'\n    ) |&gt;\n    mutate(\n        sig2 = exp(eta)\n    ) |&gt;\n    pivot_longer(\n        -draw, \n        names_to = \"param\",\n        values_to = \"value\"\n    ) \n    \ndf_param_draws |&gt;\n    ggplot(aes(x = value)) + \n    geom_density() + \n    facet_grid(param ~ .) + \n    geom_vline(xintercept=0)\n\n\n\n\n\nThere are a number of things to note here: firstly, that the average of the \\(\\beta_0\\) and \\(\\beta_1\\) values appear close to their known ‘true’ values of 2.5 and 1.4 respectively. Secondly, that whereas the \\(\\eta\\) values are normally distributed, the \\(\\sigma^2\\) values derived from them are not, and are never below zero; this is the effect of the exponential link between quantities. Thirdly, that the implied values of \\(\\sigma^2\\) do appear to be centred around 0.25, as they should be as \\(\\sigma\\) was set to 0.50 in the model.\nAnd forthly, that the density around \\(\\beta_1\\) is more peaked than around \\(\\beta_0\\). This concords with what we saw previously in the filled contour map: both the horizontal beta0 axis and vertical beta1 axis are on the same scale, but the oval is broader along the horizontal axis than the vertical axis. This in effect implies that we have more information about the true value of \\(\\beta_1\\), the slope, than about the true value of \\(\\beta_0\\), the intercept.\nWe can also use these draws to reproduce something similar to, but not identical to, 11 the previous filled contour map:\n\n\nCode\n# param_draws |&gt;\n#     as_tibble(\n#         rownames = 'draw'\n#     ) |&gt;\n#     ggplot(aes(x = beta0, y = beta1)) + \n#     geom_point(alpha = 0.1) + \n#     coord_cartesian(xlim = c(-10, 10), ylim = c(-10, 10))\n\nparam_draws |&gt;\n    as_tibble(\n        rownames = 'draw'\n    ) |&gt;\n    ggplot(aes(x = beta0, y = beta1)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nOnce again, we see the same qualities as the contour map produced by interrogating the likelihood surface exhaustively: the distribution appears bivariate normal; there is a greater range in the distribution along the beta0 than the beta1 axis; and there is evidence of some negative correlation between the two parameters.\n\n\nSummary\nThis post has shown how optim(), which in its vanilla state only returns point estimates, can be configured to also calculater and report the Hessian, a record of instantaneous curvature around the point estimates. Even without a fine-grained and exhausive search throughout the likelihood surface, this measure of curvature can be used to produce similar measures of uncertainty to the more exhausive approach, in a fraction of the number of computations.\nMore importantly, it can be used to generate draws of plausible combinations of parameter values, something denoted as \\(\\tilde{\\theta}\\) earlier. This is something especially useful for producing honest quantities of interest, which both tell users of models something they want to know, while also representing how uncertain we are in this knowledge."
  },
  {
    "objectID": "pages/main-course/likelihood-and-simulation-theory/index.html#quantities-of-interest",
    "href": "pages/main-course/likelihood-and-simulation-theory/index.html#quantities-of-interest",
    "title": "Likelihood and Simulation Theory",
    "section": "Quantities of interest",
    "text": "Quantities of interest\nWe’ll now, finally, show how this knowledge can be applied to do something with statistical models that ought to be done far more often: report on what King, Tomz, and Wittenberg (2000) calls quantities of interest, including predicted values, expected values, and first differences. Quantities of interest are not the direction and statistical significance (P-values) that many users of statistical models convince themselves matter, leading to the kind of mindless stargazing summaries of model outputs described in section one. Instead, they’re the kind of questions that someone, not trained to think that stargazing is satisfactory, might reasonably want answers to. These might include:\n\nWhat is the expected income of someone who completes course X in the five years after graduation? (Expected values)\nWhat is the expected range of incomes of someone who completes course X in the five years after graduation? (Predicted values)\nWhat is the expected difference in incomes between someone who completes course X, compared to course Y, in the five years after graduation? (First Differences)\n\nIn section one, we showed how to answer some of the questions of this form, for both standard linear regression and logistic regression. We showed that for linear regression such answers tend to come directly from the summary of coefficients, but that for logistic regression such answers tend to be both more ambiguous and dependent on other factors (such as gender of graduate, degree, ethnicity, age and so on), and require more processing in order to produce estimates for.\nHowever, we previously produced only point estimates for these questions, and so in a sense misled the questioner with the apparent certainty of our estimates. We now know, from earlier in this section, that we can use information about parameter uncertainty to produce parameter estimates \\(\\tilde{\\theta}\\) that do convey parameter uncertainty, and so we can do better than the point estimates alone to answer such questions in way that takes into account such uncertainty, with a range of values rather than a single value.\n\nMethod\nLet’s make use of our toy dataset one last time, and go through the motions to produce the \\(\\tilde{\\theta}\\) draws we ended with on the last post:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\n\n\nCode\npoint_estimates &lt;- fuller_optim_output$par\n\nvcov &lt;- -solve(fuller_optim_output$hessian)\nparam_draws &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = point_estimates, \n    Sigma = vcov\n)\n\ncolnames(param_draws) &lt;- c(\n    \"beta0\", \"beta1\", \"eta\"\n)\n\n\nLet’s now look at our toy data again, and decide on some specific questions to answer:\n\n\nCode\nlibrary(tidyverse)\ntoy_df &lt;- tibble(x = x, y = y)\n\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() \n\n\n\n\n\nWithin the data itself, we have only supplied x and y values for whole numbers of x between -3 and 5. But we can use the model to produce estimates for non-integer values of x. Let’s try 2.5. For this single value of x, we can produce both predicted values and expected values, by passing the same value of x to each of the plausible estimates of \\(\\theta\\) returned by the multivariate normal function above.\n\n\nCode\ncandidate_x &lt;- 2.5\n\n\n\n\nExpected values\nHere’s an example of estimating the expected value of y for x = 2.5 using loops and standard algebra:\n\n\nCode\n# Using standard algebra and loops\nN &lt;- nrow(param_draws)\nexpected_y_simpler &lt;- vector(\"numeric\", N)\nfor (i in 1:N){\n    expected_y_simpler[i] &lt;- param_draws[i, \"beta0\"] + candidate_x * param_draws[i, \"beta1\"]\n}\n\nhead(expected_y_simpler)\n\n\n[1] 6.004068 5.859547 6.121791 5.987509 5.767047 6.395820\n\n\nWe can see just from the first few values that each estimate is slightly different. Let’s order the values from lowest to highest, and find the range where 95% of values sit:\n\n\nCode\nev_range &lt;- quantile(expected_y_simpler,  probs = c(0.025, 0.500, 0.975)) \n\nev_range\n\n\n    2.5%      50%    97.5% \n5.505104 5.898148 6.291150 \n\n\nThe 95% interval is therefore between 5.51 and 6.29, with the median (similar but not quite the point estimate) being 5.90. Let’s plot this against the data:\n\n\nCode\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() + \n    annotate(\"point\", x = candidate_x, y =  median(expected_y_simpler), size = 1.2, shape = 2, colour = \"blue\") + \n    annotate(\"segment\", x = candidate_x, xend=candidate_x, y = ev_range[1], yend = ev_range[3], colour = \"blue\")\n\n\n\n\n\nThe vertical blue line therefore shows the range of estimates for \\(Y|x=2.5\\) that contain 95% of the expected values given the draws of \\(\\beta = \\{\\beta_0, \\beta_1\\}\\) which we produced from the Multivariate Normal given the point estimates and Hessian from optim(). This is our estimated range for the expected value, not predicted value. What’s the difference?\n\n\nPredicted values\nOne clue about the difference between expected value lies in the parameters from optim() we did and did not use: Whereas we have both point estimates and uncertainty estimates for the parameters \\(\\{\\beta_0, \\beta_1, \\sigma^2\\}\\),12 we only made use of the the two \\(\\beta\\) parameters when producing this estimate.\nNow let’s recall the general model formula, from the start of King, Tomz, and Wittenberg (2000), which we repeated for the first few posts in the series:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nThe manual for Zelig, the (now defunct) R package that used to support analysis using this approach, states that for Normal Linear Regression these two components are resolved as follows:\nStochastic Component\n\\[\nY_i \\sim Normal(\\mu_i, \\sigma^2)\n\\]\nSystematic Component\n\\[\n\\mu_i = x_i \\beta\n\\]\nThe page then goes onto state that the expected value, \\(E(Y)\\), is :\n\\[\nE(Y) = \\mu_i = x_i \\beta\n\\]\nSo, in this case, the expected value is the systematic component only, and does not involve the dispersion parameter in the stochastic component, which for normal linear regression is the \\(\\sigma^2\\) term. That’s why we didn’t use estimates of \\(\\sigma^2\\) when simulating the expected values.\nBut why is this? Well, it comes from the expectation operator, \\(E(.)\\). This operator means something like, return to me the value that would be expected if this experiment were performed an infinite number of times.\nThere are two types of uncertainty which give rise to variation in the predicted estimate: sampling uncertainty, and stochastic variation. In the expected value condition, this second source of variation falls to zero,13 leaving only the influence of sampling uncertainty, as in uncertainty about the true value of the \\(\\beta\\) parameters, remaining on uncertainty on the predicted outputs.\nFor predicted values, we therefore need to reintroduce stochastic variation as a source of variation in the range of estimates produced. Each \\(\\eta\\) value we have implies a different \\(\\sigma^2\\) value in the stochastic part of the equation, which we can then add onto the variation caused by parameter uncertainty alone:\n\n\nCode\nN &lt;- nrow(param_draws)\npredicted_y_simpler &lt;- vector(\"numeric\", N)\nfor (i in 1:N){\n    predicted_y_simpler[i] &lt;- param_draws[i, \"beta0\"] + candidate_x * param_draws[i, \"beta1\"] + \n        rnorm(\n            1, mean = 0, \n            sd = sqrt(exp(param_draws[i, \"eta\"]))\n        )\n}\n\nhead(predicted_y_simpler)\n\n\n[1] 4.802092 6.706397 7.073450 6.118750 6.757717 7.461254\n\n\nLet’s now get the 95% prediction interval for the predicted values, and compare them with the expected values predicted interval earlier\n\n\nCode\npv_range &lt;- \n    quantile(\n        predicted_y_simpler, \n        probs = c(0.025, 0.500, 0.975)\n    )\n\npv_range\n\n\n    2.5%      50%    97.5% \n4.766300 5.895763 7.055408 \n\n\nSo, whereas the median is similar to before, 5.90, the 95% interval is now from 4.77 to 7.0614. This compares with the 5.51 to 6.29 range for the expected values. Let’s now plot this predicted value range just as we did with the expected values:\n\n\nCode\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() + \n    annotate(\"point\", x = candidate_x, y =  pv_range[2], size = 1.2, shape = 2, colour = \"blue\") + \n    annotate(\"segment\", x = candidate_x, xend=candidate_x, y = pv_range[1], yend = pv_range[3], colour = \"red\")\n\n\n\n\n\nClearly considerably wider."
  },
  {
    "objectID": "pages/main-course/likelihood-and-simulation-theory/index.html#log-likelihood-for-logistic-regression",
    "href": "pages/main-course/likelihood-and-simulation-theory/index.html#log-likelihood-for-logistic-regression",
    "title": "Likelihood and Simulation Theory",
    "section": "Log likelihood for logistic regression",
    "text": "Log likelihood for logistic regression\nPreviously we derived the log likelihood for Normal (Gaussian) regression and did some cool things with it. Let’s now do the same with logistic regression. We need to start with definition, then calculate log likelihood, then write it as a function in R that optim() can work its magic with.\nAccording to the relevant section of the Zelig website:\nStochastic component \\[\nY_i \\sim Bernoulli(y_i | \\pi_i )\n\\]\n\\[\nY_i = \\pi_i^{y_i}(1 - \\pi_i)^{1-y_i}\n\\]\nwhere \\(\\pi_i = P(Y_i = 1)\\)\nAnd\nSystematic Component\n\\[\n\\pi_i = \\frac{1}{1 + \\exp{(-x_i \\beta)}}\n\\]\nThe likelihood is the product of the above for all observations in the dataset \\(i \\in N\\)\n\\[\nL(.) = \\prod{\\pi_i^{y_i}(1 - \\pi_i)^{1-y_i}}\n\\]\nThe effect of logging the above15:\n\\[\n\\log{L(.)} = \\sum{[y_i \\log{\\pi_i} + (1-y_i)\\log{(1-y_i)}]}\n\\]\nThis can now be implemented as a function:\n\n\nCode\nllogit &lt;- function(par, y, X){\n    xform &lt;- function(z) {1 / (1 + exp(-z))}\n    p &lt;- xform(X%*%par)\n    sum(y * log(p) + (1-y) * log(1 - p))\n}\n\n\nLet’s pick an appropriate dataset. How about… picking a Palmer Penguin!?\n\n\nCode\nlibrary(tidyverse)\npalmerpenguins::penguins\n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nLet’s say we want to predict whether a penguin is of the Chinstrap species\n\n\nCode\npalmerpenguins::penguins %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(is_chinstrap = species == \"Chinstrap\") |&gt;\n    ggplot(aes(x = bill_length_mm, y = bill_depth_mm, colour = is_chinstrap, shape = sex)) + \n    geom_point()\n\n\n\n\n\nNeither bill length nor bill depth alone appears to distinguish between chinstrap and other species. But perhaps the interaction (product) of the two terms would do:\n\n\nCode\npalmerpenguins::penguins %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(is_chinstrap = species == \"Chinstrap\") |&gt;\n    mutate(bill_size = bill_length_mm * bill_depth_mm) |&gt;\n    ggplot(aes(x = bill_size, fill = is_chinstrap)) + \n    facet_wrap(~sex) + \n    geom_histogram()\n\n\n\n\n\nThe interaction term isn’t great at separating the two classes, but seems to be better than either length or size alone. So I’ll include it in the model.\n\n\nCode\ndf &lt;- palmerpenguins::penguins %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(is_chinstrap = species == \"Chinstrap\") |&gt;\n    mutate(bill_size = bill_length_mm * bill_depth_mm) |&gt;\n    mutate(is_male = as.numeric(sex == \"male\"))\n\ny &lt;- df$is_chinstrap\n\nX &lt;- cbind(1, df[,c(\"bill_length_mm\", \"bill_depth_mm\", \"bill_size\", \"is_male\")]) |&gt;\nas.matrix()\n\n\nSo, including the intercept term, our predictor matrix \\(X\\) contains 5 columns, including the interaction term bill_size. 16\nLet’s try now to use the above in optim()\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = rep(0, 5), \n    fn = llogit,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1] 82.9075239 -2.4368673 -6.4311531  0.1787047 -6.4900678\n\n$value\n[1] -33.31473\n\n$counts\nfunction gradient \n     137       45 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n             [,1]         [,2]          [,3]         [,4]         [,5]\n[1,]   -12.103063    -550.0621    -209.30944    -9674.925    -3.700623\n[2,]  -550.062097  -25256.3082   -9500.55848  -443670.225  -184.360139\n[3,]  -209.309443   -9500.5585   -3650.65107  -168517.417   -68.158844\n[4,] -9674.924703 -443670.2251 -168517.41718 -7846293.352 -3464.964868\n[5,]    -3.700623    -184.3601     -68.15884    -3464.965    -3.700623\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n            [,1]         [,2]         [,3]          [,4]         [,5]\n[1,] 41.95816335 -0.156192235 -0.309892876 -4.036895e-02  9.329019450\n[2,] -0.15619224 -0.005017392 -0.024806420  1.070652e-03 -0.139430425\n[3,] -0.30989288 -0.024806420 -0.042869947  2.854565e-03 -0.337480429\n[4,] -0.04036895  0.001070652  0.002854565 -7.331214e-05  0.003098092\n[5,]  9.32901945 -0.139430425 -0.337480429  3.098092e-03  1.202424836\n\n\nNow let’s compare with glm()\n\n\nCode\nmod_glm &lt;- glm(is_chinstrap ~ bill_length_mm * bill_depth_mm +is_male, data = df, \nfamily = binomial())\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = is_chinstrap ~ bill_length_mm * bill_depth_mm + \n    is_male, family = binomial(), data = df)\n\nCoefficients:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                  365.2924    88.3341   4.135 3.54e-05 ***\nbill_length_mm                -8.9312     2.0713  -4.312 1.62e-05 ***\nbill_depth_mm                -23.6184     5.5003  -4.294 1.75e-05 ***\nis_male                      -11.8725     2.6121  -4.545 5.49e-06 ***\nbill_length_mm:bill_depth_mm   0.5752     0.1292   4.452 8.53e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 337.113  on 332  degrees of freedom\nResidual deviance:  49.746  on 328  degrees of freedom\nAIC: 59.746\n\nNumber of Fisher Scoring iterations: 9\n\n\nUh oh! On this occasion it appears one or both approaches have become confused. A five dimensional search space might be too much for the algorithms to cope with, especially with collinearity 17 between some of the terms. Let’s simplify the task a bit, and just use intercept, bill size, and is_male as covariates. First with the standard package:\n\n\nCode\nmod_glm_simpler &lt;- glm(is_chinstrap ~ bill_size +is_male,   data = df, \nfamily = binomial())\nsummary(mod_glm_simpler)\n\n\n\nCall:\nglm(formula = is_chinstrap ~ bill_size + is_male, family = binomial(), \n    data = df)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -32.815339   4.325143  -7.587 3.27e-14 ***\nbill_size     0.043433   0.005869   7.400 1.36e-13 ***\nis_male      -7.038215   1.207740  -5.828 5.62e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 337.11  on 332  degrees of freedom\nResidual deviance:  90.60  on 330  degrees of freedom\nAIC: 96.6\n\nNumber of Fisher Scoring iterations: 7\n\n\nAnd now with the bespoke function and optim\n\n\nCode\nX &lt;- cbind(1, df[,c(\"bill_size\", \"is_male\")]) |&gt;\nas.matrix()\n\nfuller_optim_output &lt;- optim(\n    par = rep(0, 3), \n    fn = llogit,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1] -32.60343219   0.04314546  -6.98585077\n\n$value\n[1] -45.30114\n\n$counts\nfunction gradient \n      73       18 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]         [,2]         [,3]\n[1,]    -13.008605   -10662.078    -5.201308\n[2,] -10662.078251 -8846787.584 -4846.390833\n[3,]     -5.201308    -4846.391    -5.201308\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n             [,1]          [,2]         [,3]\n[1,] -536.7022079  0.7206703142 -134.7923170\n[2,]    0.7206703 -0.0009674672    0.1807806\n[3,] -134.7923170  0.1807806218  -33.4602664\n\n\nThe estimates from the two approaches are now much closer, even if they aren’t as close to each other as in the earlier examples. Using optim(), we have parameter estimates \\(\\beta = \\{\\beta_0 = -32.60, \\beta_1 = 0.04, \\beta_2 = -6.99\\}\\), and using glm(), we have estimates \\(\\beta = \\{\\beta_0 = -32.82, \\beta_1 = 0.04, \\beta_2 = -7.04 \\}\\)\nIf we cheat a bit, and give the five dimensional version starting values closer to the estimates from glm(), we can probably get similar estimates too.\n\n\nCode\nX &lt;- cbind(1, df[,c(\"bill_length_mm\", \"bill_depth_mm\", \"bill_size\", \"is_male\")]) |&gt;\nas.matrix()\n\nfuller_optim_output &lt;- optim(\n    par = c(300, -10, -29, 0.5, -10), \n    fn = llogit,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1] 299.5512512  -7.3684567 -19.3951742   0.4747209  -9.7521255\n\n$value\n[1] -25.33208\n\n$counts\nfunction gradient \n     153       22 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n             [,1]          [,2]          [,3]         [,4]         [,5]\n[1,]    -8.378918    -370.41592    -140.86865    -6342.301    -1.800406\n[2,]  -370.415921  -16580.87909   -6238.75358  -284403.350   -91.239716\n[3,]  -140.868648   -6238.75358   -2387.19776  -107598.410   -33.018551\n[4,] -6342.300809 -284403.34960 -107598.40987 -4906697.476 -1685.235507\n[5,]    -1.800406     -91.23972     -33.01855    -1685.236    -1.800406\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n            [,1]         [,2]        [,3]          [,4]         [,5]\n[1,] -59.5448267  2.316365876  5.14842594 -0.1737609491 10.383684649\n[2,]   2.3163659 -0.064512887 -0.16844980  0.0044962968 -0.166413655\n[3,]   5.1484259 -0.168449797 -0.33888931  0.0106735535 -0.387558164\n[4,]  -0.1737609  0.004496297  0.01067355 -0.0002712683  0.004068597\n[5,]  10.3836846 -0.166413655 -0.38755816  0.0040685965  1.904433768\n\n\nWell, they are closer, but they aren’t very close. As mentioned, the glm() model produced warnings, and some of the variables are likely to be collinear, so this initial specification may have been especially difficult to fit. Both approaches found an answer, but neither seem happy about it!\n\nSummary\nIn the exercise above we did for logistic regression what the previous few posts in section two did for standard regression: i.e. we derived the log likelihood, applied it using optim, and compared with results from the glm() package. We saw in this case that fitting models isn’t always straightforward. We were - well, I was - overly ambitious in building and applying an overly parameterised model specification. But we eventually got to similar parameter values using both approaches.\nThough this wasn’t as straightforward as I was hoping for, I’m presenting it warts-and-all. In principle, the log-likelihood maximisation approach generalises to a great many model specifications, even if in practice some model structures aren’t as straightforward to fit as others."
  },
  {
    "objectID": "pages/main-course/likelihood-and-simulation-theory/index.html#footnotes",
    "href": "pages/main-course/likelihood-and-simulation-theory/index.html#footnotes",
    "title": "Likelihood and Simulation Theory",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf \\(Y_i\\) is what the model predicts given observations \\(X_i\\), and \\(y_i\\) is the outcome observed to have occurred alongside \\(X_i\\), then we can call \\(\\delta_i = h(y_i, Y_i)\\) the difference, or error, between predicted and observed value. The function \\(h(.,.)\\) is typically the squared difference between predicted and observed values, \\((Y_i - y_i)^2\\), but could also in principle be the absolute difference \\(|Y_i - y_i|\\). Term-fitting algorithms usually compare not any individual \\(\\delta_i\\), but a sum of these error terms \\(\\delta\\). The aim of the algorithm is to find the set of \\(\\beta\\) terms that is least wrong for the whole dataset \\(D\\), rather than any specific row in the dataset \\(D_i\\).↩︎\nAs King (1998) (p. 59) describes it, “\\(k(y)\\) is an unknown fuction of the data. Whereas traditional probability is a measure of absolute uncertainty … the constant \\(k(y)\\) means that likelihood is only a relative measure of uncertainty”↩︎\nFrequentist approaches can thus be considered a kind of ‘improper Bayesian’ approach by considering \\(k(y)\\) in the Likelihood formula as a stand-in for \\(\\frac{P(\\tilde{\\theta})}{P(y)}\\) in Bayes’ Rule. Roughly speaking, it’s because of the improperness of treating the two terms as equivalent, and the relativeness of \\(k(y)\\), that mean frequentist probability statements can’t be interpreted as Bayesian probability statements. But thinking of the two terms as equivalent can be helpful for spotting the similarity between the two formulae.↩︎\ni.e. the square of the sd passed to rnorm() of 0.5↩︎\n\\(101^2 = 10201\\)↩︎\n\\(101^3 = 1030301\\)↩︎\nThough I had assumed Hessian matrices are called Hessian matrices because they sort-of resemble the criss-crossing grids of Hessian bags, they’re actually named after Otto Hesse, who proposed them.↩︎\nI’ve narrowed the space between values slightly, and increased the range of permutations of values to search through, for an even more precise recovery of the likelihood landscape.↩︎\nIn practice, the algorithm seeks to minimise the value returned by the function, not maximise it, hence the negative being applied through the argument fnscale = -1 in the control argument. But the principle is identical.↩︎\nThis means that, whereas the standard Normal returns a single output, the Multivariate Normal returns a vector of outputs, one for each parameter in \\(\\theta\\), which should also be the length of the diagonal (or alternatively either the number of rows or columns) of \\(\\Sigma\\).↩︎\nThe values will not be identical because the values for \\(\\eta\\), and so \\(\\sigma^2\\), have not been fixed at the true value in this example.↩︎\nWhere \\(\\sigma^2\\) is from \\(\\eta\\) and we defined \\(e^{\\eta} = \\sigma^2\\), a transformation which allowed optim() to search over an unbounded rather than bounded real number line↩︎\nIt can be easier to see this by using the more conventional way of expressing Normal linear regression: \\(Y_i = x_i \\beta + \\epsilon\\), where \\(\\epsilon \\sim Normal(0, \\sigma^2)\\). The expectation is therefore \\(E(Y_i) = E( x_i \\beta + \\epsilon ) = E(x_i \\beta) + E(\\epsilon)\\). For the first part of this equation, \\(E(x_i \\beta) = x_i \\beta\\), because the systematic component is always the same value, no matter how many times a draw is taken from the model. And for the second part, \\(E(\\epsilon) = 0\\), because Normal distributions are symmetrical around their central value over the long term: on average, every large positive value drawn from this distribution will become cancelled out by an equally large negative value, meaning the expected value returned by the distribution is zero. Hence, \\(E(Y) = x_i \\beta\\).↩︎\nBecause these estimates depend on random variation, these intervals may be slightly different to two decimal places than the values I’m quoting here.↩︎\nThanks to this post. My calculus is a bit rusty these days.↩︎\nAn important point to note is that, though bill_size is derived from other variables, it’s its own variable, and so has another distinct ‘slot’ in the vector of \\(\\beta\\) parameters. It’s just another dimension in the search space for optim to search through.↩︎\nThis is fancy-speak for when two terms aren’t independent, or both adding unique information. For example, length in mm, length in cm, and length in inches would all be perfectly collinear, so shouldn’t all be included in the model.↩︎"
  },
  {
    "objectID": "pages/main-course/complete-simulation-example/index.html",
    "href": "pages/main-course/complete-simulation-example/index.html",
    "title": "Statistical Simulation: A Complete Example",
    "section": "",
    "text": "Section One of this course introduced generalised linear models (GLMs) and statistical simuation. Section Two then delved more into the underlying theory and technicalities involved in implementing GLMs and using them for simulation.\nThis section gives a complete example of the methodology developed in these two sections, from start to finish. It also shows how the methodology is similar enough to Bayesian methods of statistical inference that applying a fully Bayesian modelling framework is just a small hop and jump from where we already are."
  },
  {
    "objectID": "pages/main-course/complete-simulation-example/index.html#recap",
    "href": "pages/main-course/complete-simulation-example/index.html#recap",
    "title": "Statistical Simulation: A Complete Example",
    "section": "",
    "text": "Section One of this course introduced generalised linear models (GLMs) and statistical simuation. Section Two then delved more into the underlying theory and technicalities involved in implementing GLMs and using them for simulation.\nThis section gives a complete example of the methodology developed in these two sections, from start to finish. It also shows how the methodology is similar enough to Bayesian methods of statistical inference that applying a fully Bayesian modelling framework is just a small hop and jump from where we already are."
  },
  {
    "objectID": "pages/main-course/complete-simulation-example/index.html#modelling-hamster-tooth-growth",
    "href": "pages/main-course/complete-simulation-example/index.html#modelling-hamster-tooth-growth",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Modelling Hamster Tooth Growth",
    "text": "Modelling Hamster Tooth Growth\nLet’s start with one of the built-in datasets, ToothGrowth, which is described as follows:\n\nThe response is the length of odontoblasts (cells responsible for tooth growth) in 60 guinea pigs. Each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods, orange juice or ascorbic acid (a form of vitamin C and coded as VC).\n\nLet’s load the dataset and visualise\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nWhat does it look like?\n\n\nCode\ndf |&gt;\n    ggplot(aes(y = len, x = dose, shape = supp, colour = supp)) + \n    geom_point() + \n    expand_limits(x = 0, y = 0)\n\n\n\n\n\nSo, although this has just three variables, there is some complexity involved in thinking about how the two predictor variables, supp and dose, relate to the response variable len. These include:\n\nWhether the relationship between len and dose is linear in a straightforward sense, or associated in a more complicated wway\nWhether supp has the same effect on len regardless of dose, or whether there is an interaction between dose and supp.\n\n\nStage One: model fitting\nWe can address each of these questions in turn, but should probably start with a model which includes both predictors:\n\n\nCode\nmod_01 &lt;- lm(len ~ dose + supp, data = df)\n\nsummary(mod_01)\n\n\n\nCall:\nlm(formula = len ~ dose + supp, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.600 -3.700  0.373  2.116  8.800 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.2725     1.2824   7.231 1.31e-09 ***\ndose          9.7636     0.8768  11.135 6.31e-16 ***\nsuppVC       -3.7000     1.0936  -3.383   0.0013 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.236 on 57 degrees of freedom\nMultiple R-squared:  0.7038,    Adjusted R-squared:  0.6934 \nF-statistic: 67.72 on 2 and 57 DF,  p-value: 8.716e-16\n\n\nEach term is statistically significant at the conventional thresholds (P &lt; 0.05), with higher doses associated with greater lengths. Compared to OJ, the reference category, a vitamin C (VC) supplement is associated with lower lengths.\nTurning to the first question, about the type of relationship between len and dose, one possibility is that greater doses lead to greater lengths, but there are diminishing marginal returns: the first mg has the biggest marginal effect, then the second mg has a lower marginal effect. An easy way to model this would be to include the log of dose in the regression model, rather than the dose itself.1 We can get a sense of whether this log dose specification might be preferred by plotting the data with a log scale on the x axis, and seeing if the points look like they ‘line up’ better:\n\n\nCode\ndf |&gt;\n    ggplot(aes(y = len, x = dose, shape = supp, colour = supp)) + \n    geom_point() + \n    scale_x_log10() + \n    expand_limits(x = 0.250, y = 0)\n\n\n\n\n\nYes, with this scaling, the points associated with the three dosage regimes look like they line up better. Let’s now build this model specification:\n\n\nCode\nmod_02 &lt;- lm(len ~ log(dose) + supp, data = df)\n\nsummary(mod_02)\n\n\n\nCall:\nlm(formula = len ~ log(dose) + supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2108 -2.9896 -0.5633  2.2842  9.1892 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.6633     0.7033   29.38  &lt; 2e-16 ***\nlog(dose)    11.1773     0.8788   12.72  &lt; 2e-16 ***\nsuppVC       -3.7000     0.9947   -3.72 0.000457 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.852 on 57 degrees of freedom\nMultiple R-squared:  0.755, Adjusted R-squared:  0.7464 \nF-statistic: 87.81 on 2 and 57 DF,  p-value: &lt; 2.2e-16\n\n\nAgain, the same kind of relationship between variables is observed: higher log dose: greater length; and VC rather than OJ is associated with lower growth. But is this model actually any better? The model summary for the linear dose model gives an adjusted \\(R^2\\) of 0.69, whereas for the log dose model the adjusted \\(R^2\\) is 0.75. So, as the data are fundamentally the same,2 this suggests it is. However, as we know that linear regression models are really just another kind of generalised linear models, and that model fitting tends to involve trying to maximise the log likelihood, we can also compare the log likelihoods of the two models, using the logLik() function, and so which is higher:\n\n\nCode\nlogLik(mod_01)\n\n\n'log Lik.' -170.2078 (df=4)\n\n\nCode\nlogLik(mod_02)\n\n\n'log Lik.' -164.5183 (df=4)\n\n\nBoth report the same number of degrees of freedom (‘df’), which shouldn’t be suprising as they involve the same number of parameters. But the log likelihood for mod_02 is higher, which like the Adjusted R-squared metric suggests a better fit.\nAnother approach, which generalises better to other types of model, is to compare the AICs, which are metrics that try to show the trade off between model complexity (based on number of parameters), and model fit (based on the log likelihood). By this criterion, the lower the score, the better the model:\n\n\nCode\nAIC(mod_01, mod_02)\n\n\n       df      AIC\nmod_01  4 348.4155\nmod_02  4 337.0367\n\n\nAs both models have exactly the same number of parameters, it should be of no surprise that mod_02 is still preferred.\nLet’s now address the second question: is there an interaction between dose and supp. This interaction term can be specified in one of two ways:\n\n\nCode\n# add interaction term explicitly, using the : symbol\nmod_03a &lt;- lm(len ~ log(dose) + supp + log(dose) : supp, data = df)\n\n# add interaction term implicitly, using the * symbol \nmod_03b &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(mod_03a)\n\n\n\nCall:\nlm(formula = len ~ log(dose) + supp + log(dose):supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(mod_03b)\n\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nWe can see from the summaries that both ways of specifying the models lead to exactly the same model, with exactly the same estimates, standared errors, adjusted \\(R^2\\)s, and so on. The adjusted \\(R^2\\) is now 0.76, a slight improvement on the 0.75 value for the model without the interaction term. As before, we can also compare the trade-off between additional complexity and improved fit using AIC\n\n\nCode\nAIC(mod_02, mod_03a)\n\n\n        df      AIC\nmod_02   4 337.0367\nmod_03a  5 333.7750\n\n\nSo, the AIC of the more complex model is lower, suggesting a better model, but the additional improvement in fit is small.\nWe can also compare the fit, and answer the question of whether the two models can be compared, in a couple of other ways. Firstly, we can use BIC, AIC’s (usually) stricter cousin, which tends to penalise model complexity more harshly:\n\n\nCode\nBIC(mod_02, mod_03a)\n\n\n        df      BIC\nmod_02   4 345.4140\nmod_03a  5 344.2467\n\n\nEven using BIC, the more complex model is still preferred, though the difference in values is now much smaller.\nThe other way we can compare the models is using an F-test using the anova (analysis of variance) function:\n\n\nCode\nanova(mod_02, mod_03a)\n\n\nAnalysis of Variance Table\n\nModel 1: len ~ log(dose) + supp\nModel 2: len ~ log(dose) + supp + log(dose):supp\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     57 845.91                              \n2     56 774.89  1    71.022 5.1327 0.02737 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere anova compares the two models, notes that the first model can be understood as a restricted variant of the second model,3 and compares the change in model fit between the two models against the change in number of parameters used to fit the model. The key parts of the summary to look at are the F test value, 5.13, and the associated P value, which is between 0.01 and 0.05. This, again, suggests the interaction term is worth keeping.\nSo, after all that, we finally have a fitted model. Let’s look now at making some predictions from it.\n\n\nStage Two: Model predictions\nThe simplest approach to getting model predictions is to use the predict function, passing it a dataframe of values for which we want predictions:\n\n\nCode\npredictor_df &lt;- expand_grid(\n    supp = c('VC', 'OJ'), \n    dose = seq(0.25, 2.25, by = 0.01)\n)\npreds_predictors_df &lt;- predictor_df |&gt;\n    mutate(pred_len = predict(mod_03a, predictor_df))\n\npreds_predictors_df\n\n\n# A tibble: 402 × 3\n   supp   dose pred_len\n   &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 VC     0.25   -1.20 \n 2 VC     0.26   -0.683\n 3 VC     0.27   -0.189\n 4 VC     0.28    0.288\n 5 VC     0.29    0.748\n 6 VC     0.3     1.19 \n 7 VC     0.31    1.62 \n 8 VC     0.32    2.04 \n 9 VC     0.33    2.44 \n10 VC     0.34    2.83 \n# ℹ 392 more rows\n\n\nWe can visualise these predictions as follows, with the predicted values as lines, and the observed values as points:\n\n\nCode\npreds_predictors_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, y = pred_len, colour = supp, linetype = interextrap)) + \n    geom_line() + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\"\n    )\n\n\n\n\n\nIn the above, I’ve shown the lines as solid when they represent interpolations of the data, i.e. are in the range of measured doses, and as dashed when they represent extrapolations from the data, meaning they are are predictions made outside the range of observed values. We can see an obvious issue when we extrapolate too far to the left: for low doses, and for the VC supplement, the model predicts negative tooth lengths. Extrapolation is dangerous! And gets more dangerous the further we extrapolate from available observations.\nWe can also use the predict function to produce uncertainty intervals, either of expected values, or predicted values. By default these are 95% intervals, meaning they are expected to contain 95% of the range of expected or predicted values from the model.\nLet’s first look at expected values, which include uncertainty about parameter estimates, but not observed variation in outcomes:\n\n\nCode\ndf_pred_intvl &lt;- predict(mod_03a, newdata = predictor_df, interval = \"confidence\")\n\npreds_predictors_intervals_df &lt;- \n    bind_cols(predictor_df, df_pred_intvl)\n\npreds_predictors_intervals_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, linetype = interextrap)) + \n    geom_line(aes(y = fit, colour = supp)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = supp), alpha = 0.2) + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\",\n        subtitle = \"Range of expected values\"\n    )\n\n\n\n\n\nAnd the following shows the equivalent prediction intervals, which also incorporate known variance, as well as parameter uncertainty:\n\n\nCode\ndf_pred_intvl &lt;- predict(mod_03a, newdata = predictor_df, interval = \"prediction\")\n\npreds_predictors_intervals_df &lt;- \n    bind_cols(predictor_df, df_pred_intvl)\n\npreds_predictors_intervals_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, linetype = interextrap)) + \n    geom_line(aes(y = fit, colour = supp)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = supp), alpha = 0.2) + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\",\n        subtitle = \"Range of predicted values\"\n    )\n\n\n\n\n\nAs should be clear from the above, and discussion of the difference between expected and predicted values in previous posts, predicted values and expected values are very different, and it is important to be aware of the difference between these two quantities of interest. Regardless, we can see once again how dangerous it is to use this particular model specification to extrapolate beyond the range of observations, expecially for lower doses."
  },
  {
    "objectID": "pages/main-course/complete-simulation-example/index.html#a-lower-level-example",
    "href": "pages/main-course/complete-simulation-example/index.html#a-lower-level-example",
    "title": "Statistical Simulation: A Complete Example",
    "section": "A lower level example",
    "text": "A lower level example\nWe’re now going to use the same hamster tooth growth model we used previously, but do some slightly more complicated things with it, relying a bit less on convenience functions and a bit more on our understanding of some of the fundamentals from Section Two.\n\nRecap of core concepts\nBack in Section Two we stated that estimates of the cloud of uncertainty in model parameters, that results from having limited numbers of observations in the data, can be represented as:\n\\[\n\\tilde{\\theta} \\sim MVN(\\mu = \\dot{\\theta}, \\sigma^2 = \\Sigma)\n\\]\nWhere MVN means multivariate normal, and needs the two quantities \\(\\dot{\\theta}\\) and \\(\\Sigma\\) as parameters.\nPreviously we showed how to extract (estimates of) these two quantities from optim(), where the first quantity, \\(\\dot{\\theta}\\), was taken from the converged parameter point estimate slot par, and the second quantity, \\(\\Sigma\\), was derived from the hessian slot.\nBut we don’t need to use optim() directly in order to recover these quantities. Instead we can get them from the standard model objects produced by either lm() or glm(). Let’s check this out…\nWith the model developed previously, let’s now look at some convenience functions, other than just summary, that work with lm() and glm() objects, and recover the quantities required from MVN to represent the uncertainty cloud.\n\n\nExtracting quantities for modelling uncertainty\nFirstly, for the point estimates \\(\\dot{\\theta}\\), we can use the coefficients() function\n\n\nCode\n## Building our model \n\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nCode\nbest_model &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(best_model)\n\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nCode\ncoef &lt;- coefficients(best_model)\n\ncoef\n\n\n     (Intercept)        log(dose)           suppVC log(dose):suppVC \n       20.663333         9.254889        -3.700000         3.844782 \n\n\nAnd for the variance-covariance matrix, for representing joint uncertainty about the above estimates, we can use the vcov function\n\n\nCode\nSig &lt;- vcov(best_model)\n\nSig\n\n\n                   (Intercept)     log(dose)        suppVC log(dose):suppVC\n(Intercept)       4.612422e-01 -8.768056e-17 -4.612422e-01    -7.224251e-17\nlog(dose)        -8.768056e-17  1.440023e+00  1.753611e-16    -1.440023e+00\nsuppVC           -4.612422e-01  1.753611e-16  9.224843e-01     1.748938e-16\nlog(dose):suppVC -7.224251e-17 -1.440023e+00  1.748938e-16     2.880045e+00\n\n\nFinally, we can extract the point estimate for stochastic variation in the model, i.e. variation assumed by the model even if parameter uncertainty were minimised, using the sigma function:\n\n\nCode\nsig &lt;- sigma(best_model)\n\nsig\n\n\n[1] 3.719847\n\n\nWe now have three quantities, coef, Sig and sig (note the upper and lower case s in the above). These provide something almost but not exactly equivalent to the contents of par and that derived from hessian when using optim() previously. The section below explains this distinction in more detail.\n\nBack to the weeds (potentially skippable)\nRecall the ‘grandmother formulae’, from King, Tomz, and Wittenberg (2000), which the first few posts in this series started with:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nFor standard linear regression this becomes:\nStochastic Component\n\\[\nY_i \\sim Norm(\\theta_i, \\sigma^2)\n\\]\nSystematic Component\n\\[\n\\theta_i =X_i \\beta\n\\]\nOur main parameters are \\(\\theta\\), which combined our predictors \\(X_i\\) and our model parameter estimates \\(\\beta\\). Of these two components we know the data - they are what they are - but are merely estimating our model parameters \\(\\beta\\). So, any estimation uncertainty in this part of the equation results from \\(\\beta\\) alone.\nOur ancillary parameter is \\(\\sigma^2\\). This is our estimate of how much fundamental variation there is in how the data (the response variables \\(Y\\)) is drawn from the stochastic data generating process.\nWhen we used optim() directly, we estimated \\(\\sigma^2\\) along with the other \\(\\beta\\) parameters, via the \\(\\eta\\) parameter eta, defined as \\(\\sigma^2 = e^{\\eta}\\) to allow optim() to search over an unbounded real number range. If there are k \\(\\beta\\) parameters, therefore, optim()’s par vector contained k + 1 values, with this last value being the point estimate for the eta parameter. Similarly, the number of rows, columns, and length of diagonal elements in the variance-covariance matrix recoverable through optim’s hessian slot was also k + 1 rather than k, with the last row, last column, and last diagonal element being measures of covariance between \\(\\eta\\) and the \\(\\beta\\) elements, and variance in \\(\\eta\\) itself.\nBy contrast, the length of coefficients returned by coefficients(best_model) is k, the number of \\(\\beta\\) parameters being estimated, and the dimensions of vcov(best_model) returned are also k by k.\nThis means there is one fewer piece/type of information about model parameters returned by coefficients(model), vcov(model) and sigma(model) than was potentially recoverable by optim()’s par and hessian parameter slots: namely, uncertainty about the true value of the ancillary parameter \\(\\sigma^2\\). The following table summarises this difference:\n\n\n\n\n\n\n\n\nInformation type\nvia optim\nvia lm and glm\n\n\n\n\nMain parameters: point\nfirst k elements of par\ncoefficients() function\n\n\nMain parameters: uncertainty\nfirst k rows and columns of hessian\nvcov() function\n\n\nAncillary parameters: point\nk+1th through to last element of par\nsigma() function or equivalent for glm()\n\n\nAncillary parameters: uncertainty\nlast columns and rows of hessian (after rows and columns k)\n—\n\n\n\nSo long as capturing uncertainty about the fundamental variability in the stochastic part of the model isn’t critical to our predictions then omission of a measure of uncertainty in the ancillary parameters \\(\\alpha\\) is likely a price worth paying for the additional convenience of being able to use the model objects directly. However we should be aware that, whereas with optim we potentially have both \\(\\tilde{\\beta}\\) and \\(\\tilde{\\alpha}\\) to represent model uncertainty, when using the three convenience functions coefficients(), vcov() and sigma() we technically ‘only’ have \\(\\tilde{\\beta}\\) and \\(\\dot{\\alpha}\\) (i.e. point estimates alone for the ancillary parameters).\nWith the above caveat in mind, let’s now look at using the results of coefficients(), vcov() and sigma() to generate (mostly) honest representations of expected values, predicted values, and first differences\n\n\n\nModel predictions\nAs covered in section two, we can use the mvrnorm function from the MASS package to create \\(\\tilde{\\beta}\\), our parameter estimates with uncertainty:\n\nParameter simulation\n\n\nCode\nbeta_tilde &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = coef, \n    Sigma = Sig\n)\n\nhead(beta_tilde)\n\n\n     (Intercept) log(dose)    suppVC log(dose):suppVC\n[1,]    21.38498  8.705757 -4.750950         4.674708\n[2,]    21.28967  8.056243 -4.050778         5.367108\n[3,]    20.18314 10.530421 -3.231182         3.878748\n[4,]    19.81736  8.345250 -1.732652         3.125173\n[5,]    20.64979  9.966520 -2.747275         4.414104\n[6,]    20.98720  9.180350 -3.032385         2.646055\n\n\nLet’s first look at each of these parameters individually:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    pivot_longer(everything(), names_to = \"coefficient\", values_to = \"value\") |&gt; \n    ggplot(aes(x = value)) + \n    facet_grid(coefficient ~ .) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nNow let’s look at a couple of coefficients jointly, to see how they’re correlated. Firstly the association between the intercept and the log dosage:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `(Intercept)`, y = `log(dose)`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nHere the covariance between the two parameters appears very low. Now let’s look at how log dosage and Vitamin C supplement factor are associated:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `log(dose)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nAgain, the covariance appears low. Finally, the association between log dose and the interaction term\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `log(dose)`, y = `log(dose):suppVC`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nHere we have a much stronger negative covariance between the two coefficients. Let’s look at the variance-covariance extracted from the model previously to confirm this:\n\n\nCode\nknitr::kable(Sig)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\nlog(dose)\nsuppVC\nlog(dose):suppVC\n\n\n\n\n(Intercept)\n0.4612422\n0.000000\n-0.4612422\n0.000000\n\n\nlog(dose)\n0.0000000\n1.440023\n0.0000000\n-1.440023\n\n\nsuppVC\n-0.4612422\n0.000000\n0.9224843\n0.000000\n\n\nlog(dose):suppVC\n0.0000000\n-1.440023\n0.0000000\n2.880045\n\n\n\n\n\nHere we can see that the covariance between intercept and log dose is effectively zero, as is the covariance between the intercept and the interaction term, and the covariance between the log(dose) and suppVC factor. However, there is a negative covariance between log dose and the interaction term, i.e. what we have plotted above, and also between the intercept and the VC factor. For completeness, let’s look at this last assocation, which we expect to show negative association:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `(Intercept)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nYes it is! The parameter estimates follow the covariance provided by Sigma, as we would expect.\n\n\n\nExpected values\nLet’s stay we are initially interested in the expected values for a dosage of 1.25mg, with the OJ (rather than VC) supplement:\n\n\nCode\n# first element is 1 due to intercept\npredictor &lt;- c(1, log(1.25), 0, 0) \n\npredictions_ev &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor %*% this_beta\n)\n\nhead(predictions_ev)\n\n\n[1] 23.32761 23.08737 22.53293 21.67955 22.87376 23.03573\n\n\nLet’s now get a 95% credible interval:\n\n\nCode\nquantile(predictions_ev, probs = c(0.025, 0.500, 0.975))\n\n\n    2.5%      50%    97.5% \n21.30301 22.72359 24.17352 \n\n\nSo, the 95% interval for the expected value is between 21.31 and 24.14, with a middle (median) estimate of 22.73.4 Let’s check this against estimates from the predict() function:\n\n\nCode\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'confidence')\n\n\n      fit      lwr      upr\n1 22.7285 21.26607 24.19093\n\n\nThe expected values using the predict function give a 95% confidence interval of 21.27 to 24.19, with a point estimate of 22.73. These are not identical, as the methods employed are not identical,5 but they are hopefully similar enough to demonstrate they are attempts at getting at the same quantities of interest.\n\n\nPredicted values\nPredicted values also include inherent stochastic variation from the ancillary parameters \\(\\alpha\\), which for linear regression is \\(\\sigma^2\\). We can simply add these only the expected values above to produce predicted values:\n\n\nCode\nn &lt;- length(predictions_ev)\n\nshoogliness &lt;- rnorm(n=n, mean = 0, sd = sig)\n\npredictions_pv &lt;- predictions_ev + shoogliness\n\n\nhead(predictions_pv)\n\n\n[1] 24.28755 23.32435 16.16326 23.39975 26.59984 24.70898\n\n\nLet’s get the 95% interval from the above using quantile\n\n\nCode\nquantile(predictions_pv, probs = c(0.025, 0.5000, 0.975))\n\n\n    2.5%      50%    97.5% \n15.25717 22.68875 30.22538 \n\n\nAs expected, the interval is now much wider, with a 95% interval from 15.34 to 30.11. The central estimate should in theory, with an infinite number of runs, be the same, however because of random variation it will never be exactly the same to an arbitrary number of decimal places. In this case, the middle estimate is 22.75, not identical to the central estimate from the expected values distribution of 22.72. The number of simulations can always be increased to produce greater precision if needed.\nLet’s now compare this with the prediction interval produce by the predict function:\n\n\nCode\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'prediction')\n\n\n      fit      lwr     upr\n1 22.7285 15.13461 30.3224\n\n\nAgain, the interval estimates are not exactly the same, but they are very similar.\n\n\nFirst differences\nIt’s in the production of estimates of first differences - this, compared to that, holding all else constant - that the simulation approach shines for producing estimates with credible uncertainty. In our case, let’s say we are interested in asking:\n\nWhat is the expected effect of using the VC supplement, rather than the OJ supplement, where the dose is 1.25mg?\n\nSo, the first difference is from switching from OJ to VC, holding the other factor constant.\nWe can answer this question by using the same selection of \\(\\tilde{\\beta}\\) draws, but passing two different scenarios:\n\n\nCode\n#scenario 0: supplement is OJ\npredictor_x0 &lt;- c(1, log(1.25), 0, 0) \n\n#scenario 1: supplement is VC\npredictor_x1 &lt;- c(1, log(1.25), 1, 1 * log(1.25)) \n\n\npredictions_ev_x0 &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x0 %*% this_beta\n)\n\npredictions_ev_x1 &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x1 %*% this_beta\n)\n\npredictions_df &lt;- \n    tibble(\n        x0 = predictions_ev_x0,\n        x1 = predictions_ev_x1\n    ) |&gt;\n    mutate(\n        fd = x1 - x0\n    )\n\npredictions_df\n\n\n# A tibble: 10,000 × 3\n      x0    x1    fd\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  23.3  19.6 -3.71\n 2  23.1  20.2 -2.85\n 3  22.5  20.2 -2.37\n 4  21.7  20.6 -1.04\n 5  22.9  21.1 -1.76\n 6  23.0  20.6 -2.44\n 7  22.1  19.8 -2.31\n 8  21.8  20.0 -1.83\n 9  22.7  20.2 -2.57\n10  22.9  20.7 -2.17\n# ℹ 9,990 more rows\n\n\nLet’s look at the distribution of both scenarios individually:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario != \"fd\") |&gt;\n    ggplot(aes(x = estimate)) + \n    facet_wrap(~scenario, ncol = 1) + \n    geom_histogram(bins = 100)\n\n\n\n\n\nAnd the distribution of the pairwise differences between them:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario == \"fd\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nIt’s this last distribution which shows our first differences, i.e. our answer, hedged with an appropriate dose of uncertainty, to the specific question shown above. We can get a 95% interval of the first difference as follows:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario == \"fd\") |&gt; \n    pull('estimate') |&gt;\n    quantile(probs = c(0.025, 0.500, 0.975))\n\n\n      2.5%        50%      97.5% \n-4.8591797 -2.8407341 -0.8408581 \n\n\nSo, 95% of estimates of the first difference are between -4.85 and -0.81, with the middle of this distribution (on this occasion) being -2.83.\nUnlike with the expected values and predicted values, the predict() function does not return first differences with honest uncertainty in this way. What we have above is something new.\n\n\nSummary\nIn this subsection we’ve finally combined all the learning we’ve developed over the two previous sectionsto answer three specific ‘what if?’ questions: one on expected values, one on predicted values, and one on first differences. These are what King, Tomz, and Wittenberg (2000) refer to as quantities of interest, and I hope you agree these are more organic and reasonable types of question to ask of data and statistical models than simply looking at coefficients and p-values and reporting which ones are ‘statistically significant’.\nIf you’ve been able to follow everything in these posts, and can generalise the approach shown above to other types of statistical model, then congratulations! You’ve learned the framework for answering meaningful questions using statistical models which is at the heart of one of the toughest methods courses for social scientists offered by one of the most prestigious universities in the world."
  },
  {
    "objectID": "pages/main-course/complete-simulation-example/index.html#introducing-bayesian-statistics-on-marbles-and-jumping-beans",
    "href": "pages/main-course/complete-simulation-example/index.html#introducing-bayesian-statistics-on-marbles-and-jumping-beans",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Introducing Bayesian Statistics: On marbles and jumping beans",
    "text": "Introducing Bayesian Statistics: On marbles and jumping beans\nSection Two introduced Bayes’ Rule and the Likelihood axiom. It pointed out that, at heart, Bayes’ Rule is a way of expressing that given this in terms of this given that; and that Likelihood is also a claim about how that given this relates to this given that. More specifically, the claim of Likelihood is:\n\nThe likelihood of the model given the data is proportional to the probability of the data given the model.\n\nThere are two aspects to the model: firstly its structure; secondly its parameters. The structure includes the type of statistical model - whether it is a standard linear regression, negative binomial regression, logistic regression, Poisson regression model and so on - and also the specific types of columns from the dataset selected as either predictor variables (\\(X\\)) or response variables (\\(Y\\)). It is only after both the higher level structure of the model family, and the lower level structure of the data inputs (what’s being regressed on what?) have been decided that the Likelihood theory is used.\nAnd how is Likelihood theory used? Well, it defines a landscape over which an algorithm searches. This landscape has as many dimensions as there are parameters to fit. Where there are just two parameters, \\(\\beta_0\\) and \\(\\beta_1\\) to fit, we can visualise this landscape using something like a contour plot, with \\(\\beta_0\\) as latitude, \\(\\beta_1\\) as longitude, and the likelihood at this position its elevation or depth. Each possible joint value \\(\\beta = \\{\\beta_0, \\beta_1\\}\\) which the algorithm might wish to propose leads to a different long-lat coordinate over the surface, and each coordinate has a different elevation or depth. Although we can’t see beyond three dimensions (latitude, longitude, and elevation/depth), mathematics has no problem extending the concept of multidimensional space into far more dimensions than we can see or meaningfully comprehenend. If a model has ten parameters to fit, for example, the likelihood search space really is ten dimensional, and so on.\nNoticed I used elevation and depth interchangably in the description above. Well, this is because it really doesn’t matter whether an optimisation algorithm is trying to find the greatest elevation over a surface, or the greatest depth over the surface. The aim of maximum likelihood estimation is to find the configuration of parameters that maximises the likelihood, i.e. finds the top of the surface. However we saw that when passing the likelihood function to optim() we often inverted the function by multiplying it by -1. This is because the optimisation algorithms themselves seek to minimise the objective function they’re passed, not maximise it. By multiplying the likelihood function by -1 we made what we were trying to seek compatible with what the optimisation algorithms seek to do: find the greatest depth over a surface, rather than the highest elevation over the surface.\nTo make this all a bit less abstract let’s develop the intuition of an algorithm that seeks to minimise a function by way of a(nother) weird little story:\n\nImagine there is a landscape made out of transparent perspex. It’s not just transparent, it’s invisible to the naked eye. And you want to know where the lowest point of this surface is. All you have to do this is a magical leaking marble. The marble is just like any other marble, except every few moments, at regular intervals (say every tenth of a second), it dribbles out a white dye that you can see. And this dye sticks on and stains the otherwise invisible landscape whose lowest point you wish to find.\n\n\nNow, you drop the marble somewhere on the surface. You see the first point it hits on the surface - a white blob appears. The second blob appears some distance away from the first blob; and the third blob slightly less far away from the second blob as the second was to the second. After a few seconds, a trail of white spots is visible, the first few of which form something like a straight line, each consecutive point slightly less closer to the previous one. A second or two later, and the rumbling sounds of the marble rolling over the surface cease; the marble has clearly run out of momentum. And as you look at the trail of dots it’s generated, and is still generating, and you see it keeps highlighting the same point on the otherwise invisible surface, again and again.\n\nPreviously I used the analogy of a magical robo-chauffer, taking you to the top of a landscape. But the falling marble is probably a closer analogy to how many of optim()’s algorithms actually work. Using gravity and its shape alone, it finds the lowest point on the surface, and with its magical leaking dye, it tells you where this lowest point is.\nNow let’s extend the story to convert the analogy of the barefoot-and-blind person from section two as well:\n\nThe marble has now ‘told’ you where the lowest point on the invisible surface is. However you also want to know more about the shape of the depression it’s in. You want to know if it’s a steep depression, or a shallow depression. And you want to know if it’s as steep or shallow in every direction, or if it’s steeper in some ways than the other.\n\n\nSo you now have to do a bit more work. You move your hand to just above the marble, and with your forefinger ‘flick’ it in a particular direction (say east-west): you see it move in the direction you flick it briefly, before rolling back towards (and beyond, and then towards) the depression point. As it does so, it leaks dye onto the surface, revealing a bit more about the landscape’s steepness or shallowness in this dimension. Then you do the same, but along a different dimension (say, north-south). After you’ve done this enough times, you are left with a collection of dyed points on the part of the surface closest to its deepest depression. The spacing and shape of these points tells you something about the nature of the depression and the part of the landscape it’s surrounding.\n\nNotice in this analogy you had to do extra work to get the marble to reveal more information about the surface. By default, the marble tells you the specific location of the depression, but not what the surface is like around this point. Instead, you need to intervene twice: firstly by dropping the marble onto the surface; secondly by flicking it around once it’s reached the lowest point on the surface.\nNow, let’s imagine swapping out our magical leaking marble for something even weirder: a magical leaking jumping bean.\n\nThe magical jumping bean does two things: it leaks and it jumps. (Okay, it does three things: when it leaks it also sticks to the surface it’s dying). When the bean is first dropped onto the surface, it marks the location it lands on. Then, it jumps up and across in a random direction. After jumping, it drops onto another part of the surface, marks it, and the process starts again. Jumping, sticking, marking; jumping, sticking, marking; jumping, sticking, marking… potentially forever.\n\n\nBecause of the effect of gravity, though the jumping bean jumps in a random direction, after a few jump-stick-mark steps it’s still, like the marble, very likely to move towards the depression. However, unlike the marble, even when it gets towards the lowest point in the depression, it’s not going to just rest there. The magical jumping bean is never at rest. It’s forever jump-stick-marking, jump-stick-marking.\n\n\nHowever, once the magical bean has moved towards the depression, though it keeps moving, it’s likely never to move too far from the depression. Instead, it’s likely to bounce around the depression. And as it does so, it drops ever more marks on the surface, which keep showing what the surface looks like around the depression in ever more detail.\n\nSo, because of the behaviour of the jumping bean, you only have to act on it once, by choosing where to drop it, rather than twice as with the marble: first choosing where to drop it, then flicking it around once it’s reached the lowest point on the surface.\n\nSo what?\nIn the analogies above, the marble is to frequentist statistics as the jumping bean is to Bayesian statistics. A technical distinction between the marble and the jumping bean is that the marble converges towards a point (meaning it reaches a point of rest on the surface) whereas the jumping bean converges towards a distribution (meaning it never rests).\nIt’s Bayesian statistics’ 6 property of converging to a distribution rather than a point that makes the converged posterior distribution of parameter estimates Bayesian models produce ideal for the kind of honest prediction so much of this blog series has been focused on.\nLet’s now do some Bayesian modelling to compare…\n\n\nBayesian modelling: now significantly less terrifying than it used to be\nThere are a lot of packages and approaches for building Bayesian models. In fact there are whole statistical programming languages - like JAGS, BUGS 7 and Stan - dedicated to precisely describing every assumption the statistician wants to make about how a Bayesian model should be built. For more complicated and bespoke models these are ideal.\nHowever there are also an increasingly large number of Bayesian modelling packages that abstract away some of the assumptions and complexity apparent in the above specialised Bayesian modelling languages, and allow Bayesian versions of the kinds of model we’re already familiar with to be specified using formulae interfaces almost identical to what we’ve already worked with. Let’s look at one of them, rstanarm, which allows us to use stan, a full Bayesian statistical programming language, without quite as much thinking and set-up being required on our part.\nLet’s try to use this to build a Bayesian equivalent of the hamster tooth model we worked on in the last couple of posts.\n\n\nData Preparation and Frequentist modelling\nLet’s start by getting the dataset and building the frequentist version of the model we’re already familiar with:\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nCode\nbest_model_frequentist &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(best_model_frequentist)\n\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nBuilding the Bayesian equivalent\nNow how would we build a Bayesian equivalent of this? Firstly let’s load (and if necessary install8) rstanarm.\n\n\nCode\nlibrary(rstanarm)\n\n\nWhereas for the frequentist model we used the function lm(), rstanarm has what looks like a broadly equivalent function stan_lm(). However, as I’ve just discovered, it’s actually more straightforward with stan_glm instead:\n\n\nCode\nbest_model_bayesian &lt;- stan_glm(len ~ log(dose) * supp, data = df)\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.001295 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 12.95 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.03 seconds (Warm-up)\nChain 1:                0.037 seconds (Sampling)\nChain 1:                0.067 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 7e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.032 seconds (Warm-up)\nChain 2:                0.025 seconds (Sampling)\nChain 2:                0.057 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 4e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.033 seconds (Warm-up)\nChain 3:                0.028 seconds (Sampling)\nChain 3:                0.061 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 8e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.03 seconds (Warm-up)\nChain 4:                0.028 seconds (Sampling)\nChain 4:                0.058 seconds (Total)\nChain 4: \n\n\nCode\nsummary(best_model_bayesian)\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      len ~ log(dose) * supp\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 60\n predictors:   4\n\nEstimates:\n                   mean   sd   10%   50%   90%\n(Intercept)      20.7    0.7 19.8  20.7  21.6 \nlog(dose)         9.2    1.2  7.7   9.2  10.8 \nsuppVC           -3.7    1.0 -4.9  -3.7  -2.6 \nlog(dose):suppVC  3.9    1.8  1.6   3.9   6.1 \nsigma             3.8    0.4  3.3   3.8   4.3 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 18.8    0.7 17.9  18.8  19.7 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                 mcse Rhat n_eff\n(Intercept)      0.0  1.0  3921 \nlog(dose)        0.0  1.0  2761 \nsuppVC           0.0  1.0  4014 \nlog(dose):suppVC 0.0  1.0  2782 \nsigma            0.0  1.0  4003 \nmean_PPD         0.0  1.0  4098 \nlog-posterior    0.0  1.0  1797 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nSome parts of the summary for the Bayesian model look fairly familiar compared with the frequentist model summary; other bits a lot more exotic. We’ll skip over a detailed discussion of these outputs for now, though it is worth comparing the estimates section of the summary directly above, from the Bayesian approach, with the frequentist model produced earlier.\nThe frequentist model had point estimates of \\(\\{20.7, 9.3, -3.7, 3.8\\}\\). The analogous section of the Bayesian model summary is the mean column of the estimates section. These are reported to fewer decimal places by default - Bayesians are often more mindful of spurious precision - but are also \\(\\{20.7, 9.3, -3.7, 3.8\\}\\), so the same to this number of decimal places.\nNote also the Bayesian model reports an estimate for an additional parameter, sigma. This should be expected if we followed along with some of the examples using optim() for linear regression: the likelihood function required the ancillary parameters (referred to as \\(\\alpha\\) in the ‘mother model’ which this series started with, and part of the stochastic component \\(f(.)\\)) be estimated as well as the primary model parameters (referred to as \\(\\beta\\) in the ‘mother model’, and part of the systematic component \\(g(.)\\)). The Bayesian model’s coefficients (Intercept), log(dose), suppVC and the interaction term log(dose):suppVC are all part of \\(\\beta\\), whereas the sigma parameter is part of \\(\\alpha\\). The Bayesian model has just been more explicit about exactly which parameters it’s estimated from the data.\nFor the \\(\\beta\\) parameters, the Std. Error column in the Frequentist model summary is broadly comparable with the sd column in the Bayesian model summary. For the \\(\\beta\\) parameters these values are \\(\\{0.7, 1.2, 1.0, 1.7\\}\\) in the Frequentist model, and \\(\\{0.7, 1.2, 1.0, 1.7\\}\\) in the Bayesian model the summary. i.e. they’re the same to the degree of precision offered in the Bayesian model summary.\nBut let’s get to the crux of the argument: with Bayesian models honest predictions are easier.\nAnd they are, with the posterior_predict() function, passing what we want to predict on through the newdata argument, much as we did with the predict() function with frequentist models.\n\n\nScenario modelling\nLet’s recall the scenarios we looked at previously:\n\npredicted and expected values: length when dosage is 1.25mg and supplement is OJ\nfirst difference difference between OJ and VC supplement when dosage is 1.25mg\n\nLet’s start with the first question:\n\n\nCode\npredictors &lt;- data.frame(supp = \"OJ\", dose = 1.25)\n\npredictions &lt;- rstanarm::posterior_predict(\n    best_model_bayesian,\n    newdata = predictors\n)\n\nhead(predictions)\n\n\n            1\n[1,] 16.45905\n[2,] 26.91309\n[3,] 28.30747\n[4,] 25.59066\n[5,] 21.48906\n[6,] 22.22209\n\n\nCode\ndim(predictions)\n\n\n[1] 4000    1\n\n\nBy default posterior_predict() returns a matrix, which in this case has 4000 rows and just a single column. Let’s do a little work on this and visualise the distribution of estimates it produces:\n\n\nCode\npreds_df &lt;- tibble(estimate = predictions[,1])\n\n# lower, median, upper\nlmu &lt;- quantile(preds_df$estimate, c(0.025, 0.500, 0.975))\n\nlwr &lt;- lmu[1]\nmed &lt;- lmu[2]\nupr &lt;- lmu[3]\n\npreds_df |&gt;\n    mutate(\n        in_range = between(estimate, lwr, upr)\n    ) |&gt;\n    ggplot(aes(x = estimate, fill = in_range)) + \n    geom_histogram(bins = 100) + \n    scale_fill_manual(\n        values = c(`FALSE` = 'lightgray', `TRUE` = 'darkgray')\n    ) +\n    theme(legend.position = \"none\") + \n    geom_vline(xintercept = med, linewidth = 1.2, colour = \"steelblue\")\n\n\n\n\n\nThe darker-shaded parts of the histogram show the 95% uncertainty interval, and the blue vertical line the median estimate. This 95% interval range is 14.95 to 30.04.\nRemember we previously estimated both the expected values and the predicted values for this condition. Our 95% range for the expected values were 20.27 to 24.19 (or thereabouts), whereas our 95% range for the predicted values were (by design) wider, at 15.34 to 30.11. The 95% uncertainty interval above is therefore of predicted values, which include fundamental variation due to the ancillary parameters \\(\\sigma\\), rather than expected values, which result from parameter uncertainty alone.\nThere are a couple of other functions in rstanarm we can look at: predictive_error() and predictive_interval()\nFirst here’s predictive_interval. It is a convenience function that the posterior distribution generated previously, predictions, and returns an uncertainty interval:\n\n\nCode\npredictive_interval(\n    predictions\n)\n\n\n        5%      95%\n1 16.22679 28.92694\n\n\nWe can see by default the intervals returned are from 5% to 95%, i.e. are the 90% intervals rather than the 95% intervals considered previously. We can change the intervals requested with the prob argument:\n\n\nCode\npredictive_interval(\n    predictions, \n    prob = 0.95\n)\n\n\n      2.5%    97.5%\n1 14.95077 30.03604\n\n\nAs expected, this requested interval returns an interval closer to (but not identical to) the interval estimated using the quantile function.\nLet’s see if we can also use the model directly, specifying newdata directly to predictive_interval:\n\n\nCode\npredictive_interval(\n    best_model_bayesian,\n    newdata = predictors, \n    prob = 0.95\n)\n\n\n      2.5%   97.5%\n1 14.95388 30.5151\n\n\nYes. This approach works too. The values aren’t identical as, no doubt, a more sophisticated approach is used by predictive_interval to estimate the interval than simply arranging the posterior estimates in order using quantile.\nFor producing expected values we can use the function posterior_epred:\n\n\nCode\nepreds &lt;- posterior_epred(\n    best_model_bayesian,\n    newdata = predictors\n)\n\nexp_values &lt;- epreds[,1]\n\nquantile(exp_values, probs = c(0.025, 0.500, 0.975))\n\n\n    2.5%      50%    97.5% \n21.29030 22.75297 24.20892 \n\n\nFor comparison, the expected value 95% interval we obtained from the Frequentist model was 21.3 to 24.2 when drawing from the quasi-posterior distribution, and 22.7 to 24.2 when using the predict() function with the interval argument set to \"confidence\".\nNow, finally, let’s see if we can produce first differences: the estimated effect of using VC rather than OJ as a supplement when the dose is 1.25mg\n\n\nCode\npredictors_x0 &lt;- data.frame(supp = \"OJ\", dose = 1.25)\npredictors_x1 &lt;- data.frame(supp = \"VC\", dose = 1.25)\n\npredictors_fd &lt;- rbind(predictors_x0, predictors_x1)\n\npredictions_fd &lt;- rstanarm::posterior_predict(\n    best_model_bayesian,\n    newdata = predictors_fd\n)\n\nhead(predictions_fd)\n\n\n            1        2\n[1,] 20.43321 16.39788\n[2,] 20.82978 19.84081\n[3,] 27.13488 22.76374\n[4,] 18.45427 14.22702\n[5,] 24.62406 18.13178\n[6,] 19.00336 22.00924\n\n\nThe newdata argument to posterior_predict now has two rows, one for the OJ supplement and the other for the VC supplement scenario. And the predictions matrix returned by posterior_predict now has two columns: one for each scenario (row) in predictors_fd. We can look at the distribution of both of these columns, as well as the rowwise comparisions between columns, which will give our distribution of first differences for the predicted values:\n\n\nCode\npreds_fd_df &lt;- \n    predictions_fd |&gt;\n        as_tibble(rownames = \"draw\") |&gt;\n        rename(x0 = `1`, x1 = `2`) |&gt;\n        mutate(fd = x1 - x0)\n\npreds_fd_df |&gt; \n    select(-fd) |&gt;\n    pivot_longer(cols = c(\"x0\", \"x1\"), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    facet_wrap(~ scenario, nrow = 2)\n\n\n\n\n\nTo reiterate, these are predicted values for the two scenarios, not the expected values shown in the first differences section of post 12. This explains why there is greater overlap between the two distributions. Let’s visualise and calculate the first differences in predicted values:\n\n\nCode\npreds_fd_df |&gt;\n    select(fd) |&gt;\n    ggplot(aes(x = fd)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nWe can see that the average of the distribution is below 0, but as we are looking at predicted values the range of distributions is much higher. Let’s get 95% intervals:\n\n\nCode\nquantile(preds_fd_df$fd, probs = c(0.025, 0.500, 0.975))\n\n\n      2.5%        50%      97.5% \n-13.857756  -2.952790   8.078398 \n\n\nThe 95% intervals for first differences in predicted values is from -13.6 to +7.9, with the median estimate at -3.0. As expected, the median is similar to the equivalent value from using expected values (-2.9) but the range is wider.\nNow let’s use posterior_epred to produce estimates of first differences in expected values, which will be more directly comparable to our first differences estimates in section two:\n\n\nCode\npredictions_fd_ev &lt;- posterior_epred(\n    best_model_bayesian,\n    newdata = predictors_fd\n)\n\nhead(predictions_fd_ev)\n\n\n          \niterations        1        2\n      [1,] 22.32912 19.16732\n      [2,] 23.11436 19.15944\n      [3,] 23.21977 20.45930\n      [4,] 22.52365 19.96163\n      [5,] 22.82536 20.36854\n      [6,] 22.69024 19.94984\n\n\n\n\nCode\npreds_fd_df_ev &lt;- \n    predictions_fd_ev |&gt;\n        as_tibble(rownames = \"draw\") |&gt;\n        rename(x0 = `1`, x1 = `2`) |&gt;\n        mutate(fd = x1 - x0)\n\npreds_fd_df_ev |&gt; \n    select(-fd) |&gt;\n    pivot_longer(cols = c(\"x0\", \"x1\"), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    facet_wrap(~ scenario, nrow = 2)\n\n\n\n\n\nThis time, as the stochastic variation related to the \\(\\sigma\\) term has been removed, the distributions of the expected values are more distinct, with less overlap. Let’s visualise and compare the first differences of the expected values:\n\n\nCode\npreds_fd_df_ev |&gt;\n    select(fd) |&gt;\n    ggplot(aes(x = fd)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\n\n\nCode\nquantile(preds_fd_df_ev$fd, probs = c(0.025, 0.500, 0.975))\n\n\n      2.5%        50%      97.5% \n-4.9253769 -2.8496558 -0.7829446 \n\n\nWe now have a 95% interval for the first difference in expected values of -4.9 to -0.7. By contrast, the equivalent range estimated using the Frequentist model in part 12 was -4.8 to -0.8. So, although they’re not identical, they do seem to be very similar.\n\n\nBayesian Statistics Summary\nUp until now we’ve been using Frequentist approaches to modelling. However the simulation approach required to produce honest uncertainty depends on ‘tricking’ Frequentist models into producing something like the converged posterior distributions which, in Bayesian modelling approaches, come ‘for free’ from the way in which Bayesian frameworks estimate model parameters.\nAlthough Bayesian models are generally more technically and computationally demanding than Frequentist models, we have shown the folllowing:\n\nThat packages like rstanarm abstract away some of the challenges of building Bayesian models from scratch;\nThat the posterior distributions produced by Bayesian models produce estimates of expected values, predicted values, and first differences - our substantive quantities of interest - that are similar to those produced previously from Frequentist models\nThat for the estimation of these quantities of interest, the posterior distributions Bayesian models generate make it more straightforward, not less, to produce using Bayesian methods than using Frequentist methods.\n\nThanks for reading, and congratulations on getting this far through the series."
  },
  {
    "objectID": "pages/main-course/complete-simulation-example/index.html#footnotes",
    "href": "pages/main-course/complete-simulation-example/index.html#footnotes",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr perhaps more accurately Bayesian statistical model estimation rather than Bayesian statistics more generally? Bayes’ Rule can be usefully applied to interpret results derived from frequentist models. But the term Bayesian Modelling generally implies that Bayes’ Rule is used as part of the model parameter estimation process, in which a prior distribution is updated according to some algorithm, and then crucially the posterior distribution produced then forms the prior distribution at the next step in the estimation. The specific algorithm that works as the ‘jumping bean’ is usually something like Hamiltonian Monte Carlo, HMC, and the general simulation framework in which a posterior distribution generated from applying Bayes’ Rule is repeatedly fed back into the Bayes’ Rule equation as the prior distribution is known as Markov Chain Monte Carlo, MCMC.↩︎\nBecause the simulation approach relies on random numbers, the draws will never be the same unless the same random number seed is using using set.seed(). However with more simulations, using the n parameter from mvrnorm, the distributions of estimates should become ever closer to each other.↩︎\nIn this example, our more complex model has coefficients fit from the data for the intercept, log(dose), supp and the interaction term log(dose):supp, whereas the less complex model has coefficients fit from the data for the intercept, log(dose), and supp. This means the less complex model can be specified as a restricted version of the more complex model, where the value of the coefficient on the interaction term log(dose):supp is set to be equal to zero, rather than determined from the data. An equivalent way of phrasing and thinking about this is that the two model specifications are nested, with the restricted model nested inside the unrestricted model, which includes the interaction term. It’s this requirement for models to be nested in this way which meant that mod_01 and mod_02 could not be compared using an F-test, as neither model could be described strictly as restricted variants of the other model: they’re siblings, not mothers and daughters. However, both mod_01 and mod_02 could be compared against a common ancestor model which only includes the intercept term.↩︎\nOr perhaps more accurately Bayesian statistical model estimation rather than Bayesian statistics more generally? Bayes’ Rule can be usefully applied to interpret results derived from frequentist models. But the term Bayesian Modelling generally implies that Bayes’ Rule is used as part of the model parameter estimation process, in which a prior distribution is updated according to some algorithm, and then crucially the posterior distribution produced then forms the prior distribution at the next step in the estimation. The specific algorithm that works as the ‘jumping bean’ is usually something like Hamiltonian Monte Carlo, HMC, and the general simulation framework in which a posterior distribution generated from applying Bayes’ Rule is repeatedly fed back into the Bayes’ Rule equation as the prior distribution is known as Markov Chain Monte Carlo, MCMC.↩︎\nBecause the simulation approach relies on random numbers, the draws will never be the same unless the same random number seed is using using set.seed(). However with more simulations, using the n parameter from mvrnorm, the distributions of estimates should become ever closer to each other.↩︎\nOr perhaps more accurately Bayesian statistical model estimation rather than Bayesian statistics more generally? Bayes’ Rule can be usefully applied to interpret results derived from frequentist models. But the term Bayesian Modelling generally implies that Bayes’ Rule is used as part of the model parameter estimation process, in which a prior distribution is updated according to some algorithm, and then crucially the posterior distribution produced then forms the prior distribution at the next step in the estimation. The specific algorithm that works as the ‘jumping bean’ is usually something like Hamiltonian Monte Carlo, HMC, and the general simulation framework in which a posterior distribution generated from applying Bayes’ Rule is repeatedly fed back into the Bayes’ Rule equation as the prior distribution is known as Markov Chain Monte Carlo, MCMC.↩︎\nOminously named.↩︎\nrstanarm has a lot of dependencies. It’s the friendly, cuddly face of a beast!↩︎"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-14/index.html",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-14/index.html",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "",
    "text": "Henry Dundas, as observed\n\n\n\n\n\n\n\nHenry Dundas, the unobserved good counterfactual\n\n\n\n\n\n\n\nHenry Dundas, the unobserved bad counterfactual"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-14/index.html#high-level-notewarning",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-14/index.html#high-level-notewarning",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "High level note/warning",
    "text": "High level note/warning\nThere are broadly two schools of thought when it comes to thinking about the problems of causal inference. One which interprets the challenge of causal inference mainly as a missing data problem; and another which interprets it mainly in terms of a modelling problem. The posts in this series are largely drawn from the missing data interpretation. If you want an overview of the two approaches (albeit subject to my own ignorance and biases), please skip briefly to the last post in this series before continuing."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-14/index.html#henry-dundas-hero-or-villain",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-14/index.html#henry-dundas-hero-or-villain",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "Henry Dundas: Hero or Villain?",
    "text": "Henry Dundas: Hero or Villain?\nA few minutes’ walk from where I live is St Andrew Square. And in the middle of St Andrew Square is the Melville Monument, a 40 metre tall column, on which stands a statue of Henry Dundas, 1st Viscount Melville.\nThough the Melville Monument was constructed in the 19th century to commemorate and celebrate this 18th century figure, in 2020 the City of Edimburgh Council chose to add more context to Dundas’ legacy by unveiling a plaque with the following message::\n\nAt the top of this neoclassial column stands a statue of Hentry Dundas, 1st Viscount Melville (1742-1811). He was the Scottish Lord Advocate, an MP for Edinburgh and Midlothian, and the First Lord of the Admiralty. Dundas was a contentious figure, provoking controversies that resonate to this day. While Home Secretary in 1792, and first Secretary of State for War in 1796 he was instrumental in deferring the abolition of the Atlantic slave trade. Slave trading by British ships was not abolished until 1807. As a result of this delay, more than half a million enslaved Africans crossed the Atlantic.\n\nSo, the claim of the council plaque was that Dundas caused the enslavement of hundreds of thousands of Africans, by promoting a gradualist policy of abolition.\nThe descendents of Dundas contested these claims, however, instead arguing:\n\nThe claim that Henry Dundas caused the enslavement of more than half a million Africans is patently false. The truth is: Dundas was the first MP to advocate in Parliament for the emancipation of slaves in the British territories along with the abolition of the slave trade. Dundas’s efforts resulted in the House of Commons voting in favour of ending the Atlantic slave trade for the first time in its history.\n\nSo, the claim of the descendents was that Dundas prevented the enslavement of (at least) hundreds of thousands of Africans, by promoting a gradualist policy of abolition.\nHow can the same agreed-upon historical facts lead to such diametrically opposing interpretations of the effects of Dundas and his actions?\nThe answer to this question is at the heart of causal inference, and an example of why, when trying to estimate causal effects, at least half of the data are always missing."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-14/index.html#the-unobserved-counterfactual",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-14/index.html#the-unobserved-counterfactual",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "The unobserved counterfactual",
    "text": "The unobserved counterfactual\nBoth parties in the Dundas debate have, as mentioned, access to the same historical facts. They agree on the same observed historical reality. And both are making bold claims about the impact of Dundas in relation to the Transatlantic slave trade. In doing this, they are both comparing this observed historical reality with something else: the unobserved counterfactual.\nThe unobserved counterfactual is the data that would have been observed if what had happened, hadn’t happened 1 However, what happened did happen, so this data isn’t observed. So, as it hasn’t been observed, it doesn’t exist in any historic facts. Instead, the unobserved counterfactual has to be imputed, or inferred… in effect, made up.\nCausal inference always involves some kind of comparison between an observed reality and an unobserved counterfactual. The issue at heart of the Dundas debate is that both parties have compared the observed reality with a different unobserved counterfactual, and from this different Dundas effects have been inferred.\nFor the council, the unobserved counterfactual appears to be something like the following:\n\nDundas doesn’t propose a gradualist amendment to a bill in parliament. The more radical and rapid version of the bill passes, and slavery is abolished earlier, leading to fewer people becoming enslaved.\n\nWhereas for the descendents, the unobserved counterfactual appears to be something like this:\n\nDundas doesn’t propose a gradualist amendment to a bill in parliament. Because of this, the more radical version of the bill doesn’t have enough support in parliament (perhaps because it would be acting too much against the financial interests of some parliamentarians and powerful business interests), and so is defeated. As a result of this, the abolition of slavery is delayed, leading to more people becoming enslaved.\n\nSo, by having the same observed historical facts, the observed Dundas, but radically different counterfactuals, the two parties have used the same methodology to derive near antithetical estimates of the ‘Dundas Effect’."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-14/index.html#coming-up",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-14/index.html#coming-up",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "Coming up",
    "text": "Coming up\nThe next post offers more of a technical treatment of the key concept introduced here: namely that causal effect estimation depends on comparing observed with counterfactual data, and as the counterfactual is unobserved, causal effect estimation is fundamentally a missing data problem."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-14/index.html#footnotes",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-14/index.html#footnotes",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data that would have been observed if what hadn’t happened, had happened, is the other type of unobserved counterfactual.↩︎"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-16/index.html",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-16/index.html",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "",
    "text": "This is the third post in a short mini-series on causal inference, which extends a much longer series on statistical theory and practice. After introducing the fundamental issue of causal inference, namely that the counterfactual is unobserved, through description alone in part 14, part 15 provided a more technical treatment of the same issues. We described the Platinum 1 Standard of data required for causal inference as involving observing the same individuals in two different scenarios - treated2 and untreated3 - which is not possible; and the Gold Standard as being a randomised controlled trial (RCT), which is sometimes possible, but tends to be time and resource intensive. The RCT is a mechanism for breaking the association between assignment to treatment \\(Z_i\\) and both known/included covariates \\(X^*_i\\) and unknown/unincluded characteristics \\(W_i\\); this link-breaking is described as orthogonality and represented algebraically as \\(Z_i \\perp X_i^*\\) and \\(Z_i \\perp W^*_i\\).\nThe purpose of this post is to introduce some of the statistical approaches used when the only data available are observational, and so do not meet the special properties required for robust causal inference estimation of an RCT."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-16/index.html#introduction",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-16/index.html#introduction",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "",
    "text": "This is the third post in a short mini-series on causal inference, which extends a much longer series on statistical theory and practice. After introducing the fundamental issue of causal inference, namely that the counterfactual is unobserved, through description alone in part 14, part 15 provided a more technical treatment of the same issues. We described the Platinum 1 Standard of data required for causal inference as involving observing the same individuals in two different scenarios - treated2 and untreated3 - which is not possible; and the Gold Standard as being a randomised controlled trial (RCT), which is sometimes possible, but tends to be time and resource intensive. The RCT is a mechanism for breaking the association between assignment to treatment \\(Z_i\\) and both known/included covariates \\(X^*_i\\) and unknown/unincluded characteristics \\(W_i\\); this link-breaking is described as orthogonality and represented algebraically as \\(Z_i \\perp X_i^*\\) and \\(Z_i \\perp W^*_i\\).\nThe purpose of this post is to introduce some of the statistical approaches used when the only data available are observational, and so do not meet the special properties required for robust causal inference estimation of an RCT."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-16/index.html#method-one-controlling-for-variables",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-16/index.html#method-one-controlling-for-variables",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Method One: ‘Controlling for’ variables",
    "text": "Method One: ‘Controlling for’ variables\nThe most familiar approach for trying to estimate the causal effect of treatment \\(Z\\) on outcome \\(Y\\) is to construct a multivariate 4 regression model. Here we make sure to include those ‘nuisance parameters’ \\(X^*\\) on the predictor side of the model’s equation, along with our treatment parameter of interest \\(Z\\). For each individual \\(i\\) in the dataset \\(D\\) we can therefore use the model, calibrated on the data, to produce a prediction of the outcome \\(Y_i\\) under both the treated scenario \\(Z_i = 1\\) and the untreated scenario \\(Z_i = 0\\). As post four discussed, in the specific case of linear regression, but few other model specifications, this causal effect estimate of treatment \\(Y_i | Z=1 - Y_i | Z = 0\\) can be gleamed directly from the \\(\\beta\\) coefficient for \\(Z\\). As post four also makes clear, for other model specifications, the process for estimating causal effects can be more involved.\nIt is worth pointing out that, when using models in this way, we are really ‘just’ producing estimates of first differences, the quantity of interest which we focused on in posts 11, 12, and 13. The model prediction approach is not fundamentally any different to that discussed previously, except for two things: firstly, that we will usually be averaging across first differences for multiple observations rather a single scenario; and secondly, that we will be interpreting the first differences (or rather their aggregation) as being a causal effect estimate.\nThere are actually two types of causal effect estimate we can produce using this approach, the Average Treatment Effect (ATE), and the Average Treatment Effect on the Treated (ATT). 5 The difference between ATE and ATT is that, for ATE, the counterfactuals are simulated for all observations in the dataset \\(D\\), and that these counterfactuals will be both for individuals which were observed as treated \\(Z=1\\) and untreated \\(Z=0\\). By contrast, for ATT, only those observations in the data which were observed as treated \\(Z=1\\) are included in the causal effect estimation,6 meaning that the counterfactual being modelled will always be of the scenario \\(Z=0\\).\nSo, what are the potential problems with modelling in this way?\n\nUnobserved and unincluded covariates. Remember in the previous part we introduced the term \\(W_i^*\\)? This refers to those factors which could affect assignment \\(Z_i\\) but which are not included in our model. They could either be: i) covariates that exist in the dataset \\(D\\) but we chose not to include in the model \\(M\\); or ii) covariates that are simply not recorded in the dataset \\(D\\), so even if we wanted to, we couldn’t include them. In an RCT, the random allocation mechanism breaks both the \\(X^* \\rightarrow Z\\) and the \\(W \\rightarrow Z\\) causal paths; we don’t have to observe or even know what these factors \\(W\\) might be for an RCT to block their influence. But a regression model can only really operate to attempt to attenuate the \\(X^* \\rightarrow Z\\) pathway.\nInsufficient or improper controls. Returning to our hamster tooth growth example of post 11, recall we looked at a number of different model specifications. Our starter model specification included ‘controls for’ both dosage and supplement, and so did our final model specification. But does this mean either model is equally good at ‘controlling for’ these factors? I’d suggest they aren’t, as though our final model specification included the same covariates \\(X\\) as the initial model specification, it represented the relationship between the predictor and response variables in a qualitatively different way. For the final model specification, the dosage variable was transformed by logging it; additionally, an interaction term was included between (transformed) dosage and supplement. The reasons for this were justified by the observed relationships and by measures of penalised model fit, but we do not know if this represents the ‘best possible’ model specification. And the specification used, and the assumptions contained and represented by the model specification, will affect the predictions the model produces, including the first differences used to produce the ATE and ATT causal effect estimates.\n\nOverall, just remember that, when a researcher states in a paper that they have used a model to ‘control for’ various factors and characteristics, this can often be more a statement of what the researcher aspired to do with the model rather than managed to do. There are often a great many researcher degrees of freedom in terms of how a particular observational dataset can be used to produce modelled estimates of causal effects, and these can markedly affect the effect estimates produced.\nSo, what are some alternatives?"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-16/index.html#method-two-matching-methods",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-16/index.html#method-two-matching-methods",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Method Two: Matching methods",
    "text": "Method Two: Matching methods\nRemember the Platinum Standard: For each individual, with their own personal characteristics (\\(X_i^*\\)), we known if they were treated \\(Z_i = 1\\) or untreated \\(Z_i = 0\\). In the sci-fi scenario of the genuine Platinum Standard, we are able to observe a clone of each of these individuals in the parallel universe of the unobserved counterfactual.\nObviously we can’t do that in reality. But maybe was can do something, with the data we have, which allows us to do something like the Platinum Standard, individual level pairwise comparison, \\(Y_i | Z_i = 1 - Y_i | Z_i = 0\\), even though we only precisely observe each individual \\(i\\) in one of the two scenarios \\(Z=1\\) or \\(Z=0\\).\nWe can do this by relaxing the requirement that the counterfactual be of a clone of the observed individual, and so identical in every way except for treatment status, and instead allow them to be compared to someone who’s merely similar to them.\nLet’s think through an example:\n\nBilly is 72 years old, male, overweight but not obese, works part time as a carpenter but is largely retired, married for five years but before that a widower for three, hypertensive; scores in the 85th percentile for conscentiousness, and 40th percentile for openness, in the Big Five Personality scale; owns his own home, worked in a factory in his twenties, likes baked beans with his biweekly fish suppers, enjoys war films but also musicals, liked holidaying in Spain back in the 1990s when his children were still children; owns a thirteen year old dog with advancing arthritis, who when younger used to take him on regular brisk walks, but now has to be cajoled to leave the house, especially when it’s cold and wet outside. He lives in the North East of England, and when that young woman - who seemed friendly but a bit nervous and had that weird piece of metal through the middle of her nose - from the survey company knocked on the door four months ago, and asked him to rate his level of agreement to the statement, “I am satisfied with my life” on a seven point scale, he answered with ‘6 - agree’, but pursed his lips and took five seconds to answer this question.\n\nObviously we have a lot of information about Billy. But that doesn’t mean the survey company, and thus our dataset \\(D\\), knows all that we now know. So, some of the information in the above is contained in \\(X^*_i\\), but others is part of \\(W_i\\).\nAnd what’s our treatment, and what’s our outcome? Let’s say the outcome is the response to the life satisfaction question, and the treatment is UK region, with the South East excluding London as the ‘control’ region.\nSo, how do matching methods work? Well, they can of course only work with the data available to them, \\(D\\). The basic approach is as follows:\n\nFor each person like Billy, who’s in the ‘treatment’ group \\(Z = 1\\) (‘treated’ to living in the North of England), we know various recorded characteristics about them \\(X_j^*\\), and so we want to look for one or more people on the ‘control’ group \\(Z=0\\) who are like the treated individual.\nSo, for Billy, we’re looking for someone in the part of the dataset where \\(Z=0\\) whose characteristics other than treatment assignment, i.e. \\(X^*\\) not \\(Z\\), are similar to Billy’s. Let’s say that, on paper, the person who’s most similar to Billy in the dataset is Mike, who’s 73 (just one year older), also owns his own home, also married, has a BMI of 26.3 (Billy’s is 26.1), and also diagnosed with hypertension. But, whereas Billy lives in the North of England, Mike lives in the South East.\nWe then compare the recorded response for Billy (6 - agree) with the recorded response for Mike (5 - mildly agree), to get an estimated treatment effect for Billy. 7\nWe then repeat the exercise for everyone else who, like Billy, is in the treatment/exposure group, trying to match them up with one or more individuals in the control group pool.\nOnce we’ve done that, we then average up the paired differences in responses - between each treated individual, and each person the’ve been paired up with - to produce an average treatment effect on the treated (ATT) estimate.\n\nHow do we go about about matchmaking Billy and other treated individuals? There are a variety of approaches, and as with using regression to ‘control for’ variables quite a lot of researcher degrees of freedom, different ways of matching, that can lead to different causal effect estimates. These include:\n\nExact matching: Find someone for all available characteristics other than assignment is exactly the same as the individual in the treated group to be matched. Obviously this is seldom possible, so an alternative is:\nCoarsened Exact Matching: Lump the characteristics into broader groups, such as 10 year age groups rather than age in single years, and match on someone who’s exactly roughly the same, i.e. matches the target within the more lumped/aggregated categories rather than exactly the same to the finest level of data resolution agailable.\nPropensity Score Matching: Use the known characteristics of individiduals to predict their probability of being in the treatment group, then use these predicted probabilities to try to balance the known characteristics of the populations in both treatment and control arms.\nSynthetic Controls: Combine and ‘mix’ observed characteristics from multiple untreated/unexposed individuals so that their average/admixed/combined characteristics is closely similar to those of individuals in the treated/exposed population.\n\nThese approaches are neither exhaustive nor mutually exclusive, and there are a great many ways that they could be applied in practice. One of the general aims of matching approaches is to reduce the extent to which ATT or ATE estimates depend on the specific modelling approach adopted, 8 and for Propensity Score Matching, it’s often to try to break the \\(X^* \\rightarrow Z\\) link, and so achieve orthogonality (\\(X^* \\perp Z\\)). However, it can’t necessarily do the same with unobserved characteristics (\\(W \\rightarrow Z\\))."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-16/index.html#method-three-utilise-natural-experiments",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-16/index.html#method-three-utilise-natural-experiments",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Method Three: Utilise ‘natural experiments’",
    "text": "Method Three: Utilise ‘natural experiments’\nThe idea with a ‘natural experiment’ is that something happens in the world that just happens to break the links between individual characteristics and assignment to exposure/treatment. The world has therefore created a situation for us where the orthogonality assumptions \\(W \\perp Z\\) and \\(X^* \\perp Z\\) which are safe to assume when working with RCT data can also, probably, possibly, be made with certain types of observational data too. When such factors are proposed and used by economists, they tend to call them instrumental variables. Some examples include:\n\nLottery winnings to estimate the effect of money on happiness: A lottery win is an increase in money available to someone that ‘just happens’ (at least amongst lottery players). Do lottery winners’ subjective wellbeing scores increase following a win? If so for how long? Why is this preferable to just looking at the relationship between income/assets and happiness? Well, the causality could go the other way: perhaps happier people work harder, increasing their income. Or perhaps a common underlying personality factor - something like ‘conscientious stoicism’, which isn’t measured - affects both income and happiness. By utilising the randomness of a big win allocation to just a small minority of players, 9 such alternative explanations for why there are differences between populations being compared can be more safely discounted.\nComparing educational outcomes for pupils who only just got into, and only just got rejected from, selective schools and universities: Say a selective school runs its own standardised entry exam, for which a pass mark of 70 or higher is required to be accepted. An applicant who achieves a mark of 69 isn’t really that different in their aptitude than one who achieves a of 70, but this one point difference sadly appears to make the world of difference for the applicant with a 69, and gladly appears to make the world of difference for the applicant with a 70. For years afterwards, the 70-scoring applicant will have access to a fundemntally different educational environment than the 69-scoring applicant. And presumably both applicants 10 both applied because they thought the selective educational institution really would make a substantial and positive difference for their long-term educational outcomes. But does it really? By following the actual educational outcomes of pupils just north of the selection boundary, and of non-pupils just south of the selection boundaries, we have something like a treatment and control group, whose only main difference is that some are in the selective school and some are not.\n\nNote that neither of these examples are perfect substitutes for an RCT. Perhaps the people who win lotteries, or win big, are different enough from those who don’t that the winner/non-winner group’s aren’t similar in important ways. And perhaps the way people process and feel about money they get through lottery winnings isn’t the same as they they receive through earnings or social security, so the idea of there being a single money-to-happiness pathway isn’t valid. For the second example there are other concerns: of course applicants only one mark apart won’t be very different to each other, but there won’t be many of these, meaning the precision of the estimate will tend to be low. So how about expanding the ‘catchment’ to each arm, either side of the boundary line, to 2 marks, 3 marks, 5 marks? Now there should be more people in both the control and treatment arms, but they’ll also be more different to each other. 11\nAs you might expect, if using instrumental variables, the quality of the instrument matters a lot. But generally the quality of the instrument isn’t something that can be determined through any kind of formal or statistical test. It tends to be, for want of a better term, a matter of story telling. If the story the researcher can tell their audience, about the instrument and why it’s able to break the causal links it needs to break, is convincing to the audience, then the researcher and audience will both be more willing to assume that the estimates produced at the end of the analysis are causal."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-16/index.html#summing-up",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-16/index.html#summing-up",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Summing up",
    "text": "Summing up\nSo, three methods for trying to do something technically impossible: using observational data to estimate causal effects. These methods aren’t mutually exclusive, nor are they likely to be exhaustive, and nor are any of them failsafe.\nIn the absence of being able to really know, to peak behind the veil and see the causal chains working their magic, a good pragmatic strategy tends to be to try multiple approaches. At its extreme, this can mean asking multiple teams of researchers the same question, and giving them access to the same dataset, and encouraging each team to not contact any other teams until they’ve finished their analysis, then compare the results they produce. If many different teams, with many different approaches, all tend to produce similar estimates, then maybe the estimates are really tapping into genuine causal effects, and not just reflecting some of the assumptions and biases built into the specific models and methods we’re using?"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-16/index.html#coming-up",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-16/index.html#coming-up",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Coming up",
    "text": "Coming up\nThe next post attempts to apply matching methods to a relatively complex dataset on an economic intervention, using the MatchIt package. The post largely follows an introductory example from the package, but at some points goes ‘off piste’. I hope it does so, however, in ways that are interesting, useful, and help bridge the gaps between the theoretical discussions in this and previous posts, with the practical challenges involved in applying such theory."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-16/index.html#footnotes",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-16/index.html#footnotes",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Footnotes",
    "text": "Footnotes\n\n\npronounced ‘unobtainium’↩︎\nAKA ‘exposed’↩︎\nAKA ‘unexposed’↩︎\nOr multivariable, if we wish to reserve the term multivariate to models with multiple response columns.↩︎\nLogically, we should assume there is also an Average Treatment Effect on the Untreated (ATU), but this is seldom discussed in practice.↩︎\nThis might be represented as something like \\(D^{(T)} \\subset D \\iff Z_i = 1\\), i.e. the data used are filtered based on the value of \\(Z\\) matching a condition.↩︎\nThis data is really ordinal, meaning we know ‘agree’ is higher than ‘mildly agree’, but don’t know how much higher, so should really be modelled as such, with something like an ordered logit or ordered probit model specification. However it’s often either treated as cardinal - 1, 2, 3, 4, 5, 6, 7 - with something like a linear regression, or collapsed into two categories (agree/ don’t agree) so standard logit or probit regression could be used.↩︎\nEven B-A is a modelling approach, to an extent.↩︎\nIt could be you. But it probably won’t be.↩︎\nOr their pushy parents…↩︎\nAn example of a bias/variance trade-off↩︎"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-17/index.html",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-17/index.html",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "",
    "text": "The previous post (re)introduced three ways to try to allow causal effect estimation using observational data: i) ‘controlling for’ variables using multiple regression; ii) matching methods; iii) Identifying possible ‘natural experiments’ in observational datasets. The fundamental challenge of using observational data to estimate causal effects is that we cannot be sure either the observed (\\(X^*\\)) or unobserved (\\(W\\)) characteristics of observations do not influence allocation to exposure/treatment, i.e. cannot rule out \\(X^* \\rightarrow Z\\) or \\(W \\rightarrow Z\\), meaning that statistical estimates of the effect of Z on the outcome \\(Z \\rightarrow y_i\\) may be biased.\nThe first two approaches will, within limits, generally attenuate the link between \\(X^*\\) and \\(Z\\), but can do little to break the link between \\(W\\) and \\(Z\\), as \\(W\\) is by definition those features of observational units that are not contained in the dataset \\(D\\), and so any statistical method will be ‘blind’ to. The last approach, if the instrumental variable possesses the properties we expect and hope it will, should be able to break the \\(W \\rightarrow Z\\) link too. But unfortunately that can be a big if: the instrument may not have the properties we hope it does.\nThis post will go explore some application of the first two approaches: controlling for variables using multiple regression; and using matching methods. A fuller consideration of the issues is provided in Ho et al. (2007), and the main package and dataset used will be that of the associated MatchIt package Ho et al. (2011) and vignette using the lalonde dataset."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-17/index.html#recap-and-aim",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-17/index.html#recap-and-aim",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "",
    "text": "The previous post (re)introduced three ways to try to allow causal effect estimation using observational data: i) ‘controlling for’ variables using multiple regression; ii) matching methods; iii) Identifying possible ‘natural experiments’ in observational datasets. The fundamental challenge of using observational data to estimate causal effects is that we cannot be sure either the observed (\\(X^*\\)) or unobserved (\\(W\\)) characteristics of observations do not influence allocation to exposure/treatment, i.e. cannot rule out \\(X^* \\rightarrow Z\\) or \\(W \\rightarrow Z\\), meaning that statistical estimates of the effect of Z on the outcome \\(Z \\rightarrow y_i\\) may be biased.\nThe first two approaches will, within limits, generally attenuate the link between \\(X^*\\) and \\(Z\\), but can do little to break the link between \\(W\\) and \\(Z\\), as \\(W\\) is by definition those features of observational units that are not contained in the dataset \\(D\\), and so any statistical method will be ‘blind’ to. The last approach, if the instrumental variable possesses the properties we expect and hope it will, should be able to break the \\(W \\rightarrow Z\\) link too. But unfortunately that can be a big if: the instrument may not have the properties we hope it does.\nThis post will go explore some application of the first two approaches: controlling for variables using multiple regression; and using matching methods. A fuller consideration of the issues is provided in Ho et al. (2007), and the main package and dataset used will be that of the associated MatchIt package Ho et al. (2011) and vignette using the lalonde dataset."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-17/index.html#getting-started",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-17/index.html#getting-started",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Getting started",
    "text": "Getting started\nWe start by loading the Matchit package and exploring the lalonde dataset.\n\n\nCode\nlibrary(tidyverse)\nlibrary(MatchIt)\nunmatched_data &lt;- tibble(lalonde)\n\nunmatched_data\n\n\n# A tibble: 614 × 9\n   treat   age  educ race   married nodegree  re74  re75   re78\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1    37    11 black        1        1     0     0  9930.\n 2     1    22     9 hispan       0        1     0     0  3596.\n 3     1    30    12 black        0        0     0     0 24909.\n 4     1    27    11 black        0        1     0     0  7506.\n 5     1    33     8 black        0        1     0     0   290.\n 6     1    22     9 black        0        1     0     0  4056.\n 7     1    23    12 black        0        0     0     0     0 \n 8     1    32    11 black        0        1     0     0  8472.\n 9     1    22    16 black        0        0     0     0  2164.\n10     1    33    12 white        1        0     0     0 12418.\n# ℹ 604 more rows"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-17/index.html#data",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-17/index.html#data",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Data",
    "text": "Data\nThe description of the lalonde dataset is as follows:\n\n\nCode\nhelp(lalonde)\n\n\n\nDescription\nThis is a subsample of the data from the treated group in the National Supported Work Demonstration (NSW) and the comparison sample from the Population Survey of Income Dynamics (PSID). This data was previously analyzed extensively by Lalonde (1986) and Dehejia and Wahba (1999).\nFormat\nA data frame with 614 observations (185 treated, 429 control). There are 9 variables measured for each individual.\n\n“treat” is the treatment assignment (1=treated, 0=control).\n“age” is age in years.\n“educ” is education in number of years of schooling.\n“race” is the individual’s race/ethnicity, (Black, Hispanic, or White). Note previous versions of this dataset used indicator variables black and hispan instead of a single race variable.\n“married” is an indicator for married (1=married, 0=not married).\n“nodegree” is an indicator for whether the individual has a high school degree (1=no degree, 0=degree).\n“re74” is income in 1974, in U.S. dollars.\n“re75” is income in 1975, in U.S. dollars.\n“re78” is income in 1978, in U.S. dollars.\n\n“treat” is the treatment variable, “re78” is the outcome, and the others are pre-treatment covariates.\n\nLet’s look at the data to get a sense of it:\n\n\nCode\nunmatched_data |&gt;\n    mutate(treat = as.factor(treat)) |&gt;\n    filter(re78 &lt; 25000) |&gt;\n    ggplot(aes(y = re78, x = re75, shape = treat, colour = treat)) + \ngeom_point() + \ngeom_abline(intercept = 0, slope = 1) +\ncoord_equal() + \nstat_smooth(se = FALSE, method = \"lm\")\n\n\n\n\n\nClearly this is quite complicated data, where the single implied control, wages in 1975 (re75) is not sufficient. There are also a great many observations where wages in either of both years were 0, hence the horizontal and vertical streaks apparent.\nThe two lines are the linear regression lines for the two treatment groups as a function of earlier wage. The lines are not fixed to have the same slope, so the differences in any crude treatment effect estimate vary by earlier wage, but for most previous wages the wages in 1978 appear to be lower in the treatment group (blue), than the control group (red). This would suggest either that the treatment may be harmful to wages… or that there is severe imbalance between the characteristics of persons in both treatment conditions.\nLet’s now start to use a simple linear regression to estimate an average treatment effect, before adding more covariates to see how these model-derived estimates change\n\n\nCode\n# Model of treatment assignment only\nmod_01 &lt;- lm(re78 ~ treat, unmatched_data)\nsummary(mod_01) \n\n\n\nCall:\nlm(formula = re78 ~ treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6984  -6349  -2048   4100  53959 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6984.2      360.7  19.362   &lt;2e-16 ***\ntreat         -635.0      657.1  -0.966    0.334    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7471 on 612 degrees of freedom\nMultiple R-squared:  0.001524,  Adjusted R-squared:  -0.0001079 \nF-statistic: 0.9338 on 1 and 612 DF,  p-value: 0.3342\n\n\nOn average the treated group had (annual?) wages $635 lower than the control group. However the difference is not statistically significant.\nNow let’s add previous wage from 1975\n\n\nCode\nmod_02 &lt;- lm(re78 ~ re75 + treat, unmatched_data)\nsummary(mod_02)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-15918  -5457  -2025   3824  54103 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5547.63718  412.84637  13.438  &lt; 2e-16 ***\nre75           0.58242    0.08937   6.517  1.5e-10 ***\ntreat        -90.79498  641.40291  -0.142    0.887    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7230 on 611 degrees of freedom\nMultiple R-squared:  0.06642,   Adjusted R-squared:  0.06336 \nF-statistic: 21.73 on 2 and 611 DF,  p-value: 7.611e-10\n\n\nPreviously observed wage is statistically significant and positive. The point estimate on treatment is smaller, and even less ‘starry’.\nNow let’s add all possible control variables and see what the treatment effect estimate produced is:\n\n\nCode\nmod_03 &lt;- lm(re78 ~ re75 + age + educ + race + married + nodegree + re74 + treat, unmatched_data)\nsummary(mod_03)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + age + educ + race + married + nodegree + \n    re74 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13595  -4894  -1662   3929  54570 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.174e+03  2.456e+03  -0.478   0.6328    \nre75         2.315e-01  1.046e-01   2.213   0.0273 *  \nage          1.298e+01  3.249e+01   0.399   0.6897    \neduc         4.039e+02  1.589e+02   2.542   0.0113 *  \nracehispan   1.740e+03  1.019e+03   1.708   0.0882 .  \nracewhite    1.241e+03  7.688e+02   1.614   0.1071    \nmarried      4.066e+02  6.955e+02   0.585   0.5590    \nnodegree     2.598e+02  8.474e+02   0.307   0.7593    \nre74         2.964e-01  5.827e-02   5.086 4.89e-07 ***\ntreat        1.548e+03  7.813e+02   1.982   0.0480 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6948 on 604 degrees of freedom\nMultiple R-squared:  0.1478,    Adjusted R-squared:  0.1351 \nF-statistic: 11.64 on 9 and 604 DF,  p-value: &lt; 2.2e-16\n\n\nWith all of these variables as controls, the effect of treatment is now statistically significant and positive, associated with on average an increase of $155 over the control group.\nHowever, we should probably be concerned about how dependent this estimate is on the specific model specification we used. For example, it is fairly common to try to ‘control for’ nonlinearities in age effects by adding a squared term. If modeller decisions like this don’t make much difference, then its addition shouldn’t affect the treatment effect estimate. Let’s have a look:\n\n\nCode\nmod_04 &lt;- lm(re78 ~ re75 + poly(age, 2) + educ + race + married + nodegree + re74 + treat, unmatched_data)\nsummary(mod_04)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + poly(age, 2) + educ + race + married + \n    nodegree + re74 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13692  -4891  -1514   3884  54313 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -5.395e+02  2.172e+03  -0.248   0.8039    \nre75           2.190e-01  1.057e-01   2.072   0.0387 *  \npoly(age, 2)1  3.895e+03  7.994e+03   0.487   0.6262    \npoly(age, 2)2 -6.787e+03  7.918e+03  -0.857   0.3917    \neduc           3.889e+02  1.599e+02   2.432   0.0153 *  \nracehispan     1.682e+03  1.021e+03   1.648   0.0999 .  \nracewhite      1.257e+03  7.692e+02   1.634   0.1028    \nmarried        2.264e+02  7.267e+02   0.312   0.7555    \nnodegree       3.185e+02  8.504e+02   0.375   0.7081    \nre74           2.948e-01  5.832e-02   5.055 5.73e-07 ***\ntreat          1.369e+03  8.090e+02   1.692   0.0911 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6949 on 603 degrees of freedom\nMultiple R-squared:  0.1488,    Adjusted R-squared:  0.1347 \nF-statistic: 10.54 on 10 and 603 DF,  p-value: &lt; 2.2e-16\n\n\nThe inclusion of the squared term to age has changed the point estimate of treatment from around $1550 to $1370. However it has also changed the statistical significance of the effect from p &lt; 0.05 to p &lt; 0.10, i.e. from ‘statistically significant’ to ‘not statistically significant’. If we were playing the stargazing game, this might be the difference between a publishable finding and an unpublishable finding.\nAnd what if we excluded age, because none of the terms are statistically significant at the standard level?\n\n\nCode\nmod_05 &lt;- lm(re78 ~ re75 + educ + race + married + nodegree + re74 + treat, unmatched_data)\nsummary(mod_05)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + educ + race + married + nodegree + \n    re74 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13681  -4912  -1652   3877  54648 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -676.43048 2115.37702  -0.320   0.7493    \nre75           0.22705    0.10395   2.184   0.0293 *  \neduc         389.00786  154.33865   2.520   0.0120 *  \nracehispan  1710.16654 1015.15590   1.685   0.0926 .  \nracewhite   1241.00510  768.22972   1.615   0.1067    \nmarried      478.55017  671.28910   0.713   0.4762    \nnodegree     201.04497  833.99164   0.241   0.8096    \nre74           0.30209    0.05645   5.351 1.24e-07 ***\ntreat       1564.68896  779.65173   2.007   0.0452 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6943 on 605 degrees of freedom\nMultiple R-squared:  0.1475,    Adjusted R-squared:  0.1363 \nF-statistic: 13.09 on 8 and 605 DF,  p-value: &lt; 2.2e-16\n\n\nNow the exclusion of this term, which the coefficient tables suggested wasn’t statistically significant, but intuitively we recognise as an important determinant of labour market activity, has led to yet another point estimate. It’s switched back to ‘statistically significant’ again, but now the point estimate is about $1565 more. Such estimates aren’t vastly different, but they definitely aren’t the same, and come from just a tiny same of the potentially hundreds of different model specifications we could have considered and decided to present to others."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-17/index.html#matching-with-matchit",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-17/index.html#matching-with-matchit",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Matching with MatchIt",
    "text": "Matching with MatchIt\nAs the title of Ho et al. (2007) indicates, matching methods are presented as a way of preprocessing the data to reduce the kind of model dependence we’ve just started to explore. Let’s run the first example they present in the MatchIt vignette then discuss what it means:\n\n\nCode\nm.out0 &lt;- matchit(treat ~ age + educ + race + married + \n                   nodegree + re74 + re75, data = lalonde,\n                 method = NULL, distance = \"glm\")\nsummary(m.out0)\n\n\n\nCall:\nmatchit(formula = treat ~ age + educ + race + married + nodegree + \n    re74 + re75, data = lalonde, method = NULL, distance = \"glm\")\n\nSummary of Balance for All Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.1822          1.7941     0.9211    0.3774\nage              25.8162       28.0303         -0.3094     0.4400    0.0813\neduc             10.3459       10.2354          0.0550     0.4959    0.0347\nraceblack         0.8432        0.2028          1.7615          .    0.6404\nracehispan        0.0595        0.1422         -0.3498          .    0.0827\nracewhite         0.0973        0.6550         -1.8819          .    0.5577\nmarried           0.1892        0.5128         -0.8263          .    0.3236\nnodegree          0.7081        0.5967          0.2450          .    0.1114\nre74           2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75           1532.0553     2466.4844         -0.2903     0.9563    0.1342\n           eCDF Max\ndistance     0.6444\nage          0.1577\neduc         0.1114\nraceblack    0.6404\nracehispan   0.0827\nracewhite    0.5577\nmarried      0.3236\nnodegree     0.1114\nre74         0.4470\nre75         0.2876\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       429     185\nUnmatched       0       0\nDiscarded       0       0\n\n\nWith method = NULL, the matchit function presents some summary estimates of differences in characteristics between the Treatment and Control groups. For example, the treated group has an average age of around 25, compared with 28 in the control group, have a slightly higher education score, are more likely to be Black, less likely to be Hispanic, and much less likely to be White (all important differences in the USA context, especially perhaps of the 1970s). They are also less likely to be married, more likely to have no degree, and have substantially earlier wages in both 1974 and 1975. Clearly a straightforward comparision between average outcomes is far from a like-with-like comparisons between groups. The inclusion of other covariates (\\(X^*\\)) does seem to have made a difference, switching the reported direction of effect and its statistical significance, but if we could find a subsample of the control group whose characteristics better match those of the treatment groups, we would hopefully get a more precise and reliable estimate of the effect of the labour market programme.\nThe next part of the vignette shows MatchIt working with some fairly conventional settings:\n\n\nCode\nm.out1 &lt;- matchit(treat ~ age + educ + race + married + \n                   nodegree + re74 + re75, data = lalonde,\n                 method = \"nearest\", distance = \"glm\")\nm.out1\n\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score\n             - estimated with logistic regression\n - number of obs.: 614 (original), 370 (matched)\n - target estimand: ATT\n - covariates: age, educ, race, married, nodegree, re74, re75\n\n\nThe propensity score, i.e. the probability of being in the treatment group, has been predicted using the other covariates, and using logistic regression. For each individual in the treatment group, a ‘nearest neighbour’ in the control group has been identified with the most similar propensity score, which we hope also will also mean the characteristics of the treatment group, and matched pairs from the control group, will be more similar too.\nWe can start to see what this means in practice by looking at the summary of the above object\n\n\nCode\nsummary(m.out1)\n\n\n\nCall:\nmatchit(formula = treat ~ age + educ + race + married + nodegree + \n    re74 + re75, data = lalonde, method = \"nearest\", distance = \"glm\")\n\nSummary of Balance for All Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.1822          1.7941     0.9211    0.3774\nage              25.8162       28.0303         -0.3094     0.4400    0.0813\neduc             10.3459       10.2354          0.0550     0.4959    0.0347\nraceblack         0.8432        0.2028          1.7615          .    0.6404\nracehispan        0.0595        0.1422         -0.3498          .    0.0827\nracewhite         0.0973        0.6550         -1.8819          .    0.5577\nmarried           0.1892        0.5128         -0.8263          .    0.3236\nnodegree          0.7081        0.5967          0.2450          .    0.1114\nre74           2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75           1532.0553     2466.4844         -0.2903     0.9563    0.1342\n           eCDF Max\ndistance     0.6444\nage          0.1577\neduc         0.1114\nraceblack    0.6404\nracehispan   0.0827\nracewhite    0.5577\nmarried      0.3236\nnodegree     0.1114\nre74         0.4470\nre75         0.2876\n\nSummary of Balance for Matched Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.3629          0.9739     0.7566    0.1321\nage              25.8162       25.3027          0.0718     0.4568    0.0847\neduc             10.3459       10.6054         -0.1290     0.5721    0.0239\nraceblack         0.8432        0.4703          1.0259          .    0.3730\nracehispan        0.0595        0.2162         -0.6629          .    0.1568\nracewhite         0.0973        0.3135         -0.7296          .    0.2162\nmarried           0.1892        0.2108         -0.0552          .    0.0216\nnodegree          0.7081        0.6378          0.1546          .    0.0703\nre74           2095.5737     2342.1076         -0.0505     1.3289    0.0469\nre75           1532.0553     1614.7451         -0.0257     1.4956    0.0452\n           eCDF Max Std. Pair Dist.\ndistance     0.4216          0.9740\nage          0.2541          1.3938\neduc         0.0757          1.2474\nraceblack    0.3730          1.0259\nracehispan   0.1568          1.0743\nracewhite    0.2162          0.8390\nmarried      0.0216          0.8281\nnodegree     0.0703          1.0106\nre74         0.2757          0.7965\nre75         0.2054          0.7381\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       185     185\nUnmatched     244       0\nDiscarded       0       0\n\n\nPreviously, there were 185 people in the treatment group, and 429 people in the control group. After matching there are 185 people in the treatment group… and also 185 people in the control group. So, each of the 185 people in the treatment group has been matched up with a ‘data twin’ in the control group, so the ATT should involve more of a like-with-like comparison.\nThe summary presents covariate-wise differences between the Treatment and Control groups for All Data, then for Matched Data. We would hope that, in the Matched Data, the differences are smaller for each covariate, though this isn’t necessarily the case. After matching, for example, we can see that the Black proportion in the Control group is now 0.47 rather than 0.20, and that the earlier income levels are lower, in both cases bringing the values in the Control group closer to, but not identical to, those in the Treatment group. Another way of seeing how balancing has changed things is to look at density plots:\n\n\nCode\nplot(m.out1, type = \"density\", interactive = FALSE,\n     which.xs = ~age + married + re75+ race + nodegree + re74)\n\n\n\n\n\n\n\n\nIn these density charts, the darker lines indicate the Treatment group and the lighter lines the Control groups. The matched data are on the right hand side, with All data on the left. We are looking to see if, on the right hand side, the two sets of density lines are more similar than they are on the right. Indeed they do appear to be, though we can also tell they are far from identical."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-17/index.html#estimating-treatment-effect-sizes-after-matching",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-17/index.html#estimating-treatment-effect-sizes-after-matching",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Estimating Treatment Effect Sizes after matching",
    "text": "Estimating Treatment Effect Sizes after matching\nHistorically, the MatchIt package was designed to work seamlessly with Zelig, which made it much easier to use a single library and framework to produce ‘quantities of interest’ using multiple model structures. However Zelig has since been deprecated, meaning the vignette now recommends using the marginaleffects package. We’ll follow their lead:\nFirst the vignette recommends extracting matched data from the matchit output:\n\n\nCode\nm.data &lt;- match.data(m.out1)\n\nm.data &lt;- as_tibble(m.data)\nm.data\n\n\n# A tibble: 370 × 12\n   treat   age  educ race   married nodegree  re74  re75   re78 distance weights\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1     1    37    11 black        1        1     0     0  9930.   0.639        1\n 2     1    22     9 hispan       0        1     0     0  3596.   0.225        1\n 3     1    30    12 black        0        0     0     0 24909.   0.678        1\n 4     1    27    11 black        0        1     0     0  7506.   0.776        1\n 5     1    33     8 black        0        1     0     0   290.   0.702        1\n 6     1    22     9 black        0        1     0     0  4056.   0.699        1\n 7     1    23    12 black        0        0     0     0     0    0.654        1\n 8     1    32    11 black        0        1     0     0  8472.   0.790        1\n 9     1    22    16 black        0        0     0     0  2164.   0.780        1\n10     1    33    12 white        1        0     0     0 12418.   0.0429       1\n# ℹ 360 more rows\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nWhereas the unmatched data contains 614 observations, the matched data contains 370 observations. Note that the Treatment group contained 185 observations, and that 370 is 185 times two. So, the matched data contains one person in the Control group for each person in the Treatment group.\nWe can also see that, in addition to the metrics originally included, the matched data contains three additional variables: ‘distance’, ‘weights’ and ‘subclass’. The ‘subclass’ field is perhaps especially useful for understanding the intuition of the approach, because it helps show which individual in the Control group has been paired with which individual in the Treatment group. Let’s look at the first three subgroups:\n\n\nCode\nm.data |&gt; filter(subclass == '1')\n\n\n# A tibble: 2 × 12\n  treat   age  educ race  married nodegree   re74  re75  re78 distance weights\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    37    11 black       1        1     0      0 9930.    0.639       1\n2     0    22     8 black       1        1 16961.     0  959.    0.203       1\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nSo, for the first subclass, a 37 year old married Black person with no degree has been matched to a 22 year old Black married person with no degree.\n\n\nCode\nm.data |&gt; filter(subclass == '2')\n\n\n# A tibble: 2 × 12\n  treat   age  educ race  married nodegree  re74  re75   re78 distance weights\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    33    12 white       1        0    0      0 12418.   0.0429       1\n2     0    39    12 white       1        0 1289.     0  1203.   0.0430       1\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nFor the second subclass a 33 year old married White person with a degree has been paired with a 39 year old White person with a degree.\n\n\nCode\nm.data |&gt; filter(subclass == '3')\n\n\n# A tibble: 2 × 12\n  treat   age  educ race   married nodegree  re74  re75   re78 distance weights\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    31     9 hispan       0        1     0    0  26818.    0.250       1\n2     0    16    10 white        0        1     0  190.  2137.    0.105       1\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nFor the third subclass, a 31 year old unmarried Hispanic person with no degree has been paired with a 16 year old White person with no degree.\nIn each case, we can see the pairings are similar in some ways but (as with the last example) quite dissimilar in others. The matching algorithm is trying to do the best it can with the data available, especially with the constraint1 that once a person in the Control group has been paired up once to someone in the Treatment group, they can’t be paired up again with someone else in the Treatment group.\nThe identification of these specific pairings suggests we can used a fairly crude strategy to produce an estimate of the ATT: namely just compare the outcome across each of these pairs. Let’s have a look at this:\n\n\nCode\ntrt_effects &lt;- \n    m.data |&gt;\n        group_by(subclass) |&gt;\n        summarise(\n            ind_treat_effect = re78[treat == 1] - re78[treat == 0]\n        ) |&gt; \n        ungroup()\n\ntrt_effects |&gt;\n    ggplot(aes(ind_treat_effect)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = mean(trt_effects$ind_treat_effect), colour = \"red\") + \n    geom_vline(xintercept = 0, colour = 'lightgray', linetype = 'dashed')\n\n\n\n\n\nThis crude paired comparison suggests an average difference that’s slightly positive, of $894.37.\nThis is not a particularly sophisticated or ‘kosher’ approach however. Instead the vignette suggests calculating the treatment effect estimate as follows:\n\n\nCode\nlibrary(\"marginaleffects\")\n\nfit &lt;- lm(re78 ~ treat * (age + educ + race + married + nodegree + \n             re74 + re75), data = m.data, weights = weights)\n\navg_comparisons(fit,\n                variables = \"treat\",\n                vcov = ~subclass,\n                newdata = subset(m.data, treat == 1),\n                wts = \"weights\")\n\n\n\n  Term          Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n treat mean(1) - mean(0)     1121        837 1.34    0.181 2.5  -520   2763\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\nUsing the recommended approach, the ATT estimate is now $1121. Not statistically significant at the conventional 95% threshold, but also more likely to be positive than negative."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-17/index.html#summary",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-17/index.html#summary",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Summary",
    "text": "Summary\nIn this post we have largely followed along with the introductionary vignette from the MatchIt package, in order to go from the fairly cursory theoretical overview in the previous post, to showing how some of the ideas and methods relating to multiple regression and matching methods work in practice. There are a great many ways that both matching, and multiple regression, can be implemented in practice, and both are likely to affect any causal effect estimates we produce. However, the aspiration of using matching methods is to somewhat reduce the dependency that causal effect estimates have on the specific model specifications we used."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-17/index.html#coming-up",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-17/index.html#coming-up",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Coming up",
    "text": "Coming up\nThe next post concludes this series on causal inference, by discussing in more detail a topic many users of causal inference will assume I should have started with: the Pearlean school of causal inference. In brief: the approach to causal inference I’m used to interprets the problem, fundamentally, as a missing data problem; whereas the Pearlean approach interprets it more as a modelling problem. I see value in both sides, as well as some points of overlap, but in general I’m both more used to, and more comfortable with, the missing data interpretation."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-17/index.html#footnotes",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-17/index.html#footnotes",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI think this is implied by the use of method = \"nearest\", which is the default, meaning ‘greedy nearest neighbour matching’.↩︎"
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/post-stratification/index.html",
    "href": "pages/extra-courses/hacker-stats/post-stratification/index.html",
    "title": "Resampling for post-stratification",
    "section": "",
    "text": "In the introductionary post in this series on Hacker Stats, I mentioned that resampling methods can be used to perform post-stratification, meaning reweighting of observations from a sample in such a way as to make them more representative of the population of interest to us. Let’s look at this using a variation of the red coin/blue coin example from a couple of posts ago."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/post-stratification/index.html#introduction",
    "href": "pages/extra-courses/hacker-stats/post-stratification/index.html#introduction",
    "title": "Resampling for post-stratification",
    "section": "",
    "text": "In the introductionary post in this series on Hacker Stats, I mentioned that resampling methods can be used to perform post-stratification, meaning reweighting of observations from a sample in such a way as to make them more representative of the population of interest to us. Let’s look at this using a variation of the red coin/blue coin example from a couple of posts ago."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/post-stratification/index.html#red-coinblue-coin",
    "href": "pages/extra-courses/hacker-stats/post-stratification/index.html#red-coinblue-coin",
    "title": "Resampling for post-stratification",
    "section": "Red Coin/Blue Coin",
    "text": "Red Coin/Blue Coin\nImagine we have a population of two types of coin:\n\nRed Coins, which come up heads 65% of the time\nBlue Coins, which come up heads 47% of the time\n\nWithin our population, we know 75% of the coins are Blue coins, and 25 of the coins are Red Coins.\nHowever, our sample contains 20 red coins, and 20 blue coins. i.e. the distribution of coin types in our sample is different to that in our population.\nLet’s first create this sample dataset:\n\n\nCode\nlibrary(tidyverse)\n\nset.seed(9)\n\ndraws_red &lt;- rbinom(n=20, size = 1, prob = 0.65)\ndraws_blue &lt;- rbinom(n=20, size = 1, prob = 0.47)\n\ncoin_colour &lt;- c(\n    rep(\"red\", 20),\n    rep(\"blue\", 20)\n)\n\nreal_sample_data &lt;- data.frame(\n    coin_colour = coin_colour, \n    outcome = c(draws_red, draws_blue)\n)\n\nrm(draws_red, draws_blue, coin_colour)\n\nhead(real_sample_data)\n\n\n  coin_colour outcome\n1         red       1\n2         red       1\n3         red       1\n4         red       1\n5         red       1\n6         red       1\n\n\nWhat’s the expected probability of heads in the sample?\n\n\nCode\nmean(real_sample_data$outcome)\n\n\n[1] 0.65\n\n\nCode\nreal_sample_data |&gt;\n    group_by(coin_colour) |&gt;\n    summarise(prop = mean(outcome))\n\n\n# A tibble: 2 × 2\n  coin_colour  prop\n  &lt;chr&gt;       &lt;dbl&gt;\n1 blue          0.5\n2 red           0.8\n\n\nOverall, 65% of the sample - 20 reds, 20 blues - are heads. The proportion of blues is 50%, and of reds is 80%. So, it so happens that, with this random number seed, the proportions in the sample of both reds and blues are higher than the theoretical average (the prob value arguments in the code above).\nLet’s now try to use bootstrapping to calculate a distribution around the sample mean:\n\n\nCode\nbootstrap_means &lt;- function(x, nReps = 10000){\n    out &lt;- vector(\"numeric\", nReps) \n\n    for (i in 1:nReps){\n        this_resample &lt;- sample(\n            x=x, \n            size = length(x), \n            replace = TRUE # This is what makes it bootstrapping\n        )\n        out[i] &lt;- mean(this_resample)\n    }\n    out\n}\n\nbootstrapped_means &lt;- bootstrap_means(real_sample_data$outcome)\n\nhead(bootstrapped_means)\n\n\n[1] 0.750 0.625 0.700 0.775 0.800 0.700\n\n\nWhat does this look like as a histogram?\n\n\nCode\ntibble(value = bootstrapped_means) |&gt;\n    ggplot(aes(x = value)) + \n    geom_histogram(bins = 50)\n\n\n\n\n\nWe can see the familiar bell-shaped distribution of values here. What about for blues and reds separately?\n\n\nCode\nbootstrapped_means_reds &lt;- bootstrap_means(\n    real_sample_data |&gt;\n        filter(coin_colour == \"red\") |&gt;\n        pull('outcome')  \n    )\n\nbootstrapped_means_blues &lt;- bootstrap_means(\n    real_sample_data |&gt;\n        filter(coin_colour == \"blue\") |&gt;\n        pull('outcome')  \n    )\n\n\n\n\nhead(bootstrapped_means_reds)\n\n\n[1] 0.65 0.70 0.85 0.85 1.00 0.60\n\n\nCode\nhead(bootstrapped_means_blues)\n\n\n[1] 0.45 0.60 0.50 0.45 0.70 0.55\n\n\nAnd what do these two distributions look like?\n\n\nCode\ntibble(\n    rep = 1:length(bootstrapped_means_reds),\n    red = bootstrapped_means_reds,\n    blue = bootstrapped_means_blues\n) |&gt;\n    pivot_longer(\n        cols = c(red, blue),\n        names_to = \"colour\",\n        values_to = \"value\"\n    ) |&gt;\n    ggplot(aes(x = value, fill = colour)) + \n    geom_histogram(bins = 50, position = \"dodge\")\n\n\n\n\n\nSo it’s clear the distributions for mean values of the two different coin types are different, even though there’s some overlap.\nLet’s now look at doing some post-stratification, where we sample from the two groups in proportion to the relative probabilities of encountering observations from the two groups in the population as compared with the sample. Let’s think through what this means:\n\nProportions by group in sample and population\n\n\nGroup\nSample\nPopulation\nRatio\n\n\n\n\nBlue\n0.5\n0.75\n\\(3/2\\)\n\n\nRed\n0.5\n0.25\n\\(1/2\\)\n\n\nColumn Sum\n1.00\n1.00\n\n\n\n\nIn this table, the ratio is the row-wise ratio of the population value divided by the sample value. Note that the ratios have a common denominator, 2, which we can drop in defining the probability weights, leaving us with 3 for blue and 1 for red.\nWe can adapt the standard bootstrapping approach by using the prob argument in the sample() function, using these weights:\n\n\nCode\nsample_weights &lt;- \n    tibble(\n        coin_colour = c(\"blue\", \"red\"),\n        wt = c(3, 1)\n    )\n\nreal_sample_data_wt &lt;- \n    left_join(\n        real_sample_data, sample_weights\n    )\n\nreal_sample_data_wt\n\n\n   coin_colour outcome wt\n1          red       1  1\n2          red       1  1\n3          red       1  1\n4          red       1  1\n5          red       1  1\n6          red       1  1\n7          red       1  1\n8          red       1  1\n9          red       0  1\n10         red       0  1\n11         red       1  1\n12         red       1  1\n13         red       0  1\n14         red       1  1\n15         red       1  1\n16         red       1  1\n17         red       1  1\n18         red       0  1\n19         red       1  1\n20         red       1  1\n21        blue       1  3\n22        blue       0  3\n23        blue       0  3\n24        blue       0  3\n25        blue       0  3\n26        blue       1  3\n27        blue       0  3\n28        blue       0  3\n29        blue       1  3\n30        blue       1  3\n31        blue       0  3\n32        blue       1  3\n33        blue       0  3\n34        blue       1  3\n35        blue       0  3\n36        blue       1  3\n37        blue       1  3\n38        blue       1  3\n39        blue       0  3\n40        blue       1  3\n\n\nAnd now a slightly modified version of the bootstrapping function:\n\n\nCode\nbootstrap_means_wt &lt;- function(x, wt, nReps = 10000){ #wt is the weighting\n    out &lt;- vector(\"numeric\", nReps) \n\n    for (i in 1:nReps){\n        this_resample &lt;- sample(\n            x=x, \n            size = length(x), \n            prob = wt, # This is the new argument\n            replace = TRUE # This is what makes it bootstrapping\n        )\n        out[i] &lt;- mean(this_resample)\n    }\n    out\n}\n\n\nAnd to run:\n\n\nCode\nbootstrapped_means_poststratified &lt;- bootstrap_means_wt(\n    x = real_sample_data_wt$outcome,\n    wt = real_sample_data_wt$wt\n)\n\nhead(bootstrapped_means_poststratified)\n\n\n[1] 0.750 0.550 0.625 0.525 0.575 0.600\n\n\nNow, analytically, we can calculate what the mean of the population should be given the proportion of blues and reds, and the proportion of blues that are heads, and proportion of reds that are heads:\n\n\nCode\nheads_if_blue &lt;- 0.47\nheads_if_red &lt;- 0.65\n\nexpected_pop_prop_heads &lt;- (3/4) * heads_if_blue + (1/4) * heads_if_red\n\nexpected_pop_prop_heads\n\n\n[1] 0.515\n\n\nSo within the population we would expect 51.5% of coins to come up heads.\nLet’s now look at the bootstrapped and reweighted distribution to see where 0.515 fits within this distribution:\n\n\nCode\nggplot() + \n    geom_histogram(aes(x = bootstrapped_means_poststratified), bins=50) + \n    geom_vline(aes(xintercept = expected_pop_prop_heads), linewidth = 1.2, colour = \"purple\")\n\n\n\n\n\nSo we can see that the true population mean falls within the reweighted bootstrapped distribution of the values of the mean estimated. How about if we had not performed reweighting on the sample?\n\n\nCode\ntibble(value = bootstrapped_means) |&gt;\n    ggplot() + \n    geom_histogram(aes(x = value), bins=50) + \n    geom_vline(aes(xintercept = expected_pop_prop_heads), linewidth = 1.2, colour = \"purple\")\n\n\n\n\n\nSo, although on this occasion, the true population value is also within the range of the un-reweighted bootstrapped distribution, it is further from the centre of this distribution’s mass.\nLet’s give some numbers to the above. What proportion of the bootstrapped values are below the true population value?\nFirst without reweighting:\n\n\nCode\nmean(bootstrapped_means &lt; expected_pop_prop_heads)\n\n\n[1] 0.0343\n\n\nOnly about 3.4% of the means from the unweighted bootstrapping were more extreme than the true population value.\nAnd now with reweighting:\n\n\nCode\nmean(bootstrapped_means_poststratified &lt; expected_pop_prop_heads)\n\n\n[1] 0.2102\n\n\nNow 22.4% of values of the means from the reweighted/post-stratified bootstrapped distribution are below the true value. This is the difference between the true value being in the 90% central interval or not."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/post-stratification/index.html#summary",
    "href": "pages/extra-courses/hacker-stats/post-stratification/index.html#summary",
    "title": "Resampling for post-stratification",
    "section": "Summary",
    "text": "Summary\nIn this post we’ve illustrated the importance of post-stratifying data were we know a sample is biased in terms of the relative weight given to the strata it contains as compared with the population. We’ve also shown, using Base R functions alone, how to perform this post-stratification using just two additional changes: a vector of weights, which was fairly straightforward to calculate; and the passing of this vector of weights to the prob argument in the sample() function.\nIn this post we’ve focused on a hypothetical example, and built the requisite functions and code from scratch. In practice, packages like survey can be used to perform post-stratification in fewer lines, svrep, and boot can make the process much more straightforward."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/bootstrapping/index.html",
    "href": "pages/extra-courses/hacker-stats/bootstrapping/index.html",
    "title": "A brief introduction to bootstrapping",
    "section": "",
    "text": "I met with Neil Pettinger earlier today. He traded eggs benedict for some statistical advice, mainly on what bootstrapping is, and whether it could be helpful for analysing hospital length of stay data.\nHere’s a brief post on bootstrapping with some example code:"
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/bootstrapping/index.html#what-is-bootstrapping",
    "href": "pages/extra-courses/hacker-stats/bootstrapping/index.html#what-is-bootstrapping",
    "title": "A brief introduction to bootstrapping",
    "section": "What is bootstrapping?",
    "text": "What is bootstrapping?\nAccording to Wikipedia:\n\nBootstrapping is any test or metric that uses random sampling with replacement (e.g. mimicking the sampling process), and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates.[1][2] This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.[3][4]\n\n\nBootstrapping estimates the properties of an estimand (such as its variance) by measuring those properties when sampling from an approximating distribution. One standard choice for an approximating distribution is the empirical distribution function of the observed data. In the case where a set of observations can be assumed to be from an independent and identically distributed population, this can be implemented by constructing a number of resamples with replacement, of the observed data set (and of equal size to the observed data set).\n\n\nIt may also be used for constructing hypothesis tests.[5] It is often used as an alternative to statistical inference based on the assumption of a parametric model when that assumption is in doubt, or where parametric inference is impossible or requires complicated formulas for the calculation of standard errors.\n\nn.b. The same page (In History) also states: “Other names … suggested for the ‘bootstrap’ method were: Swiss Army Knife, Meat Axe, Swan-Dive, Jack-Rabbit, and Shotgun.” So, there might not be good reasons to fear statistics, but given this list of suggestions there might be good reasons to fear some statisticians! Of these alternative names, perhaps Swiss Army Knife is the most appropriate, as it’s a very widely applicable approach!"
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/bootstrapping/index.html#a-brief-example",
    "href": "pages/extra-courses/hacker-stats/bootstrapping/index.html#a-brief-example",
    "title": "A brief introduction to bootstrapping",
    "section": "A brief example",
    "text": "A brief example\nI’m not going to look for length-of-stay data; instead I’m going to look at length-of-teeth, and the hamster experiment dataset I used in a few previous posts.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nLet’s say, instead of building a statistical model, I’m just interested in the following question:\n\nWhere the dose is 1mg, is using the OJ supplement instead of the VC supplement associated with a significant and detectable difference in the median tooth length?\n\nWe can do this in at least a couple of ways:\n\nCalculate the median of OJ at 1mg tooth lengths, and compare it to a bootstrapped distribution of medians from VC at 1mg.\nBootstrap both the OJ and VC (both at 1mg) populations, get the medians for each bootstrapped population, and record the difference in the medians.\n\nThese are asking slightly different questions, but both ways of using bootstrapping to address the general type of question framed above.\n\nApproach One\n\nNreps &lt;- 10000 # Number of bootstrap replicates\n\nbs_med_vc &lt;- vector(mode = 'numeric',  length = Nreps) #Vector for holding bootstrapped medians\n\ndta_vc &lt;- df |&gt;\n    filter(supp == \"VC\", dose == 1.0) # The equivalent of our 'control' population\n\ncontrol_toothlengths &lt;- dta_vc |&gt; pull(len) # Literally pulling teeth!\n\nNcontrol &lt;- length(control_toothlengths) #Length of 'control' population sample\n\nfor (i in 1:Nreps){\n    bs_c_length &lt;- sample(\n        control_toothlengths, \n        size = Ncontrol,\n        replace = TRUE\n    ) # resampling to the same length as the 'control' population\n\n    this_bs_control_median &lt;- median(bs_c_length)\n    bs_med_vc[i] &lt;- this_bs_control_median\n}\n\nWe’ve now done the bootstrapping on the ‘control’ population. Let’s look at this bootstrapped distribution of medians in comparison with the observed median from the ‘treatment’ group.\n\ntreatment_toothlengths &lt;- df |&gt;\n    filter(supp == \"OJ\", dose == 1.0) |&gt;\n    pull(len) # pulling teeth for the 'treatment' population\n\nobs_med_oj &lt;- median(treatment_toothlengths)\n\ntibble(bs_control_median = bs_med_vc) |&gt;\n    ggplot(aes(x=bs_control_median)) +\n    geom_histogram() +\n    geom_vline(xintercept = obs_med_oj, colour = \"red\", linetype = \"dashed\") + \n    geom_vline(xintercept = median(control_toothlengths), colour = \"blue\", linetype = \"dashed\") + \n    labs(\n       x = \"Median toothlength\",\n       y = \"Number of bootstraps\",\n       title = \"Bootstrapping approach One\",\n       subtitle = \"Red line: Observed median toothlength in 'treatment' arm. Blue line: Observed median in 'control' arm\"\n    )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe can see here that the red line, which is the observed median in the ‘treatment’ arm, is higher than all of the bootstrapped medians from the ‘control’ arm. The blue line shows the equivalent in the observed median in the ‘control’ arm.\nSo, without even performing a calculation, we can feel more confident that the OJ supplement is associated with larger tooth length, even though both arms comprise just ten observations."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/bootstrapping/index.html#approach-two",
    "href": "pages/extra-courses/hacker-stats/bootstrapping/index.html#approach-two",
    "title": "A brief introduction to bootstrapping",
    "section": "Approach Two",
    "text": "Approach Two\nLet’s now use bootstrapping to produce a distributions of differences in medians between the two arms. So, this time we repeatedly resample from both the control and the treatment arm.\n\nNreps &lt;- 10000 # Number of bootstrap replicates\n\nbs_diff_meds &lt;- vector(mode = 'numeric',  length = Nreps) #Vector for holding differences in bootstrapped medians\n\ndta_vc &lt;- df |&gt;\n    filter(supp == \"VC\", dose == 1.0) # The equivalent of our 'control' population\n\ncontrol_toothlengths &lt;- dta_vc |&gt; pull(len) # Literally pulling teeth!\n\nNcontrol &lt;- length(control_toothlengths) #Length of 'control' population sample\n\ndta_oj &lt;- df |&gt;\n    filter(supp == \"OJ\", dose == 1.0) # The equivalent of our 'treamtnet' population\n\ntreatment_toothlengths &lt;- dta_oj |&gt; pull(len) # Literally pulling teeth!\n\nNtreatment &lt;- length(treatment_toothlengths) #Length of 'treatment' population sample\n\n\nfor (i in 1:Nreps){\n    bs_c_length &lt;- sample(\n        control_toothlengths, \n        size = Ncontrol,\n        replace = TRUE\n    ) # resampling to the same length as the 'control' population\n\n    this_bs_control_median &lt;- median(bs_c_length)\n\n    bs_t_length &lt;- sample(\n        treatment_toothlengths, \n        size = Ntreatment,\n        replace = TRUE\n    ) # resampling to the same length as the 'control' population\n\n    this_bs_treat_median &lt;- median(bs_t_length)\n\n    bs_diff_meds[i] &lt;- this_bs_treat_median - this_bs_control_median\n}\n\nWe now have a bootstrapped distribution of differences, each time subtracting the bootstrapped control median from the bootstrapped treat median. So, values above 0 indicate the treatment is more effective (at lengthening teeth) than the control.\nLet’s look at this distribution\n\ntibble(bs_diffs_median = bs_diff_meds) |&gt;\n    ggplot(aes(x=bs_diffs_median)) +\n    geom_histogram() +\n    geom_vline(xintercept = 0) + \n    geom_vline(\n        xintercept = median(treatment_toothlengths) - median(control_toothlengths), linetype = \"dashed\", colour = \"green\"         \n        ) + \n    labs(\n       x = \"Differences in medians\",\n       y = \"Number of bootstraps\",\n       title = \"Bootstrapping approach Two\",\n       subtitle = \"Values above 0: medians are higher in treatment group\"\n    )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nI’ve added the observed difference in medians as a vertical green line. This corresponds with the highest peak in bootstrapped differences in medians, as we might expect.\nAlmost all bootstrapped differences in medians are above 0, which again suggests we don’t even need to calculate the proportion above 0 to work out if there is likely to be a difference in medians between the two groups.\nHowever if we wanted to get this empirical p-value, we could do it as follows:\n\nsum(bs_diff_meds &lt; 0) / Nreps\n\n[1] 5e-04\n\n\nTiny!"
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/bootstrapping/index.html#going-further",
    "href": "pages/extra-courses/hacker-stats/bootstrapping/index.html#going-further",
    "title": "A brief introduction to bootstrapping",
    "section": "Going further",
    "text": "Going further\nI suggested to Neil that writing some R code to do the bootstrapping can be a ‘good’ learning experience. This is what I’ve done in the above, using for loops as they’re easiest to reason through, even though not the most computationally efficient. Once the intuition of what bootstrapping is, how it works, and what it can do is embedded through writing out a few examples like this, there are plenty of packages that make bootstrapping even easier to do (and likely faster to run too).\nI also mentioned and can (for pedagogic purposes) recommend the infer package, which uses bootstrapping to produce estimates of distributions under the Null hypothesis, alongside parametric approaches, and produces pretty visualisations to boot!"
  },
  {
    "objectID": "pages/extra-courses/time-series/lms-are-glms-part-23/index.html",
    "href": "pages/extra-courses/time-series/lms-are-glms-part-23/index.html",
    "title": "Part Twenty Three: Time series and seasonality",
    "section": "",
    "text": "In previous posts on time series, we decomposed then applied a common general purpose modelling strategy for working with time series data called ARIMA. ARIMA model can involve autoregressive components (AR(p)), integration/differencing components (I(d)), and moving average components (MA(q)). As we saw, the time series data can also be pre-transformed, in ways other than just differencing; the example of this we saw was the application of the Box-Cox transformation for regularising the variance of the outcome, and includes logging of values as one possible transformation within the framework.\nThe data we used previous was annual data, showing the numbers of airmiles travelled in the USA by year up to the 1960s. Of course, however, many types of time series data are sub-annual, reported not just by year, but by quarter, or month, or day as well. Data disaggregated into sub-annual units often exhibit seasonal variation, patterns that repeat themselves at regular intervals within a 12 month cycle. 1\nIn this post we will look at some seasonal data, and consider two strategies for working with this data: STL decomposition; and Seasonal ARIMA (SARIMA)."
  },
  {
    "objectID": "pages/extra-courses/time-series/lms-are-glms-part-23/index.html#recap-and-introduction",
    "href": "pages/extra-courses/time-series/lms-are-glms-part-23/index.html#recap-and-introduction",
    "title": "Part Twenty Three: Time series and seasonality",
    "section": "",
    "text": "In previous posts on time series, we decomposed then applied a common general purpose modelling strategy for working with time series data called ARIMA. ARIMA model can involve autoregressive components (AR(p)), integration/differencing components (I(d)), and moving average components (MA(q)). As we saw, the time series data can also be pre-transformed, in ways other than just differencing; the example of this we saw was the application of the Box-Cox transformation for regularising the variance of the outcome, and includes logging of values as one possible transformation within the framework.\nThe data we used previous was annual data, showing the numbers of airmiles travelled in the USA by year up to the 1960s. Of course, however, many types of time series data are sub-annual, reported not just by year, but by quarter, or month, or day as well. Data disaggregated into sub-annual units often exhibit seasonal variation, patterns that repeat themselves at regular intervals within a 12 month cycle. 1\nIn this post we will look at some seasonal data, and consider two strategies for working with this data: STL decomposition; and Seasonal ARIMA (SARIMA)."
  },
  {
    "objectID": "pages/extra-courses/time-series/lms-are-glms-part-23/index.html#an-example-dataset",
    "href": "pages/extra-courses/time-series/lms-are-glms-part-23/index.html#an-example-dataset",
    "title": "Part Twenty Three: Time series and seasonality",
    "section": "An example dataset",
    "text": "An example dataset\nLet’s continue to use the examples and convenience functions from the forecast package used in the previous post, and for which the excellent book Forecasting: Principles and Practice is available freely online.\nFirst some packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(forecast)\nlibrary(fable)\n\n\nNow some seasonal data\n\n\nCode\n# Using this example dataset: https://otexts.com/fpp3/components.html\ndata(us_employment)\nus_retail_employment &lt;- us_employment |&gt;\n  filter(Title == \"Retail Trade\")\n\nus_retail_employment\n\n\n# A tsibble: 969 x 4 [1M]\n# Key:       Series_ID [1]\n      Month Series_ID     Title        Employed\n      &lt;mth&gt; &lt;chr&gt;         &lt;chr&gt;           &lt;dbl&gt;\n 1 1939 Jan CEU4200000001 Retail Trade    3009 \n 2 1939 Feb CEU4200000001 Retail Trade    3002.\n 3 1939 Mar CEU4200000001 Retail Trade    3052.\n 4 1939 Apr CEU4200000001 Retail Trade    3098.\n 5 1939 May CEU4200000001 Retail Trade    3123 \n 6 1939 Jun CEU4200000001 Retail Trade    3141.\n 7 1939 Jul CEU4200000001 Retail Trade    3100 \n 8 1939 Aug CEU4200000001 Retail Trade    3092.\n 9 1939 Sep CEU4200000001 Retail Trade    3191.\n10 1939 Oct CEU4200000001 Retail Trade    3242.\n# ℹ 959 more rows\n\n\nThere are two differences we can see with this dataset compared with previous time series data we’ve looked at.\nFirstly, the data looks like a data.frame object, or more specifically a tibble() (due to the additional metadata at the top). In fact they are of a special type of tibble called a tsibble, which is basically a modified version of a tibble optimised to work with time series data. We can check this by interrogating the class attributes of us_employment:\n\n\nCode\nclass(us_retail_employment)\n\n\n[1] \"tbl_ts\"     \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThese class attributes go broadly from the most specific type of object class: tbl_ts (the tsibble); to the most general type of object class: the data.frame.\nSecondly, we can see that the data are disaggregated not by year as in the last post’s example, but also by month. So, what does this monthly data actually look like?\n\n\nCode\nautoplot(us_retail_employment, Employed) +\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\n\n\n\n\nThis data looks… spikey. There’s clearly both a long-term trend - including periods of faster and slower growth, and occasionally some falls - but there’s also what looks like a series of near-vertical spikes along this trend, at what may be regular intervals. What happens if we zoom into a smaller part of the time series?\n\n\nCode\nautoplot(\n    us_retail_employment |&gt;\n        filter(year(Month) &gt;=1990), \n    Employed) +\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\n\n\n\n\nHere we can start to see there’s not just a single repeating ‘vertical spike’, but a pattern that appears to repeat within each year, for each year. Let’s zoom in even further, for just three years:\n\n\nCode\nautoplot(\n    us_retail_employment |&gt;\n        filter(between(year(Month), 1994, 1996)), \n    Employed) +\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\n\n\n\n\nAlthough each of these three years is different in terms of the average number of persons employed in retail, they are similar in terms of having a spike in employment towards the end of the year, then a drop off at the start of the year, then a relative plateau for the middle of the year.\nThis is an example of a seasonal pattern, information that gets revealed about a time series when we use a sub-annual resolution that might not be apparent it we used only annual data. How do we handle this kind of data?"
  },
  {
    "objectID": "pages/extra-courses/time-series/lms-are-glms-part-23/index.html#approach-one-reannualise",
    "href": "pages/extra-courses/time-series/lms-are-glms-part-23/index.html#approach-one-reannualise",
    "title": "Part Twenty Three: Time series and seasonality",
    "section": "Approach one: reannualise",
    "text": "Approach one: reannualise\nOf course we could simply reaggregate the data to an annual series:\n\n\nCode\nus_retail_employment |&gt;\n    mutate(\n        year = year(Month)\n    ) |&gt;\n    ungroup() |&gt;\n    index_by(year) |&gt;\n    summarise(\n        Employed = sum(Employed)\n    ) %&gt;%\n    autoplot(., Employed)\n\n\n\n\n\nOne thing we can notice with this is that there appears to be a big drop in total employment for the last year. This is likely because the last year is incomplete, so whereas previous years are summing up 12 months’ observations, for the last year a smaller number of months are being summed up. We could then drop the last year:\n\n\nCode\nus_retail_employment |&gt;\n    mutate(\n        year = year(Month)\n    ) |&gt;\n    ungroup() |&gt;\n    index_by(year) |&gt;\n    summarise(\n        Employed = sum(Employed)\n    ) |&gt;\n    filter(year != max(year)) %&gt;%\n    autoplot(., Employed)\n\n\n\n\n\nBut then we are losing some data that we really have. Even if we don’t have the full year, we might be able to get a sense from just the first few months worth of data whether the overall values for the last year are likely to be up or down compared to the same month in the previous years. We could even turn this single annual time series into 12 separate series: comparing Januaries with Januaries, Februaries with Februaries, and so on.\n\n\nCode\nus_retail_employment |&gt;\n    mutate(\n        year = year(Month), \n        month = month(Month, label = TRUE )\n    ) |&gt;\n    ggplot(\n        aes(year, Employed)\n    ) + \n    facet_wrap(~month) + \n    geom_line()\n\n\n\n\n\nHere we can see that comparing annual month-by-month shows a very similar trend overall. It’s as if each month’s values could be thought of as part of an annual ‘signal’ (an underlying long-term trend) plus a seasonal adjustment up or down: compared with the annual trend, Novembers and Decembers are likely to be high, and Januaries and Februaries to be low; and so on.\nIt’s this intuition - That we have a trend component, and a seasonal component - which leads us to our second strategy: decomposition."
  },
  {
    "objectID": "pages/extra-courses/time-series/lms-are-glms-part-23/index.html#approach-two-seasonal-composition",
    "href": "pages/extra-courses/time-series/lms-are-glms-part-23/index.html#approach-two-seasonal-composition",
    "title": "Part Twenty Three: Time series and seasonality",
    "section": "Approach Two: Seasonal Composition",
    "text": "Approach Two: Seasonal Composition\nThe basic intuition of decomposition is to break sub-annual data into a series of parts: The underling long term trend component; and repeating (usually) annual seasonal component.\nA common method for performing this kind of decomposition is known as STL. This actually stands for Seasonal and Trend Decomposition using Loess (Where Loess is itself another acronym). However it’s heuristically easier to imagine it stands for Season-Trend-Leftover, as it tends to generate three outputs from a single time-series input that correspond to these three components. Let’s regenerate the example in the forecasting book and then consider the outputs further:\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n  model(\n    STL(Employed ~ trend(window = 7) +\n                   season(window = \"periodic\"),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nThe plotted output contain four rows. These are, respectively:\n\nTop Row: The input data from the dataset\nSecond Row: The trend component from STL decomposition\nThird Row: The seasonal component from the STL decomposition\nBottom Row: The remainder (or leftover) component from the STL decomposition.\n\nSo, what’s going on?\nSTL uses an algorithm to find a repeated sequence (the seasonal component) in the data that, once subtracted from a long term trend, leaves a remainder (set of errors or deviations from observations) that is minimised in some way, and ideally random like white noise.\nIf you expanded the code chunk above, you will see two parameters as part of the STL model: the window argument for a trend() function; and the window argument for a season() function. This implies there are ways of setting up STL differently, and these would produce different output components. What happens if we change the window argument to 1 (which I think is its smallest allowable value)?\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n    filter(year(Month) &lt;= 2017) |&gt;\n  model(\n    STL(Employed ~ trend(window = 1) +\n                   season(window = \"periodic\"),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nHere the trend component becomes, for want of a better term, ‘wigglier’. And the remainder term, except for a strange data artefact at the end, appears much smaller. So what does the window argument do?\nConceptually, what the window argument to trend() does is adjust the stiffness of the curve that the trendline uses to fit to the data. A longer window, indicated by a higher argument value, makes the curve stiffer, and a shorter window, indicated by a lower argument value, makes the curve less stiff. We’ve adjusted from the default window length of 7 to a much shorter length of 1, making it much less stiff.2 Let’s look at the effect of increasing the window length instead:\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n  model(\n    STL(Employed ~ trend(window = 31) +\n                   season(window = \"periodic\"),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nHere we can see that, as well as the trend term being somewhat smoother than when a size 7 window length was used, the remainder term, though looking quite noisy, doesn’t really look random anymore. In particular, there seems to be a fairly big jump in the remainder component in the late 2000s. The remainder series also does not particularly stationary, lurching up and down at particular points in the series.\nIn effect, the higher stiffness of the trend component means it is not able to capture and represent enough signal in the data, and so some of that ‘signal’ is still present in the remainder term, when it should be extracted instead.\nNow what happens if we adjust the window argument in the season() function instead?\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n  model(\n    STL(Employed ~ trend(window = 7) +\n                   season(window = 5),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nIn the above I’ve reduced the season window size (by default it’s infinite). Whereas before this seasonal pattern was forced to be constant for the whole time period, this time we an see that it changes, or ‘evolves’, over the course of the time series. We can also see that the remainder component, though looking quite random, now looks especially ‘spiky’, suggesting that the kinds of residuals left are somewhat further from Guassian white noise than in the first example.\n\nSection concluding thoughts\nSTL decomposition is one of a number of strategies for decomposition available to us. Other examples are described here. However the aims and principles of decomposition are somewhat similar no matter what approach is used.\nHaving performed a decomposition on time series data, we could potentially apply something like an ARIMA model to the trend component of the data alone for purposes of projection. If using a constant seasonal component, we could then add this component onto forecast values from the trend component, along with noise consistent with the properties of the remainder component. However, there is a variant of the ARIMA model specification that can work with this kind of seasonal data directly. Let’s look at that now"
  },
  {
    "objectID": "pages/extra-courses/time-series/lms-are-glms-part-23/index.html#approach-three-sarima",
    "href": "pages/extra-courses/time-series/lms-are-glms-part-23/index.html#approach-three-sarima",
    "title": "Part Twenty Three: Time series and seasonality",
    "section": "Approach Three: SARIMA",
    "text": "Approach Three: SARIMA\nSARIMA stands for ‘Seasonal ARIMA’ (where of course ARIMA stands for Autoregressive-Integrated-Moving Average). Whereas an ARIMA model has a specification shorthand ARIMA(p, d, q), a SARIMA model has an extended specification: SARIMA(p, d, q) (P, D, Q)_S. This means that whereas ARIMA has three parameters to specify, a SARIMA model has seven. This might appear like a big jump in model complexity, but the gap from ARIMA to SARIMA is smaller than it first appears.\nTo see this it’s first noticing that, as well as terms p, d and q, there are also terms P, D and Q. This would suggest that whatever Autoregressive (p), integration (d) and moving average (q) processes are involved in standard ARIMA are also involved in another capacity in SARIMA. And what’s this other capacity? The clue to this is in the S term.\nS 3 stands for the seasonal component of the model, and specifies the number of observations that are expected to include a repeating seasonal cycle. As most seasonal cycles are annual, this means S will be 12 if the data are monthly, 4 if the data are quarterly, and so on.\nThe UPPERCASE P, D and Q terms then specify which standard ARIMA processes should be modelled as occurring every S steps in the data series. Although algebraically this means SARIMA models may look a lot more complicated than standard ARIMA models, it’s really the same process, and the same intuition, applied twice: to characterising the seasonal ‘signals’ in the time series, and to characteristing the non-seasonal ‘signals’ in the time series.\nAlthough there are important diagnostic charts and heuristics to use when determining and judging which SARIMA specification may be most appropriate for modelling seasonal data, such as the PACF and ACF, we can still use the auto.arima() function to see if the best SARIMA specification can be identified algorithmically:\n\n\nCode\nbest_sarima_model &lt;- auto.arima(as.ts(us_retail_employment, \"Employed\"))\nbest_sarima_model\n\n\nSeries: as.ts(us_retail_employment, \"Employed\") \nARIMA(1,1,2)(2,1,2)[12] \n\nCoefficients:\n         ar1      ma1     ma2     sar1     sar2    sma1     sma2\n      0.8784  -0.8428  0.1028  -0.6962  -0.0673  0.2117  -0.3873\ns.e.  0.0374   0.0481  0.0332   0.0977   0.0691  0.0937   0.0776\n\nsigma^2 = 1442:  log likelihood = -4832.08\nAIC=9680.16   AICc=9680.31   BIC=9719.06\n\n\nHere auto.arima() produced an ARIMA(1, 1, 2) (2, 1, 2)_12 specification, meaning p=1, d=1, q=2 for the non-seasonal part; and P=2, D=1, Q=2 for the seasonal part.\nWhat kind of forecasts does this produce?\n\n\nCode\nbest_sarima_model |&gt; \n  forecast(h=48) |&gt;\n  autoplot()\n\n\n\n\n\nWe can see the forecasts tend to repeat the seasonal pattern apparent throughout the observed data, and also widen in the usual way the further we move from the observed data."
  },
  {
    "objectID": "pages/extra-courses/time-series/lms-are-glms-part-23/index.html#summing-up",
    "href": "pages/extra-courses/time-series/lms-are-glms-part-23/index.html#summing-up",
    "title": "Part Twenty Three: Time series and seasonality",
    "section": "Summing up",
    "text": "Summing up\nIn this post we have looked at three approaches for working with seasonal data: aggregating seasonality away; decomposition; and SARIMA. These are far from an exhaustive list, but hopefully illustrate some common strategies for working with this kind of data."
  },
  {
    "objectID": "pages/extra-courses/time-series/lms-are-glms-part-23/index.html#footnotes",
    "href": "pages/extra-courses/time-series/lms-are-glms-part-23/index.html#footnotes",
    "title": "Part Twenty Three: Time series and seasonality",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOccasionally, we might also see repeated patterns over non-annual timescales. For example, we might see the apparent population size of a country shifting abruptly every 10 years, due to information from national censuses run every decade being incorporated into the population estimates. Or if we track sales by day we might see a weekly cycle, because trade during the weekends tends to be different than during the weekdays.↩︎\nHow this works is due to the acronym-in-the-acronym: LOESS, meaning local estimation. Effectively for each data point a local regression slope is calculated based on values a certain number of observations ahead and behind the value in question. The number of values ahead and behind considered is the ‘window’ size.↩︎\nSometimes m is used instead of S.↩︎"
  },
  {
    "objectID": "pages/extra-courses/time-series/lms-are-glms-part-24/index.html",
    "href": "pages/extra-courses/time-series/lms-are-glms-part-24/index.html",
    "title": "Part Twenty Four: Time series - Vector Autoregression and multivariate models",
    "section": "",
    "text": "So far in this short series on time series, we’ve looked at time series modelling from some first principles, learning how the types of data and challenge in time series analysis both are similar and different from those of statistical modelling more generally. We started by looking at the concept of auto-regression, then differentiation and integration, and then the moving average model specification, before combining these three components - AR, I, and MA - to produce the ARIMA model specification common in time series analysis. Afterwards, we then extended the ARIMA specification slightly to deal with seasonally varying data, the ARIMA specification begetting the Seasonal ARIMA modelling framework, or SARIMA. As part of the post on Seasonality, we also looked at time series decomposition, using the STL decomposition framework."
  },
  {
    "objectID": "pages/extra-courses/time-series/lms-are-glms-part-24/index.html#time-series-recap",
    "href": "pages/extra-courses/time-series/lms-are-glms-part-24/index.html#time-series-recap",
    "title": "Part Twenty Four: Time series - Vector Autoregression and multivariate models",
    "section": "",
    "text": "So far in this short series on time series, we’ve looked at time series modelling from some first principles, learning how the types of data and challenge in time series analysis both are similar and different from those of statistical modelling more generally. We started by looking at the concept of auto-regression, then differentiation and integration, and then the moving average model specification, before combining these three components - AR, I, and MA - to produce the ARIMA model specification common in time series analysis. Afterwards, we then extended the ARIMA specification slightly to deal with seasonally varying data, the ARIMA specification begetting the Seasonal ARIMA modelling framework, or SARIMA. As part of the post on Seasonality, we also looked at time series decomposition, using the STL decomposition framework."
  },
  {
    "objectID": "pages/extra-courses/time-series/lms-are-glms-part-24/index.html#aim-of-this-post",
    "href": "pages/extra-courses/time-series/lms-are-glms-part-24/index.html#aim-of-this-post",
    "title": "Part Twenty Four: Time series - Vector Autoregression and multivariate models",
    "section": "Aim of this post",
    "text": "Aim of this post\nIn this post, we’ll take time series in a different direction, to show an application of multivariate regression common in time series, called vector autoregression (VAR). VAR is both simpler in some ways, and more complex in other ways, than SARIMA modelling. It’s simpler in that, as the name suggests, moving average (MA) terms tend to not be part of VAR models; we’ll also not be considering seasonality either. But it’s more complicated in the sense that we are jointly modelling two outcomes at the same time."
  },
  {
    "objectID": "pages/extra-courses/time-series/lms-are-glms-part-24/index.html#model-family-tree",
    "href": "pages/extra-courses/time-series/lms-are-glms-part-24/index.html#model-family-tree",
    "title": "Part Twenty Four: Time series - Vector Autoregression and multivariate models",
    "section": "Model family tree",
    "text": "Model family tree\nThe following figure aims to show the family resemblances between model specifications and challenges:\n\n\n\n\nflowchart TB \n    uvm(univariate models)\n    mvm(multivariate models)\n\n    ar(\"AR(p)\")\n    i(\"I(d)\")\n    ma(\"MA(q)\")\n    arima(\"ARIMA(p, d, q)\")\n    sarima(\"ARIMA(p, d, q)[P, D, Q]_s\")\n    var(VAR)\n\n    ar --&gt; var\n    mvm --&gt; var\n\n    i -.-&gt; var\n\n    uvm -- autoregression --&gt; ar\n    uvm -- differencing --&gt; i\n    uvm -- moving average --&gt; ma\n    ar & i & ma --&gt; arima\n\n    arima -- seasonality --&gt;  sarima\n\n\n\n\n\n\nSo, the VAR model is an extension of the autoregressive component of a standard, univariate AR(p) specification models to multivariate models. It can also include both predictor and response variables that are differenced, hence the the dashed line from I(d) to VAR."
  },
  {
    "objectID": "pages/extra-courses/time-series/lms-are-glms-part-24/index.html#so-what-is-a-multivariate-model",
    "href": "pages/extra-courses/time-series/lms-are-glms-part-24/index.html#so-what-is-a-multivariate-model",
    "title": "Part Twenty Four: Time series - Vector Autoregression and multivariate models",
    "section": "So what is a multivariate model?",
    "text": "So what is a multivariate model?\nYou might have seen the term multivariate model before, and think you’re familiar with what it means.\nIn particular, you might have been taught that whereas a univariate regression model looks something like this:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\epsilon\n\\]\nA multivariate regression model looks more like this:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n\\]\ni.e. You might have been taught that, if the predictors include one term for the intercept (the \\(\\beta_0\\) term) and one term for the slope (the \\(\\beta_1\\) term), then this is a univariate model. But if there are two or more terms that can claim to be ‘the slope’ then this is a multivariate model.\nHowever, this isn’t the real distinction between a univariate model and a multivariate model. To see this distinction we have to return, for the umpeenth time, to the ‘grandmother model’ specification first introduced at the start of the very first post:\nStochastic Component\n\\[\nY \\sim f(\\theta, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta = g(X, \\beta)\n\\]\nNow, both the response data, \\(Y\\), and the predictor data, \\(X\\), are both taken from the same rectangular dataset, \\(D\\). Let’s say this dataset, \\(D\\), has six rows and five columns. As a matrix it would look something like this:\n\\[\nD =\n\\begin{pmatrix}\nd_{1,1} & d_{1,2} & d_{1,3} & d_{1, 4} & d_{1,5} \\\\\nd_{2,1} & d_{2,2} & d_{2,3} & d_{2, 4} & d_{2,5} \\\\\nd_{3,1} & d_{3,2} & d_{3,3} & d_{3, 4} & d_{3,5} \\\\\nd_{4,1} & d_{4,2} & d_{4,3} & d_{4, 4} & d_{4,5} \\\\\nd_{5,1} & d_{5,2} & d_{5,3} & d_{5, 4} & d_{5,5} \\\\\nd_{6,1} & d_{6,2} & d_{6,3} & d_{6, 4} & d_{6,5}\n\\end{pmatrix}\n\\]\nHere the dataset \\(D\\) is made up of a whole series of elements \\(d_{i,j}\\), where the first subset value indicates the row number \\(i\\) and the second subset value indicates the column number \\(j\\). So, for example, \\(d_{5, 2}\\) indicates the value of the 5th row and 2nd column, whereas \\(d_{2, 5}\\) indicates the value of the 2nd row and 5th column.\nFundamentally, the first challenge in building a model is deciding which columns from \\(D\\) we put in the predictor matrix \\(X\\), and which parts we put into the response matrix \\(Y\\). For example, if we wanted to predict the third column \\(j=3\\) given the fifth column \\(j=5\\) our predictor and response matrices would look as follows:\n\n\n\\[\nY = \\begin{pmatrix}\nd_{1,3} \\\\\nd_{2,3} \\\\\nd_{3,3} \\\\\nd_{4,3} \\\\\nd_{5,3} \\\\\nd_{6,3}  \n\\end{pmatrix}\n\\]\n\n\\[\nX = \\begin{pmatrix}\n1 & d_{1,5} \\\\\n1 & d_{2,5} \\\\\n1 & d_{3,5} \\\\\n1 & d_{4,5} \\\\\n1 & d_{5,5} \\\\\n1 & d_{6,5}  \n\\end{pmatrix}\n\\]\n\n\nWhere does the column of 1s come from? This is how we specify, in matrix notation, that we want an intercept term to be calculated. Models don’t have to have intercept terms, but in almost all cases we’re likely to be familiar with, they tend to.\nLet’s say we now want to include two columns, 2 and 5, from \\(D\\) in the predictor matrix, leading to what’s commonly (and wrongly) called a ‘multivariate regression’. This means that \\(Y\\) stays the same, but X is now as follows:\n\\[\nX = \\begin{pmatrix}\n1 & d_{1,2}  & d_{1,5}\\\\\n1 & d_{2,2}  & d_{2,5}\\\\\n1 & d_{3,2}  & d_{3,5}\\\\\n1 & d_{4,2}  & d_{4,5}\\\\\n1 & d_{5,2}  & d_{5,5}\\\\\n1 & d_{6,2}  & d_{6,5}\n\\end{pmatrix}\n\\]\nNo matter now many columns we include in the predictor matrix, X, however, we still don’t have a real multivariate regression model specification. Even if X had a hundred columns, or a thousand, it would still not be a multivariate regression in the more technical sense of the term.\nInstead, here’s an example of a multivariate regression model:\n\n\n\\[\nY = \\begin{pmatrix}\nd_{1,1} & d_{1,3} \\\\\nd_{2,1} & d_{2,3} \\\\\nd_{3,1} & d_{3,3} \\\\\nd_{4,1} & d_{4,3} \\\\\nd_{5,1} & d_{5,3} \\\\\nd_{6,1} & d_{6,3}  \n\\end{pmatrix}\n\\]\n\n\\[\nX = \\begin{pmatrix}\n1 & d_{1,5} \\\\\n1 & d_{2,5} \\\\\n1 & d_{3,5} \\\\\n1 & d_{4,5} \\\\\n1 & d_{5,5} \\\\\n1 & d_{6,5}  \n\\end{pmatrix}\n\\]\n\n\nThis is an example of a multivariate regression model. We encountered it before when we used the multivariate normal distribution in post 12, and when we draw from the posterior distribution of Bayesian models in post 13, but this is the first time we’ve considered multivariate modelling in the context of trying to represent something we suspect to be true about the world, rather than our uncertainty about the world. And it’s the first example of multivariate regression we’ve encountered in this series. For every previous model, no matter how apparently disparate, complicated or exotic they may appear, they’ve been univariate regression models in the sense that the response component \\(Y\\) has always only contained one column only.\nSo, with this definition of multivariate regression, let’s now look at VAR as a particular application of multivariate regression used in time series."
  },
  {
    "objectID": "pages/extra-courses/time-series/lms-are-glms-part-24/index.html#vector-autoregression",
    "href": "pages/extra-courses/time-series/lms-are-glms-part-24/index.html#vector-autoregression",
    "title": "Part Twenty Four: Time series - Vector Autoregression and multivariate models",
    "section": "Vector Autoregression",
    "text": "Vector Autoregression\nLet’s start with a semi-technical definition:\n\nIn vector autoregression (VAR) the values of two or more outcomes, \\(\\{Y_1(T), Y_2(T)\\}\\), are predicted based on previous values of those same outcomes \\(\\{Y_1(T-k), Y_2(T-k)\\}\\), for various lag periods \\(k\\).\n\nWhere \\(Y\\) has two columns, and an AR(1) specification (i.e. k is just 1), how is this different from simply running two separate AR(1) regression models, one for \\(Y_1\\), and the other for \\(Y_2\\)?\nWell, graphically, two separate AR(1) models proposes the following paths of influence:\n\n\n\n\nflowchart LR\nY1_T[\"Y1(T)\"]\nY2_T[\"Y2(T)\"]\n\nY1_T1[\"Y1(T-1)\"]\nY2_T1[\"Y2(T-1)\"]\n\nY1_T1 --&gt; Y1_T\nY2_T1 --&gt; Y2_T\n\n\n\n\n\nBy contrast, the paths implied and allowed in the corresponding VAR(1) model look more like the following:\n\n\n\n\nflowchart LR\nY1_T[\"Y1(T)\"]\nY2_T[\"Y2(T)\"]\n\nY1_T1[\"Y1(T-1)\"]\nY2_T1[\"Y2(T-1)\"]\n\nY1_T1 & Y2_T1 --&gt; Y1_T & Y2_T\n\n\n\n\n\nSo, each of the two outcomes at time T is influenced both by its own previous value, but also by the previous value of the other outcome. This other outcome influence is what is represented in the figure above by the diagonal lines: from Y2(T-1) to Y1(T), and from Y1(T-1) to Y2(T).\nExpressed verbally, if we imagine two entities - self and other - tracked through time, self is influenced both by self’s history, but also by other’s history too."
  },
  {
    "objectID": "pages/extra-courses/time-series/lms-are-glms-part-24/index.html#example-and-application-in-r",
    "href": "pages/extra-courses/time-series/lms-are-glms-part-24/index.html#example-and-application-in-r",
    "title": "Part Twenty Four: Time series - Vector Autoregression and multivariate models",
    "section": "Example and application in R",
    "text": "Example and application in R\nIn a more substantivelly focused post, I discussed how I suspect economic growth and longevity growth trends are correlated. What I proposed doesn’t exactly lend itself to the simplest kind of VAR(1) model specification, because I suggested a longer lag between the influence of economic growth on longevity growth, and a change in the fundamentals of growth in both cases. However, as an example of VAR I will ignore these complexities, and use the data I prepared for that post:\n\n\nCode\nlibrary(tidyverse)\n\ngdp_growth_pct_series &lt;- read_csv(\"still-the-economy-both-series.csv\") \n\ngdp_growth_pct_series\n\n\n# A tibble: 147 × 5\n    ...1  year series            pct_change period\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt; \n 1     1  1949 1. Per Capita GDP     NA     Old   \n 2     2  1950 1. Per Capita GDP      2.20  Old   \n 3     3  1951 1. Per Capita GDP      2.92  Old   \n 4     4  1952 1. Per Capita GDP      1.36  Old   \n 5     5  1953 1. Per Capita GDP      5.07  Old   \n 6     6  1954 1. Per Capita GDP      3.87  Old   \n 7     7  1955 1. Per Capita GDP      3.55  Old   \n 8     8  1956 1. Per Capita GDP      1.28  Old   \n 9     9  1957 1. Per Capita GDP      1.49  Old   \n10    10  1958 1. Per Capita GDP      0.815 Old   \n# ℹ 137 more rows\n\n\nWe need to do a certain amount of reformatting to bring this into a useful format:\n\n\nCode\nwide_ts_series &lt;- \ngdp_growth_pct_series |&gt;\n    select(-c(`...1`, period)) |&gt;\n    mutate(\n        short_series = case_when(\n            series == \"1. Per Capita GDP\" ~ 'gdp',\n            series == \"2. Life Expectancy at Birth\" ~ 'e0',\n            TRUE ~ NA_character_\n        )\n    ) |&gt;\n    select(-series) |&gt;\n    pivot_wider(names_from = short_series, values_from = pct_change) |&gt;\n    arrange(year) |&gt;\n    mutate(\n        lag_gdp = lag(gdp),\n        lag_e0 = lag(e0)\n    ) %&gt;%\n    filter(complete.cases(.))\n\n\nwide_ts_series\n\n\n# A tibble: 71 × 5\n    year   gdp      e0 lag_gdp  lag_e0\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1  1951 2.92  -0.568    2.20   0.763 \n 2  1952 1.36   1.89     2.92  -0.568 \n 3  1953 5.07   0.388    1.36   1.89  \n 4  1954 3.87   0.530    5.07   0.388 \n 5  1955 3.55  -0.0570   3.87   0.530 \n 6  1956 1.28   0.385    3.55  -0.0570\n 7  1957 1.49   0.170    1.28   0.385 \n 8  1958 0.815  0.255    1.49   0.170 \n 9  1959 3.70   0.184    0.815  0.255 \n10  1960 5.72   0.268    3.70   0.184 \n# ℹ 61 more rows\n\n\nSo, we can map columns to parts of the VAR specification as follows:\n\nY1: gdp\nY2: e0 (life expectancy at birth)\nperiod T: gdp and e0\nperiod T-1: lag_gdp and lag_e0\n\nTo include two or more variables as the response part, \\(Y\\), of a linear model we can use the cbind() function to combine more than one variable to the left hand side of the linear regression formula for lm or glm:\n\n\nCode\nvar_model &lt;- lm(\n    cbind(gdp, e0) ~ lag_gdp + lag_e0,\n    data = wide_ts_series\n)\n\nvar_model\n\n\n\nCall:\nlm(formula = cbind(gdp, e0) ~ lag_gdp + lag_e0, data = wide_ts_series)\n\nCoefficients:\n             gdp       e0      \n(Intercept)   1.99440   0.28108\nlag_gdp       0.06173   0.01179\nlag_e0       -0.48525  -0.33063\n\n\nWe can see here that the model reports a small matrix of coefficients: three rows (one for each coefficient term) and two columns: one for each of the response variables. This is as we should expect.\nBack in part 12 of the series, we saw we could extract the coefficients, variance-covariance matrix, and error terms of a linear regression model using the functions coefficients, vcov, and sigma respectively.1 Let’s use those functions here too:\nFirst the coefficients\n\n\nCode\ncoefficients(var_model)\n\n\n                    gdp          e0\n(Intercept)  1.99439587  0.28108028\nlag_gdp      0.06172721  0.01178781\nlag_e0      -0.48524808 -0.33062640\n\n\nAnd now the variance-covariance matrix:\n\n\nCode\nvcov(var_model)\n\n\n                gdp:(Intercept)   gdp:lag_gdp    gdp:lag_e0 e0:(Intercept)\ngdp:(Intercept)    0.1804533467 -0.0272190933 -0.1129646084   0.0039007790\ngdp:lag_gdp       -0.0272190933  0.0164590434 -0.0180517467  -0.0005883829\ngdp:lag_e0        -0.1129646084 -0.0180517467  0.6290771233  -0.0024419053\ne0:(Intercept)     0.0039007790 -0.0005883829 -0.0024419053   0.0037904364\ne0:lag_gdp        -0.0005883829  0.0003557878 -0.0003902165  -0.0005717391\ne0:lag_e0         -0.0024419053 -0.0003902165  0.0135984779  -0.0023728303\n                   e0:lag_gdp     e0:lag_e0\ngdp:(Intercept) -0.0005883829 -0.0024419053\ngdp:lag_gdp      0.0003557878 -0.0003902165\ngdp:lag_e0      -0.0003902165  0.0135984779\ne0:(Intercept)  -0.0005717391 -0.0023728303\ne0:lag_gdp       0.0003457235 -0.0003791783\ne0:lag_e0       -0.0003791783  0.0132138133\n\n\nAnd finally the error terms\n\n\nCode\nsigma(var_model)\n\n\n      gdp        e0 \n2.6906051 0.3899529 \n\n\nThe coefficients returns the same kind of 3x2 matrix we saw previously: two models run simultaneously. The error terms is now a vector of length 2: one for each of these models. The variance-covariance matrix is a square matrix of dimension 6: i.e. 6 rows and 6 columns. This is the number of predictor coefficients in each model (the number of columns of \\(X\\), i.e. 3) times the number of models simultaneously run, i.e. 2.\n\\(6^2\\) is 36, which is the number of elements in the variance-covariance matrix of this VAR model. By contrast, if we had run two independent models - one for gdp and the other for e0 - we would have two 3x3 variance-variance matrices, producing a total of 18 2 terms. This should provide some reassurance that, when we run a multivariate regression model of two outcomes, we’re not just doing the equivalent of running separate regression models for each outcome, but in slightly fewer lines.\nNow, let’s look at the model summary:\n\n\nCode\nsummary(var_model)\n\n\nResponse gdp :\n\nCall:\nlm(formula = gdp ~ lag_gdp + lag_e0, data = wide_ts_series)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.679  -1.061   0.143   1.483   6.478 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.99440    0.42480   4.695 1.34e-05 ***\nlag_gdp      0.06173    0.12829   0.481    0.632    \nlag_e0      -0.48525    0.79314  -0.612    0.543    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.691 on 68 degrees of freedom\nMultiple R-squared:  0.007555,  Adjusted R-squared:  -0.02163 \nF-statistic: 0.2588 on 2 and 68 DF,  p-value: 0.7727\n\n\nResponse e0 :\n\nCall:\nlm(formula = e0 ~ lag_gdp + lag_e0, data = wide_ts_series)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.45680 -0.19230 -0.00385  0.24379  1.38706 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.28108    0.06157   4.565 2.15e-05 ***\nlag_gdp      0.01179    0.01859   0.634  0.52823    \nlag_e0      -0.33063    0.11495  -2.876  0.00537 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.39 on 68 degrees of freedom\nMultiple R-squared:  0.1086,    Adjusted R-squared:  0.08243 \nF-statistic: 4.144 on 2 and 68 DF,  p-value: 0.02003\n\n\nThe summary is now reported for each of the two outcomes: first gdp, then e0.\nRemember that the outcome is percentage annual change in the outcome of interest from the previous year. i.e. both series have already been ‘differenced’ to produce approximately stationary series. It also means that the intercept terms are especially important, as they indicate the long-term trends observed in each series.\nIn this case the intercepts for both series are positive and statistically significant: over the long term, GDP has grown on average around 2% each year, and life expectancy by around 0.28%. As the post this relates to makes clear, however, these long-term trends may no longer apply.\nOf the four lag (AR(1)) terms in the model(s), three are not statistically significant; not even close. The exception is the lag_e0 term for the e0 response model, which is statistically significant and negative. Its coefficient is also of similar magnitude to the intercept too.\nWhat does this mean in practice? In effect, that annual mortality improvement trends have a tendency to oscillate: a better-than-average year tends to be followed by a worse-than-average year, and a worse-than-average year to be followed by a better-than-average year, in both cases at higher-than-chance rates.\nWhat could be the cause of this oscillatory phenomenon? When it comes to longevity, the phenomenon is somewhat well understood (though perhaps not widely enough), and referred to as either ‘forward mortality displacement’ or, more chillingly, ‘harvesting’. This outcome likely comes about because, if there were an exceptionally bad year in terms of (say) influenza mortality, the most frail and vulnerable are likely to be those who die disproportionately from this additional mortality event. This means that the ‘stock’ of people remaining the following year have been selected, on average, to be slightly less frail and vulnerable than those who started the previous year. Similarly, an exceptionally ‘good’ year can mean that the average ‘stock’ of the population in the following year is slightly more frail than in an average year, so more susceptible to mortality. And so, by this means, comparatively-bad-years tend to be followed by comparatively-good-years, and comparatively-good-years by comparatively-bad-years.\nThough this general process is not pleasant to think about or reason through, statistical signals such as the negative AR(1) coefficient identified here tend to keep appearing, whether we are looking for them or not."
  },
  {
    "objectID": "pages/extra-courses/time-series/lms-are-glms-part-24/index.html#conclusion",
    "href": "pages/extra-courses/time-series/lms-are-glms-part-24/index.html#conclusion",
    "title": "Part Twenty Four: Time series - Vector Autoregression and multivariate models",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post we’ve both concluded the time series subseries, and returned to and expanded on a few posts earlier in the series. This includes the very first post, where we were first introduced to the grandmother formulae, the posts on statistical modelling using both frequentist and Bayesian methods, and a substantive post linking life expectancy with economic growth.\nAlthough we’ve now first encountered multivariate regression models in the context of time series, they are a much more general phenomenon. Pretty much any type of model we can think of and apply in a univariate fashion - where \\(Y\\) has just a single column - can conceivably be expanded to two mor more columns, leading to their more complicated multiple regression variants."
  },
  {
    "objectID": "pages/extra-courses/time-series/lms-are-glms-part-24/index.html#footnotes",
    "href": "pages/extra-courses/time-series/lms-are-glms-part-24/index.html#footnotes",
    "title": "Part Twenty Four: Time series - Vector Autoregression and multivariate models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA primary aim extracting these components from a linear regression in this way is to allow something approximating a Bayesian posterior distribution of coefficients to be generated, using a multivariate normal distribution (the first place we actually encountered a multivariate regression), without using a Bayesian modelling approach. This allows for the estimating and propagation of ‘honest uncertainty’ in predicted and expected outcomes. However, as we saw in part 13, it can sometimes be as or more straightforward to just use a Bayesian modelling approach.↩︎\ni.e. two times three squared.↩︎"
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/infer-introduction/index.html",
    "href": "pages/extra-courses/hacker-stats/infer-introduction/index.html",
    "title": "Getting started with the infer package",
    "section": "",
    "text": "This post continues a short series on resampling methods, sometimes also known as ‘Hacker Stats’, for hypothesis testing. To recap: resampling with replacement is known as bootstrapping. Resampling without replacement can be used for permutation tests: testing whether apparent patterns in the data, including apparent associations between variables in the data, could likely have emerged from the Null distribution.\nIn a previous post introducing bootstrapping, I showed how the approach can be used to perform something like hypothesis tests for quantities of interest that aren’t as easily amenable as means to being assessed parametrically, such as differences in medians. In the next post, on resampling and permutation tests, I described the intuition and methodology behind resampling with replacement to produce Null distributions, and how to implement the procedure using base R.\nIn this post, I show how the infer package, can be used to perform both bootstrapping and permutation testing in a way that’s slightly easier, and more declarative in the context of a general hypothesis testing framework."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/infer-introduction/index.html#introduction",
    "href": "pages/extra-courses/hacker-stats/infer-introduction/index.html#introduction",
    "title": "Getting started with the infer package",
    "section": "",
    "text": "This post continues a short series on resampling methods, sometimes also known as ‘Hacker Stats’, for hypothesis testing. To recap: resampling with replacement is known as bootstrapping. Resampling without replacement can be used for permutation tests: testing whether apparent patterns in the data, including apparent associations between variables in the data, could likely have emerged from the Null distribution.\nIn a previous post introducing bootstrapping, I showed how the approach can be used to perform something like hypothesis tests for quantities of interest that aren’t as easily amenable as means to being assessed parametrically, such as differences in medians. In the next post, on resampling and permutation tests, I described the intuition and methodology behind resampling with replacement to produce Null distributions, and how to implement the procedure using base R.\nIn this post, I show how the infer package, can be used to perform both bootstrapping and permutation testing in a way that’s slightly easier, and more declarative in the context of a general hypothesis testing framework."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/infer-introduction/index.html#setting-up",
    "href": "pages/extra-courses/hacker-stats/infer-introduction/index.html#setting-up",
    "title": "Getting started with the infer package",
    "section": "Setting up",
    "text": "Setting up\nLet’s install the infer packge and try a couple of examples from the documentation.\n\n# install.packages(\"infer\") # First time around\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(infer)"
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/infer-introduction/index.html#the-infer-package",
    "href": "pages/extra-courses/hacker-stats/infer-introduction/index.html#the-infer-package",
    "title": "Getting started with the infer package",
    "section": "The infer package",
    "text": "The infer package\nFrom the vignette page we can see that infer’s workflow is framed around four verbs:\n\nspecify() allows you to specify the variable, or relationship between variables, that you’re interested in.\nhypothesize() allows you to declare the null hypothesis.\ngenerate() allows you to generate data reflecting the null hypothesis.\ncalculate() allows you to calculate a distribution of statistics from the generated data to form the null distribution.\n\nThe package describes the problem of hypothesis testing as being somewhat generic, regardless of the specific test, hypothesis, or dataset being used:\n\nRegardless of which hypothesis test we’re using, we’re still asking the same kind of question: is the effect/difference in our observed data real, or due to chance? To answer this question, we start by assuming that the observed data came from some world where “nothing is going on” (i.e. the observed effect was simply due to random chance), and call this assumption our null hypothesis. (In reality, we might not believe in the null hypothesis at all—the null hypothesis is in opposition to the alternate hypothesis, which supposes that the effect present in the observed data is actually due to the fact that “something is going on.”) We then calculate a test statistic from our data that describes the observed effect. We can use this test statistic to calculate a p-value, giving the probability that our observed data could come about if the null hypothesis was true. If this probability is below some pre-defined significance level \\(\\alpha\\), then we can reject our null hypothesis."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/infer-introduction/index.html#the-gss-dataset",
    "href": "pages/extra-courses/hacker-stats/infer-introduction/index.html#the-gss-dataset",
    "title": "Getting started with the infer package",
    "section": "The gss dataset",
    "text": "The gss dataset\nLet’s look through - and in some places adapt - the examples used. These mainly make use of the gss dataset.\n\ndata(gss)\n\n\nglimpse(gss)\n\nRows: 500\nColumns: 11\n$ year    &lt;dbl&gt; 2014, 1994, 1998, 1996, 1994, 1996, 1990, 2016, 2000, 1998, 20…\n$ age     &lt;dbl&gt; 36, 34, 24, 42, 31, 32, 48, 36, 30, 33, 21, 30, 38, 49, 25, 56…\n$ sex     &lt;fct&gt; male, female, male, male, male, female, female, female, female…\n$ college &lt;fct&gt; degree, no degree, degree, no degree, degree, no degree, no de…\n$ partyid &lt;fct&gt; ind, rep, ind, ind, rep, rep, dem, ind, rep, dem, dem, ind, de…\n$ hompop  &lt;dbl&gt; 3, 4, 1, 4, 2, 4, 2, 1, 5, 2, 4, 3, 4, 4, 2, 2, 3, 2, 1, 2, 5,…\n$ hours   &lt;dbl&gt; 50, 31, 40, 40, 40, 53, 32, 20, 40, 40, 23, 52, 38, 72, 48, 40…\n$ income  &lt;ord&gt; $25000 or more, $20000 - 24999, $25000 or more, $25000 or more…\n$ class   &lt;fct&gt; middle class, working class, working class, working class, mid…\n$ finrela &lt;fct&gt; below average, below average, below average, above average, ab…\n$ weight  &lt;dbl&gt; 0.8960034, 1.0825000, 0.5501000, 1.0864000, 1.0825000, 1.08640…"
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/infer-introduction/index.html#example-1-categorical-predictor-continuous-response",
    "href": "pages/extra-courses/hacker-stats/infer-introduction/index.html#example-1-categorical-predictor-continuous-response",
    "title": "Getting started with the infer package",
    "section": "Example 1: Categorical Predictor; Continuous Response",
    "text": "Example 1: Categorical Predictor; Continuous Response\nLet’s go slightly off piste and say we are interested in seeing if there is a relationship between age, a cardinal variable, and sex, a categorical variable. We can start by stating our null and alternative hypotheses explicitly:\n\nNull hypothesis: There is no difference between age and sex\nAlt hypothesis: There is a difference between age and sex\n\nLet’s see if we can start by just looking at the data to see if, informally, it looks like it might better fit the Null or Alt hypothesis.\n\ngss |&gt; \n    ggplot(aes(x=age, group = sex, colour = sex)) + \n    geom_density()\n\n\n\n\nIt looks like the densities of age distributions are similar for both sexes. However, they’re not identical. Are the differences more likely to be due to chance, or are they more structural?\nWe can start by calculating, say, the differences in average ages between males and females:\n\ngss |&gt;\n    group_by(sex) |&gt;\n    summarise(n = n(), mean_age = mean(age))\n\n# A tibble: 2 × 3\n  sex        n mean_age\n  &lt;fct&gt;  &lt;int&gt;    &lt;dbl&gt;\n1 male     263     40.6\n2 female   237     39.9\n\n\n\nOur first testable hypothesis (using permutation testing/sampling without replacement)\nThe mean age is 40.6 for males and 39.9 for females, a difference of about 0.7 years of age. Could this have occurred by chance?\nThere are 263 male observations, and 237 female observations, in the dataset. Imagine that the ages are values, and the sexes are labels that are added to these values.\nOne approach to operationalising the concept of the Null Hypothesis is to ask: If we shifted around the labels assigned to the values, so there were still as many male and female labels, but they were randomly reassigned, what would the difference in mean age between these two groups be? What would happen if we did this many times?\nThis is the essence of building a Null distribution using a permutation test, which is similar to a bootstrap except it involves resampling with replacement rather than without replacement.\nWe can perform this permutation test using the infer package as follows:\n\nmodel &lt;- gss |&gt;\n    specify(age ~ sex) |&gt;\n    hypothesize(null = 'independence') |&gt;\n    generate(reps = 10000, type = 'permute')\n\nmodel\n\nResponse: age (numeric)\nExplanatory: sex (factor)\nNull Hypothesis: independence\n# A tibble: 5,000,000 × 3\n# Groups:   replicate [10,000]\n     age sex    replicate\n   &lt;dbl&gt; &lt;fct&gt;      &lt;int&gt;\n 1    23 male           1\n 2    38 female         1\n 3    56 male           1\n 4    47 male           1\n 5    20 male           1\n 6    23 female         1\n 7    25 female         1\n 8    23 female         1\n 9    24 female         1\n10    27 female         1\n# ℹ 4,999,990 more rows\n\n\nThe infer package has now arbitrarily shifted around the labels assigned to the age values 10000 times. Each time is labelled with a different replicate number. Let’s take the first nine replicates and show what the densities by sex look like:\n\nmodel |&gt;\n    filter(replicate &lt;= 9) |&gt;\n    ggplot(aes(x=age, group = sex, colour = sex)) + \n    geom_density() + \n    facet_wrap(~replicate)\n\n\n\n\nWhat if we now look at the differences in means apparent in each of these permutations\n\nmodel |&gt;\n    calculate(stat = \"diff in means\", order = c(\"male\", \"female\")) |&gt;\n    visualize()\n\n\n\n\nHere we can see the distribution of differences in means follows broadly a normal distribution, which appears to be centred on 0.\nLet’s now calculate and save the observed difference in means.\n\ntmp &lt;- gss |&gt;\n    group_by(sex) |&gt;\n    summarise(mean_age = mean(age))\n\ntmp \n\n# A tibble: 2 × 2\n  sex    mean_age\n  &lt;fct&gt;     &lt;dbl&gt;\n1 male       40.6\n2 female     39.9\n\ndiff_means &lt;- tmp$mean_age[tmp$sex == \"male\"] - tmp$mean_age[tmp$sex == \"female\"]\n\ndiff_means\n\n[1] 0.7463541\n\n\n\n\nA two-sided hypothesis\nLet’s now show where the observed difference in means falls along the distribution of differences in means generated by this permutation-based Null distribution:\n\nmodel |&gt;\n    calculate(stat = \"diff in means\", order = c(\"male\", \"female\")) |&gt;\n    visualize() +\n    shade_p_value(obs_stat = diff_means, direction = \"two-sided\")\n\n\n\n\nThe observed difference in means appears to be quite close to the centre of mass for the distribution of differences in means generated by the Null distribution. So it appears very likely that this observed difference could be generated from a data generating process in which there’s no real difference in mean ages between the two groups. We can formalise this slightly by calcuating a p-value:\n\nmodel |&gt;\n    calculate(stat = \"diff in means\", order = c(\"male\", \"female\")) |&gt;\n    get_p_value(obs_stat = diff_means, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.540\n\n\nThe p value is much, much greater than 0.05, suggesting there’s little evidence to reject the Null hypothesis, that in this dataset age is not influenced by sex."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/infer-introduction/index.html#example-2-categorical-predictor-categorical-response",
    "href": "pages/extra-courses/hacker-stats/infer-introduction/index.html#example-2-categorical-predictor-categorical-response",
    "title": "Getting started with the infer package",
    "section": "Example 2: Categorical Predictor; Categorical Response",
    "text": "Example 2: Categorical Predictor; Categorical Response\nNow let’s look at the two variables college and partyid:\n\ncollege: Can be degree or no degree\npartyid: Can be ind rep, dem, other\n\nThe simplest type of hypothesis to state is probably something like:\n\nNull Hypothesis: There is no relationship between partyid and college\nAlt Hypothesis: There is a relationship between partyid and college\n\nWe can then consider more specific and targetted hypotheses at a later date.\nLet’s see how we could use infer to help decide between these hypotheses, using a permutation test:\n\nmodel &lt;- gss |&gt;\n    specify(partyid ~ college) |&gt;\n    hypothesize(null = 'independence') |&gt;\n    generate(reps = 10000, type = 'permute')\n\nDropping unused factor levels DK from the supplied response variable 'partyid'.\n\nmodel\n\nResponse: partyid (factor)\nExplanatory: college (factor)\nNull Hypothesis: independence\n# A tibble: 5,000,000 × 3\n# Groups:   replicate [10,000]\n   partyid college   replicate\n   &lt;fct&gt;   &lt;fct&gt;         &lt;int&gt;\n 1 dem     degree            1\n 2 rep     no degree         1\n 3 ind     degree            1\n 4 dem     no degree         1\n 5 rep     degree            1\n 6 dem     no degree         1\n 7 dem     no degree         1\n 8 dem     degree            1\n 9 ind     degree            1\n10 dem     no degree         1\n# ℹ 4,999,990 more rows\n\n\nLet’s visualise the relationship between partyid and college in the first nine replicates:\n\nmodel |&gt;\n    filter(replicate &lt;= 9) |&gt;\n    ggplot(aes(x = college, fill = partyid)) + \n    geom_bar(position = \"fill\") + \n    facet_wrap(~replicate) +\n    labs(title = \"Permuted (fake) datasets\")\n\n\n\n\nAnd how does this compare with the observed dataset?\n\ngss |&gt;\n    ggplot(aes(x = college, fill = partyid)) + \n    geom_bar(position = \"fill\") + \n    labs(title = \"Relationship in real dataset\")\n\n\n\n\nBut what summary statistic can we use for comparing the observed level of extremeness of any apparent association between the two variables, with summary statistics under the Null hypothesis (i.e. using permutation testing)? The standard answer is to calculate the Chi-squared statistic, as detailed here.\nFirst, what’s the Chi-squared value we get from the observed data?\n\nChisq_obs &lt;- gss |&gt;\n    specify(partyid ~ college) |&gt;\n    hypothesize(null = \"independence\") |&gt;\n    calculate(stat = \"Chisq\")\n\nDropping unused factor levels DK from the supplied response variable 'partyid'.\n\nChisq_obs\n\nResponse: partyid (factor)\nExplanatory: college (factor)\nNull Hypothesis: independence\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  4.15\n\n\nSo, the value is 4.15. Is this a big or a small value?\nTo answer that let’s calculate the same statistic from the Null distribution\n\nchi_dist_null &lt;- model |&gt;\n    calculate(stat = \"Chisq\")\n\nchi_dist_null\n\nResponse: partyid (factor)\nExplanatory: college (factor)\nNull Hypothesis: independence\n# A tibble: 10,000 × 2\n   replicate  stat\n       &lt;int&gt; &lt;dbl&gt;\n 1         1 2.91 \n 2         2 5.63 \n 3         3 9.93 \n 4         4 2.48 \n 5         5 0.421\n 6         6 1.61 \n 7         7 4.09 \n 8         8 3.19 \n 9         9 1.57 \n10        10 2.99 \n# ℹ 9,990 more rows\n\n\nSo, is the observed value something that could have been plausibly generated from the Null distribution? We can answer this by seeing how extreme the observed Chi-squared value is compared with the distribution of values under the Null:\n\nvisualise(chi_dist_null) +\n    shade_p_value(obs_stat = Chisq_obs, direction = \"greater\")\n\n\n\n\nSo, it looks like it’s fairly likely that the value we observed could have been observed under the Null, a scenario in which there’s no true relationship between the variables. But how likely?\n\nchi_dist_null |&gt;\n    get_p_value(obs_stat = Chisq_obs, direction = \"greater\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.257\n\n\nAround a quarter of Chi-squared values under the Null are as greater or greater than that observed in the real dataset. So there’s not great evidence of there being a relationship between having a degree and distribution of party affiliations.\nInfer makes it fairly straightforward to calculate the extremeness of our observed test statistic using the analytic/theoretical approach too, using the assume() verb:\n\nnull_dist_theory &lt;- gss %&gt;%\n    specify(partyid ~ college) |&gt;\n    assume(distribution = \"Chisq\")\n\nDropping unused factor levels DK from the supplied response variable 'partyid'.\n\nvisualize(null_dist_theory) +\n  shade_p_value(obs_stat = Chisq_obs, direction = \"greater\")\n\n\n\n\n\nnull_dist_theory |&gt;\n    get_p_value(obs_stat = Chisq_obs, direction = \"greater\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.386\n\n\nHere the theoretical distribution suggests the observed value is even more likely to have been observed by chance under the Null, than using the permutation-based approach.\nAnd we can show both approaches together:\n\nchi_dist_null |&gt;\n    visualise(method = \"both\") +\n    shade_p_value(obs_stat = Chisq_obs, direction = \"greater\")\n\nWarning: Check to make sure the conditions have been met for the theoretical method.\ninfer currently does not check these for you.\n\n\n\n\n\nHere we can see the resampling-based distribution (the histogram) has more values lower than the observed value, and fewer values higher than the observed value, than the theoretical distribution (the density line), which helps to explain the difference in p-values calculated."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/infer-introduction/index.html#summing-up",
    "href": "pages/extra-courses/hacker-stats/infer-introduction/index.html#summing-up",
    "title": "Getting started with the infer package",
    "section": "Summing up",
    "text": "Summing up\nSo, that’s a brief introduction to the infer package. It provides a clear and opinionated way of thinking about and constructing hypothesis tests using a small series of verbs, and as part of this handles a lot of the code for performing permutation tests, visualising data, and comparing resampling-based estimates of the Null distribution with theoretical estimates of the same quantities. And, though both of the examples I’ve shown above are about permutation testing, it also allows for bootstrapped calculations to be performed too.\nIn some ways, infer seems largely intended as a pedagogic/teaching tool, for understanding the intuition behind the concept of the Null hypothesis and distribution, and so what a p-value actually means. However you can see that it does abstract away some of the computational complexity involved in producing Null distributions using both resampling and ‘traditional’ approaches. In previous posts we showed that it’s not necessarily too difficult to produce resampled distributions without this, but there’s still potentially some quality-of-life benefits to using it."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/resampling-approaches-intro/index.html",
    "href": "pages/extra-courses/hacker-stats/resampling-approaches-intro/index.html",
    "title": "Hacker Stats: Intro and overview",
    "section": "",
    "text": "This is the first post in a small series on resampling approaches to statistical inference. 1 Resampling approaches are a powerful and highly adaptable set of approaches for trying to get ‘good enough’ estimates of how statistically significant some observed value or summary of observed values is likely to be, or equivalently how likely what one’s observed is to have been observed by chance. They can also be extended and applied to performing post-stratification, which allows samples of the population with known biases to be adjusted in ways that aim to mitigate such biases, and so produce summary estimates more representative of the population of interest."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/resampling-approaches-intro/index.html#introduction",
    "href": "pages/extra-courses/hacker-stats/resampling-approaches-intro/index.html#introduction",
    "title": "Hacker Stats: Intro and overview",
    "section": "",
    "text": "This is the first post in a small series on resampling approaches to statistical inference. 1 Resampling approaches are a powerful and highly adaptable set of approaches for trying to get ‘good enough’ estimates of how statistically significant some observed value or summary of observed values is likely to be, or equivalently how likely what one’s observed is to have been observed by chance. They can also be extended and applied to performing post-stratification, which allows samples of the population with known biases to be adjusted in ways that aim to mitigate such biases, and so produce summary estimates more representative of the population of interest."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/resampling-approaches-intro/index.html#resampling-as-hacker-stats",
    "href": "pages/extra-courses/hacker-stats/resampling-approaches-intro/index.html#resampling-as-hacker-stats",
    "title": "Hacker Stats: Intro and overview",
    "section": "Resampling as Hacker Stats",
    "text": "Resampling as Hacker Stats\nResampling methods are sometimes called Hacker Stats, which might be a slightly derogatory term, but is also an informative one. Broadly, Resampling Methods:\n\nSubstitute meat brain effort (deriving and recalling analytic solutions) for silicon brain effort (i.e. they’re computationally intensive rather than human knowledge and reasoning intensive).\nAre theoretically and methodologically thin rather than theoretically and methodologically fat.\nAre approximate, stochastic and general; rather than precise, deterministic and specialist.\n\nPut another way, Hacker Stats are methods that data scientists and more casual users of statistics can use to get good enough approximations of the kinds of careful, analytic solutions and tests that, with many years of specialist training and memorisation, a degree in statistics would provide. They’re a good example of the 80:20 Principle: part of the 20% of stats know-how that’s used for 80% of the tasks."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/resampling-approaches-intro/index.html#types-of-permutation-method",
    "href": "pages/extra-courses/hacker-stats/resampling-approaches-intro/index.html#types-of-permutation-method",
    "title": "Hacker Stats: Intro and overview",
    "section": "Types of permutation method",
    "text": "Types of permutation method\nThe following flowchart shows the ‘family tree’ of types of resampling method:\n\n\n\n\nflowchart TB\n    sd[Sample Data]\n    us(Uniform Sampling)\n    nus(Non-Uniform Sampling)\n    pt[Permutation Testing]\n    bs[Bootstrapping]\n    ps[Post-Stratification]\n\n    pw[Population Weights]\n\n    dec1{Equal Probability?}\n    dec2{With Replacement?}\n\n    sd --sampling--&gt; dec1\n\n    us --&gt; dec2\n\n    dec1 --Yes--&gt; us\n    dec1 --No--&gt; nus\n    nus --&gt; ps\n\n    dec2 --Yes--&gt; bs\n    dec2 --No--&gt; pt\n\n    pw --&gt; nus\n\n\n\n\n\n\n\n\nn.b. Bootstrapping and permutation testing can be applied to post-stratified data too!"
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/resampling-approaches-intro/index.html#the-thin-but-deep-theories",
    "href": "pages/extra-courses/hacker-stats/resampling-approaches-intro/index.html#the-thin-but-deep-theories",
    "title": "Hacker Stats: Intro and overview",
    "section": "The thin-but-deep theories",
    "text": "The thin-but-deep theories\nBoth Bootstrapping, which is resampling with replacement, and Permutation Testing, which is resampling without replacement, use computation to explore the implications of two distinct, simple, and important theories about the sample data, and any observations we may think we’ve observed within it. Let’s try to talk through these two thin-but-deep theories:\n\nBootstrapping\nBootstrapping starts and ends with something like the following claim:\n\nEvery observation in our dataset is equally likely.\n\nWhy is this?\n\nBecause each specific observation in our dataset has been observed the same number of times.\n\nWhy do you say that?\n\nBecause each observation in the dataset has been observed exactly one time, and 1=1!\n\nAnd why does this matter?\n\nBecause, if we can accept the above, we can say that another dataset, made up by resampling the real sample data, so that each observation (row) is as likely to be picked as every other one, is as likely as the dataset we actually observed. And so long as this other dataset has the same number of observations as the original dataset, then it’s also as precise as the original dataset.\n\nIt’s this line of reasoning - and the two conditions for another dataset: equally likely; and equally precise - which lead to the justification, in bootstrapping, for resampling with replacement.\n\n\nPermutation Tests\nSay we have a sample dataset, \\(D\\), which is a big rectangle of data with rows (observations) and columns (variables). To simplify, imagine \\(D\\) comprises five observations and two variables, so it looks like this:\n\\[\nD =\n\\begin{pmatrix}\nd_{1,1} & d_{1,2} \\\\\nd_{2,1} & d_{2,2} \\\\\nd_{3,1} & d_{3,2} \\\\\nd_{4,1} & d_{4,2} \\\\\nd_{5,1} & d_{5,2}  \n\\end{pmatrix}\n\\]\nThere are a number of different ways of describing and thinking about this kind of data, which is really just a structured collection of elements. One approach is to think about from the perspective of observations, which leads to a row-wise interpretation of the dataset:\n\\[\nD =\n\\begin{pmatrix}\nd_{1} = \\{d_{1,1} , d_{1,2}\\} \\\\\nd_{2} = \\{d_{2,1} , d_{2,2}\\} \\\\\nd_{3} = \\{d_{3,1} , d_{3,2}\\} \\\\\nd_{4} = \\{d_{4,1} , d_{4,2}\\} \\\\\nd_{5} = \\{d_{5,1} , d_{5,2}\\}  \n\\end{pmatrix}\n\\]\nAnd another way of thinking about the data is from the perspective of variables, which leads to a column-wise interpretation of the data:\n\\[\nD = \\{X, Y\\}\n\\]\n\\[\nX = \\{d_{1,1}, d_{2,1}, d_{3, 1}, d_{4, 1}, d_{5, 1}\\}\n\\]\n\\[\nY = \\{d_{1,2}, d_{2,2}, d_{3, 2}, d_{4, 2}, d_{5, 2}\\}\n\\]\nNow, imagine we’ve looked at our dataset, and we think there’s an association between the two variables \\(X\\) and \\(Y\\). What would be a very generalisable way of testing for whether we’re correct in assuming this association?\nThe key piece of reasoning behind resampling without replacement for permutation testing is as follows:\n\nIf there is a real association between the variables then the way values are paired up as observations matters, and should be preserved. If there’s no real association between the variables then the pairing up of values into observations doesn’t matter, so we can break this pairing and still get outcomes similar to what we actually observed.\n\nThere’s another term for resampling with replacement: shuffling. We can break-up the observational pairing seen in the dataset by shuffling one or both of the variables, then putting back the data into the same kind of rectangular structure it was before.\nFor instance, say we shuffle variable \\(Y\\), and end up with the following new vector of observations:\n\\[\nY^{shuffled} = \\{ d_{2,2}, d_{5, 2}, d_{3, 2}, d_{1,2}, d_{4, 2} \\}\n\\]\nWe could then make a new fake dataset, with all the same values as in the original dataset, but not necessarily in the same order:\n\\[\nX = \\{d_{1,1}, d_{2,1}, d_{3, 1}, d_{4, 1}, d_{5, 1}\\}\n\\]\n\\[\nY^{shuffled} = \\{d_{4,2}, d_{2,2}, d_{1, 2}, d_{3, 2}, d_{5, 2}\\}\n\\]\n\\[\nD^{fake} = \\{X, Y^{shuffled}\\}\n\\]\n\\[\nD^{fake} =\n\\begin{pmatrix}\nd_{1}^{fake} = \\{d_{1,1} , d_{4,2}\\} \\\\\nd_{2}^{fake} = \\{d_{2,1} , d_{2,2}\\} \\\\\nd_{3}^{fake} = \\{d_{3,1} , d_{1,2}\\} \\\\\nd_{4}^{fake} = \\{d_{4,1} , d_{3,2}\\} \\\\\nd_{5}^{fake} = \\{d_{5,1} , d_{5,2}\\}  \n\\end{pmatrix}\n\\]\nSo, in \\(D^{fake}\\) the observed (row-wise) association between each \\(X\\) and corresponding \\(Y\\) value has broken, even though the same values \\(d_{i,j}\\) are present.\nHowever, if the assumption/‘hunch’ about there being an association between \\(X\\) and \\(Y\\) from the real dataset \\(D\\) was justified through some kind of summary statistic, such as a correlation coefficient, \\(r(X, Y)\\), then we calculate the same summary statistic for the fake dataset too, \\(r(X, Y^{fake})\\).\nIn fact (and in practice) we can repeat the fakery, permuting the values again and again, and each time calculating the summary statistic of interest. This produces a distribution of values for this summary statistic, against which we can compare the observed value of this summary statistic.\nThis distribution of summary statistics produced from a large selection of permutated (fake) datasets is the distribution we would expect to see under the Null Hypothesis, which is that the apparent association is illusionary, and that no real association exists: the appearance of association comes from chance alone."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/resampling-approaches-intro/index.html#post-stratification",
    "href": "pages/extra-courses/hacker-stats/resampling-approaches-intro/index.html#post-stratification",
    "title": "Hacker Stats: Intro and overview",
    "section": "Post-stratification",
    "text": "Post-stratification\nResampling methods can also be used as a method for post-stratification, reweighting sample data to try to make it more representative of the population of interest. Consider two scenarios where this might be important:\n\nIntentional Oversampling: Say we know that 95% of people working in a particular occupation tend to be female, and 5% male. We are interested both in the typical characteristics of people who work in this occupation, but also in properly understanding the characteristics of males and females separately, and the differences between males and females within the occupation. And we know that, if we take a purely random sample of the population, we’ll only get, on average, 5% of the sample being males, which won’t give us enough precision/resolution to properly understand males in the population. So, we intentionally oversample from the male population, meaning our sample contains 20% males and 80% females, even though this isn’t representative of the population as a whole.\n\n\nUnintentional Undersampling: Say we are interested in political party voting intentions at an upcoming election. However for reasons of convenience we decide only to poll people who play console games, by asking someone about to play a game if they’re more likely to vote for the Blue Party or the Red Party. We know that our sample has very different characteristics to the population at large. However we also know so many people play console games that we have a reasonably large (and so sufficiently precise) set of estimates for each of the main demographic stratas of interest to us. So what do we do to convert the very biased sample data into unbiased population estimates? 2\n\nIn either case resampling methods can be applied. Just go from equal probability sampling to weighted probability sampling, in which samples from our dataset is more likely to be selected if they are under-represented in the sample dataset compared with the population, and less likely to be selected if they are under-represented in the sample dataset compared with the population."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/resampling-approaches-intro/index.html#summary",
    "href": "pages/extra-courses/hacker-stats/resampling-approaches-intro/index.html#summary",
    "title": "Hacker Stats: Intro and overview",
    "section": "Summary",
    "text": "Summary\nIn this post we’ve discussed the key ideas behind resampling methods, AKA Hacker Stats. These approaches are computationally intensive as compared with analytical solutions, which would have been a big barrier to their use until, perhaps, the mid 1980s. However computationally intensive these days might just mean it takes five seconds to perform many times, whereas the analytic solution takes five microseconds: still a large relative difference in computing time, but practically both kinds of approaches are similarly fast to perform.\nThese days, whether you know an analytic approximation for performing the test or calculation of interest, or whether you don’t, the Hacker Stats approach is still worth trying out. Even at their slowest, the worst case scenario with Hacker Stats is your computer might whirr a bit more loudly than usual, and you’ll finally have a good excuse to get that much-deserved tea- or coffee-break!"
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/resampling-approaches-intro/index.html#footnotes",
    "href": "pages/extra-courses/hacker-stats/resampling-approaches-intro/index.html#footnotes",
    "title": "Hacker Stats: Intro and overview",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough it wasn’t written as the first post in the series, so a challenge for me is to figure out how to present these in something other than date order!↩︎\nThis isn’t a made-up example, but broadly the approach used by Wang et al 2014 to produce pretty accurate estimates of a then-upcoming US election↩︎"
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/permutation-with-base-r/index.html",
    "href": "pages/extra-courses/hacker-stats/permutation-with-base-r/index.html",
    "title": "Permutation Testing, and the intuition of the Null hypothesis, with Base R",
    "section": "",
    "text": "In this post I’ll cover the intuition of permutation tests through a little toy example. In a follow-up post I’ll discuss how this intuition can be implemented (and made a bit easier) using the infer package."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/permutation-with-base-r/index.html#core-intuition-for-permutation-tests",
    "href": "pages/extra-courses/hacker-stats/permutation-with-base-r/index.html#core-intuition-for-permutation-tests",
    "title": "Permutation Testing, and the intuition of the Null hypothesis, with Base R",
    "section": "Core intuition for permutation tests",
    "text": "Core intuition for permutation tests\nLet’s try to understand the intuition of permutation tests using a (rather boring) story:\n\nImagine you have two types of index cards: red cards and blue cards.\nSay there are 12 red cards and 8 blue cards, so a total of 20 cards.\nOn each of the cards is a value. Let’s say it’s a binary value: 1 (maybe for a ‘success’) or 0 (a ‘failure’).\nLet’s say the values from the red card came from flipping a specific coin, Coin A, 12 times, and writing a 1 on a blank red index card if the coin came up heads, and 0 on a blank red index card if the coin came up tails.\nThe values on the blue cards came from flipping a different coin, Coin B, 8 times, and doing the same thing, but with blue cards instead of red cards.\n\nWhat you want to know is whether Coin A or Coin B are different, i.e. one has a different probability of producing heads than the other one. However, you don’t have access either Coin A or Coin B. The only information you have to go on is the 20 index cards: 12 red, 8 blue.\nHow do you go about determining if the two coins are different, when you don’t have access to either coin, and all you have are the 20 index cards?\nOne approach is to perform permutation tests. This is a way of using computation to produce a Null Distribution, meaning a distribution of some kind of summary statistic that you would expect to observe if there were really no difference between Coin A and Coin B. This Null Distribution is a distribution of summary values that you would expect to observe if the Null Hypothesis were true, where the Null Hypothesis is that Coin A and Coin B behave in exactly the same way. You then compare the corresponding summary statistic from the observed data against this Null Distribution. If the observed summary statistic is far from the range of summary statistics, then you have more reason to Reject the Null Hypothesis, which generally corresponds to evidence for the Alternative Hypothesis, which in this case is that Coin A and Coin B are different.\nThe way you would manually perform a permutation test (without computers) in this example is as follows:\n\nYou get a big box of only red index cards, and a big box of blue index cards, all of which are blank.\nFrom the big box of red index cards, you take 12 cards, and put them into a little pile.\nFrom the big box of blue index cards, you take 8 cards, and put them into the same pile containing the 12 red index cards.\nYou then randomly shuffle the 20 cards with values written on them (your data), and place this randomly shuffled pile face down.\nYou take the top card from the data pile, turn it over, and write its value on the first card in the small pile of 20 blank cards you’ve just made. You then take this now-not-blank card from the small pile, and place it next to the pile of now 19 blank cards.\nYou then repeat with the next card in the data pile, and the next card in the small blank card pile, until all cards in the blank card pile have had a value (1 or 0) written onto them.\nYou then repeat steps 2 through 6 a large number of times: say another 999 times. At the end of this, you now have one real dataset, comprising 20 index cards - 12 red, 8 blue - and 1000 ‘fake datasets’, i.e. 1000 piles of 20 index cards each - 12 red, 8 blue - which also each have 1 or 0 written on them.\nAfter you have done this, you calculate a summary statistic for both the one real dataset, and the 1000 ‘fake datasets’. Say this is the difference in the proportions of 1 in the red subset of cards, and the blue subset in cards. You calculate this for the real dataset, and call it the observed statistic. And you also calculate it for each of the 1000 fake datasets, which provides your Null distribution for this same statistic.\nFinally, you compare the observed statistic (from the real dataset), with the Null distribution of summary statistics. If the observed statistic is somewhere in the middle of the Null distribution, there’s little reason to reject the Null Hypothesis; if it’s quite far from the Null distribution, there’s much more reason to reject the Null Hypothesis.\n\nAs you can tell from the description above, this would be quite a slow approach to making a Null distribution if we were to follow the steps manually. This is why historically many of the approaches for producing Null distributions that you might be familiar with involve algebra-based theoretical distributions. In the example above a classic way of calculating the Null distribution would be using the Chi-Squared distribution. Historically, it was much quicker for one person to figure out the algebra once, and perform calculation based on the algebraic solution, than to perform a permutation test. These days, even if we have an algebraic solution, it can still be as quick or quicker to perform a permutation test.\nLet’s actually make the dataset I’ve described above (using a random number seed so the answers don’t change). Let’s say in our example the true proportion for Coin A is 0.55, and for Coin B it’s 0.50. (Something we’d never know in practice.)\n\n\nCode\nset.seed(7) # Random number set.seed\n\ndraws_A &lt;- rbinom(n=12, size=1, prob=0.55)\ndraws_B &lt;- rbinom(n=8, size=1, prob=0.50)\n\ncard_colour &lt;- c(\n    rep(\"red\", 12),\n    rep(\"blue\", 8)\n)\n\nreal_data &lt;- data.frame(\n    card_colour = card_colour,\n    outcome = c(draws_A, draws_B)\n)\n\nreal_data\n\n\n   card_colour outcome\n1          red       0\n2          red       1\n3          red       1\n4          red       1\n5          red       1\n6          red       0\n7          red       1\n8          red       0\n9          red       1\n10         red       1\n11         red       1\n12         red       1\n13        blue       1\n14        blue       0\n15        blue       0\n16        blue       0\n17        blue       1\n18        blue       0\n19        blue       1\n20        blue       0\n\n\nIn this example, what is the proportion of 1s in the red card subgroup, and the blue card subgroup?\n\n\nCode\nprop_in_red &lt;- real_data$outcome[real_data$card_colour == \"red\"] |&gt;\n    mean()\n\nprop_in_blue &lt;- real_data$outcome[real_data$card_colour == \"blue\"] |&gt;\n    mean()\n\ndiff_in_props &lt;- prop_in_red - prop_in_blue\n\ndiff_in_props\n\n\n[1] 0.375\n\n\nIn this example the proportion ‘heads’ in the red subgroup (from coin A) is 0.750, and in the blue subgroup (from coin B) happens to be exactly 0.375. This means the difference in proportions is 0.375.\nHow would we use a permutation test to produce a Null distribution of differences in proportions between the two groups?\nHere’s one approach:\n\n\nCode\nnReps &lt;- 1000 # We'll perform 1000 replications/resamples\n\nnullVector &lt;- vector(mode = \"numeric\", length = 1000)\n\n\noutcomes &lt;- real_data$outcome\nlabels &lt;- real_data$card_colour\n\nnObs &lt;- length(outcomes)\n\nfor (i in 1:nReps){\n\n    random_draw_of_outcomes &lt;- sample(outcomes, size = nObs, replace = FALSE)\n\n    fake_prop_red &lt;- mean(\n        random_draw_of_outcomes[labels == \"red\"]\n    )\n\n    fake_prop_blue &lt;- mean(\n        random_draw_of_outcomes[labels == \"blue\"]\n    )\n\n    fake_diff_outcomes &lt;- fake_prop_red - fake_prop_blue\n\n    nullVector[i] &lt;- fake_diff_outcomes\n}\n\nhead(nullVector)\n\n\n[1] -0.25000000 -0.04166667  0.16666667 -0.04166667 -0.25000000 -0.04166667\n\n\nWhat does the distribution of differences look like?\n\n\nCode\nhist(nullVector)\n\n\n\n\n\nHere we can see quite a wide range of differences in proportions are generated by the permutation-based Null distribution. We can use the quantile function to get a sense of the range:\n\n\nCode\nquantile(nullVector, prob = c(0.025, 0.050, 0.25, 0.50, 0.75, 0.95, 0.975))\n\n\n       2.5%          5%         25%         50%         75%         95% \n-0.45833333 -0.45833333 -0.25000000 -0.04166667  0.16666667  0.37500000 \n      97.5% \n 0.37500000 \n\n\nHere the median value of the proportion of differences is -0.042. Half of the values are between -0.025 and 0.0167; 90% of the values are between -0.458 and 0.375, and 95% of values are between -0.458 and 0.375.\nFor reference, the real observed difference in proportions is 0.375. This seems to be at the far right end of the Null distribution. We can calculate what is in effect a p-value, of the probability of seeing a value as or more extreme than the observed value from the Null distribution, by counting up the proportion of Null distribution values that were as or more extreme than the observed value:\n\n\nCode\nsum(nullVector &gt;= diff_in_props) / length(nullVector)\n\n\n[1] 0.103\n\n\nSo, the proportion of times the Null distribution generates a value as great or greater than the observed value is about 10%. This wouldn’t meet conventional thresholds of statistical significance, which would be less than 5% of values being this or more extreme. However it does seem from the data that it’s more likely than not the two coins may be different. (And we know, as a fact, the two coins are different, because we made them to be!)\nFinally, let’s use the Chi-squared test to try to answer the same sort of question1:\nFirst we make a cross-tab out of the real data:\n\n\nCode\nxtab &lt;- xtabs(~card_colour + outcome, data = real_data)\nxtab\n\n\n           outcome\ncard_colour 0 1\n       blue 5 3\n       red  3 9\n\n\nAnd then we pass the cross-tab to the function chisq.test:\n\n\nCode\nchisq.test(xtab)\n\n\nWarning in chisq.test(xtab): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  xtab\nX-squared = 1.467, df = 1, p-value = 0.2258\n\n\nHere the function produces a p-value that’s even larger than the approximately 0.10 value from the permutation approach, giving even less confidence that there may be a difference between the two groups. However it also gives a warnings that the assumptions made in producing this p-value may not be appropriate. In particular, two of the four cells (so 50% of the cells) in the cross-tab have values less than 5, whereas a rule-of-thumb when calculating a Chi-squared statistic is that no more than 20% of cells shoudl have values less than 5.\nAn alternative to the Chi-Square test, when there are small sample sizes, is the Fisher Exact test. This is more computationally intensive than the Chi-Square test, but can be more appropriate when there are small sample sizes. Unlike with the Chi-Square test, we can perform one sided as well as two sided tests using this method, with the default being two sided. Let’s see what this produces:\n\n\nCode\nfisher.test(xtab)\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  xtab\np-value = 0.1675\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.5182846 52.4512095\nsample estimates:\nodds ratio \n  4.564976 \n\n\nHere the p-value is slightly smaller than for the Chi-squared test, but slightly larger than for the (one-sided) permutation based p-value. Let’s see what the corresponding p-value is if we specify we want a one-sided test, by setting the alternative argument to \"greater\":\n\n\nCode\nfisher.test(xtab, alternative = \"greater\")\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  xtab\np-value = 0.1132\nalternative hypothesis: true odds ratio is greater than 1\n95 percent confidence interval:\n 0.6835727       Inf\nsample estimates:\nodds ratio \n  4.564976 \n\n\nThis time, we get a p value of 0.113, which is much closer to the permutation-based one-sided p-value of 0.103 we derived previously."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/permutation-with-base-r/index.html#summary",
    "href": "pages/extra-courses/hacker-stats/permutation-with-base-r/index.html#summary",
    "title": "Permutation Testing, and the intuition of the Null hypothesis, with Base R",
    "section": "Summary",
    "text": "Summary\nIn this post we’ve used only Base R functions to understand the intuition and implementation of permutation based tests for trying to either reject or not reject the Null hypothesis. Permutation methods, like bootstrapping, fall under the broader umbrella of resampling methods, and are immensely versatile and applicable to a great many types of data and question.\nApproaches like these are sometimes referred to as ‘Hacker Stats’, as being able to implement them correctly depends much more on having some computer science knowledge - such as for loops or equivalent - than much knowledge of statistical methods and tests. In this example I happened to know of a couple of classic conventional statistical tests that were broadly appropriate to the type of question we were trying to answer, but a reasonable programmer, once they understand the intuition behind the approach, would be able to produce a p-value and Null distribution in the way I did, and get to roughly the right answer even without knowing or implementing either of the classical statistical methods shown here.\nFrom my perspective, I don’t think it’s a case of either-or when it comes to which kind of approach we use - Hacker Stats or ‘Proper’ Stats. Indeed, I think it’s from these simulation based examples, where we can run a little experiment and see what happens, that we can develop the kind of deep intuition about the Null hypothesis - and so p-values, statistical significance, and the bread-and-butter of a lot of conventional statistical learning - that we need to be effective statisticians. It’s likely only by historical accident, in my view, that Hacker Stats are often only taught later in courses, and classical approaches taught first. Resampling methods can be both the Alpha of statistics, because they help to develop the deep intuitions through clear examples that don’t rely on much algebra, and also the Omega of statistics, because some quantities of interest just aren’t easy (and in some cases may be impossible) to derive analytic solutions to."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/permutation-with-base-r/index.html#footnotes",
    "href": "pages/extra-courses/hacker-stats/permutation-with-base-r/index.html#footnotes",
    "title": "Permutation Testing, and the intuition of the Null hypothesis, with Base R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, the Chi-Squared test here is two sided, looking for much smaller and much higher values than the Null distribution, whereas in the example below where we used a one-sided test.↩︎"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-18/index.html",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-18/index.html",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "",
    "text": "Over posts 14 through to 17 I’ve discussed causal inference. However, readers who’ve been involved and interested in the topic of causal inference over the last few years might be less surprised by what I have covered than by what I’ve not, namely the causal inference framework developed by Judea Pearl, and (somewhat) popularised by his co-authored book, The Book of Why: The New Science of Cause and Effect. (Pearl and Mackenzie (2018))\nThis ‘oversight’ in posts so far has been intentional, but in this post the Pearl framework will finally be discussed. I’ll aim to: i) give an overview of the two primary ways of thinking about causal inference: either as a missing data problem; or as a ‘do-logic’ problem; ii) discuss the concept of the omitted variable vs post treatment effect bias trade-off as offering something of a bridge between the two paradigms; iii) give some brief examples of directed acyclic graphs (DAGs) and do-logic, two important ideas from the Pearl framework, as described in Pearl and Mackenzie (2018); iv) make some suggestions about the benefits and uses of the Pearl framework; and finally v) advocate for epistemic humility when it comes to trying to draw causal inferences from observational data, even where a DAG has been clearly articulated and agreed upon within a research community. 1 Without further ado, let’s begin:"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-18/index.html#introduction-correcting-an-oversight-in-discussing-causality",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-18/index.html#introduction-correcting-an-oversight-in-discussing-causality",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "",
    "text": "Over posts 14 through to 17 I’ve discussed causal inference. However, readers who’ve been involved and interested in the topic of causal inference over the last few years might be less surprised by what I have covered than by what I’ve not, namely the causal inference framework developed by Judea Pearl, and (somewhat) popularised by his co-authored book, The Book of Why: The New Science of Cause and Effect. (Pearl and Mackenzie (2018))\nThis ‘oversight’ in posts so far has been intentional, but in this post the Pearl framework will finally be discussed. I’ll aim to: i) give an overview of the two primary ways of thinking about causal inference: either as a missing data problem; or as a ‘do-logic’ problem; ii) discuss the concept of the omitted variable vs post treatment effect bias trade-off as offering something of a bridge between the two paradigms; iii) give some brief examples of directed acyclic graphs (DAGs) and do-logic, two important ideas from the Pearl framework, as described in Pearl and Mackenzie (2018); iv) make some suggestions about the benefits and uses of the Pearl framework; and finally v) advocate for epistemic humility when it comes to trying to draw causal inferences from observational data, even where a DAG has been clearly articulated and agreed upon within a research community. 1 Without further ado, let’s begin:"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-18/index.html#causal-inference-two-paradigms",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-18/index.html#causal-inference-two-paradigms",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "Causal Inference: Two paradigms",
    "text": "Causal Inference: Two paradigms\nIn the posts so far, I’ve introduced and kept returning to the idea that the fundamental problem of causal inference is that at least half of the data is always missing. i.e., for each individual observation, who has either been treated or not treated, if they had been treated then we do not observe them in the untreated state, and if they had not been treated we do not observe them in the treated state. It’s this framing of the problem which\nIn introducing causal inference from this perspective, I’ve ‘taken a side’ in an ongoing debate, or battle, or even war, between two clans of applied epistemologists. Let’s call them the Rubinites, and the Pearlites. Put crudely, the Rubinites adopt a data-centred framing of the challenge of causal inference, whereas the Pearlites adopt a model-centred framing of the challenge of causal inference. For the Rubinites, the data-centred framing leads to an intepretation of causal inference as a missing data problem, for which the solution is therefore to perform some kind of data imputation. For the Pearlites, by contrast, the solution is focused on developing, describing and drawing out causal models, which describe how we believe one thing leads to another and the paths of effect and influence that one variable has on each other variable.\nIt is likely no accident that the broader backgrounds and interests of Rubin and Pearl align with type of solution each proposes. Rubin’s other main interests are in data imputation more generally, including methods of multiple imputation which allow ‘missing values’ to be filled in stochastically, rather than deterministically, to allow some representation of uncertainty and variation in the missing values to be indicated by the range of values that are generated for a missing hole in the data. Pearl worked as a computer scientist, whose key contribution to the field was the development of Bayesian networks, which share many similarities with neural networks. For both types of network, there are nodes, and there are directed links. The nodes have values, and these values can be influenced and altered by the values of other nodes that are connected to the node in question. This influence that each node has on other nodes, through the paths indicated in the directed links, is perhaps more likely to be described as updating from the perspective of a Bayesian network, and propagation from the perspective of a neural network. But in either case, it really is correct to say that one node really does cause another node’s value to change through the causal pathway of the directed link. The main graphical tool Pearl proposes for reasoning about causality in obervational data is the directed acyclic graph (DAG), and again it should be unsurprising that DAGs look much like Bayesian networks."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-18/index.html#the-omitted-variable-bias-vs-post-treatment-bias-trade-off-as-a-potential-bridge-between-the-two-paradigms",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-18/index.html#the-omitted-variable-bias-vs-post-treatment-bias-trade-off-as-a-potential-bridge-between-the-two-paradigms",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "The Omitted Variable Bias vs Post Treatment Bias Trade-off as a potential bridge between the two paradigms",
    "text": "The Omitted Variable Bias vs Post Treatment Bias Trade-off as a potential bridge between the two paradigms\nThe school of inference I’m most familiar with is that of Gary King, a political scientist, methodologist and (in the hallowed halls of Harvard) populariser of statistical methods in the social sciences. In the crude paradigmatic split I’ve sketched out above, King is a Rubinite, and so I guess - mainly through historical accident but partly through conscious decision - I am too. However, I have read Pearl and Mackenzie (2018) (maybe not recently enough nor enough times to fully digest it), consider it valuable and insightful in many places, and think there’s at least one place where the epistemic gap between the two paradigms can be bridged.\nThe bridge point on the Rubinite side,2 I’d suggest, comes from thinking carefully about the sources of bias enumerated in section 3.2 of King and Zeng (2006), which posits that:\n\\[\nbias = \\Delta_o + \\Delta_p + \\Delta_i + \\Delta_e\n\\]\nThis section states:\n\nThese four terms denote exactly the four sources of bias in using observational data, with the subscripts being mnemonics for the components … . The bias components are due to, respectively, omitted variable bias (\\(\\Delta_o\\)), post-treatment bias (\\(\\Delta_p\\)), interpolation bias (\\(\\Delta_i\\)) and extrapolation bias (\\(\\Delta_e\\)). [Emphases added]\n\nOf the four sources of bias listed, it’s the first two which appear to offer a potential link between the two paradigms, and so suggest to Rubinites why some engagement with the Pearlite approach may be valuable. The section continues:\n\nBriefly, \\(\\Delta_o\\) is the bias due to omitting relevant variables such as common causes of both the treatment and the outcome variables [whereas] \\(\\Delta_p\\) is bias due to controlling for the consequences of the treatment. [Emphases added]\n\nFrom the Rubinite perspective, it seems that omitted variable bias and post-treatment bias are recognised, in combination, as constituting a wicked problem. This is because the inclusion of an specific variable can simultaneously affect both types of bias: reducing omitted variable bias, but also potentially increasing post treatment bias. You’re doomed if you do, but you’re also doomed if you don’t."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-18/index.html#with-apologies-to-economists-and-epidemiologists-alike",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-18/index.html#with-apologies-to-economists-and-epidemiologists-alike",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "With apologies to economists and epidemiologists alike…",
    "text": "With apologies to economists and epidemiologists alike…\nOf the two sources of bias, omitted variable bias seems to be the more discussed. And historically, it seems different social and health science disciplines have placed a different weight of addressing these two sources of bias. In particular, at least in the UK context, it’s seemed that economists tend to be more concerned about omitted variable bias, leading to the inclusion of a large number of variables in their statistical models, whereas epidemiologists (though they might not be familiar with and use the term) tend to be more concerned about post-treatment bias, leading a statistical models with fewer variables.\nThe issue of post treatment bias is especially important to consider in the context of root or fundamental causes, which again is often something more of interest to epidemiologists than economists. And the importance of the issue comes into sharp relief if considering factors like sex or race. An economist/econometrician, if asked to estimate the effect of race on (say) the probability of a successful job application to an esteemed organisation, might be very liable to try to include many additional covariates, such as previous work experience and job qualifications, as ‘control variables’ in a statistical model in addition to race. From this, they might find that the covariate associated with race is neither statistically nor substantively, and from this conclude that there is no evidence of (say) racial discrimination in employment, because any disparities in outcomes between racial groups appear to be ‘explained by’ other factors like previous experience and job qualifications.\nTo this, a methodologically minded epidemiologist might counter - very reasonably - that the econometrician’s model is over-controlling, and that the inclusion of factors like educational outcomes and previous work experience in the model risks introducing post treatment bias. If there were discrimination on the basis of race, or sex, it would be unlikely to just affect the specific outcome on the response side of the model. Instead, discrimination (or other race-based factors) would also likely affect the kind of education available to people of different races, and the kinds of educational expectations placed on people of different racial groups. This would then affect the level of educational achievement by group as well. Similarly, both because of prior differences in educational achievement, and because of concurrent effects of discrimination, race might also be expected to affect job history too. Based on this, the epidemiologist might choose to omit both qualifications and job history from the model, because both are presumed to be causallly downstream of the key factor of interest, race.\nSo which type of model is correct? The epidemiologist’s more parsimonious model, which is mindful of post-treatment bias, or the economist’s more complicated model, which is mindful of omitted variable bias? The conclusion from the four-biases position laid out above is that we don’t know, but that all biases potentially exist in observational data, and neither model specification can claim to be free from bias. Perhaps both kinds of model can be run, and perhaps looking at the estimates from both models can give something like a plausible range of possible effects. But fundamentally, we don’t know, and can’t know, and ideally we should seek better quality data, run RCTs and so on.\nPearl and Mackenzie (2018) argues that Rubinites don’t see much (or any) value in causal diagrams, stating “The Rubin causal model treats counterfactuals as abstract mathematical objects that are managed by algebraic machinery but not derived from a model.” [p. 280] Though I think this characterisation is broadly consciously correct, the recognition within the Rubinite community that such things as post-treatment bias and omitted variables exist suggests to me that, unconsciously, even Rubinites employ something like path-diagram reasoning when considering which sources of bias are likely to affect their effect estimates. Put simply: I don’t see how claims of either omitted variable or post treatment bias could be made or believed but for the kind of graphical, path-like thinking at the centre of the Pearlite paradigm.\nLet’s draw the two types of statistical model implied in the discussion above. Firstly the economist’s model:\n\n\n\n\nflowchart LR\n\nrace(race)\nqual(qualifications)\nhist(job history)\naccept(job offer)\n\nrace --&gt;|Z| accept\nqual --&gt;|X*| accept\nhist --&gt;|X*| accept \n\n\n\n\n\n\nAnd now the epidemiologist’s model:\n\n\n\n\nflowchart LR \n\nrace(race)\naccept(job offer)\n\nrace --&gt;|Z| accept\n\n\n\n\n\n\nEmploying a DAG-like causal path diagram would at the very least allow both the economist and epidemiologist to discuss whether or not they agree that the underlying causal pathways are more likely to be something like the follows:\n\n\n\n\nflowchart LR\n\n\nrace(race)\nqual(qualifications)\nhist(job history)\naccept(job offer)\n\nrace --&gt; qual\nqual --&gt; hist\nhist --&gt; accept\n\nrace --&gt; hist\nqual --&gt; accept\nrace --&gt; accept\n\n\n\n\n\n\nIf, having drawn out their presumed causal pathways like this, the economist and epidemiologist end up with the same path diagram, then the Pearlian framework offers plenty of suggestions about how, subject to various assumptions about the types of effect each node has on each downstream node, statistical models based on observational data should be specified, and how the values of various coefficients in the statistical model should be combined in order to produce an overall estimate of the left-most node on the right-most node. Even a Rubinite who does not subscribe to some of these assumptions may still find this kind of graphical, path-based reasoning helpful for thinking through what their concerns are relating to both omitted variable and post-treatment biases are, and whether there’s anything they can do about it. In the path diagram above, for example, the importance of temporal sequence appears important: first there’s education and qualification; then there’s initial labour market experience; and then there’s contemporary labour market experience. This appreciation of the sequence of events might suggest that, perhaps, data employing a longitudinal research design might be preferred to one using only cross-sectional data; and/or that what appeared intially to be only a single research question, investigated through a single statistical model, is actually a series of linked, stepped research questions, each employing a different statistical model, breaking down the cause-effect question into a series of smaller steps."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-18/index.html#summary-thoughts-on-social-complexity-and-the-need-for-epistemic-humility",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-18/index.html#summary-thoughts-on-social-complexity-and-the-need-for-epistemic-humility",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "Summary thoughts: on social complexity and the need for epistemic humility",
    "text": "Summary thoughts: on social complexity and the need for epistemic humility\nAs mentioned before, I probably lean somewhat more towards the Rubinite than the Pearlite framework. A lot of this is simply because this is the causal effect framework I was first introduced to, but some of it comes from more fundamental concerns I have about how some users and advocates of the Pearlite framework seem to think, or suggest, it can solve issues of causal inference from observational data that, fundamentally, I don’t think it may be possible to address.\nOne clue about what the Pearlite framework can and cannot do comes from the ‘A’ in DAG: ‘acyclic’. This means that causal pathways of the following form can be specified:\n\n\n\n\nflowchart LR\nA(A)\nB(B)\n\nA --&gt; B\n\n\n\n\n\nBut causal pathways of the following form cannot:\n\n\n\n\nflowchart LR\n\nA(A)\nB(B)\n\nA --&gt; B\nB --&gt; A\n\n\n\n\n\n\nUnfortunately, cyclic relationships between two or more factors, in which the pathways of influence go in both directions, are likely extremely common in social and economic systems, because such systems are complex rather than merely complicated. 3 One approach to trying to fit a representation of a complex coupled system into a DAG-like framework would be to use time to try to break the causal paths:\n\n\n\n\nflowchart LR\n\nc0(Chicken at T0)\ne1(Egg at T1)\nc2(Chicken at T2)\ne3(Egg at T3)\n\nc0 --&gt; e1\ne1 --&gt; c2\nc2 --&gt; e3\n\n\n\n\n\n\nBut another way of reasoning about such localised coupled complexity might be to use something like factor analysis to identify patterns of co-occurence of variables which may be consistent with this kind of localised complex coupling:\n\n\n\n\nflowchart LR\n\nce((ChickenEgg))\ne[egg]\nc[chicken]\n\nce --&gt; e\nce --&gt; c\n\n\n\n\n\n\nWithin the above diagram, based on structural equation modelling, the directed arrows have a different meaning. They’re not claims of causal effects, but instead of membership. The circle is an underlying proposed ‘latent variable’, the ChickenEgg, which is presumed to manifest through the two observed/manifest variables egg and chicken represented by the rectangles. In places with a lot of ChickenEgg, such as a hen house, we would expect to observe a lot of both chickens and eggs. The statistical model in the above case is a measurement model, rather than a causal model, but in this case is one which is informed by an implicit recognition of continual causal influence operating within members of a complex, paired, causal system.\nSo, I guess my first concern relating to DAGs is that, whereas they can be really useful in allowing researchers to express some form of causal thinking and assumptions about paths of influence between factors, their acyclic requirement can also lead researchers to disregard or underplay the role of complexity even when considering inherently complex systems. In summary, they offer the potential both to expand, but also to restrict, our ability to reason effectively about causal influence.\nMy second, related, concern about the potential over-use or over-reach of DAG-like thinking comes from conventional assumptions built into the paths of influence between nodes. We can get to the heart of this latter concern by looking at , and carefully considering the implications of, something called a double pendulum, a video of which is shown below:\n\n\nA double pendulum is not a complicated system, but it is a complex system, and also a chaotic system. The variables at play include two length variables, two mass variables, a gravity variable, and time. The chaotic complexity of the system comes from the way the length and mass of the first arm interact with the length and and mass of the second arm. This complex interaction is what leads to the position of the outer-most part of the second arm (the grey ball) at any given time.\nNow imagine trying to answer a question of the form “what is the effect of the first arm’s mass on the grey ball’s position?” This kind of question is one that it’s simply not meaningful to even ask. It’s the complex interaction between all components of the system that jointly determines the ball’s position, and attempting to decompose the causal effect of any one variable in the system is simply not a fruitful way of trying to understand the system as a whole.\nThis does not mean, however, that we cannot develop a useful understanding of the double pendulum. We know, for example, that the ball cannot be further than the sum of the length of the two arms from the centre of the system. If we were thinking about placing another object near the double pendulum, for example, this would help us work out how far apart from the pendulum we should place it. Also, if one of the arms is much longer or more massive than the other, then maybe we could approximate it with a simple pendulum too. Additionally, all double pendulums tend to behave in similar ways during their initial fall. But the nature of this kind of complex system also means some types of causal question are beyond the realm of being answerable.\nThe double pendulum, for me, is an object lesson on the importance of epistemic humility. My overall concern relating to causal inference applies nearly equally to Rubinites and Pearlites alike, and is that excessive engagement with or enthusiasm for any kind of method or framework can lead to us believing we know more than we really know more about how one thing affects another. This can potentially lead both to errors of judgement - such as not planning sufficiently for eventualities our models suggest cannot happen - and potentially to intolerance towards those who ‘join the dots’ in a different way to ourselves. 4\nIn short: stay methodologically engaged, but also stay epistemically modest."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-18/index.html#footnotes",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-18/index.html#footnotes",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI might not cover these areas in the order listed above, and thinking about this further this might be too much territory for a single post. Let’s see how this post develops…↩︎\nThe bridge point on the Pearlite side might be a recognition of the apparent bloody obviousness of the fact that, if an observational unit was treated, we don’t observe untreated, and vice versa. The kind of table with missing cells, as shown in part fifteen, would appear to follow straightforwardly from conceding this point. However, Pearl and Mackenzie (2018) includes an example of this kind of table (table 8.1; p. 273), and argues forcefully against this particular framing.↩︎\nThe economist’s model is more complicated than the epidemiologist’s model, but both are equally complex, i.e. not complex at all, because they don’t involve any pathways going from right to left.↩︎\nA majority of political disagreement, for example, seems to occur when people agree on the facts, but disagree about the primary causal pathway.↩︎"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-15/index.html",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-15/index.html",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "",
    "text": "This is the second post on a short mini-series on causal inference. The previous post provided a non-technical introduction to the core challenge of causal inference, namely that the counterfactual is always unobserved, meaning at least half of the data required to really know the causal effect of something is always missing. In the previous post different historians made different assumptions about what the counterfactual would have looked like - what would have happened if something that did happen, hadn’t happened - and based on this came to very different judgements about the effect that Henry Dundas, an 18th century Scottish politician, had on the transatlantic slave trade.\nThis post is more technical, aiming to show: how awkward phrases like “What would have happened if something that did happen, hadn’t happened” are expressed algebraically; how the core problem of causal inference is expressed in this framework; the technical impossibility of addressing the question of causal inference from the Platinum Standard of estimating causal effects on individuals; and describe the reason why randomised controlled trials (RCTs) provide the Gold Standard for trying to estimate these effects for populations."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-15/index.html#introduction",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-15/index.html#introduction",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "",
    "text": "This is the second post on a short mini-series on causal inference. The previous post provided a non-technical introduction to the core challenge of causal inference, namely that the counterfactual is always unobserved, meaning at least half of the data required to really know the causal effect of something is always missing. In the previous post different historians made different assumptions about what the counterfactual would have looked like - what would have happened if something that did happen, hadn’t happened - and based on this came to very different judgements about the effect that Henry Dundas, an 18th century Scottish politician, had on the transatlantic slave trade.\nThis post is more technical, aiming to show: how awkward phrases like “What would have happened if something that did happen, hadn’t happened” are expressed algebraically; how the core problem of causal inference is expressed in this framework; the technical impossibility of addressing the question of causal inference from the Platinum Standard of estimating causal effects on individuals; and describe the reason why randomised controlled trials (RCTs) provide the Gold Standard for trying to estimate these effects for populations."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-15/index.html#models-dont-care-about-causality-but-we-do",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-15/index.html#models-dont-care-about-causality-but-we-do",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Models don’t care about causality… but we do",
    "text": "Models don’t care about causality… but we do\nThe first stage when using a statistical model is to take a big rectangle of data, \\(D\\), and split the columns of the data into two types:\n\nPredictor variables, usually denoted \\(X\\)\nResponse variables, usually denoted \\(y\\)\n\nWith the predictor variables and the response variables defined, the challenge of model fitting is then to find some combination of model parameters \\(\\theta\\) that minimises in some way the gap between the observed response values \\(y\\), and the predicted response values from the model \\(Y\\).\nThe first point to note is that, from the perspective of the model, it does not matter which variable or variables from \\(D\\) we choose to put in the predictor side \\(X\\) or the response side \\(y\\). Even if we put a variable from the future in as a predictor of something in the past, the optimisation algorithms will still work in exactly the same way, working to minimise the gap between observed and predicted responses. The only problem is such a model would make no sense from a causal perspective.\nThe model also does not ‘care’ about how we think about and go about defining any of the variables that go into the predictor side of the equation, \\(X\\). But again, we do. In particular, when thinking about causality it can be immensely helpful to imagine splitting the predictor columns up into some conceptually different types. This will be helpful for thinking about causal inference using some algebra."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-15/index.html#the-impossible-platinum-standard",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-15/index.html#the-impossible-platinum-standard",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "The (Impossible) Platinum Standard",
    "text": "The (Impossible) Platinum Standard\nIn some previous expressions of the data, \\(D\\), we used the subscript \\(i\\) to indicate the rows of the data which go into the model. Each of these rows is, by convention, a different observation. So, instead of saying the purpose of the model is to predict \\(y\\) on \\(X\\), it’s more precisely to predict \\(y_i\\) on \\(X_i\\), for all \\(i\\) in the data (i.e. all rows in \\(D\\)).\nNow let’s do some predictor variable fission and say, for our purposes, that:\n\\[\nX_i = \\{X_i^*, Z_i\\}\n\\]\nHere \\(Z_i\\) is an assignment variable, and takes either a value of 1, meaning ‘is assigned’, or 0, meaning ‘is not assigned’. The variable \\(X_i^*\\), by contrast, means ‘all other predictor variables’.\nFor individual observations \\(D_i\\) where \\(Z_i = 1\\), the individual is exposed (or treated) to something. And for individual observations \\(D_i\\) where \\(Z_i = 0\\), the individual is not exposed (or not treated) to that thing.\nThe causal effect of assignment, or treatment, for any individual observation is:\n\\[\nTE_i = y_i|(X_i^*, Z = 1) - y_i| (X_i^*, Z = 0)\n\\]\nThe fundamental problem of causal inference, however, is that for any individual observation \\(i\\), one of the two parts of this expression is always missing. If an individual \\(i\\) had been assigned, then \\(y_i|(X_i^*, Z=1)\\) is observed, but \\(y_i|(X_i^*, Z=0)\\) is unobserved. By contrast, if an individual \\(i\\) had not been assigned, then \\(y_i|(X_i^*, Z=0)\\) is observed, but \\(y_i|(X_i^*, Z=1)\\) is unobserved.\nAnother way to think about this is as a table, where the treatment effect for an individual involves comparing the outcomes reported in two columns of the same row, but the cells in one of these two columns is always missing:\n\n\n\n\n\n\n\n\n\nindividual\noutcome if treated\noutcome if not treated\ntreatment effect\n\n\n\n\n1\n4.8\n??\n??\n\n\n2\n3.7\n??\n??\n\n\n3\n??\n2.3\n??\n\n\n4\n3.1\n??\n??\n\n\n5\n??\n3.4\n??\n\n\n6\n??\n2.9\n??\n\n\n\nThe Platinum Standard of causal effect estimation would therefore be if the missing cells in the outcome columns could be accurately filled in, allowing the treatment effect for each individual to be calculated.\nHowever, this isn’t possible. It’s social science fiction, as we can’t split the universe and compare parallel realities: one in which what happened didn’t happen, and the other in which what didn’t happen happened.\nSo, what can be done?"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-15/index.html#the-everyday-fools-gold-standard",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-15/index.html#the-everyday-fools-gold-standard",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "The Everyday Fool’s Gold Standard",
    "text": "The Everyday Fool’s Gold Standard\nThere’s one thing you might be tempted to do with the kind of data shown in the table above: compare the average outcome in the treated group with the average outcome in the untreated group, i.e.:\n\\[\nATE = E(y | Z = 1) - E(y | Z = 0)\n\\]\nLet’s do this with the example above:\n\n\nCode\ne_y_z1 &lt;- mean(c(4.8, 3.7, 3.1))\ne_y_z0 &lt;- mean(c(2.3, 3.4, 2.9))\n\n\n# And the difference?\ne_y_z1 - e_y_z0\n\n\n[1] 1\n\n\nIn this example, the difference in the averages between the two groups is 1.0.1 Based on this, we might imagine the first individual, who was treated, would have had a score of 3.8 rather than 4.8, and the third individual, who was not treated, would have received a score of 3.3 rather than 2.3 if they had been treated.\nSo, what’s the problem with just comparing the averages in this way? Potentially, nothing. But potentially, a lot. It depends on the data and the problem. More specifically, it depends on the relationship between the assignment variable, \\(Z\\), and the other characteristics of the individual, which includes but is not usually entirely captured by the known additional characteristics of the individual, \\(X_i^*\\).\nLet’s give a specific example: What if I were to tell you that the outcomes \\(y_i\\) were waiting times at public toilets/bathrooms, and the assignment variable, \\(Z\\), takes the value 1 if the individual has been assigned to a facility containing urinals, and 0 if the individual has been assigned to a facility containing no urinals? Would it be right to infer that the difference in the average is the average causal effect of urinals in public toilets/bathrooms?\nI’d suggest not, because there are characteristics of the individual which govern assignment to bathroom type. What this means is that \\(Z_i\\) and \\(X_i^*\\) are coupled or related to each other in some way. So, any difference in the average outcome between those assigned to (or ‘treated with’) urinals could be due to the urinals themselves; or could be due to other ways that ‘the treated’ and ‘the untreated’ differ from each other systematically. We may be able to observe a difference, and to report that it’s statistically significant. But we don’t know how much, if any, of that difference is due to the exposure or treatment of primary interest to us, and how much is due to other ways in the ‘treated’ and ‘untreated’ groups differ.\nSo, we need some way of breaking the link between \\(Z\\) and \\(X^*\\). How do we do this?"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-15/index.html#why-randomised-controlled-trials-are-the-real-gold-standard",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-15/index.html#why-randomised-controlled-trials-are-the-real-gold-standard",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Why Randomised Controlled Trials are the real Gold Standard",
    "text": "Why Randomised Controlled Trials are the real Gold Standard\nThe clue’s in the subheading. Randomised Controlled Trials (RCTs) are known as the Gold Standard for scientific evaluation of effects for a reason, and the reason is this: they’re explicitly designed to break the link between \\(Z\\) and \\(X^*\\). And not just \\(X^*\\), but any unobserved or unincluded characteristics of the individuals, \\(W^*\\), which might also otherwise influence assignment or selection to \\(Z\\) but we either couldn’t measure or didn’t choose to include.\nThe key idea of an RCT is that assignment to either a treated or untreated group, or to any additional arms of the trial, has nothing to do with the characteristics of any individual in the trial. Instead, the allocation is random, determined by a figurature (or historically occasionally literal) coin toss. 2\nWhat this random assignment means is that assignment \\(Z\\) should be unrelated to the known characteristics \\(X^*\\), as well as unknown characteristics \\(W^*\\). The technical term for this (if I remember correctly) is that assignment is orthogonal to other characteristics, represented algebraically as \\(Z \\perp X^*\\) and \\(Z \\perp W^*\\).\nThis doesn’t mean that, for any particular trial, there will be zero correlation between \\(Z\\) and other characteristics. Nor does it mean that the characteristics of participants will be the same across trial arms. Because of random variation there are always going to be differences between arms in any specific RCT. However, we know that, because we are aware of the mechanism used to allocate participants to treated or non-treated groups (or more generally to trial arms), the expected difference in characteristics will be zero across many RCTs. Along with increased observations, this is the reason why, in principle, a meta-analysis of methodologically identical RCTs should offer even greater precision as to the causal effect of a treatment than just relying on a single RCT. 3"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-15/index.html#summing-up-and-coming-up",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-15/index.html#summing-up-and-coming-up",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Summing up and coming up",
    "text": "Summing up and coming up\nA key point to note is that, when analysing a properly conducted RCT to estimate a treatment effect, the ATE formula shown above, which is naive and likely to be biased when working with observational data, is likely to produce an unbiased estimate of the treatment effect. Because the trial design is sophisticated in the way it breaks the link between \\(Z\\) and everything else, the statistical analysis does not have to be sophisticated.\nThe flip side of this, however, is that when the data are observational, and it would be naive (as with the urinals and waiting times example) to assume that \\(Z\\) is unlinked to everything else known (\\(X^*\\)) and unknown (\\(W^*\\)), then more careful and bespoke statistical modelling approaches are likely to be required to recover non-biased causal effects. Such modelling approaches need to be mindful of both the platinum and gold standards presented above, and rely on modelling and other assumptions to try to simulate what the treatment effects would be if these unobtainable (platinum) and unobtained (gold) standards had been obtained.\nThe next post will start to delve into some of these approaches."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/lms-are-glms-part-15/index.html#footnotes",
    "href": "pages/extra-courses/causal-inference/lms-are-glms-part-15/index.html#footnotes",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is pure fluke. I didn’t choose the values to get a difference of exactly 1, but there we go…↩︎\nIn the gold-plated gold standard of the double-blind RCT, not even the people running the trial and interacting with participants would be aware of which treatment a participant has been assigned. They would simply be given a participant ID, find a pack containing the participant’s treatment, and give this pack to the participant. Only a statistician, who has access to a random number cypher, would know which participants are assigned to which treatment, and they might not know until the trial has concluded. The idea of all of these layers of secrecy in assignment is to reduce the possibility that those running the experiment could intentionally or unintentially inform participants about which treatment they’re receiving, and so create expectations in participants about the effectiveness or otherwise of the treatments, which could have an additional effect on the outcomes.↩︎\nIn practice, issues like methodological variation, and publication bias, mean that meta-analyses of RCTs are unlikely to provide as accurate and unbiased an estimate of treatment effect as we would hope for.↩︎"
  },
  {
    "objectID": "pages/main-course/intro-to-glms/index.html",
    "href": "pages/main-course/intro-to-glms/index.html",
    "title": "Introduction to Generalised Linear Models",
    "section": "",
    "text": "The aims of this web page is to provide an overview of generalised linear models, and ways of thinking about modelling that go beyond ‘star-gazing’."
  },
  {
    "objectID": "pages/main-course/intro-to-glms/index.html#aim",
    "href": "pages/main-course/intro-to-glms/index.html#aim",
    "title": "Introduction to Generalised Linear Models",
    "section": "",
    "text": "The aims of this web page is to provide an overview of generalised linear models, and ways of thinking about modelling that go beyond ‘star-gazing’."
  },
  {
    "objectID": "pages/main-course/intro-to-glms/index.html#what-are-statistical-models-and-how-are-they-fit",
    "href": "pages/main-course/intro-to-glms/index.html#what-are-statistical-models-and-how-are-they-fit",
    "title": "Introduction to Generalised Linear Models",
    "section": "What are statistical models and how are they fit?",
    "text": "What are statistical models and how are they fit?\nIt’s common for different statistical methods to be taught as if they’re completely different species or families. In particular, for standard linear regression to be taught first, then additional, more exotic models, like logistic or Poisson regression, to be introduced at a later stage, in an advanced course.\nThe disadvantage with this standard approach to teaching statistics is that it obscures the way that almost all statistical models are, fundamentally, trying to do something very similar, and work in very similar ways.\nSomething I’ve found immensely helpful over the years is the following pair of equations:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nIn words, the above is saying something like:\n\nThe predicted response \\(Y_i\\) for a set of predictors \\(X_i\\) is assumed to be drawn from (the \\(\\sim\\) symbol) a stochastic distribution (\\(f(.,.)\\))\nThe stochastic distribution contains both parameters we’re interested in, and which are determined by the data \\(\\theta_i\\), and parameters we’re not interested in and might just have to assume, \\(\\alpha\\).\nThe parameters we’re interested in determining from the data \\(\\theta_i\\) are themselves determined by a systematic component \\(g(.,.)\\) which take and transform two inputs: The observed predictor data \\(X_i\\), and a set of coefficients \\(\\beta\\)\n\nAnd graphically this looks something like:\n\n\n\n\nflowchart LR\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  \n  X --&gt; g\n  beta --&gt; g\n  g --&gt; theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt; Y\n\n\n\n\n\n\nTo understand how this fits into the ‘whole game’ of modelling, it’s worth introducing another term, \\(D\\), for the data we’re using, and to say that \\(D\\) is partitioned into observed predictors \\(X_i\\), and observed responses, \\(y_i\\).\nFor each observation, \\(i\\), we therefore have a predicted response, \\(Y_i\\), and an observed response, \\(y_i\\). We can compare \\(Y_i\\) with \\(y_i\\) to get the difference between the two, \\(\\delta_i\\).\nNow, obviously can’t change the data to make it fit our model better. But what we can do is calibrate the model a little better. How do we do this? Through adjusting the \\(\\beta\\) parameters that feed into the systematic component \\(g\\). Graphically, this process of comparison, adjustment, and calibration looks as follows:\n\n\n\n\nflowchart LR\n  D\n  y\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  diff\n  \n  D --&gt;|partition| X\n  D --&gt;|partition| y\n  X --&gt; g\n  beta --&gt;|rerun| g\n  g --&gt;|transform| theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt;|predict| Y\n  \n  Y --&gt;|compare| diff\n  y --&gt;|compare| diff\n  \n  diff --&gt;|adjust| beta\n  \n  \n  \n  linkStyle default stroke:blue, stroke-width:1px\n\n\n\n\n\n\nPretty much all statistical model fitting involves iterating along this \\(g \\to \\beta\\) and \\(\\beta \\to g\\) feedback loop until some kind of condition is met involving minimising \\(\\delta\\)."
  },
  {
    "objectID": "pages/main-course/intro-to-glms/index.html#systematic-components-and-link-functions",
    "href": "pages/main-course/intro-to-glms/index.html#systematic-components-and-link-functions",
    "title": "Introduction to Generalised Linear Models",
    "section": "Systematic components and link functions",
    "text": "Systematic components and link functions\nThe two part equation shown above is too general and abstract to be implemented directly. Instead, specific choices about the \\(f(.)\\) and \\(g(.)\\) need to be made. King, Tomz, and Wittenberg (2000) gives the following examples:\nLogistic Regression\n\\[\nY_i \\sim Bernoulli(\\pi_i)\n\\]\n\\[\n\\pi_i = \\frac{1}{1 + e^{-X_i\\beta}}\n\\]\nLinear Regression\n\\[\nY_i \\sim N(\\mu_i, \\sigma^2)\n\\] \\[\n\\mu_i = X_i\\beta\n\\]\nSo, what’s so special about linear regression, in this framework?\nIn one sense, not so much. It’s got a systematic component, and it’s got a stochastic component. But so do other models. But in another sense, quite a lot. It’s a rare case where the systematic component, \\(g(.)\\), doesn’t transform its inputs in some weird and wonderful way. We can say that \\(g(.)\\) is the identity transform, \\(I(.)\\), which in words means take what you’re given, do nothing to it, and pass it on.\nBy contrast, the systematic component for logistic regression is known as the logistic function. \\(logistic(x) := \\frac{1}{1 + e^{-x}}\\) It transforms inputs that could be anywhere on the real number line to values that lay somewhere between 0 and 1. Why 0 to 1? Because what logistic regression models produce aren’t predicted values, but predicted probabilities, and nothing can be more probable than certain (1) or less probable than impossible (0).\nWe can compare the transformations used in linear and logistic regression as follows:1\n\n\nCode\n# Define transformations\nident &lt;- function(x) {x}\nlgt &lt;- function(x) {1 / (1 + exp(-x))}\n\n\n# Draw the associations\ncurve(ident, -6, 6,\n      xlab = \"x (before transform)\",\n      ylab = \"z (after transform)\",\n      main = \"The Identity 'Transformation'\"\n      )\ncurve(lgt, -6, 6, \n      xlab = \"x (before transform)\", \n      ylab = \"z (after transform)\",\n      main = \"The Logistic Transformation\"\n      )\n\n\n\n\n\n\n\nIdentity Transformation\n\n\n\n\n\n\n\nLogistic Transformation\n\n\n\n\n\n\nThe usual input to the transformation function \\(g(.)\\) is a sum of products. For three variables, for example, this could be \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\). In matrix algebra this generalises to \\(\\boldsymbol{X\\beta}\\) , where \\(\\boldsymbol{X}\\) is the predictor data whose rows are observations, columns are variables, and whose first column is a vector of 1s (for the intercept term). The \\(\\boldsymbol{\\beta}\\) term is a row-wise vector comprising each specific \\(\\beta\\) term, such as \\(\\boldsymbol{\\beta} = \\{ \\beta_0, \\beta_1, \\beta_2 \\}\\) in the three variable example above.\nWhat’s special about the identity transformation, and so linear regression, is that there is a fairly clear correspondence between a \\(\\beta_j\\) term and the estimated influence of changing a predictor variable \\(x_j\\) on the predicted outcome \\(Y\\), i.e. the ‘effect of \\(x_j\\) on \\(Y\\)’. For other transformations this tends to not be the case."
  },
  {
    "objectID": "pages/main-course/intro-to-glms/index.html#how-to-express-a-linear-model-as-a-generalised-linear-model",
    "href": "pages/main-course/intro-to-glms/index.html#how-to-express-a-linear-model-as-a-generalised-linear-model",
    "title": "Introduction to Generalised Linear Models",
    "section": "How to express a linear model as a generalised linear model",
    "text": "How to express a linear model as a generalised linear model\nIn R, there’s the lm function for linear models, and the glm function for generalised linear models.\nI’ve argued previously that the standard linear regression is just a specific type of generalised linear model, one that makes use of an identity transformation I(.) for its systematic component g(.). Let’s now demonstrate that by producing the same model specification using both lm and glm.\nWe can start by being painfully unimaginative and picking using one of R’s standard datasets\n\n\nCode\nlibrary(tidyverse)\n\niris |&gt; \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n\n\n\n\n\nIt looks like, where the petal length is over 2.5, the relationship with sepal length is fairly linear\n\n\nCode\niris |&gt; \n  filter(Petal.Length &gt; 2.5) |&gt; \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n\n\n\n\n\nSo, let’s make a linear regression just of this subset\n\n\nCode\niris_ss &lt;- \n  iris |&gt; \n  filter(Petal.Length &gt; 2.5) \n\n\nWe can produce the regression using lm as follows:\n\n\nCode\nmod_lm &lt;- lm(Sepal.Length ~ Petal.Length, data = iris_ss)\n\n\nAnd we can use the summary function (which checks the type of mod_lm and evokes summary.lm implicitly) to get the following:\n\n\nCode\nsummary(mod_lm)\n\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = iris_ss)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.09194 -0.26570  0.00761  0.21902  0.87502 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3731 on 98 degrees of freedom\nMultiple R-squared:  0.6864,    Adjusted R-squared:  0.6832 \nF-statistic: 214.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nWoohoo! Three stars next to the Petal.Length coefficient! Definitely publishable!\nTo do the same using glm.\n\n\nCode\nmod_glm &lt;- glm(Sepal.Length ~ Petal.Length, data = iris_ss)\n\n\nAnd we can use the summary function for this data too. In this case, summary evokes summary.glm because it knows the class of mod_glm contains glm.\n\n\nCode\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = Sepal.Length ~ Petal.Length, data = iris_ss)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1391962)\n\n    Null deviance: 43.496  on 99  degrees of freedom\nResidual deviance: 13.641  on 98  degrees of freedom\nAIC: 90.58\n\nNumber of Fisher Scoring iterations: 2\n\n\nSo, the coefficients are exactly the same. But there’s also some additional information in the summary, including on the type of ‘family’ used. Why is this?\nIf we look at the help for glm we can see that, by default, the family argument is set to gaussian.\nAnd if we delve a bit further into the help file, in the details about the family argument, it links to the family help page. The usage statement of the family help file is as follows:\nfamily(object, ...)\n\nbinomial(link = \"logit\")\ngaussian(link = \"identity\")\nGamma(link = \"inverse\")\ninverse.gaussian(link = \"1/mu^2\")\npoisson(link = \"log\")\nquasi(link = \"identity\", variance = \"constant\")\nquasibinomial(link = \"logit\")\nquasipoisson(link = \"log\")\nEach family has a default link argument, and for this gaussian family, this link is the identity function.\nWe can also see that, for both the binomial and quasibinomial family, the default link is logit, which transforms all predictors onto a 0-1 scale, as shown in the last post.\nSo, by using the default family, the Gaussian family is selected, and by using the default Gaussian family member, the identity link is selected.\nWe can confirm this by setting the family and link explicitly, showing that we get the same results\n\n\nCode\nmod_glm2 &lt;- glm(Sepal.Length ~ Petal.Length, family = gaussian(link = \"identity\"), data = iris_ss)\nsummary(mod_glm2)\n\n\n\nCall:\nglm(formula = Sepal.Length ~ Petal.Length, family = gaussian(link = \"identity\"), \n    data = iris_ss)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1391962)\n\n    Null deviance: 43.496  on 99  degrees of freedom\nResidual deviance: 13.641  on 98  degrees of freedom\nAIC: 90.58\n\nNumber of Fisher Scoring iterations: 2\n\n\nIt’s the same!\nHow do these terms used in the glm function, family and link, relate to the general framework in King, Tomz, and Wittenberg (2000)?\n\nfamily is the stochastic component, f(.)\nlink is the systematic component, g(.)\n\nThey’re different terms, but it’s the same broad framework.\nLinear models are just one type of general linear model!"
  },
  {
    "objectID": "pages/main-course/intro-to-glms/index.html#why-only-betas-look-at-betas",
    "href": "pages/main-course/intro-to-glms/index.html#why-only-betas-look-at-betas",
    "title": "Introduction to Generalised Linear Models",
    "section": "Why only betas look at betas",
    "text": "Why only betas look at betas\n\nWhy overuse of linear regression leads people to look at models in the wrong way\nThough it’s not always phrased this way, a motivating question behind the construction of most statistical models is, “What influence does a single input to the model, \\(x_j\\), have on the output, \\(Y\\)?”2 For a single variable \\(x_j\\) which is either present (1) or absent (0), this is in effect asking what is \\(E(Y | x_j = 1) - E(Y | x_j = 0)\\) ?3\nLet’s look at a linear regression case, then a logistic regression case.\n\n\nLinear Regression example\nUsing the iris dataset, let’s try to predict Sepal Width (a continuous variable) on Sepal Length (a continuous variable) and whether the species is setosa or not (a discrete variable). As a reminder, the data relating these three variables look as follows:\n\n\nCode\nlibrary(ggplot2)\n\niris |&gt;\n    ggplot(aes(Sepal.Length, Sepal.Width, group = Species, colour = Species, shape = Species)) + \n    geom_point()\n\n\n\n\n\nLet’s now build the model:\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- iris |&gt; mutate(is_setosa = Species == 'setosa')\n\nmod_lm &lt;- lm(Sepal.Width ~ Sepal.Length + is_setosa, data = df)\n\nmod_lm\n\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length + is_setosa, data = df)\n\nCoefficients:\n  (Intercept)   Sepal.Length  is_setosaTRUE  \n       0.7307         0.3420         0.9855  \n\n\nThe coefficients \\(\\boldsymbol{\\beta} = \\{\\beta_0, \\beta_1, \\beta_2\\}\\) are \\(\\{0.73, 0.34, 0.99\\}\\), and refer to the intercept, Sepal Length and is_setosa respectively.\nIf we assume a Sepel Length of 6, for example, then the expected Sepal Width (the thing we are predicting) is 0.73 + 6 * 0.34 + 0.99 or about 3.77 in the case where is_setosa is true, and 0.73 + 6 * 0.34 or about 2.78 where is_setosa is false.\nThe difference between these two values, 3.77 and 2.78, i.e. the ‘influence of setosa’ on the outcome, is 0.99, i.e. the \\(\\beta_2\\) coefficient shown before. In fact, for any conceivable (and non-conceivable, i.e. negative) value of Sepal Length, the difference is still 0.99.\nThis is the \\(\\beta_2\\) coefficient, and the reason why, for linear regression, and almost exclusively linear regression, looking at the coefficients themselves provides substantively meaningful information (something King, Tomz, and Wittenberg (2000) calls a ‘quantity of interest’) about the size of influence that a predictor has on a response.\n\n\nLogistic Regression example\nNow let’s look at an example using logistic regression. We will use another tiresomely familiar dataset, mtcars. We are interested in estimating the effect that having a straight engine (vs=1) has on the probability of the car having a manual transmission (am=1). Our model also tries to control for the miles-per-gallon (mpg). The model specification is shown, the model is run, and the coefficeints are all shown below:\n\n\nCode\nmod_logistic &lt;- glm(\n    am ~ mpg + vs,\n    data = mtcars, \n    family = binomial()\n    )\n\nmod_logistic\n\n\n\nCall:  glm(formula = am ~ mpg + vs, family = binomial(), data = mtcars)\n\nCoefficients:\n(Intercept)          mpg           vs  \n    -9.9183       0.5359      -2.7957  \n\nDegrees of Freedom: 31 Total (i.e. Null);  29 Residual\nNull Deviance:      43.23 \nResidual Deviance: 24.94    AIC: 30.94\n\n\nHere the coefficients \\(\\boldsymbol{\\beta} = \\{\\beta_0, \\beta_1, \\beta_2\\}\\) are \\(\\{-9.92, 0.54, -2.80\\}\\), and refer to the intercept, mpg, and vs respectively.\nBut what does this actually mean, substantively?\n\n\n(Don’t) Stargaze\nA very common approach to trying to answer this question is to look at the statistical significance of the coefficients, which we can do with the summary() function\n\n\nCode\nsummary(mod_logistic)\n\n\n\nCall:\nglm(formula = am ~ mpg + vs, family = binomial(), data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -9.9183     3.4942  -2.839  0.00453 **\nmpg           0.5359     0.1967   2.724  0.00644 **\nvs           -2.7957     1.4723  -1.899  0.05758 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 24.944  on 29  degrees of freedom\nAIC: 30.944\n\nNumber of Fisher Scoring iterations: 6\n\n\nA common practice in many social and health sciences is to offer something like a narrative summary of the above, something like:\n\nOur logistic regression model indicates that manualness is positively and significantly associated with our measure of fuel efficiency (p &lt; 0.01). There is also an indication of a negative association with straight engine, but this effect does not quite meet conventional thresholds for statistical significance (p &lt; 0.10).\n\nThis above practice is known as ‘star-gazing’, because summary tables like those above tend to have one or more * symbols in the final row, if the value of the Pr(&gt;|z|) is below 0.05, and narrative summaries like those just above tend to involve looking at the number of stars in each row, alongside whether the Estimate values have a minus sign in front of them.\nStar gazing is a very common practice. It’s also a terrible practice, which - ironically - turns the final presented output of a quantitative model into the crudest of qualitative summaries (positive, negative; significant, not significant). Star gazing is what researchers tend to default to when presented with model outputs from the above because, unlike in the linear regression example, the extent to which the \\(\\beta\\) coefficients answer substantive ‘how-much’-ness questions, like “How much does having a straight engine change the probability of manual transmission?, is not easily apparent from the coefficients themselves.\n\n\nStandardisation\nSo, how can we do better?\nOne approach is to standardise the data that goes into the model before passing them to the model. Standardisation means attempting to make the distribution and range of different variables more similar, and is especially useful when comparing between different continuous variables.\nTo give an example of this, let’s look at a specification with weight (wt) and horsepower (hp) in place of mpg, but keeping engine-type indicator (vs):\n\n\nCode\nmod_logistic_2 &lt;- glm(\n    am ~ vs + wt + hp,\n    data = mtcars, \n    family = binomial()\n    )\n\nsummary(mod_logistic_2)\n\n\n\nCall:\nglm(formula = am ~ vs + wt + hp, family = binomial(), data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) 25.35510   11.24613   2.255   0.0242 *\nvs          -3.12906    2.92958  -1.068   0.2855  \nwt          -9.64982    4.05528  -2.380   0.0173 *\nhp           0.03242    0.01959   1.655   0.0979 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.2297  on 31  degrees of freedom\nResidual deviance:  8.5012  on 28  degrees of freedom\nAIC: 16.501\n\nNumber of Fisher Scoring iterations: 8\n\n\nHere both wt and hp are continuous variables.\nA star gazing zombie might say something like\n\nmanualness is negatively and significantly associated with weight (p &lt; 0.05); there is a positive association with horsepower but this does not meet standard thresholds of statistical significance (0.05 &lt; p &lt; 0.10).\n\nA slightly better approach would be to standardise the variables wt and hp before passing to the model. Standardising means trying to set the variables to a common scale, and giving the variables more similar statistical characteristics.\n\n\nCode\nstandardise &lt;- function(x){\n  (x - mean(x)) / sd(x)\n}\n\nmtcars_z &lt;- mtcars\nmtcars_z$wt_z = standardise(mtcars$wt)\nmtcars_z$hp_z = standardise(mtcars$hp)\n\nmod_logistic_2_z &lt;- glm(\n    am ~ vs + wt_z + hp_z,\n    data = mtcars_z, \n    family = binomial()\n    )\n\nsummary(mod_logistic_2_z)\n\n\n\nCall:\nglm(formula = am ~ vs + wt_z + hp_z, family = binomial(), data = mtcars_z)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -0.9348     1.4500  -0.645   0.5191  \nvs           -3.1291     2.9296  -1.068   0.2855  \nwt_z         -9.4419     3.9679  -2.380   0.0173 *\nhp_z          2.2230     1.3431   1.655   0.0979 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.2297  on 31  degrees of freedom\nResidual deviance:  8.5012  on 28  degrees of freedom\nAIC: 16.501\n\nNumber of Fisher Scoring iterations: 8\n\n\nwt_z is the standardised version of wt, and hp_z is the standardised version of hp. By convention, whereas unstandardised coefficients are usually referred to as \\(\\beta\\) (‘beta’) coefficients, standardised coefficients are instead referred to as \\(b\\) coefficients. But really, it’s the same model.\nNote the p value of wt_z is the same as for wt, and the p value of hp_z is the same as that for hp. Note also the directions of effect are the same: the coefficients on wt and wt_z are both negative, and the coefficients of hp and hp_z are both positive.\nThis isn’t a coincidence. Of course standardising can’t really add any new information, can’t really change the relationship between a predictor and a response. It’s not really a new variable, it’s the same old variable, so the relationship between predictor and response that there used to be is still there now.\nSo why bother standardising?\nOne reason is it gives, subject to some assumptions and caveats, a way of gauging the relative importance of the two different continuous variables, by allowing a slightly more meaningful comparison between the two coefficients.\nIn this case, we have a standardised \\(b\\) coefficient of -9.44 for wt_z, and of 2.22 for hp_z. As with the unstandardised coefficients we can still assert that manualness is negatively associated with weight, and positively associated with horsepower. But now we can also compare the two numbers -9.44 and 2.22. The ratio of these two numbers is around 4.3. So, we might hazard to suggest something like:\n\na given increase in weight is around four times as important in negatively predicting manual transmission (i.e. in predicting an automatic transmission) as an equivalent increase in horsepower is in positively predicting manual transmission.\n\nThis isn’t a statement that’s easy to parse, but does at least allow slightly more information to be gleamed from the model. For example, it implies that, if a proposed change to a vehicle leads to similar relative (standardised) increases in both weight and horsepower then, as the weight effect is greater than the horsepower effect, the model will predict a decreased probability of manualness as a result.\nBut what about the motivating question, “What’s the effect of a straight engine (vs=1) on the probability of manual transmission (am=1)?”\nThe problem, unlike with the linear regression, is this is now a badly formulated question, based on an incorrect premise. The problem is with the word ‘the’, which implies there should be a single answer to this question, i.e. that the effect of vs on the probability of am=1 should always be the same. But, at least when it comes to absolute changes in the probability of am=1, this is no longer the case, as it depends on the values of the other variables in the model.\nInstead of assuming vs=1 has a single effect on P(am=1), we instead need to think about predictions of the marginal effects of vs on am in the context of other plausible values of the other predictors in the model, wt and hp. This involves asking the model a series of well formulated and specific questions.\n\n\nMaximum marginal effects: Divide-by-four\nBefore we do that, however, there’s a useful heuristic that can be employed when looking at discrete variables and using a logistic regression specification. The heuristic, which is based on the properties of the logistic function,4 is called divide-by-four. What this means is that, if we take the coefficient on vs of -3.13, and divide this value by four, we get a value of -0.78. Notice that the absolute value of -0.78 is between 0 and 1.5 What this value gives is the maximum possible effect that the discrete variable (the presence rather than absence of a straight engine) has on the probability of being a manual transmission. We can say, “a straight engine reduces the probability of a manual transmission by up to 78%”\nBut, as mentioned, this doesn’t quite answer the motivating question, it gives an upper bound to the answer, not the answer itself.6 We can instead start to get a sense of ‘the’ effect of the variable vs on P(am=1) by asking the model a series of questions.\n\n\nPredictions on a matrix\nWe can start by getting the range of observed values for the two continuous variables, hp and mpg:\n\n\nCode\nmin(mtcars$hp)\n\n\n[1] 52\n\n\nCode\nmax(mtcars$hp)\n\n\n[1] 335\n\n\nCode\nmin(mtcars$wt)\n\n\n[1] 1.513\n\n\nCode\nmax(mtcars$wt)\n\n\n[1] 5.424\n\n\nWe can then ask the model to make predictions of \\(P(am=1)\\) for a large number of values of hp and wt within the observed range, both in the condition in which vs=0 and in the condition in which vs=1. The expand_grid function7 can help us do this:\n\n\nCode\npredictors &lt;- expand_grid(\n  hp = seq(min(mtcars$hp), max(mtcars$hp), length.out = 100),\n  wt = seq(min(mtcars$wt), max(mtcars$wt), length.out = 100)\n)\n\npredictors_straight &lt;- predictors |&gt; \n  mutate(vs = 1)\n\npredictors_vshaped &lt;- predictors |&gt; \n  mutate(vs = 0)\n\n\nFor each of these permutations of inputs, we can use the model to get a conditional prediction. For convenience, we can also attach this as an additional column to the predictor data frame:\n\n\nCode\npredictions_predictors_straight &lt;- predictors_straight |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictors_straight)\n  )\n\npredictions_predictors_vshaped &lt;- predictors_vshaped |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictors_vshaped)\n  )\n\n\nWe can see how the predictions vary over hp and wt using a heat map or contour map:\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  ggplot(aes(x = hp, y = wt, z = p_manual)) + \n  geom_contour_filled() + \n  facet_wrap(~vs) +\n  labs(\n    title = \"Predicted probability of manual transmission by wt, hp, and vs\"\n  )\n\n\n\n\n\nWe can also produce a contour map of the differences between these two contour maps, i.e. the effect of a straight (vs=1) compared with v-shaped (vs=0) engine, which gets us a bit closer to the answer:\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  group_by(hp, wt) |&gt; \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |&gt; \n  ungroup() |&gt; \n  ggplot(\n    aes(x = hp, y = wt, z = diff_p_manual)\n  ) + \n  geom_contour_filled() + \n  labs(\n    title = \"Marginal effect of vs=1 given wt and hp on P(am=1)\"\n  )\n\n\n\n\n\nWe can see here that, for large ranges of wt and hp, the marginal effect of vs=1 is small. However, for particular combinations of hp and wt, such as where hp is around 200 and wt is slightly below 3, then the marginal effect of vs=1 becomes large, up to around a -70% reduction in the probability of manual transmission. (i.e. similar to the theoretical maximum marginal effect of around -78%).\nSo, what’s the effect of vs=1 on P(am=1)? i.e. how should we boil down all these 10,000 predicted effect sizes into a single effect size?\nI guess, if we have to try to answer this silly question, then we could take the average effect size…\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  group_by(hp, wt) |&gt; \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |&gt; \n  ungroup() |&gt; \n  summarise(\n    mean_diff_p_manual = mean(diff_p_manual)\n  )\n\n\n# A tibble: 1 × 1\n  mean_diff_p_manual\n               &lt;dbl&gt;\n1            -0.0821\n\n\nSo, we get an average difference of around -0.08, i.e. about an 8% reduction in probability of manual transmission.\n\n\nMarginal effects on observed data\nIs this a reasonable answer? Probably not, because although the permutations of wt and hp we looked at come from the observed range, most of these combinations are likely very ‘theoretical’. We can get a sense of this by plotting the observed values of wt and hp onto the above contour map:\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  group_by(hp, wt) |&gt; \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |&gt; \n  ungroup() |&gt; \n  ggplot(\n    aes(x = hp, y = wt, z = diff_p_manual)\n  ) + \n  geom_contour_filled(alpha = 0.2, show.legend = FALSE) + \n  labs(\n    title = \"Observations from mtcars on the predicted probability surface\"\n  ) +\n  geom_point(\n    aes(x = hp, y = wt), inherit.aes = FALSE,\n    data = mtcars\n  )\n\n\n\n\n\nPerhaps a better option, then, would be to calculate an average marginal effect using the observed values, but switching the observations for vs to 1 in one scenario, and 0 in another scenario:\n\n\nCode\npredictions_predictors_observed_straight &lt;- mtcars |&gt; \n  select(hp, wt) |&gt; \n  mutate(vs = 1)\n\npredictions_predictors_observed_straight &lt;- predictions_predictors_observed_straight |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictions_predictors_observed_straight)\n  )\n\npredictions_predictors_observed_vshaped &lt;- mtcars |&gt; \n  select(hp, wt) |&gt; \n  mutate(vs = 0) \n\npredictions_predictors_observed_vshaped &lt;- predictions_predictors_observed_vshaped |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictions_predictors_observed_vshaped)\n  )\n  \n\npredictions_predictors_observed &lt;- \n  bind_rows(\n    predictions_predictors_observed_straight,\n    predictions_predictors_observed_vshaped\n  )\n\npredictions_marginal &lt;- \n  predictions_predictors_observed |&gt; \n    group_by(hp, wt) |&gt; \n    summarise(\n      diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n    )\n\npredictions_marginal |&gt; \n  ggplot(aes(x = diff_p_manual)) + \n  geom_histogram() +\n  geom_vline(aes(xintercept = mean(diff_p_manual)), colour = \"red\") + \n  geom_vline(aes(xintercept = median(diff_p_manual)), colour = \"green\")\n\n\n\n\n\nIn the above the red line indicates the mean value of these marginal differences, which is -0.12, and the green line the median value of these differences, which is around -0.02. So, even with just these two measures of central tendency, there’s around a six-fold difference in the estimate of ‘the effect’. We can also see there’s a lot of variation, from around nothing (right hand side), to around a 65% reduction (left hand side).\nIf forced to give a simple answer (to this overly simplistic question), we might plump for the mean for theoretical reasons, and say something like “The effect of a straight engine is to reduce the probability of a manual transmission by around an eighth”. But I’m sure, having seen how much variation there is in these marginal effects, we can agree this ‘around an eighth’ answer, or any single number answer, is likely to be overly reductive.\nHopefully, however, it is more informative than ‘statistically significant and negative’, (the stargazing approach) or ‘up to around 78%’ (the divide-by-four approach).\n\n\nConclusion\nLinear regression tends to give a false impression about how straightforward it is to use a model to answer questions of the form “What is the effect of x on y?”. This is because, for linear regression, but few other model specifications, the answer to this question is in the \\(\\beta\\) coefficients themselves. For other model specifications, like the logistic regression example above, the correct-but-uninformative answer tends to be “it depends”, and potentially more informative answers tend to require a bit more work to derive and interpret."
  },
  {
    "objectID": "pages/main-course/intro-to-glms/index.html#page-discussion",
    "href": "pages/main-course/intro-to-glms/index.html#page-discussion",
    "title": "Introduction to Generalised Linear Models",
    "section": "Page discussion",
    "text": "Page discussion\nThis section of the course has aimed to reintroduce statistics from the perspective of generalised linear models (GLMs), in order to make the following clearer:\n\nThat linear regression is just one member of a broader ‘family’ of regression models\nThat all regression models can be thought of as just ‘types’ of GLM, with more in common than divides them\nThat we can and should aim for substantive significance when using the outputs of GLMs, i.e. use them for prediction and simulation rather than focus on whether individual coefficients are ‘statistically significant’ or not.\n\nThe next section of the course delves further into the fundamentals of model fitting and statistical inference, including likelihood theory."
  },
  {
    "objectID": "pages/main-course/intro-to-glms/index.html#footnotes",
    "href": "pages/main-course/intro-to-glms/index.html#footnotes",
    "title": "Introduction to Generalised Linear Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote here I’m using \\(x_j\\), not \\(x_i\\), and that \\(X\\beta\\) is shorthand for \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) and so on. In using the \\(j\\) suffix, I’m referring to just one of the specific \\(x\\) values, \\(x_1\\), \\(x_2\\), \\(x_3\\), which is equivalent to selecting one of the columns in \\(X\\). By contrast \\(i\\) should be considered shorthand for selection of one of the rows of \\(X\\), i.e. one of the series of observations that goes into the dataset \\(D\\).↩︎\nNote here I’m using \\(x_j\\), not \\(x_i\\), and that \\(X\\beta\\) is shorthand for \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) and so on. In using the \\(j\\) suffix, I’m referring to just one of the specific \\(x\\) values, \\(x_1\\), \\(x_2\\), \\(x_3\\), which is equivalent to selecting one of the columns in \\(X\\). By contrast \\(i\\) should be considered shorthand for selection of one of the rows of \\(X\\), i.e. one of the series of observations that goes into the dataset \\(D\\).↩︎\n\\(E(.)\\) is the expectation operator, and \\(|\\) indicates a condition. So, the two terms mean, respectively, what is the expected value of the outcome if the variable of interest is ‘switched on’?, and what is the expected value of the outcome if the variable of interest is ‘switched off’?↩︎\nThe logistic function maps any real number z onto the value range 0 to 1. z is \\(X\\beta\\), which in non-matrix notation is equivalent to a sum of products \\(\\sum_{k=0}^{K}x_k\\beta_k\\) (where, usually, \\(x_0\\) is 1, i.e. the intercept term). Another way of expressing this would be something like \\(\\sum_{k \\in S}x_k\\beta_k\\) where by default \\(S = \\{0, 1, 2, ..., K\\}\\). We can instead imagine partitioning out \\(S = \\{S^{-J}, S^{J}\\}\\) where the superscript \\(J\\) indicates the Jth variable, and \\(-J\\) indicates everything in \\(S\\) apart from the Jth variable. Where J is a discrete variable, the effect of J on \\(P(Y=1)\\) is \\(logistic({\\sum_{k \\in S^{-J}}x_k\\beta_k + \\beta_J}) - logistic({\\sum_{k \\in S^{-J}}x_k\\beta_k})\\), where \\(logistic(z) = \\frac{1}{1 + e^{-z}}\\). The marginal effect of the \\(\\beta_J\\) coefficient thus depends on the other term \\(\\sum_{k \\in S^{-J}}x_k\\beta_k\\). Where this other term is set to 0 the marginal effect of \\(\\beta_J\\) becomes \\(logistic(\\beta_J) - logistic(0)\\). According to p.82 of this chapter by Gelman we can equivalently ask the question ‘what is the first derivative of the logistic regression with respect to \\(\\beta\\)?’. Asking more about this to Wolfram Alpha we get this page of information, and scrolling down to the section on the global minimum we indeed get an absolute value of \\(\\frac{1}{4}\\), so the maximum change in \\(P(Y=1)\\) given a unit change in \\(\\beta\\) is indeed one quarter of the value of \\(\\beta\\), hence why the ‘divide-by-four’ heuristic ‘works’. This isn’t quite a full derivation, but more explanation than I was planning for a footnote! In general, it’s better just to remember ‘divide-by-four’ than go down the rabbit warren of derivation each time! (As I’ve just learned, to my cost, writing this footnote!)↩︎\nWe should always expect the absolute value of a coefficient for a discrete variable to be less than four, for this reason.↩︎\nThe lower bound for the marginal effect of a discrete variable, or any variable, is zero. This is when the absolute value of the sum of the product of the other variables is infinite.↩︎\nOr the base R expand.grid function↩︎"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html",
    "href": "pages/extra-courses/causal-inference/index.html",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "",
    "text": "There are broadly two schools of thought when it comes to thinking about the problems of causal inference. One which interprets the challenge of causal inference mainly as a missing data problem; and another which interprets it mainly in terms of a modelling problem. The posts in this series are largely drawn from the missing data interpretation. If you want an overview of the two approaches (albeit subject to my own ignorance and biases), please skip briefly to the end of these notes before continuing."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#high-level-notewarning",
    "href": "pages/extra-courses/causal-inference/index.html#high-level-notewarning",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "",
    "text": "There are broadly two schools of thought when it comes to thinking about the problems of causal inference. One which interprets the challenge of causal inference mainly as a missing data problem; and another which interprets it mainly in terms of a modelling problem. The posts in this series are largely drawn from the missing data interpretation. If you want an overview of the two approaches (albeit subject to my own ignorance and biases), please skip briefly to the end of these notes before continuing."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#henry-dundas-hero-or-villain",
    "href": "pages/extra-courses/causal-inference/index.html#henry-dundas-hero-or-villain",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Henry Dundas: Hero or Villain?",
    "text": "Henry Dundas: Hero or Villain?\nA few minutes’ walk from where I live is St Andrew Square. And in the middle of St Andrew Square is the Melville Monument, a 40 metre tall column, on which stands a statue of Henry Dundas, 1st Viscount Melville.\nThough the Melville Monument was constructed in the 19th century to commemorate and celebrate this 18th century figure, in 2020 the City of Edimburgh Council chose to add more context to Dundas’ legacy by unveiling a plaque with the following message::\n\nAt the top of this neoclassial column stands a statue of Hentry Dundas, 1st Viscount Melville (1742-1811). He was the Scottish Lord Advocate, an MP for Edinburgh and Midlothian, and the First Lord of the Admiralty. Dundas was a contentious figure, provoking controversies that resonate to this day. While Home Secretary in 1792, and first Secretary of State for War in 1796 he was instrumental in deferring the abolition of the Atlantic slave trade. Slave trading by British ships was not abolished until 1807. As a result of this delay, more than half a million enslaved Africans crossed the Atlantic.\n\nSo, the claim of the council plaque was that Dundas caused the enslavement of hundreds of thousands of Africans, by promoting a gradualist policy of abolition.\nThe descendents of Dundas contested these claims, however, instead arguing:\n\nThe claim that Henry Dundas caused the enslavement of more than half a million Africans is patently false. The truth is: Dundas was the first MP to advocate in Parliament for the emancipation of slaves in the British territories along with the abolition of the slave trade. Dundas’s efforts resulted in the House of Commons voting in favour of ending the Atlantic slave trade for the first time in its history.\n\nSo, the claim of the descendents was that Dundas prevented the enslavement of (at least) hundreds of thousands of Africans, by promoting a gradualist policy of abolition.\nHow can the same agreed-upon historical facts lead to such diametrically opposing interpretations of the effects of Dundas and his actions?\nThe answer to this question is at the heart of causal inference, and an example of why, when trying to estimate causal effects, at least half of the data are always missing."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#the-unobserved-counterfactual",
    "href": "pages/extra-courses/causal-inference/index.html#the-unobserved-counterfactual",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "The unobserved counterfactual",
    "text": "The unobserved counterfactual\nBoth parties in the Dundas debate have, as mentioned, access to the same historical facts. They agree on the same observed historical reality. And both are making bold claims about the impact of Dundas in relation to the Transatlantic slave trade. In doing this, they are both comparing this observed historical reality with something else: the unobserved counterfactual.\nThe unobserved counterfactual is the data that would have been observed if what had happened, hadn’t happened 1 However, what happened did happen, so this data isn’t observed. So, as it hasn’t been observed, it doesn’t exist in any historic facts. Instead, the unobserved counterfactual has to be imputed, or inferred… in effect, made up.\nCausal inference always involves some kind of comparison between an observed reality and an unobserved counterfactual. The issue at heart of the Dundas debate is that both parties have compared the observed reality with a different unobserved counterfactual, and from this different Dundas effects have been inferred.\nFor the council, the unobserved counterfactual appears to be something like the following:\n\nDundas doesn’t propose a gradualist amendment to a bill in parliament. The more radical and rapid version of the bill passes, and slavery is abolished earlier, leading to fewer people becoming enslaved.\n\nWhereas for the descendents, the unobserved counterfactual appears to be something like this:\n\nDundas doesn’t propose a gradualist amendment to a bill in parliament. Because of this, the more radical version of the bill doesn’t have enough support in parliament (perhaps because it would be acting too much against the financial interests of some parliamentarians and powerful business interests), and so is defeated. As a result of this, the abolition of slavery is delayed, leading to more people becoming enslaved.\n\nSo, by having the same observed historical facts, the observed Dundas, but radically different counterfactuals, the two parties have used the same methodology to derive near antithetical estimates of the ‘Dundas Effect’."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#coming-up",
    "href": "pages/extra-courses/causal-inference/index.html#coming-up",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Coming up",
    "text": "Coming up\nThe next post concludes this series on causal inference, by discussing in more detail a topic many users of causal inference will assume I should have started with: the Pearlean school of causal inference. In brief: the approach to causal inference I’m used to interprets the problem, fundamentally, as a missing data problem; whereas the Pearlean approach interprets it more as a modelling problem. I see value in both sides, as well as some points of overlap, but in general I’m both more used to, and more comfortable with, the missing data interpretation."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#introduction-1",
    "href": "pages/extra-courses/causal-inference/index.html#introduction-1",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Introduction",
    "text": "Introduction\nThis is the second post on a short mini-series on causal inference. The previous post provided a non-technical introduction to the core challenge of causal inference, namely that the counterfactual is always unobserved, meaning at least half of the data required to really know the causal effect of something is always missing. In the previous post different historians made different assumptions about what the counterfactual would have looked like - what would have happened if something that did happen, hadn’t happened - and based on this came to very different judgements about the effect that Henry Dundas, an 18th century Scottish politician, had on the transatlantic slave trade.\nThis post is more technical, aiming to show: how awkward phrases like “What would have happened if something that did happen, hadn’t happened” are expressed algebraically; how the core problem of causal inference is expressed in this framework; the technical impossibility of addressing the question of causal inference from the Platinum Standard of estimating causal effects on individuals; and describe the reason why randomised controlled trials (RCTs) provide the Gold Standard for trying to estimate these effects for populations."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#models-dont-care-about-causality-but-we-do",
    "href": "pages/extra-courses/causal-inference/index.html#models-dont-care-about-causality-but-we-do",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Models don’t care about causality… but we do",
    "text": "Models don’t care about causality… but we do\nThe first stage when using a statistical model is to take a big rectangle of data, \\(D\\), and split the columns of the data into two types:\n\nPredictor variables, usually denoted \\(X\\)\nResponse variables, usually denoted \\(y\\)\n\nWith the predictor variables and the response variables defined, the challenge of model fitting is then to find some combination of model parameters \\(\\theta\\) that minimises in some way the gap between the observed response values \\(y\\), and the predicted response values from the model \\(Y\\).\nThe first point to note is that, from the perspective of the model, it does not matter which variable or variables from \\(D\\) we choose to put in the predictor side \\(X\\) or the response side \\(y\\). Even if we put a variable from the future in as a predictor of something in the past, the optimisation algorithms will still work in exactly the same way, working to minimise the gap between observed and predicted responses. The only problem is such a model would make no sense from a causal perspective.\nThe model also does not ‘care’ about how we think about and go about defining any of the variables that go into the predictor side of the equation, \\(X\\). But again, we do. In particular, when thinking about causality it can be immensely helpful to imagine splitting the predictor columns up into some conceptually different types. This will be helpful for thinking about causal inference using some algebra."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#the-impossible-platinum-standard",
    "href": "pages/extra-courses/causal-inference/index.html#the-impossible-platinum-standard",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "The (Impossible) Platinum Standard",
    "text": "The (Impossible) Platinum Standard\nIn some previous expressions of the data, \\(D\\), we used the subscript \\(i\\) to indicate the rows of the data which go into the model. Each of these rows is, by convention, a different observation. So, instead of saying the purpose of the model is to predict \\(y\\) on \\(X\\), it’s more precisely to predict \\(y_i\\) on \\(X_i\\), for all \\(i\\) in the data (i.e. all rows in \\(D\\)).\nNow let’s do some predictor variable fission and say, for our purposes, that:\n\\[\nX_i = \\{X_i^*, Z_i\\}\n\\]\nHere \\(Z_i\\) is an assignment variable, and takes either a value of 1, meaning ‘is assigned’, or 0, meaning ‘is not assigned’. The variable \\(X_i^*\\), by contrast, means ‘all other predictor variables’.\nFor individual observations \\(D_i\\) where \\(Z_i = 1\\), the individual is exposed (or treated) to something. And for individual observations \\(D_i\\) where \\(Z_i = 0\\), the individual is not exposed (or not treated) to that thing.\nThe causal effect of assignment, or treatment, for any individual observation is:\n\\[\nTE_i = y_i|(X_i^*, Z = 1) - y_i| (X_i^*, Z = 0)\n\\]\nThe fundamental problem of causal inference, however, is that for any individual observation \\(i\\), one of the two parts of this expression is always missing. If an individual \\(i\\) had been assigned, then \\(y_i|(X_i^*, Z=1)\\) is observed, but \\(y_i|(X_i^*, Z=0)\\) is unobserved. By contrast, if an individual \\(i\\) had not been assigned, then \\(y_i|(X_i^*, Z=0)\\) is observed, but \\(y_i|(X_i^*, Z=1)\\) is unobserved.\nAnother way to think about this is as a table, where the treatment effect for an individual involves comparing the outcomes reported in two columns of the same row, but the cells in one of these two columns is always missing:\n\n\n\n\n\n\n\n\n\nindividual\noutcome if treated\noutcome if not treated\ntreatment effect\n\n\n\n\n1\n4.8\n??\n??\n\n\n2\n3.7\n??\n??\n\n\n3\n??\n2.3\n??\n\n\n4\n3.1\n??\n??\n\n\n5\n??\n3.4\n??\n\n\n6\n??\n2.9\n??\n\n\n\nThe Platinum Standard of causal effect estimation would therefore be if the missing cells in the outcome columns could be accurately filled in, allowing the treatment effect for each individual to be calculated.\nHowever, this isn’t possible. It’s social science fiction, as we can’t split the universe and compare parallel realities: one in which what happened didn’t happen, and the other in which what didn’t happen happened.\nSo, what can be done?"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#the-everyday-fools-gold-standard",
    "href": "pages/extra-courses/causal-inference/index.html#the-everyday-fools-gold-standard",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "The Everyday Fool’s Gold Standard",
    "text": "The Everyday Fool’s Gold Standard\nThere’s one thing you might be tempted to do with the kind of data shown in the table above: compare the average outcome in the treated group with the average outcome in the untreated group, i.e.:\n\\[\nATE = E(y | Z = 1) - E(y | Z = 0)\n\\]\nLet’s do this with the example above:\n\n\nCode\ne_y_z1 &lt;- mean(c(4.8, 3.7, 3.1))\ne_y_z0 &lt;- mean(c(2.3, 3.4, 2.9))\n\n\n# And the difference?\ne_y_z1 - e_y_z0\n\n\n[1] 1\n\n\nIn this example, the difference in the averages between the two groups is 1.0.2 Based on this, we might imagine the first individual, who was treated, would have had a score of 3.8 rather than 4.8, and the third individual, who was not treated, would have received a score of 3.3 rather than 2.3 if they had been treated.\nSo, what’s the problem with just comparing the averages in this way? Potentially, nothing. But potentially, a lot. It depends on the data and the problem. More specifically, it depends on the relationship between the assignment variable, \\(Z\\), and the other characteristics of the individual, which includes but is not usually entirely captured by the known additional characteristics of the individual, \\(X_i^*\\).\nLet’s give a specific example: What if I were to tell you that the outcomes \\(y_i\\) were waiting times at public toilets/bathrooms, and the assignment variable, \\(Z\\), takes the value 1 if the individual has been assigned to a facility containing urinals, and 0 if the individual has been assigned to a facility containing no urinals? Would it be right to infer that the difference in the average is the average causal effect of urinals in public toilets/bathrooms?\nI’d suggest not, because there are characteristics of the individual which govern assignment to bathroom type. What this means is that \\(Z_i\\) and \\(X_i^*\\) are coupled or related to each other in some way. So, any difference in the average outcome between those assigned to (or ‘treated with’) urinals could be due to the urinals themselves; or could be due to other ways that ‘the treated’ and ‘the untreated’ differ from each other systematically. We may be able to observe a difference, and to report that it’s statistically significant. But we don’t know how much, if any, of that difference is due to the exposure or treatment of primary interest to us, and how much is due to other ways in the ‘treated’ and ‘untreated’ groups differ.\nSo, we need some way of breaking the link between \\(Z\\) and \\(X^*\\). How do we do this?"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#why-randomised-controlled-trials-are-the-real-gold-standard",
    "href": "pages/extra-courses/causal-inference/index.html#why-randomised-controlled-trials-are-the-real-gold-standard",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Why Randomised Controlled Trials are the real Gold Standard",
    "text": "Why Randomised Controlled Trials are the real Gold Standard\nThe clue’s in the subheading. Randomised Controlled Trials (RCTs) are known as the Gold Standard for scientific evaluation of effects for a reason, and the reason is this: they’re explicitly designed to break the link between \\(Z\\) and \\(X^*\\). And not just \\(X^*\\), but any unobserved or unincluded characteristics of the individuals, \\(W^*\\), which might also otherwise influence assignment or selection to \\(Z\\) but we either couldn’t measure or didn’t choose to include.\nThe key idea of an RCT is that assignment to either a treated or untreated group, or to any additional arms of the trial, has nothing to do with the characteristics of any individual in the trial. Instead, the allocation is random, determined by a figurature (or historically occasionally literal) coin toss. 3\nWhat this random assignment means is that assignment \\(Z\\) should be unrelated to the known characteristics \\(X^*\\), as well as unknown characteristics \\(W^*\\). The technical term for this (if I remember correctly) is that assignment is orthogonal to other characteristics, represented algebraically as \\(Z \\perp X^*\\) and \\(Z \\perp W^*\\).\nThis doesn’t mean that, for any particular trial, there will be zero correlation between \\(Z\\) and other characteristics. Nor does it mean that the characteristics of participants will be the same across trial arms. Because of random variation there are always going to be differences between arms in any specific RCT. However, we know that, because we are aware of the mechanism used to allocate participants to treated or non-treated groups (or more generally to trial arms), the expected difference in characteristics will be zero across many RCTs. Along with increased observations, this is the reason why, in principle, a meta-analysis of methodologically identical RCTs should offer even greater precision as to the causal effect of a treatment than just relying on a single RCT. 4"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#summing-up-and-coming-up",
    "href": "pages/extra-courses/causal-inference/index.html#summing-up-and-coming-up",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Summing up and coming up",
    "text": "Summing up and coming up\nA key point to note is that, when analysing a properly conducted RCT to estimate a treatment effect, the ATE formula shown above, which is naive and likely to be biased when working with observational data, is likely to produce an unbiased estimate of the treatment effect. Because the trial design is sophisticated in the way it breaks the link between \\(Z\\) and everything else, the statistical analysis does not have to be sophisticated.\nThe flip side of this, however, is that when the data are observational, and it would be naive (as with the urinals and waiting times example) to assume that \\(Z\\) is unlinked to everything else known (\\(X^*\\)) and unknown (\\(W^*\\)), then more careful and bespoke statistical modelling approaches are likely to be required to recover non-biased causal effects. Such modelling approaches need to be mindful of both the platinum and gold standards presented above, and rely on modelling and other assumptions to try to simulate what the treatment effects would be if these unobtainable (platinum) and unobtained (gold) standards had been obtained.\nThe next post will start to delve into some of these approaches."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#recap-and-aim",
    "href": "pages/extra-courses/causal-inference/index.html#recap-and-aim",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Recap and aim",
    "text": "Recap and aim\nThe previous post (re)introduced three ways to try to allow causal effect estimation using observational data: i) ‘controlling for’ variables using multiple regression; ii) matching methods; iii) Identifying possible ‘natural experiments’ in observational datasets. The fundamental challenge of using observational data to estimate causal effects is that we cannot be sure either the observed (\\(X^*\\)) or unobserved (\\(W\\)) characteristics of observations do not influence allocation to exposure/treatment, i.e. cannot rule out \\(X^* \\rightarrow Z\\) or \\(W \\rightarrow Z\\), meaning that statistical estimates of the effect of Z on the outcome \\(Z \\rightarrow y_i\\) may be biased.\nThe first two approaches will, within limits, generally attenuate the link between \\(X^*\\) and \\(Z\\), but can do little to break the link between \\(W\\) and \\(Z\\), as \\(W\\) is by definition those features of observational units that are not contained in the dataset \\(D\\), and so any statistical method will be ‘blind’ to. The last approach, if the instrumental variable possesses the properties we expect and hope it will, should be able to break the \\(W \\rightarrow Z\\) link too. But unfortunately that can be a big if: the instrument may not have the properties we hope it does.\nThis post will go explore some application of the first two approaches: controlling for variables using multiple regression; and using matching methods. A fuller consideration of the issues is provided in Ho et al. (2007), and the main package and dataset used will be that of the associated MatchIt package Ho et al. (2011) and vignette using the lalonde dataset."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#getting-started",
    "href": "pages/extra-courses/causal-inference/index.html#getting-started",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Getting started",
    "text": "Getting started\nWe start by loading the Matchit package and exploring the lalonde dataset.\n\n\nCode\nlibrary(tidyverse)\nlibrary(MatchIt)\nunmatched_data &lt;- tibble(lalonde)\n\nunmatched_data\n\n\n# A tibble: 614 × 9\n   treat   age  educ race   married nodegree  re74  re75   re78\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1    37    11 black        1        1     0     0  9930.\n 2     1    22     9 hispan       0        1     0     0  3596.\n 3     1    30    12 black        0        0     0     0 24909.\n 4     1    27    11 black        0        1     0     0  7506.\n 5     1    33     8 black        0        1     0     0   290.\n 6     1    22     9 black        0        1     0     0  4056.\n 7     1    23    12 black        0        0     0     0     0 \n 8     1    32    11 black        0        1     0     0  8472.\n 9     1    22    16 black        0        0     0     0  2164.\n10     1    33    12 white        1        0     0     0 12418.\n# ℹ 604 more rows"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#data",
    "href": "pages/extra-courses/causal-inference/index.html#data",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Data",
    "text": "Data\nThe description of the lalonde dataset is as follows:\n\n\nCode\nhelp(lalonde)\n\n\n\nDescription\nThis is a subsample of the data from the treated group in the National Supported Work Demonstration (NSW) and the comparison sample from the Population Survey of Income Dynamics (PSID). This data was previously analyzed extensively by Lalonde (1986) and Dehejia and Wahba (1999).\nFormat\nA data frame with 614 observations (185 treated, 429 control). There are 9 variables measured for each individual.\n\n“treat” is the treatment assignment (1=treated, 0=control).\n“age” is age in years.\n“educ” is education in number of years of schooling.\n“race” is the individual’s race/ethnicity, (Black, Hispanic, or White). Note previous versions of this dataset used indicator variables black and hispan instead of a single race variable.\n“married” is an indicator for married (1=married, 0=not married).\n“nodegree” is an indicator for whether the individual has a high school degree (1=no degree, 0=degree).\n“re74” is income in 1974, in U.S. dollars.\n“re75” is income in 1975, in U.S. dollars.\n“re78” is income in 1978, in U.S. dollars.\n\n“treat” is the treatment variable, “re78” is the outcome, and the others are pre-treatment covariates.\n\nLet’s look at the data to get a sense of it:\n\n\nCode\nunmatched_data |&gt;\n    mutate(treat = as.factor(treat)) |&gt;\n    filter(re78 &lt; 25000) |&gt;\n    ggplot(aes(y = re78, x = re75, shape = treat, colour = treat)) + \ngeom_point() + \ngeom_abline(intercept = 0, slope = 1) +\ncoord_equal() + \nstat_smooth(se = FALSE, method = \"lm\")\n\n\n\n\n\nClearly this is quite complicated data, where the single implied control, wages in 1975 (re75) is not sufficient. There are also a great many observations where wages in either of both years were 0, hence the horizontal and vertical streaks apparent.\nThe two lines are the linear regression lines for the two treatment groups as a function of earlier wage. The lines are not fixed to have the same slope, so the differences in any crude treatment effect estimate vary by earlier wage, but for most previous wages the wages in 1978 appear to be lower in the treatment group (blue), than the control group (red). This would suggest either that the treatment may be harmful to wages… or that there is severe imbalance between the characteristics of persons in both treatment conditions.\nLet’s now start to use a simple linear regression to estimate an average treatment effect, before adding more covariates to see how these model-derived estimates change\n\n\nCode\n# Model of treatment assignment only\nmod_01 &lt;- lm(re78 ~ treat, unmatched_data)\nsummary(mod_01) \n\n\n\nCall:\nlm(formula = re78 ~ treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6984  -6349  -2048   4100  53959 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6984.2      360.7  19.362   &lt;2e-16 ***\ntreat         -635.0      657.1  -0.966    0.334    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7471 on 612 degrees of freedom\nMultiple R-squared:  0.001524,  Adjusted R-squared:  -0.0001079 \nF-statistic: 0.9338 on 1 and 612 DF,  p-value: 0.3342\n\n\nOn average the treated group had (annual?) wages $635 lower than the control group. However the difference is not statistically significant.\nNow let’s add previous wage from 1975\n\n\nCode\nmod_02 &lt;- lm(re78 ~ re75 + treat, unmatched_data)\nsummary(mod_02)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-15918  -5457  -2025   3824  54103 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5547.63718  412.84637  13.438  &lt; 2e-16 ***\nre75           0.58242    0.08937   6.517  1.5e-10 ***\ntreat        -90.79498  641.40291  -0.142    0.887    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7230 on 611 degrees of freedom\nMultiple R-squared:  0.06642,   Adjusted R-squared:  0.06336 \nF-statistic: 21.73 on 2 and 611 DF,  p-value: 7.611e-10\n\n\nPreviously observed wage is statistically significant and positive. The point estimate on treatment is smaller, and even less ‘starry’.\nNow let’s add all possible control variables and see what the treatment effect estimate produced is:\n\n\nCode\nmod_03 &lt;- lm(re78 ~ re75 + age + educ + race + married + nodegree + re74 + treat, unmatched_data)\nsummary(mod_03)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + age + educ + race + married + nodegree + \n    re74 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13595  -4894  -1662   3929  54570 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.174e+03  2.456e+03  -0.478   0.6328    \nre75         2.315e-01  1.046e-01   2.213   0.0273 *  \nage          1.298e+01  3.249e+01   0.399   0.6897    \neduc         4.039e+02  1.589e+02   2.542   0.0113 *  \nracehispan   1.740e+03  1.019e+03   1.708   0.0882 .  \nracewhite    1.241e+03  7.688e+02   1.614   0.1071    \nmarried      4.066e+02  6.955e+02   0.585   0.5590    \nnodegree     2.598e+02  8.474e+02   0.307   0.7593    \nre74         2.964e-01  5.827e-02   5.086 4.89e-07 ***\ntreat        1.548e+03  7.813e+02   1.982   0.0480 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6948 on 604 degrees of freedom\nMultiple R-squared:  0.1478,    Adjusted R-squared:  0.1351 \nF-statistic: 11.64 on 9 and 604 DF,  p-value: &lt; 2.2e-16\n\n\nWith all of these variables as controls, the effect of treatment is now statistically significant and positive, associated with on average an increase of $155 over the control group.\nHowever, we should probably be concerned about how dependent this estimate is on the specific model specification we used. For example, it is fairly common to try to ‘control for’ nonlinearities in age effects by adding a squared term. If modeller decisions like this don’t make much difference, then its addition shouldn’t affect the treatment effect estimate. Let’s have a look:\n\n\nCode\nmod_04 &lt;- lm(re78 ~ re75 + poly(age, 2) + educ + race + married + nodegree + re74 + treat, unmatched_data)\nsummary(mod_04)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + poly(age, 2) + educ + race + married + \n    nodegree + re74 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13692  -4891  -1514   3884  54313 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -5.395e+02  2.172e+03  -0.248   0.8039    \nre75           2.190e-01  1.057e-01   2.072   0.0387 *  \npoly(age, 2)1  3.895e+03  7.994e+03   0.487   0.6262    \npoly(age, 2)2 -6.787e+03  7.918e+03  -0.857   0.3917    \neduc           3.889e+02  1.599e+02   2.432   0.0153 *  \nracehispan     1.682e+03  1.021e+03   1.648   0.0999 .  \nracewhite      1.257e+03  7.692e+02   1.634   0.1028    \nmarried        2.264e+02  7.267e+02   0.312   0.7555    \nnodegree       3.185e+02  8.504e+02   0.375   0.7081    \nre74           2.948e-01  5.832e-02   5.055 5.73e-07 ***\ntreat          1.369e+03  8.090e+02   1.692   0.0911 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6949 on 603 degrees of freedom\nMultiple R-squared:  0.1488,    Adjusted R-squared:  0.1347 \nF-statistic: 10.54 on 10 and 603 DF,  p-value: &lt; 2.2e-16\n\n\nThe inclusion of the squared term to age has changed the point estimate of treatment from around $1550 to $1370. However it has also changed the statistical significance of the effect from p &lt; 0.05 to p &lt; 0.10, i.e. from ‘statistically significant’ to ‘not statistically significant’. If we were playing the stargazing game, this might be the difference between a publishable finding and an unpublishable finding.\nAnd what if we excluded age, because none of the terms are statistically significant at the standard level?\n\n\nCode\nmod_05 &lt;- lm(re78 ~ re75 + educ + race + married + nodegree + re74 + treat, unmatched_data)\nsummary(mod_05)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + educ + race + married + nodegree + \n    re74 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13681  -4912  -1652   3877  54648 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -676.43048 2115.37702  -0.320   0.7493    \nre75           0.22705    0.10395   2.184   0.0293 *  \neduc         389.00786  154.33865   2.520   0.0120 *  \nracehispan  1710.16654 1015.15590   1.685   0.0926 .  \nracewhite   1241.00510  768.22972   1.615   0.1067    \nmarried      478.55017  671.28910   0.713   0.4762    \nnodegree     201.04497  833.99164   0.241   0.8096    \nre74           0.30209    0.05645   5.351 1.24e-07 ***\ntreat       1564.68896  779.65173   2.007   0.0452 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6943 on 605 degrees of freedom\nMultiple R-squared:  0.1475,    Adjusted R-squared:  0.1363 \nF-statistic: 13.09 on 8 and 605 DF,  p-value: &lt; 2.2e-16\n\n\nNow the exclusion of this term, which the coefficient tables suggested wasn’t statistically significant, but intuitively we recognise as an important determinant of labour market activity, has led to yet another point estimate. It’s switched back to ‘statistically significant’ again, but now the point estimate is about $1565 more. Such estimates aren’t vastly different, but they definitely aren’t the same, and come from just a tiny same of the potentially hundreds of different model specifications we could have considered and decided to present to others."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#matching-with-matchit",
    "href": "pages/extra-courses/causal-inference/index.html#matching-with-matchit",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Matching with MatchIt",
    "text": "Matching with MatchIt\nAs the title of Ho et al. (2007) indicates, matching methods are presented as a way of preprocessing the data to reduce the kind of model dependence we’ve just started to explore. Let’s run the first example they present in the MatchIt vignette then discuss what it means:\n\n\nCode\nm.out0 &lt;- matchit(treat ~ age + educ + race + married + \n                   nodegree + re74 + re75, data = lalonde,\n                 method = NULL, distance = \"glm\")\nsummary(m.out0)\n\n\n\nCall:\nmatchit(formula = treat ~ age + educ + race + married + nodegree + \n    re74 + re75, data = lalonde, method = NULL, distance = \"glm\")\n\nSummary of Balance for All Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.1822          1.7941     0.9211    0.3774\nage              25.8162       28.0303         -0.3094     0.4400    0.0813\neduc             10.3459       10.2354          0.0550     0.4959    0.0347\nraceblack         0.8432        0.2028          1.7615          .    0.6404\nracehispan        0.0595        0.1422         -0.3498          .    0.0827\nracewhite         0.0973        0.6550         -1.8819          .    0.5577\nmarried           0.1892        0.5128         -0.8263          .    0.3236\nnodegree          0.7081        0.5967          0.2450          .    0.1114\nre74           2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75           1532.0553     2466.4844         -0.2903     0.9563    0.1342\n           eCDF Max\ndistance     0.6444\nage          0.1577\neduc         0.1114\nraceblack    0.6404\nracehispan   0.0827\nracewhite    0.5577\nmarried      0.3236\nnodegree     0.1114\nre74         0.4470\nre75         0.2876\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       429     185\nUnmatched       0       0\nDiscarded       0       0\n\n\nWith method = NULL, the matchit function presents some summary estimates of differences in characteristics between the Treatment and Control groups. For example, the treated group has an average age of around 25, compared with 28 in the control group, have a slightly higher education score, are more likely to be Black, less likely to be Hispanic, and much less likely to be White (all important differences in the USA context, especially perhaps of the 1970s). They are also less likely to be married, more likely to have no degree, and have substantially earlier wages in both 1974 and 1975. Clearly a straightforward comparision between average outcomes is far from a like-with-like comparisons between groups. The inclusion of other covariates (\\(X^*\\)) does seem to have made a difference, switching the reported direction of effect and its statistical significance, but if we could find a subsample of the control group whose characteristics better match those of the treatment groups, we would hopefully get a more precise and reliable estimate of the effect of the labour market programme.\nThe next part of the vignette shows MatchIt working with some fairly conventional settings:\n\n\nCode\nm.out1 &lt;- matchit(treat ~ age + educ + race + married + \n                   nodegree + re74 + re75, data = lalonde,\n                 method = \"nearest\", distance = \"glm\")\nm.out1\n\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score\n             - estimated with logistic regression\n - number of obs.: 614 (original), 370 (matched)\n - target estimand: ATT\n - covariates: age, educ, race, married, nodegree, re74, re75\n\n\nThe propensity score, i.e. the probability of being in the treatment group, has been predicted using the other covariates, and using logistic regression. For each individual in the treatment group, a ‘nearest neighbour’ in the control group has been identified with the most similar propensity score, which we hope also will also mean the characteristics of the treatment group, and matched pairs from the control group, will be more similar too.\nWe can start to see what this means in practice by looking at the summary of the above object\n\n\nCode\nsummary(m.out1)\n\n\n\nCall:\nmatchit(formula = treat ~ age + educ + race + married + nodegree + \n    re74 + re75, data = lalonde, method = \"nearest\", distance = \"glm\")\n\nSummary of Balance for All Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.1822          1.7941     0.9211    0.3774\nage              25.8162       28.0303         -0.3094     0.4400    0.0813\neduc             10.3459       10.2354          0.0550     0.4959    0.0347\nraceblack         0.8432        0.2028          1.7615          .    0.6404\nracehispan        0.0595        0.1422         -0.3498          .    0.0827\nracewhite         0.0973        0.6550         -1.8819          .    0.5577\nmarried           0.1892        0.5128         -0.8263          .    0.3236\nnodegree          0.7081        0.5967          0.2450          .    0.1114\nre74           2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75           1532.0553     2466.4844         -0.2903     0.9563    0.1342\n           eCDF Max\ndistance     0.6444\nage          0.1577\neduc         0.1114\nraceblack    0.6404\nracehispan   0.0827\nracewhite    0.5577\nmarried      0.3236\nnodegree     0.1114\nre74         0.4470\nre75         0.2876\n\nSummary of Balance for Matched Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.3629          0.9739     0.7566    0.1321\nage              25.8162       25.3027          0.0718     0.4568    0.0847\neduc             10.3459       10.6054         -0.1290     0.5721    0.0239\nraceblack         0.8432        0.4703          1.0259          .    0.3730\nracehispan        0.0595        0.2162         -0.6629          .    0.1568\nracewhite         0.0973        0.3135         -0.7296          .    0.2162\nmarried           0.1892        0.2108         -0.0552          .    0.0216\nnodegree          0.7081        0.6378          0.1546          .    0.0703\nre74           2095.5737     2342.1076         -0.0505     1.3289    0.0469\nre75           1532.0553     1614.7451         -0.0257     1.4956    0.0452\n           eCDF Max Std. Pair Dist.\ndistance     0.4216          0.9740\nage          0.2541          1.3938\neduc         0.0757          1.2474\nraceblack    0.3730          1.0259\nracehispan   0.1568          1.0743\nracewhite    0.2162          0.8390\nmarried      0.0216          0.8281\nnodegree     0.0703          1.0106\nre74         0.2757          0.7965\nre75         0.2054          0.7381\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       185     185\nUnmatched     244       0\nDiscarded       0       0\n\n\nPreviously, there were 185 people in the treatment group, and 429 people in the control group. After matching there are 185 people in the treatment group… and also 185 people in the control group. So, each of the 185 people in the treatment group has been matched up with a ‘data twin’ in the control group, so the ATT should involve more of a like-with-like comparison.\nThe summary presents covariate-wise differences between the Treatment and Control groups for All Data, then for Matched Data. We would hope that, in the Matched Data, the differences are smaller for each covariate, though this isn’t necessarily the case. After matching, for example, we can see that the Black proportion in the Control group is now 0.47 rather than 0.20, and that the earlier income levels are lower, in both cases bringing the values in the Control group closer to, but not identical to, those in the Treatment group. Another way of seeing how balancing has changed things is to look at density plots:\n\n\nCode\nplot(m.out1, type = \"density\", interactive = FALSE,\n     which.xs = ~age + married + re75+ race + nodegree + re74)\n\n\n\n\n\n\n\n\nIn these density charts, the darker lines indicate the Treatment group and the lighter lines the Control groups. The matched data are on the right hand side, with All data on the left. We are looking to see if, on the right hand side, the two sets of density lines are more similar than they are on the right. Indeed they do appear to be, though we can also tell they are far from identical."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#estimating-treatment-effect-sizes-after-matching",
    "href": "pages/extra-courses/causal-inference/index.html#estimating-treatment-effect-sizes-after-matching",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Estimating Treatment Effect Sizes after matching",
    "text": "Estimating Treatment Effect Sizes after matching\nHistorically, the MatchIt package was designed to work seamlessly with Zelig, which made it much easier to use a single library and framework to produce ‘quantities of interest’ using multiple model structures. However Zelig has since been deprecated, meaning the vignette now recommends using the marginaleffects package. We’ll follow their lead:\nFirst the vignette recommends extracting matched data from the matchit output:\n\n\nCode\nm.data &lt;- match.data(m.out1)\n\nm.data &lt;- as_tibble(m.data)\nm.data\n\n\n# A tibble: 370 × 12\n   treat   age  educ race   married nodegree  re74  re75   re78 distance weights\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1     1    37    11 black        1        1     0     0  9930.   0.639        1\n 2     1    22     9 hispan       0        1     0     0  3596.   0.225        1\n 3     1    30    12 black        0        0     0     0 24909.   0.678        1\n 4     1    27    11 black        0        1     0     0  7506.   0.776        1\n 5     1    33     8 black        0        1     0     0   290.   0.702        1\n 6     1    22     9 black        0        1     0     0  4056.   0.699        1\n 7     1    23    12 black        0        0     0     0     0    0.654        1\n 8     1    32    11 black        0        1     0     0  8472.   0.790        1\n 9     1    22    16 black        0        0     0     0  2164.   0.780        1\n10     1    33    12 white        1        0     0     0 12418.   0.0429       1\n# ℹ 360 more rows\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nWhereas the unmatched data contains 614 observations, the matched data contains 370 observations. Note that the Treatment group contained 185 observations, and that 370 is 185 times two. So, the matched data contains one person in the Control group for each person in the Treatment group.\nWe can also see that, in addition to the metrics originally included, the matched data contains three additional variables: ‘distance’, ‘weights’ and ‘subclass’. The ‘subclass’ field is perhaps especially useful for understanding the intuition of the approach, because it helps show which individual in the Control group has been paired with which individual in the Treatment group. Let’s look at the first three subgroups:\n\n\nCode\nm.data |&gt; filter(subclass == '1')\n\n\n# A tibble: 2 × 12\n  treat   age  educ race  married nodegree   re74  re75  re78 distance weights\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    37    11 black       1        1     0      0 9930.    0.639       1\n2     0    22     8 black       1        1 16961.     0  959.    0.203       1\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nSo, for the first subclass, a 37 year old married Black person with no degree has been matched to a 22 year old Black married person with no degree.\n\n\nCode\nm.data |&gt; filter(subclass == '2')\n\n\n# A tibble: 2 × 12\n  treat   age  educ race  married nodegree  re74  re75   re78 distance weights\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    33    12 white       1        0    0      0 12418.   0.0429       1\n2     0    39    12 white       1        0 1289.     0  1203.   0.0430       1\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nFor the second subclass a 33 year old married White person with a degree has been paired with a 39 year old White person with a degree.\n\n\nCode\nm.data |&gt; filter(subclass == '3')\n\n\n# A tibble: 2 × 12\n  treat   age  educ race   married nodegree  re74  re75   re78 distance weights\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    31     9 hispan       0        1     0    0  26818.    0.250       1\n2     0    16    10 white        0        1     0  190.  2137.    0.105       1\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nFor the third subclass, a 31 year old unmarried Hispanic person with no degree has been paired with a 16 year old White person with no degree.\nIn each case, we can see the pairings are similar in some ways but (as with the last example) quite dissimilar in others. The matching algorithm is trying to do the best it can with the data available, especially with the constraint5 that once a person in the Control group has been paired up once to someone in the Treatment group, they can’t be paired up again with someone else in the Treatment group.\nThe identification of these specific pairings suggests we can used a fairly crude strategy to produce an estimate of the ATT: namely just compare the outcome across each of these pairs. Let’s have a look at this:\n\n\nCode\ntrt_effects &lt;- \n    m.data |&gt;\n        group_by(subclass) |&gt;\n        summarise(\n            ind_treat_effect = re78[treat == 1] - re78[treat == 0]\n        ) |&gt; \n        ungroup()\n\ntrt_effects |&gt;\n    ggplot(aes(ind_treat_effect)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = mean(trt_effects$ind_treat_effect), colour = \"red\") + \n    geom_vline(xintercept = 0, colour = 'lightgray', linetype = 'dashed')\n\n\n\n\n\nThis crude paired comparison suggests an average difference that’s slightly positive, of $894.37.\nThis is not a particularly sophisticated or ‘kosher’ approach however. Instead the vignette suggests calculating the treatment effect estimate as follows:\n\n\nCode\nlibrary(\"marginaleffects\")\n\nfit &lt;- lm(re78 ~ treat * (age + educ + race + married + nodegree + \n             re74 + re75), data = m.data, weights = weights)\n\navg_comparisons(fit,\n                variables = \"treat\",\n                vcov = ~subclass,\n                newdata = subset(m.data, treat == 1),\n                wts = \"weights\")\n\n\n\n  Term          Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n treat mean(1) - mean(0)     1121        837 1.34    0.181 2.5  -520   2763\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\nUsing the recommended approach, the ATT estimate is now $1121. Not statistically significant at the conventional 95% threshold, but also more likely to be positive than negative."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#summary",
    "href": "pages/extra-courses/causal-inference/index.html#summary",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Summary",
    "text": "Summary\nIn this post we have largely followed along with the introductionary vignette from the MatchIt package, in order to go from the fairly cursory theoretical overview in the previous post, to showing how some of the ideas and methods relating to multiple regression and matching methods work in practice. There are a great many ways that both matching, and multiple regression, can be implemented in practice, and both are likely to affect any causal effect estimates we produce. However, the aspiration of using matching methods is to somewhat reduce the dependency that causal effect estimates have on the specific model specifications we used."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#coming-up-1",
    "href": "pages/extra-courses/causal-inference/index.html#coming-up-1",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Coming up",
    "text": "Coming up\nThe next post concludes this series on causal inference, by discussing in more detail a topic many users of causal inference will assume I should have started with: the Pearlean school of causal inference. In brief: the approach to causal inference I’m used to interprets the problem, fundamentally, as a missing data problem; whereas the Pearlean approach interprets it more as a modelling problem. I see value in both sides, as well as some points of overlap, but in general I’m both more used to, and more comfortable with, the missing data interpretation."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#introduction-correcting-an-oversight-in-discussing-causality",
    "href": "pages/extra-courses/causal-inference/index.html#introduction-correcting-an-oversight-in-discussing-causality",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Introduction: Correcting an ‘oversight’ in discussing causality",
    "text": "Introduction: Correcting an ‘oversight’ in discussing causality\nOver posts 14 through to 17 I’ve discussed causal inference. However, readers who’ve been involved and interested in the topic of causal inference over the last few years might be less surprised by what I have covered than by what I’ve not, namely the causal inference framework developed by Judea Pearl, and (somewhat) popularised by his co-authored book, The Book of Why: The New Science of Cause and Effect. (Pearl and Mackenzie (2018))\nThis ‘oversight’ in posts so far has been intentional, but in this post the Pearl framework will finally be discussed. I’ll aim to: i) give an overview of the two primary ways of thinking about causal inference: either as a missing data problem; or as a ‘do-logic’ problem; ii) discuss the concept of the omitted variable vs post treatment effect bias trade-off as offering something of a bridge between the two paradigms; iii) give some brief examples of directed acyclic graphs (DAGs) and do-logic, two important ideas from the Pearl framework, as described in Pearl and Mackenzie (2018); iv) make some suggestions about the benefits and uses of the Pearl framework; and finally v) advocate for epistemic humility when it comes to trying to draw causal inferences from observational data, even where a DAG has been clearly articulated and agreed upon within a research community. 6 Without further ado, let’s begin:"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#causal-inference-two-paradigms",
    "href": "pages/extra-courses/causal-inference/index.html#causal-inference-two-paradigms",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Causal Inference: Two paradigms",
    "text": "Causal Inference: Two paradigms\nIn the posts so far, I’ve introduced and kept returning to the idea that the fundamental problem of causal inference is that at least half of the data is always missing. i.e., for each individual observation, who has either been treated or not treated, if they had been treated then we do not observe them in the untreated state, and if they had not been treated we do not observe them in the treated state. It’s this framing of the problem which\nIn introducing causal inference from this perspective, I’ve ‘taken a side’ in an ongoing debate, or battle, or even war, between two clans of applied epistemologists. Let’s call them the Rubinites, and the Pearlites. Put crudely, the Rubinites adopt a data-centred framing of the challenge of causal inference, whereas the Pearlites adopt a model-centred framing of the challenge of causal inference. For the Rubinites, the data-centred framing leads to an intepretation of causal inference as a missing data problem, for which the solution is therefore to perform some kind of data imputation. For the Pearlites, by contrast, the solution is focused on developing, describing and drawing out causal models, which describe how we believe one thing leads to another and the paths of effect and influence that one variable has on each other variable.\nIt is likely no accident that the broader backgrounds and interests of Rubin and Pearl align with type of solution each proposes. Rubin’s other main interests are in data imputation more generally, including methods of multiple imputation which allow ‘missing values’ to be filled in stochastically, rather than deterministically, to allow some representation of uncertainty and variation in the missing values to be indicated by the range of values that are generated for a missing hole in the data. Pearl worked as a computer scientist, whose key contribution to the field was the development of Bayesian networks, which share many similarities with neural networks. For both types of network, there are nodes, and there are directed links. The nodes have values, and these values can be influenced and altered by the values of other nodes that are connected to the node in question. This influence that each node has on other nodes, through the paths indicated in the directed links, is perhaps more likely to be described as updating from the perspective of a Bayesian network, and propagation from the perspective of a neural network. But in either case, it really is correct to say that one node really does cause another node’s value to change through the causal pathway of the directed link. The main graphical tool Pearl proposes for reasoning about causality in obervational data is the directed acyclic graph (DAG), and again it should be unsurprising that DAGs look much like Bayesian networks."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#the-omitted-variable-bias-vs-post-treatment-bias-trade-off-as-a-potential-bridge-between-the-two-paradigms",
    "href": "pages/extra-courses/causal-inference/index.html#the-omitted-variable-bias-vs-post-treatment-bias-trade-off-as-a-potential-bridge-between-the-two-paradigms",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "The Omitted Variable Bias vs Post Treatment Bias Trade-off as a potential bridge between the two paradigms",
    "text": "The Omitted Variable Bias vs Post Treatment Bias Trade-off as a potential bridge between the two paradigms\nThe school of inference I’m most familiar with is that of Gary King, a political scientist, methodologist and (in the hallowed halls of Harvard) populariser of statistical methods in the social sciences. In the crude paradigmatic split I’ve sketched out above, King is a Rubinite, and so I guess - mainly through historical accident but partly through conscious decision - I am too. However, I have read Pearl and Mackenzie (2018) (maybe not recently enough nor enough times to fully digest it), consider it valuable and insightful in many places, and think there’s at least one place where the epistemic gap between the two paradigms can be bridged.\nThe bridge point on the Rubinite side,7 I’d suggest, comes from thinking carefully about the sources of bias enumerated in section 3.2 of King and Zeng (2006), which posits that:\n\\[\nbias = \\Delta_o + \\Delta_p + \\Delta_i + \\Delta_e\n\\]\nThis section states:\n\nThese four terms denote exactly the four sources of bias in using observational data, with the subscripts being mnemonics for the components … . The bias components are due to, respectively, omitted variable bias (\\(\\Delta_o\\)), post-treatment bias (\\(\\Delta_p\\)), interpolation bias (\\(\\Delta_i\\)) and extrapolation bias (\\(\\Delta_e\\)). [Emphases added]\n\nOf the four sources of bias listed, it’s the first two which appear to offer a potential link between the two paradigms, and so suggest to Rubinites why some engagement with the Pearlite approach may be valuable. The section continues:\n\nBriefly, \\(\\Delta_o\\) is the bias due to omitting relevant variables such as common causes of both the treatment and the outcome variables [whereas] \\(\\Delta_p\\) is bias due to controlling for the consequences of the treatment. [Emphases added]\n\nFrom the Rubinite perspective, it seems that omitted variable bias and post-treatment bias are recognised, in combination, as constituting a wicked problem. This is because the inclusion of an specific variable can simultaneously affect both types of bias: reducing omitted variable bias, but also potentially increasing post treatment bias. You’re doomed if you do, but you’re also doomed if you don’t."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#with-apologies-to-economists-and-epidemiologists-alike",
    "href": "pages/extra-courses/causal-inference/index.html#with-apologies-to-economists-and-epidemiologists-alike",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "With apologies to economists and epidemiologists alike…",
    "text": "With apologies to economists and epidemiologists alike…\nOf the two sources of bias, omitted variable bias seems to be the more discussed. And historically, it seems different social and health science disciplines have placed a different weight of addressing these two sources of bias. In particular, at least in the UK context, it’s seemed that economists tend to be more concerned about omitted variable bias, leading to the inclusion of a large number of variables in their statistical models, whereas epidemiologists (though they might not be familiar with and use the term) tend to be more concerned about post-treatment bias, leading a statistical models with fewer variables.\nThe issue of post treatment bias is especially important to consider in the context of root or fundamental causes, which again is often something more of interest to epidemiologists than economists. And the importance of the issue comes into sharp relief if considering factors like sex or race. An economist/econometrician, if asked to estimate the effect of race on (say) the probability of a successful job application to an esteemed organisation, might be very liable to try to include many additional covariates, such as previous work experience and job qualifications, as ‘control variables’ in a statistical model in addition to race. From this, they might find that the covariate associated with race is neither statistically nor substantively, and from this conclude that there is no evidence of (say) racial discrimination in employment, because any disparities in outcomes between racial groups appear to be ‘explained by’ other factors like previous experience and job qualifications.\nTo this, a methodologically minded epidemiologist might counter - very reasonably - that the econometrician’s model is over-controlling, and that the inclusion of factors like educational outcomes and previous work experience in the model risks introducing post treatment bias. If there were discrimination on the basis of race, or sex, it would be unlikely to just affect the specific outcome on the response side of the model. Instead, discrimination (or other race-based factors) would also likely affect the kind of education available to people of different races, and the kinds of educational expectations placed on people of different racial groups. This would then affect the level of educational achievement by group as well. Similarly, both because of prior differences in educational achievement, and because of concurrent effects of discrimination, race might also be expected to affect job history too. Based on this, the epidemiologist might choose to omit both qualifications and job history from the model, because both are presumed to be causallly downstream of the key factor of interest, race.\nSo which type of model is correct? The epidemiologist’s more parsimonious model, which is mindful of post-treatment bias, or the economist’s more complicated model, which is mindful of omitted variable bias? The conclusion from the four-biases position laid out above is that we don’t know, but that all biases potentially exist in observational data, and neither model specification can claim to be free from bias. Perhaps both kinds of model can be run, and perhaps looking at the estimates from both models can give something like a plausible range of possible effects. But fundamentally, we don’t know, and can’t know, and ideally we should seek better quality data, run RCTs and so on.\nPearl and Mackenzie (2018) argues that Rubinites don’t see much (or any) value in causal diagrams, stating “The Rubin causal model treats counterfactuals as abstract mathematical objects that are managed by algebraic machinery but not derived from a model.” [p. 280] Though I think this characterisation is broadly consciously correct, the recognition within the Rubinite community that such things as post-treatment bias and omitted variables exist suggests to me that, unconsciously, even Rubinites employ something like path-diagram reasoning when considering which sources of bias are likely to affect their effect estimates. Put simply: I don’t see how claims of either omitted variable or post treatment bias could be made or believed but for the kind of graphical, path-like thinking at the centre of the Pearlite paradigm.\nLet’s draw the two types of statistical model implied in the discussion above. Firstly the economist’s model:\n\n\n\n\nflowchart LR\n\nrace(race)\nqual(qualifications)\nhist(job history)\naccept(job offer)\n\nrace --&gt;|Z| accept\nqual --&gt;|X*| accept\nhist --&gt;|X*| accept \n\n\n\n\n\n\nAnd now the epidemiologist’s model:\n\n\n\n\nflowchart LR \n\nrace(race)\naccept(job offer)\n\nrace --&gt;|Z| accept\n\n\n\n\n\n\nEmploying a DAG-like causal path diagram would at the very least allow both the economist and epidemiologist to discuss whether or not they agree that the underlying causal pathways are more likely to be something like the follows:\n\n\n\n\nflowchart LR\n\n\nrace(race)\nqual(qualifications)\nhist(job history)\naccept(job offer)\n\nrace --&gt; qual\nqual --&gt; hist\nhist --&gt; accept\n\nrace --&gt; hist\nqual --&gt; accept\nrace --&gt; accept\n\n\n\n\n\n\nIf, having drawn out their presumed causal pathways like this, the economist and epidemiologist end up with the same path diagram, then the Pearlian framework offers plenty of suggestions about how, subject to various assumptions about the types of effect each node has on each downstream node, statistical models based on observational data should be specified, and how the values of various coefficients in the statistical model should be combined in order to produce an overall estimate of the left-most node on the right-most node. Even a Rubinite who does not subscribe to some of these assumptions may still find this kind of graphical, path-based reasoning helpful for thinking through what their concerns are relating to both omitted variable and post-treatment biases are, and whether there’s anything they can do about it. In the path diagram above, for example, the importance of temporal sequence appears important: first there’s education and qualification; then there’s initial labour market experience; and then there’s contemporary labour market experience. This appreciation of the sequence of events might suggest that, perhaps, data employing a longitudinal research design might be preferred to one using only cross-sectional data; and/or that what appeared intially to be only a single research question, investigated through a single statistical model, is actually a series of linked, stepped research questions, each employing a different statistical model, breaking down the cause-effect question into a series of smaller steps."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#summary-thoughts-on-social-complexity-and-the-need-for-epistemic-humility",
    "href": "pages/extra-courses/causal-inference/index.html#summary-thoughts-on-social-complexity-and-the-need-for-epistemic-humility",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Summary thoughts: on social complexity and the need for epistemic humility",
    "text": "Summary thoughts: on social complexity and the need for epistemic humility\nAs mentioned before, I probably lean somewhat more towards the Rubinite than the Pearlite framework. A lot of this is simply because this is the causal effect framework I was first introduced to, but some of it comes from more fundamental concerns I have about how some users and advocates of the Pearlite framework seem to think, or suggest, it can solve issues of causal inference from observational data that, fundamentally, I don’t think it may be possible to address.\nOne clue about what the Pearlite framework can and cannot do comes from the ‘A’ in DAG: ‘acyclic’. This means that causal pathways of the following form can be specified:\n\n\n\n\nflowchart LR\nA(A)\nB(B)\n\nA --&gt; B\n\n\n\n\n\nBut causal pathways of the following form cannot:\n\n\n\n\nflowchart LR\n\nA(A)\nB(B)\n\nA --&gt; B\nB --&gt; A\n\n\n\n\n\n\nUnfortunately, cyclic relationships between two or more factors, in which the pathways of influence go in both directions, are likely extremely common in social and economic systems, because such systems are complex rather than merely complicated. 8 One approach to trying to fit a representation of a complex coupled system into a DAG-like framework would be to use time to try to break the causal paths:\n\n\n\n\nflowchart LR\n\nc0(Chicken at T0)\ne1(Egg at T1)\nc2(Chicken at T2)\ne3(Egg at T3)\n\nc0 --&gt; e1\ne1 --&gt; c2\nc2 --&gt; e3\n\n\n\n\n\n\nBut another way of reasoning about such localised coupled complexity might be to use something like factor analysis to identify patterns of co-occurence of variables which may be consistent with this kind of localised complex coupling:\n\n\n\n\nflowchart LR\n\nce((ChickenEgg))\ne[egg]\nc[chicken]\n\nce --&gt; e\nce --&gt; c\n\n\n\n\n\n\nWithin the above diagram, based on structural equation modelling, the directed arrows have a different meaning. They’re not claims of causal effects, but instead of membership. The circle is an underlying proposed ‘latent variable’, the ChickenEgg, which is presumed to manifest through the two observed/manifest variables egg and chicken represented by the rectangles. In places with a lot of ChickenEgg, such as a hen house, we would expect to observe a lot of both chickens and eggs. The statistical model in the above case is a measurement model, rather than a causal model, but in this case is one which is informed by an implicit recognition of continual causal influence operating within members of a complex, paired, causal system.\nSo, I guess my first concern relating to DAGs is that, whereas they can be really useful in allowing researchers to express some form of causal thinking and assumptions about paths of influence between factors, their acyclic requirement can also lead researchers to disregard or underplay the role of complexity even when considering inherently complex systems. In summary, they offer the potential both to expand, but also to restrict, our ability to reason effectively about causal influence.\nMy second, related, concern about the potential over-use or over-reach of DAG-like thinking comes from conventional assumptions built into the paths of influence between nodes. We can get to the heart of this latter concern by looking at , and carefully considering the implications of, something called a double pendulum, a video of which is shown below:\n\n\nA double pendulum is not a complicated system, but it is a complex system, and also a chaotic system. The variables at play include two length variables, two mass variables, a gravity variable, and time. The chaotic complexity of the system comes from the way the length and mass of the first arm interact with the length and and mass of the second arm. This complex interaction is what leads to the position of the outer-most part of the second arm (the grey ball) at any given time.\nNow imagine trying to answer a question of the form “what is the effect of the first arm’s mass on the grey ball’s position?” This kind of question is one that it’s simply not meaningful to even ask. It’s the complex interaction between all components of the system that jointly determines the ball’s position, and attempting to decompose the causal effect of any one variable in the system is simply not a fruitful way of trying to understand the system as a whole.\nThis does not mean, however, that we cannot develop a useful understanding of the double pendulum. We know, for example, that the ball cannot be further than the sum of the length of the two arms from the centre of the system. If we were thinking about placing another object near the double pendulum, for example, this would help us work out how far apart from the pendulum we should place it. Also, if one of the arms is much longer or more massive than the other, then maybe we could approximate it with a simple pendulum too. Additionally, all double pendulums tend to behave in similar ways during their initial fall. But the nature of this kind of complex system also means some types of causal question are beyond the realm of being answerable.\nThe double pendulum, for me, is an object lesson on the importance of epistemic humility. My overall concern relating to causal inference applies nearly equally to Rubinites and Pearlites alike, and is that excessive engagement with or enthusiasm for any kind of method or framework can lead to us believing we know more than we really know more about how one thing affects another. This can potentially lead both to errors of judgement - such as not planning sufficiently for eventualities our models suggest cannot happen - and potentially to intolerance towards those who ‘join the dots’ in a different way to ourselves. 9\nIn short: stay methodologically engaged, but also stay epistemically modest."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#footnotes",
    "href": "pages/extra-courses/causal-inference/index.html#footnotes",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data that would have been observed if what hadn’t happened, had happened, is the other type of unobserved counterfactual.↩︎\nThis is pure fluke. I didn’t choose the values to get a difference of exactly 1, but there we go…↩︎\nIn the gold-plated gold standard of the double-blind RCT, not even the people running the trial and interacting with participants would be aware of which treatment a participant has been assigned. They would simply be given a participant ID, find a pack containing the participant’s treatment, and give this pack to the participant. Only a statistician, who has access to a random number cypher, would know which participants are assigned to which treatment, and they might not know until the trial has concluded. The idea of all of these layers of secrecy in assignment is to reduce the possibility that those running the experiment could intentionally or unintentially inform participants about which treatment they’re receiving, and so create expectations in participants about the effectiveness or otherwise of the treatments, which could have an additional effect on the outcomes.↩︎\nIn practice, issues like methodological variation, and publication bias, mean that meta-analyses of RCTs are unlikely to provide as accurate and unbiased an estimate of treatment effect as we would hope for.↩︎\nI think this is implied by the use of method = \"nearest\", which is the default, meaning ‘greedy nearest neighbour matching’.↩︎\nI might not cover these areas in the order listed above, and thinking about this further this might be too much territory for a single post. Let’s see how this post develops…↩︎\nThe bridge point on the Pearlite side might be a recognition of the apparent bloody obviousness of the fact that, if an observational unit was treated, we don’t observe untreated, and vice versa. The kind of table with missing cells, as shown earlier, would appear to follow straightforwardly from conceding this point. However, Pearl and Mackenzie (2018) includes an example of this kind of table (table 8.1; p. 273), and argues forcefully against this particular framing.↩︎\nThe economist’s model is more complicated than the epidemiologist’s model, but both are equally complex, i.e. not complex at all, because they don’t involve any pathways going from right to left.↩︎\nA majority of political disagreement, for example, seems to occur when people agree on the facts, but disagree about the primary causal pathway.↩︎"
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#causal-inference-a-non-technical-introduction",
    "href": "pages/extra-courses/causal-inference/index.html#causal-inference-a-non-technical-introduction",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Causal Inference: A non-technical Introduction",
    "text": "Causal Inference: A non-technical Introduction\n\nHenry Dundas: Hero or Villain?\n\n\n\n\n\n\nHenry Dundas, as observed\n\n\n\n\n\n\n\nHenry Dundas, the unobserved good counterfactual\n\n\n\n\n\n\n\nHenry Dundas, the unobserved bad counterfactual\n\n\n\n\n\nA few minutes’ walk from where I live is St Andrew Square. And in the middle of St Andrew Square is the Melville Monument, a 40 metre tall column, on which stands a statue of Henry Dundas, 1st Viscount Melville.\nThough the Melville Monument was constructed in the 19th century to commemorate and celebrate this 18th century figure, in 2020 the City of Edimburgh Council chose to add more context to Dundas’ legacy by unveiling a plaque with the following message::\n\nAt the top of this neoclassial column stands a statue of Hentry Dundas, 1st Viscount Melville (1742-1811). He was the Scottish Lord Advocate, an MP for Edinburgh and Midlothian, and the First Lord of the Admiralty. Dundas was a contentious figure, provoking controversies that resonate to this day. While Home Secretary in 1792, and first Secretary of State for War in 1796 he was instrumental in deferring the abolition of the Atlantic slave trade. Slave trading by British ships was not abolished until 1807. As a result of this delay, more than half a million enslaved Africans crossed the Atlantic.\n\nSo, the claim of the council plaque was that Dundas caused the enslavement of hundreds of thousands of Africans, by promoting a gradualist policy of abolition.\nThe descendents of Dundas contested these claims, however, instead arguing:\n\nThe claim that Henry Dundas caused the enslavement of more than half a million Africans is patently false. The truth is: Dundas was the first MP to advocate in Parliament for the emancipation of slaves in the British territories along with the abolition of the slave trade. Dundas’s efforts resulted in the House of Commons voting in favour of ending the Atlantic slave trade for the first time in its history.\n\nSo, the claim of the descendents was that Dundas prevented the enslavement of (at least) hundreds of thousands of Africans, by promoting a gradualist policy of abolition.\nHow can the same agreed-upon historical facts lead to such diametrically opposing interpretations of the effects of Dundas and his actions?\nThe answer to this question is at the heart of causal inference, and an example of why, when trying to estimate causal effects, at least half of the data are always missing.\n\n\nThe unobserved counterfactual\nBoth parties in the Dundas debate have, as mentioned, access to the same historical facts. They agree on the same observed historical reality. And both are making bold claims about the impact of Dundas in relation to the Transatlantic slave trade. In doing this, they are both comparing this observed historical reality with something else: the unobserved counterfactual.\nThe unobserved counterfactual is the data that would have been observed if what had happened, hadn’t happened 1 However, what happened did happen, so this data isn’t observed. So, as it hasn’t been observed, it doesn’t exist in any historic facts. Instead, the unobserved counterfactual has to be imputed, or inferred… in effect, made up.\nCausal inference always involves some kind of comparison between an observed reality and an unobserved counterfactual. The issue at heart of the Dundas debate is that both parties have compared the observed reality with a different unobserved counterfactual, and from this different Dundas effects have been inferred.\nFor the council, the unobserved counterfactual appears to be something like the following:\n\nDundas doesn’t propose a gradualist amendment to a bill in parliament. The more radical and rapid version of the bill passes, and slavery is abolished earlier, leading to fewer people becoming enslaved.\n\nWhereas for the descendents, the unobserved counterfactual appears to be something like this:\n\nDundas doesn’t propose a gradualist amendment to a bill in parliament. Because of this, the more radical version of the bill doesn’t have enough support in parliament (perhaps because it would be acting too much against the financial interests of some parliamentarians and powerful business interests), and so is defeated. As a result of this, the abolition of slavery is delayed, leading to more people becoming enslaved.\n\nSo, by having the same observed historical facts, the observed Dundas, but radically different counterfactuals, the two parties have used the same methodology to derive near antithetical estimates of the ‘Dundas Effect’."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#causal-inference-technical-descriptions",
    "href": "pages/extra-courses/causal-inference/index.html#causal-inference-technical-descriptions",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Causal Inference: Technical Descriptions",
    "text": "Causal Inference: Technical Descriptions\n\nModels don’t care about causality… but we do\nThe first stage when using a statistical model is to take a big rectangle of data, \\(D\\), and split the columns of the data into two types:\n\nPredictor variables, usually denoted \\(X\\)\nResponse variables, usually denoted \\(y\\)\n\nWith the predictor variables and the response variables defined, the challenge of model fitting is then to find some combination of model parameters \\(\\theta\\) that minimises in some way the gap between the observed response values \\(y\\), and the predicted response values from the model \\(Y\\).\nThe first point to note is that, from the perspective of the model, it does not matter which variable or variables from \\(D\\) we choose to put in the predictor side \\(X\\) or the response side \\(y\\). Even if we put a variable from the future in as a predictor of something in the past, the optimisation algorithms will still work in exactly the same way, working to minimise the gap between observed and predicted responses. The only problem is such a model would make no sense from a causal perspective.\nThe model also does not ‘care’ about how we think about and go about defining any of the variables that go into the predictor side of the equation, \\(X\\). But again, we do. In particular, when thinking about causality it can be immensely helpful to imagine splitting the predictor columns up into some conceptually different types. This will be helpful for thinking about causal inference using some algebra.\n\n\nThe (Impossible) Platinum Standard\nIn some previous expressions of the data, \\(D\\), we used the subscript \\(i\\) to indicate the rows of the data which go into the model. Each of these rows is, by convention, a different observation. So, instead of saying the purpose of the model is to predict \\(y\\) on \\(X\\), it’s more precisely to predict \\(y_i\\) on \\(X_i\\), for all \\(i\\) in the data (i.e. all rows in \\(D\\)).\nNow let’s do some predictor variable fission and say, for our purposes, that:\n\\[\nX_i = \\{X_i^*, Z_i\\}\n\\]\nHere \\(Z_i\\) is an assignment variable, and takes either a value of 1, meaning ‘is assigned’, or 0, meaning ‘is not assigned’. The variable \\(X_i^*\\), by contrast, means ‘all other predictor variables’.\nFor individual observations \\(D_i\\) where \\(Z_i = 1\\), the individual is exposed (or treated) to something. And for individual observations \\(D_i\\) where \\(Z_i = 0\\), the individual is not exposed (or not treated) to that thing.\nThe causal effect of assignment, or treatment, for any individual observation is:\n\\[\nTE_i = y_i|(X_i^*, Z = 1) - y_i| (X_i^*, Z = 0)\n\\]\nThe fundamental problem of causal inference, however, is that for any individual observation \\(i\\), one of the two parts of this expression is always missing. If an individual \\(i\\) had been assigned, then \\(y_i|(X_i^*, Z=1)\\) is observed, but \\(y_i|(X_i^*, Z=0)\\) is unobserved. By contrast, if an individual \\(i\\) had not been assigned, then \\(y_i|(X_i^*, Z=0)\\) is observed, but \\(y_i|(X_i^*, Z=1)\\) is unobserved.\nAnother way to think about this is as a table, where the treatment effect for an individual involves comparing the outcomes reported in two columns of the same row, but the cells in one of these two columns is always missing:\n\n\n\n\n\n\n\n\n\nindividual\noutcome if treated\noutcome if not treated\ntreatment effect\n\n\n\n\n1\n4.8\n??\n??\n\n\n2\n3.7\n??\n??\n\n\n3\n??\n2.3\n??\n\n\n4\n3.1\n??\n??\n\n\n5\n??\n3.4\n??\n\n\n6\n??\n2.9\n??\n\n\n\nThe Platinum Standard of causal effect estimation would therefore be if the missing cells in the outcome columns could be accurately filled in, allowing the treatment effect for each individual to be calculated.\nHowever, this isn’t possible. It’s social science fiction, as we can’t split the universe and compare parallel realities: one in which what happened didn’t happen, and the other in which what didn’t happen happened.\nSo, what can be done?\n\n\nThe Everyday Fool’s Gold Standard\nThere’s one thing you might be tempted to do with the kind of data shown in the table above: compare the average outcome in the treated group with the average outcome in the untreated group, i.e.:\n\\[\nATE = E(y | Z = 1) - E(y | Z = 0)\n\\]\nLet’s do this with the example above:\n\n\nCode\ne_y_z1 &lt;- mean(c(4.8, 3.7, 3.1))\ne_y_z0 &lt;- mean(c(2.3, 3.4, 2.9))\n\n\n# And the difference?\ne_y_z1 - e_y_z0\n\n\n[1] 1\n\n\nIn this example, the difference in the averages between the two groups is 1.0.2 Based on this, we might imagine the first individual, who was treated, would have had a score of 3.8 rather than 4.8, and the third individual, who was not treated, would have received a score of 3.3 rather than 2.3 if they had been treated.\nSo, what’s the problem with just comparing the averages in this way? Potentially, nothing. But potentially, a lot. It depends on the data and the problem. More specifically, it depends on the relationship between the assignment variable, \\(Z\\), and the other characteristics of the individual, which includes but is not usually entirely captured by the known additional characteristics of the individual, \\(X_i^*\\).\nLet’s give a specific example: What if I were to tell you that the outcomes \\(y_i\\) were waiting times at public toilets/bathrooms, and the assignment variable, \\(Z\\), takes the value 1 if the individual has been assigned to a facility containing urinals, and 0 if the individual has been assigned to a facility containing no urinals? Would it be right to infer that the difference in the average is the average causal effect of urinals in public toilets/bathrooms?\nI’d suggest not, because there are characteristics of the individual which govern assignment to bathroom type. What this means is that \\(Z_i\\) and \\(X_i^*\\) are coupled or related to each other in some way. So, any difference in the average outcome between those assigned to (or ‘treated with’) urinals could be due to the urinals themselves; or could be due to other ways that ‘the treated’ and ‘the untreated’ differ from each other systematically. We may be able to observe a difference, and to report that it’s statistically significant. But we don’t know how much, if any, of that difference is due to the exposure or treatment of primary interest to us, and how much is due to other ways in the ‘treated’ and ‘untreated’ groups differ.\nSo, we need some way of breaking the link between \\(Z\\) and \\(X^*\\). How do we do this?\n\n\nWhy Randomised Controlled Trials are the real Gold Standard\nThe clue’s in the subheading. Randomised Controlled Trials (RCTs) are known as the Gold Standard for scientific evaluation of effects for a reason, and the reason is this: they’re explicitly designed to break the link between \\(Z\\) and \\(X^*\\). And not just \\(X^*\\), but any unobserved or unincluded characteristics of the individuals, \\(W^*\\), which might also otherwise influence assignment or selection to \\(Z\\) but we either couldn’t measure or didn’t choose to include.\nThe key idea of an RCT is that assignment to either a treated or untreated group, or to any additional arms of the trial, has nothing to do with the characteristics of any individual in the trial. Instead, the allocation is random, determined by a figurature (or historically occasionally literal) coin toss. 3\nWhat this random assignment means is that assignment \\(Z\\) should be unrelated to the known characteristics \\(X^*\\), as well as unknown characteristics \\(W^*\\). The technical term for this (if I remember correctly) is that assignment is orthogonal to other characteristics, represented algebraically as \\(Z \\perp X^*\\) and \\(Z \\perp W^*\\).\nThis doesn’t mean that, for any particular trial, there will be zero correlation between \\(Z\\) and other characteristics. Nor does it mean that the characteristics of participants will be the same across trial arms. Because of random variation there are always going to be differences between arms in any specific RCT. However, we know that, because we are aware of the mechanism used to allocate participants to treated or non-treated groups (or more generally to trial arms), the expected difference in characteristics will be zero across many RCTs. Along with increased observations, this is the reason why, in principle, a meta-analysis of methodologically identical RCTs should offer even greater precision as to the causal effect of a treatment than just relying on a single RCT. 4\n\n\nSumming up\nA key point to note is that, when analysing a properly conducted RCT to estimate a treatment effect, the ATE formula shown above, which is naive and likely to be biased when working with observational data, is likely to produce an unbiased estimate of the treatment effect. Because the trial design is sophisticated in the way it breaks the link between \\(Z\\) and everything else, the statistical analysis does not have to be sophisticated.\nThe flip side of this, however, is that when the data are observational, and it would be naive (as with the urinals and waiting times example) to assume that \\(Z\\) is unlinked to everything else known (\\(X^*\\)) and unknown (\\(W^*\\)), then more careful and bespoke statistical modelling approaches are likely to be required to recover non-biased causal effects. Such modelling approaches need to be mindful of both the platinum and gold standards presented above, and rely on modelling and other assumptions to try to simulate what the treatment effects would be if these unobtainable (platinum) and unobtained (gold) standards had been obtained."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#multiple-regression-and-matching-approaches-in-practice",
    "href": "pages/extra-courses/causal-inference/index.html#multiple-regression-and-matching-approaches-in-practice",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "Multiple Regression and Matching Approaches in Practice",
    "text": "Multiple Regression and Matching Approaches in Practice\nThis post will go explore some application of the first two approaches: controlling for variables using multiple regression; and using matching methods. A fuller consideration of the issues is provided in Ho et al. (2007), and the main package and dataset used will be that of the associated MatchIt package Ho et al. (2011) and vignette using the lalonde dataset.\n\nGetting started\nWe start by loading the Matchit package and exploring the lalonde dataset.\n\n\nCode\nlibrary(tidyverse)\nlibrary(MatchIt)\nunmatched_data &lt;- tibble(lalonde)\n\nunmatched_data\n\n\n# A tibble: 614 × 9\n   treat   age  educ race   married nodegree  re74  re75   re78\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1    37    11 black        1        1     0     0  9930.\n 2     1    22     9 hispan       0        1     0     0  3596.\n 3     1    30    12 black        0        0     0     0 24909.\n 4     1    27    11 black        0        1     0     0  7506.\n 5     1    33     8 black        0        1     0     0   290.\n 6     1    22     9 black        0        1     0     0  4056.\n 7     1    23    12 black        0        0     0     0     0 \n 8     1    32    11 black        0        1     0     0  8472.\n 9     1    22    16 black        0        0     0     0  2164.\n10     1    33    12 white        1        0     0     0 12418.\n# ℹ 604 more rows\n\n\n\n\nData\nThe description of the lalonde dataset is as follows:\n\n\nCode\nhelp(lalonde)\n\n\n\nDescription\nThis is a subsample of the data from the treated group in the National Supported Work Demonstration (NSW) and the comparison sample from the Population Survey of Income Dynamics (PSID). This data was previously analyzed extensively by Lalonde (1986) and Dehejia and Wahba (1999).\nFormat\nA data frame with 614 observations (185 treated, 429 control). There are 9 variables measured for each individual.\n\n“treat” is the treatment assignment (1=treated, 0=control).\n“age” is age in years.\n“educ” is education in number of years of schooling.\n“race” is the individual’s race/ethnicity, (Black, Hispanic, or White). Note previous versions of this dataset used indicator variables black and hispan instead of a single race variable.\n“married” is an indicator for married (1=married, 0=not married).\n“nodegree” is an indicator for whether the individual has a high school degree (1=no degree, 0=degree).\n“re74” is income in 1974, in U.S. dollars.\n“re75” is income in 1975, in U.S. dollars.\n“re78” is income in 1978, in U.S. dollars.\n\n“treat” is the treatment variable, “re78” is the outcome, and the others are pre-treatment covariates.\n\nLet’s look at the data to get a sense of it:\n\n\nCode\nunmatched_data |&gt;\n    mutate(treat = as.factor(treat)) |&gt;\n    filter(re78 &lt; 25000) |&gt;\n    ggplot(aes(y = re78, x = re75, shape = treat, colour = treat)) + \ngeom_point() + \ngeom_abline(intercept = 0, slope = 1) +\ncoord_equal() + \nstat_smooth(se = FALSE, method = \"lm\")\n\n\n\n\n\nClearly this is quite complicated data, where the single implied control, wages in 1975 (re75) is not sufficient. There are also a great many observations where wages in either of both years were 0, hence the horizontal and vertical streaks apparent.\nThe two lines are the linear regression lines for the two treatment groups as a function of earlier wage. The lines are not fixed to have the same slope, so the differences in any crude treatment effect estimate vary by earlier wage, but for most previous wages the wages in 1978 appear to be lower in the treatment group (blue), than the control group (red). This would suggest either that the treatment may be harmful to wages… or that there is severe imbalance between the characteristics of persons in both treatment conditions.\nLet’s now start to use a simple linear regression to estimate an average treatment effect, before adding more covariates to see how these model-derived estimates change\n\n\nCode\n# Model of treatment assignment only\nmod_01 &lt;- lm(re78 ~ treat, unmatched_data)\nsummary(mod_01) \n\n\n\nCall:\nlm(formula = re78 ~ treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6984  -6349  -2048   4100  53959 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6984.2      360.7  19.362   &lt;2e-16 ***\ntreat         -635.0      657.1  -0.966    0.334    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7471 on 612 degrees of freedom\nMultiple R-squared:  0.001524,  Adjusted R-squared:  -0.0001079 \nF-statistic: 0.9338 on 1 and 612 DF,  p-value: 0.3342\n\n\nOn average the treated group had (annual?) wages $635 lower than the control group. However the difference is not statistically significant.\nNow let’s add previous wage from 1975\n\n\nCode\nmod_02 &lt;- lm(re78 ~ re75 + treat, unmatched_data)\nsummary(mod_02)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-15918  -5457  -2025   3824  54103 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5547.63718  412.84637  13.438  &lt; 2e-16 ***\nre75           0.58242    0.08937   6.517  1.5e-10 ***\ntreat        -90.79498  641.40291  -0.142    0.887    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7230 on 611 degrees of freedom\nMultiple R-squared:  0.06642,   Adjusted R-squared:  0.06336 \nF-statistic: 21.73 on 2 and 611 DF,  p-value: 7.611e-10\n\n\nPreviously observed wage is statistically significant and positive. The point estimate on treatment is smaller, and even less ‘starry’.\nNow let’s add all possible control variables and see what the treatment effect estimate produced is:\n\n\nCode\nmod_03 &lt;- lm(re78 ~ re75 + age + educ + race + married + nodegree + re74 + treat, unmatched_data)\nsummary(mod_03)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + age + educ + race + married + nodegree + \n    re74 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13595  -4894  -1662   3929  54570 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.174e+03  2.456e+03  -0.478   0.6328    \nre75         2.315e-01  1.046e-01   2.213   0.0273 *  \nage          1.298e+01  3.249e+01   0.399   0.6897    \neduc         4.039e+02  1.589e+02   2.542   0.0113 *  \nracehispan   1.740e+03  1.019e+03   1.708   0.0882 .  \nracewhite    1.241e+03  7.688e+02   1.614   0.1071    \nmarried      4.066e+02  6.955e+02   0.585   0.5590    \nnodegree     2.598e+02  8.474e+02   0.307   0.7593    \nre74         2.964e-01  5.827e-02   5.086 4.89e-07 ***\ntreat        1.548e+03  7.813e+02   1.982   0.0480 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6948 on 604 degrees of freedom\nMultiple R-squared:  0.1478,    Adjusted R-squared:  0.1351 \nF-statistic: 11.64 on 9 and 604 DF,  p-value: &lt; 2.2e-16\n\n\nWith all of these variables as controls, the effect of treatment is now statistically significant and positive, associated with on average an increase of $155 over the control group.\nHowever, we should probably be concerned about how dependent this estimate is on the specific model specification we used. For example, it is fairly common to try to ‘control for’ nonlinearities in age effects by adding a squared term. If modeller decisions like this don’t make much difference, then its addition shouldn’t affect the treatment effect estimate. Let’s have a look:\n\n\nCode\nmod_04 &lt;- lm(re78 ~ re75 + poly(age, 2) + educ + race + married + nodegree + re74 + treat, unmatched_data)\nsummary(mod_04)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + poly(age, 2) + educ + race + married + \n    nodegree + re74 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13692  -4891  -1514   3884  54313 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -5.395e+02  2.172e+03  -0.248   0.8039    \nre75           2.190e-01  1.057e-01   2.072   0.0387 *  \npoly(age, 2)1  3.895e+03  7.994e+03   0.487   0.6262    \npoly(age, 2)2 -6.787e+03  7.918e+03  -0.857   0.3917    \neduc           3.889e+02  1.599e+02   2.432   0.0153 *  \nracehispan     1.682e+03  1.021e+03   1.648   0.0999 .  \nracewhite      1.257e+03  7.692e+02   1.634   0.1028    \nmarried        2.264e+02  7.267e+02   0.312   0.7555    \nnodegree       3.185e+02  8.504e+02   0.375   0.7081    \nre74           2.948e-01  5.832e-02   5.055 5.73e-07 ***\ntreat          1.369e+03  8.090e+02   1.692   0.0911 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6949 on 603 degrees of freedom\nMultiple R-squared:  0.1488,    Adjusted R-squared:  0.1347 \nF-statistic: 10.54 on 10 and 603 DF,  p-value: &lt; 2.2e-16\n\n\nThe inclusion of the squared term to age has changed the point estimate of treatment from around $1550 to $1370. However it has also changed the statistical significance of the effect from p &lt; 0.05 to p &lt; 0.10, i.e. from ‘statistically significant’ to ‘not statistically significant’. If we were playing the stargazing game, this might be the difference between a publishable finding and an unpublishable finding.\nAnd what if we excluded age, because none of the terms are statistically significant at the standard level?\n\n\nCode\nmod_05 &lt;- lm(re78 ~ re75 + educ + race + married + nodegree + re74 + treat, unmatched_data)\nsummary(mod_05)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + educ + race + married + nodegree + \n    re74 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13681  -4912  -1652   3877  54648 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -676.43048 2115.37702  -0.320   0.7493    \nre75           0.22705    0.10395   2.184   0.0293 *  \neduc         389.00786  154.33865   2.520   0.0120 *  \nracehispan  1710.16654 1015.15590   1.685   0.0926 .  \nracewhite   1241.00510  768.22972   1.615   0.1067    \nmarried      478.55017  671.28910   0.713   0.4762    \nnodegree     201.04497  833.99164   0.241   0.8096    \nre74           0.30209    0.05645   5.351 1.24e-07 ***\ntreat       1564.68896  779.65173   2.007   0.0452 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6943 on 605 degrees of freedom\nMultiple R-squared:  0.1475,    Adjusted R-squared:  0.1363 \nF-statistic: 13.09 on 8 and 605 DF,  p-value: &lt; 2.2e-16\n\n\nNow the exclusion of this term, which the coefficient tables suggested wasn’t statistically significant, but intuitively we recognise as an important determinant of labour market activity, has led to yet another point estimate. It’s switched back to ‘statistically significant’ again, but now the point estimate is about $1565 more. Such estimates aren’t vastly different, but they definitely aren’t the same, and come from just a tiny same of the potentially hundreds of different model specifications we could have considered and decided to present to others.\n\n\nMatching with MatchIt\nAs the title of Ho et al. (2007) indicates, matching methods are presented as a way of preprocessing the data to reduce the kind of model dependence we’ve just started to explore. Let’s run the first example they present in the MatchIt vignette then discuss what it means:\n\n\nCode\nm.out0 &lt;- matchit(treat ~ age + educ + race + married + \n                   nodegree + re74 + re75, data = lalonde,\n                 method = NULL, distance = \"glm\")\nsummary(m.out0)\n\n\n\nCall:\nmatchit(formula = treat ~ age + educ + race + married + nodegree + \n    re74 + re75, data = lalonde, method = NULL, distance = \"glm\")\n\nSummary of Balance for All Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.1822          1.7941     0.9211    0.3774\nage              25.8162       28.0303         -0.3094     0.4400    0.0813\neduc             10.3459       10.2354          0.0550     0.4959    0.0347\nraceblack         0.8432        0.2028          1.7615          .    0.6404\nracehispan        0.0595        0.1422         -0.3498          .    0.0827\nracewhite         0.0973        0.6550         -1.8819          .    0.5577\nmarried           0.1892        0.5128         -0.8263          .    0.3236\nnodegree          0.7081        0.5967          0.2450          .    0.1114\nre74           2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75           1532.0553     2466.4844         -0.2903     0.9563    0.1342\n           eCDF Max\ndistance     0.6444\nage          0.1577\neduc         0.1114\nraceblack    0.6404\nracehispan   0.0827\nracewhite    0.5577\nmarried      0.3236\nnodegree     0.1114\nre74         0.4470\nre75         0.2876\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       429     185\nUnmatched       0       0\nDiscarded       0       0\n\n\nWith method = NULL, the matchit function presents some summary estimates of differences in characteristics between the Treatment and Control groups. For example, the treated group has an average age of around 25, compared with 28 in the control group, have a slightly higher education score, are more likely to be Black, less likely to be Hispanic, and much less likely to be White (all important differences in the USA context, especially perhaps of the 1970s). They are also less likely to be married, more likely to have no degree, and have substantially earlier wages in both 1974 and 1975. Clearly a straightforward comparision between average outcomes is far from a like-with-like comparisons between groups. The inclusion of other covariates (\\(X^*\\)) does seem to have made a difference, switching the reported direction of effect and its statistical significance, but if we could find a subsample of the control group whose characteristics better match those of the treatment groups, we would hopefully get a more precise and reliable estimate of the effect of the labour market programme.\nThe next part of the vignette shows MatchIt working with some fairly conventional settings:\n\n\nCode\nm.out1 &lt;- matchit(treat ~ age + educ + race + married + \n                   nodegree + re74 + re75, data = lalonde,\n                 method = \"nearest\", distance = \"glm\")\nm.out1\n\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score\n             - estimated with logistic regression\n - number of obs.: 614 (original), 370 (matched)\n - target estimand: ATT\n - covariates: age, educ, race, married, nodegree, re74, re75\n\n\nThe propensity score, i.e. the probability of being in the treatment group, has been predicted using the other covariates, and using logistic regression. For each individual in the treatment group, a ‘nearest neighbour’ in the control group has been identified with the most similar propensity score, which we hope also will also mean the characteristics of the treatment group, and matched pairs from the control group, will be more similar too.\nWe can start to see what this means in practice by looking at the summary of the above object\n\n\nCode\nsummary(m.out1)\n\n\n\nCall:\nmatchit(formula = treat ~ age + educ + race + married + nodegree + \n    re74 + re75, data = lalonde, method = \"nearest\", distance = \"glm\")\n\nSummary of Balance for All Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.1822          1.7941     0.9211    0.3774\nage              25.8162       28.0303         -0.3094     0.4400    0.0813\neduc             10.3459       10.2354          0.0550     0.4959    0.0347\nraceblack         0.8432        0.2028          1.7615          .    0.6404\nracehispan        0.0595        0.1422         -0.3498          .    0.0827\nracewhite         0.0973        0.6550         -1.8819          .    0.5577\nmarried           0.1892        0.5128         -0.8263          .    0.3236\nnodegree          0.7081        0.5967          0.2450          .    0.1114\nre74           2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75           1532.0553     2466.4844         -0.2903     0.9563    0.1342\n           eCDF Max\ndistance     0.6444\nage          0.1577\neduc         0.1114\nraceblack    0.6404\nracehispan   0.0827\nracewhite    0.5577\nmarried      0.3236\nnodegree     0.1114\nre74         0.4470\nre75         0.2876\n\nSummary of Balance for Matched Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.3629          0.9739     0.7566    0.1321\nage              25.8162       25.3027          0.0718     0.4568    0.0847\neduc             10.3459       10.6054         -0.1290     0.5721    0.0239\nraceblack         0.8432        0.4703          1.0259          .    0.3730\nracehispan        0.0595        0.2162         -0.6629          .    0.1568\nracewhite         0.0973        0.3135         -0.7296          .    0.2162\nmarried           0.1892        0.2108         -0.0552          .    0.0216\nnodegree          0.7081        0.6378          0.1546          .    0.0703\nre74           2095.5737     2342.1076         -0.0505     1.3289    0.0469\nre75           1532.0553     1614.7451         -0.0257     1.4956    0.0452\n           eCDF Max Std. Pair Dist.\ndistance     0.4216          0.9740\nage          0.2541          1.3938\neduc         0.0757          1.2474\nraceblack    0.3730          1.0259\nracehispan   0.1568          1.0743\nracewhite    0.2162          0.8390\nmarried      0.0216          0.8281\nnodegree     0.0703          1.0106\nre74         0.2757          0.7965\nre75         0.2054          0.7381\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       185     185\nUnmatched     244       0\nDiscarded       0       0\n\n\nPreviously, there were 185 people in the treatment group, and 429 people in the control group. After matching there are 185 people in the treatment group… and also 185 people in the control group. So, each of the 185 people in the treatment group has been matched up with a ‘data twin’ in the control group, so the ATT should involve more of a like-with-like comparison.\nThe summary presents covariate-wise differences between the Treatment and Control groups for All Data, then for Matched Data. We would hope that, in the Matched Data, the differences are smaller for each covariate, though this isn’t necessarily the case. After matching, for example, we can see that the Black proportion in the Control group is now 0.47 rather than 0.20, and that the earlier income levels are lower, in both cases bringing the values in the Control group closer to, but not identical to, those in the Treatment group. Another way of seeing how balancing has changed things is to look at density plots:\n\n\nCode\nplot(m.out1, type = \"density\", interactive = FALSE,\n     which.xs = ~age + married + re75+ race + nodegree + re74)\n\n\n\n\n\n\n\n\nIn these density charts, the darker lines indicate the Treatment group and the lighter lines the Control groups. The matched data are on the right hand side, with All data on the left. We are looking to see if, on the right hand side, the two sets of density lines are more similar than they are on the right. Indeed they do appear to be, though we can also tell they are far from identical.\n\n\nEstimating Treatment Effect Sizes after matching\nHistorically, the MatchIt package was designed to work seamlessly with Zelig, which made it much easier to use a single library and framework to produce ‘quantities of interest’ using multiple model structures. However Zelig has since been deprecated, meaning the vignette now recommends using the marginaleffects package. We’ll follow their lead:\nFirst the vignette recommends extracting matched data from the matchit output:\n\n\nCode\nm.data &lt;- match.data(m.out1)\n\nm.data &lt;- as_tibble(m.data)\nm.data\n\n\n# A tibble: 370 × 12\n   treat   age  educ race   married nodegree  re74  re75   re78 distance weights\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1     1    37    11 black        1        1     0     0  9930.   0.639        1\n 2     1    22     9 hispan       0        1     0     0  3596.   0.225        1\n 3     1    30    12 black        0        0     0     0 24909.   0.678        1\n 4     1    27    11 black        0        1     0     0  7506.   0.776        1\n 5     1    33     8 black        0        1     0     0   290.   0.702        1\n 6     1    22     9 black        0        1     0     0  4056.   0.699        1\n 7     1    23    12 black        0        0     0     0     0    0.654        1\n 8     1    32    11 black        0        1     0     0  8472.   0.790        1\n 9     1    22    16 black        0        0     0     0  2164.   0.780        1\n10     1    33    12 white        1        0     0     0 12418.   0.0429       1\n# ℹ 360 more rows\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nWhereas the unmatched data contains 614 observations, the matched data contains 370 observations. Note that the Treatment group contained 185 observations, and that 370 is 185 times two. So, the matched data contains one person in the Control group for each person in the Treatment group.\nWe can also see that, in addition to the metrics originally included, the matched data contains three additional variables: ‘distance’, ‘weights’ and ‘subclass’. The ‘subclass’ field is perhaps especially useful for understanding the intuition of the approach, because it helps show which individual in the Control group has been paired with which individual in the Treatment group. Let’s look at the first three subgroups:\n\n\nCode\nm.data |&gt; filter(subclass == '1')\n\n\n# A tibble: 2 × 12\n  treat   age  educ race  married nodegree   re74  re75  re78 distance weights\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    37    11 black       1        1     0      0 9930.    0.639       1\n2     0    22     8 black       1        1 16961.     0  959.    0.203       1\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nSo, for the first subclass, a 37 year old married Black person with no degree has been matched to a 22 year old Black married person with no degree.\n\n\nCode\nm.data |&gt; filter(subclass == '2')\n\n\n# A tibble: 2 × 12\n  treat   age  educ race  married nodegree  re74  re75   re78 distance weights\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    33    12 white       1        0    0      0 12418.   0.0429       1\n2     0    39    12 white       1        0 1289.     0  1203.   0.0430       1\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nFor the second subclass a 33 year old married White person with a degree has been paired with a 39 year old White person with a degree.\n\n\nCode\nm.data |&gt; filter(subclass == '3')\n\n\n# A tibble: 2 × 12\n  treat   age  educ race   married nodegree  re74  re75   re78 distance weights\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    31     9 hispan       0        1     0    0  26818.    0.250       1\n2     0    16    10 white        0        1     0  190.  2137.    0.105       1\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nFor the third subclass, a 31 year old unmarried Hispanic person with no degree has been paired with a 16 year old White person with no degree.\nIn each case, we can see the pairings are similar in some ways but (as with the last example) quite dissimilar in others. The matching algorithm is trying to do the best it can with the data available, especially with the constraint5 that once a person in the Control group has been paired up once to someone in the Treatment group, they can’t be paired up again with someone else in the Treatment group.\nThe identification of these specific pairings suggests we can used a fairly crude strategy to produce an estimate of the ATT: namely just compare the outcome across each of these pairs. Let’s have a look at this:\n\n\nCode\ntrt_effects &lt;- \n    m.data |&gt;\n        group_by(subclass) |&gt;\n        summarise(\n            ind_treat_effect = re78[treat == 1] - re78[treat == 0]\n        ) |&gt; \n        ungroup()\n\ntrt_effects |&gt;\n    ggplot(aes(ind_treat_effect)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = mean(trt_effects$ind_treat_effect), colour = \"red\") + \n    geom_vline(xintercept = 0, colour = 'lightgray', linetype = 'dashed')\n\n\n\n\n\nThis crude paired comparison suggests an average difference that’s slightly positive, of $894.37.\nThis is not a particularly sophisticated or ‘kosher’ approach however. Instead the vignette suggests calculating the treatment effect estimate as follows:\n\n\nCode\nlibrary(\"marginaleffects\")\n\nfit &lt;- lm(re78 ~ treat * (age + educ + race + married + nodegree + \n             re74 + re75), data = m.data, weights = weights)\n\navg_comparisons(fit,\n                variables = \"treat\",\n                vcov = ~subclass,\n                newdata = subset(m.data, treat == 1),\n                wts = \"weights\")\n\n\n\n  Term          Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n treat mean(1) - mean(0)     1121        837 1.34    0.181 2.5  -520   2763\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\nUsing the recommended approach, the ATT estimate is now $1121. Not statistically significant at the conventional 95% threshold, but also more likely to be positive than negative.\n\n\nSummary\nIn this post we have largely followed along with the introductionary vignette from the MatchIt package, in order to go from the fairly cursory theoretical overview in the previous post, to showing how some of the ideas and methods relating to multiple regression and matching methods work in practice. There are a great many ways that both matching, and multiple regression, can be implemented in practice, and both are likely to affect any causal effect estimates we produce. However, the aspiration of using matching methods is to somewhat reduce the dependency that causal effect estimates have on the specific model specifications we used."
  },
  {
    "objectID": "pages/extra-courses/causal-inference/index.html#the-schools-of-causal-inference",
    "href": "pages/extra-courses/causal-inference/index.html#the-schools-of-causal-inference",
    "title": "Causal Inference: An Opinionated Introduction",
    "section": "The Schools of Causal Inference",
    "text": "The Schools of Causal Inference\nReaders who’ve been involved and interested in the topic of causal inference over the last few years might be less surprised by what I have covered than by what I’ve not, namely the causal inference framework developed by Judea Pearl, and (somewhat) popularised by his co-authored book, The Book of Why: The New Science of Cause and Effect. (Pearl and Mackenzie (2018))\nThis ‘oversight’ in posts so far has been intentional, but in this post the Pearl framework will finally be discussed. I’ll aim to: i) give an overview of the two primary ways of thinking about causal inference: either as a missing data problem; or as a ‘do-logic’ problem; ii) discuss the concept of the omitted variable vs post treatment effect bias trade-off as offering something of a bridge between the two paradigms; iii) give some brief examples of directed acyclic graphs (DAGs) and do-logic, two important ideas from the Pearl framework, as described in Pearl and Mackenzie (2018); iv) make some suggestions about the benefits and uses of the Pearl framework; and finally v) advocate for epistemic humility when it comes to trying to draw causal inferences from observational data, even where a DAG has been clearly articulated and agreed upon within a research community. 6 Without further ado, let’s begin:\n\nCausal Inference: Two paradigms\nIn the posts so far, I’ve introduced and kept returning to the idea that the fundamental problem of causal inference is that at least half of the data is always missing. i.e., for each individual observation, who has either been treated or not treated, if they had been treated then we do not observe them in the untreated state, and if they had not been treated we do not observe them in the treated state. It’s this framing of the problem which\nIn introducing causal inference from this perspective, I’ve ‘taken a side’ in an ongoing debate, or battle, or even war, between two clans of applied epistemologists. Let’s call them the Rubinites, and the Pearlites. Put crudely, the Rubinites adopt a data-centred framing of the challenge of causal inference, whereas the Pearlites adopt a model-centred framing of the challenge of causal inference. For the Rubinites, the data-centred framing leads to an intepretation of causal inference as a missing data problem, for which the solution is therefore to perform some kind of data imputation. For the Pearlites, by contrast, the solution is focused on developing, describing and drawing out causal models, which describe how we believe one thing leads to another and the paths of effect and influence that one variable has on each other variable.\nIt is likely no accident that the broader backgrounds and interests of Rubin and Pearl align with type of solution each proposes. Rubin’s other main interests are in data imputation more generally, including methods of multiple imputation which allow ‘missing values’ to be filled in stochastically, rather than deterministically, to allow some representation of uncertainty and variation in the missing values to be indicated by the range of values that are generated for a missing hole in the data. Pearl worked as a computer scientist, whose key contribution to the field was the development of Bayesian networks, which share many similarities with neural networks. For both types of network, there are nodes, and there are directed links. The nodes have values, and these values can be influenced and altered by the values of other nodes that are connected to the node in question. This influence that each node has on other nodes, through the paths indicated in the directed links, is perhaps more likely to be described as updating from the perspective of a Bayesian network, and propagation from the perspective of a neural network. But in either case, it really is correct to say that one node really does cause another node’s value to change through the causal pathway of the directed link. The main graphical tool Pearl proposes for reasoning about causality in obervational data is the directed acyclic graph (DAG), and again it should be unsurprising that DAGs look much like Bayesian networks.\n\n\nThe Omitted Variable Bias vs Post Treatment Bias Trade-off as a potential bridge between the two paradigms\nThe school of inference I’m most familiar with is that of Gary King, a political scientist, methodologist and (in the hallowed halls of Harvard) populariser of statistical methods in the social sciences. In the crude paradigmatic split I’ve sketched out above, King is a Rubinite, and so I guess - mainly through historical accident but partly through conscious decision - I am too. However, I have read Pearl and Mackenzie (2018) (maybe not recently enough nor enough times to fully digest it), consider it valuable and insightful in many places, and think there’s at least one place where the epistemic gap between the two paradigms can be bridged.\nThe bridge point on the Rubinite side,7 I’d suggest, comes from thinking carefully about the sources of bias enumerated in section 3.2 of King and Zeng (2006), which posits that:\n\\[\nbias = \\Delta_o + \\Delta_p + \\Delta_i + \\Delta_e\n\\]\nThis section states:\n\nThese four terms denote exactly the four sources of bias in using observational data, with the subscripts being mnemonics for the components … . The bias components are due to, respectively, omitted variable bias (\\(\\Delta_o\\)), post-treatment bias (\\(\\Delta_p\\)), interpolation bias (\\(\\Delta_i\\)) and extrapolation bias (\\(\\Delta_e\\)). [Emphases added]\n\nOf the four sources of bias listed, it’s the first two which appear to offer a potential link between the two paradigms, and so suggest to Rubinites why some engagement with the Pearlite approach may be valuable. The section continues:\n\nBriefly, \\(\\Delta_o\\) is the bias due to omitting relevant variables such as common causes of both the treatment and the outcome variables [whereas] \\(\\Delta_p\\) is bias due to controlling for the consequences of the treatment. [Emphases added]\n\nFrom the Rubinite perspective, it seems that omitted variable bias and post-treatment bias are recognised, in combination, as constituting a wicked problem. This is because the inclusion of an specific variable can simultaneously affect both types of bias: reducing omitted variable bias, but also potentially increasing post treatment bias. You’re doomed if you do, but you’re also doomed if you don’t.\n\n\nWith apologies to economists and epidemiologists alike…\nOf the two sources of bias, omitted variable bias seems to be the more discussed. And historically, it seems different social and health science disciplines have placed a different weight of addressing these two sources of bias. In particular, at least in the UK context, it’s seemed that economists tend to be more concerned about omitted variable bias, leading to the inclusion of a large number of variables in their statistical models, whereas epidemiologists (though they might not be familiar with and use the term) tend to be more concerned about post-treatment bias, leading a statistical models with fewer variables.\nThe issue of post treatment bias is especially important to consider in the context of root or fundamental causes, which again is often something more of interest to epidemiologists than economists. And the importance of the issue comes into sharp relief if considering factors like sex or race. An economist/econometrician, if asked to estimate the effect of race on (say) the probability of a successful job application to an esteemed organisation, might be very liable to try to include many additional covariates, such as previous work experience and job qualifications, as ‘control variables’ in a statistical model in addition to race. From this, they might find that the covariate associated with race is neither statistically nor substantively, and from this conclude that there is no evidence of (say) racial discrimination in employment, because any disparities in outcomes between racial groups appear to be ‘explained by’ other factors like previous experience and job qualifications.\nTo this, a methodologically minded epidemiologist might counter - very reasonably - that the econometrician’s model is over-controlling, and that the inclusion of factors like educational outcomes and previous work experience in the model risks introducing post treatment bias. If there were discrimination on the basis of race, or sex, it would be unlikely to just affect the specific outcome on the response side of the model. Instead, discrimination (or other race-based factors) would also likely affect the kind of education available to people of different races, and the kinds of educational expectations placed on people of different racial groups. This would then affect the level of educational achievement by group as well. Similarly, both because of prior differences in educational achievement, and because of concurrent effects of discrimination, race might also be expected to affect job history too. Based on this, the epidemiologist might choose to omit both qualifications and job history from the model, because both are presumed to be causallly downstream of the key factor of interest, race.\nSo which type of model is correct? The epidemiologist’s more parsimonious model, which is mindful of post-treatment bias, or the economist’s more complicated model, which is mindful of omitted variable bias? The conclusion from the four-biases position laid out above is that we don’t know, but that all biases potentially exist in observational data, and neither model specification can claim to be free from bias. Perhaps both kinds of model can be run, and perhaps looking at the estimates from both models can give something like a plausible range of possible effects. But fundamentally, we don’t know, and can’t know, and ideally we should seek better quality data, run RCTs and so on.\nPearl and Mackenzie (2018) argues that Rubinites don’t see much (or any) value in causal diagrams, stating “The Rubin causal model treats counterfactuals as abstract mathematical objects that are managed by algebraic machinery but not derived from a model.” [p. 280] Though I think this characterisation is broadly consciously correct, the recognition within the Rubinite community that such things as post-treatment bias and omitted variables exist suggests to me that, unconsciously, even Rubinites employ something like path-diagram reasoning when considering which sources of bias are likely to affect their effect estimates. Put simply: I don’t see how claims of either omitted variable or post treatment bias could be made or believed but for the kind of graphical, path-like thinking at the centre of the Pearlite paradigm.\nLet’s draw the two types of statistical model implied in the discussion above. Firstly the economist’s model:\n\n\n\n\nflowchart LR\n\nrace(race)\nqual(qualifications)\nhist(job history)\naccept(job offer)\n\nrace --&gt;|Z| accept\nqual --&gt;|X*| accept\nhist --&gt;|X*| accept \n\n\n\n\n\n\nAnd now the epidemiologist’s model:\n\n\n\n\nflowchart LR \n\nrace(race)\naccept(job offer)\n\nrace --&gt;|Z| accept\n\n\n\n\n\n\nEmploying a DAG-like causal path diagram would at the very least allow both the economist and epidemiologist to discuss whether or not they agree that the underlying causal pathways are more likely to be something like the follows:\n\n\n\n\nflowchart LR\n\n\nrace(race)\nqual(qualifications)\nhist(job history)\naccept(job offer)\n\nrace --&gt; qual\nqual --&gt; hist\nhist --&gt; accept\n\nrace --&gt; hist\nqual --&gt; accept\nrace --&gt; accept\n\n\n\n\n\n\nIf, having drawn out their presumed causal pathways like this, the economist and epidemiologist end up with the same path diagram, then the Pearlian framework offers plenty of suggestions about how, subject to various assumptions about the types of effect each node has on each downstream node, statistical models based on observational data should be specified, and how the values of various coefficients in the statistical model should be combined in order to produce an overall estimate of the left-most node on the right-most node. Even a Rubinite who does not subscribe to some of these assumptions may still find this kind of graphical, path-based reasoning helpful for thinking through what their concerns are relating to both omitted variable and post-treatment biases are, and whether there’s anything they can do about it. In the path diagram above, for example, the importance of temporal sequence appears important: first there’s education and qualification; then there’s initial labour market experience; and then there’s contemporary labour market experience. This appreciation of the sequence of events might suggest that, perhaps, data employing a longitudinal research design might be preferred to one using only cross-sectional data; and/or that what appeared intially to be only a single research question, investigated through a single statistical model, is actually a series of linked, stepped research questions, each employing a different statistical model, breaking down the cause-effect question into a series of smaller steps.\n\n\nSummary thoughts: on social complexity and the need for epistemic humility\nAs mentioned before, I probably lean somewhat more towards the Rubinite than the Pearlite framework. A lot of this is simply because this is the causal effect framework I was first introduced to, but some of it comes from more fundamental concerns I have about how some users and advocates of the Pearlite framework seem to think, or suggest, it can solve issues of causal inference from observational data that, fundamentally, I don’t think it may be possible to address.\nOne clue about what the Pearlite framework can and cannot do comes from the ‘A’ in DAG: ‘acyclic’. This means that causal pathways of the following form can be specified:\n\n\n\n\nflowchart LR\nA(A)\nB(B)\n\nA --&gt; B\n\n\n\n\n\nBut causal pathways of the following form cannot:\n\n\n\n\nflowchart LR\n\nA(A)\nB(B)\n\nA --&gt; B\nB --&gt; A\n\n\n\n\n\n\nUnfortunately, cyclic relationships between two or more factors, in which the pathways of influence go in both directions, are likely extremely common in social and economic systems, because such systems are complex rather than merely complicated. 8 One approach to trying to fit a representation of a complex coupled system into a DAG-like framework would be to use time to try to break the causal paths:\n\n\n\n\nflowchart LR\n\nc0(Chicken at T0)\ne1(Egg at T1)\nc2(Chicken at T2)\ne3(Egg at T3)\n\nc0 --&gt; e1\ne1 --&gt; c2\nc2 --&gt; e3\n\n\n\n\n\n\nBut another way of reasoning about such localised coupled complexity might be to use something like factor analysis to identify patterns of co-occurence of variables which may be consistent with this kind of localised complex coupling:\n\n\n\n\nflowchart LR\n\nce((ChickenEgg))\ne[egg]\nc[chicken]\n\nce --&gt; e\nce --&gt; c\n\n\n\n\n\n\nWithin the above diagram, based on structural equation modelling, the directed arrows have a different meaning. They’re not claims of causal effects, but instead of membership. The circle is an underlying proposed ‘latent variable’, the ChickenEgg, which is presumed to manifest through the two observed/manifest variables egg and chicken represented by the rectangles. In places with a lot of ChickenEgg, such as a hen house, we would expect to observe a lot of both chickens and eggs. The statistical model in the above case is a measurement model, rather than a causal model, but in this case is one which is informed by an implicit recognition of continual causal influence operating within members of a complex, paired, causal system.\nSo, I guess my first concern relating to DAGs is that, whereas they can be really useful in allowing researchers to express some form of causal thinking and assumptions about paths of influence between factors, their acyclic requirement can also lead researchers to disregard or underplay the role of complexity even when considering inherently complex systems. In summary, they offer the potential both to expand, but also to restrict, our ability to reason effectively about causal influence.\nMy second, related, concern about the potential over-use or over-reach of DAG-like thinking comes from conventional assumptions built into the paths of influence between nodes. We can get to the heart of this latter concern by looking at , and carefully considering the implications of, something called a double pendulum, a video of which is shown below:\n\n\nA double pendulum is not a complicated system, but it is a complex system, and also a chaotic system. The variables at play include two length variables, two mass variables, a gravity variable, and time. The chaotic complexity of the system comes from the way the length and mass of the first arm interact with the length and and mass of the second arm. This complex interaction is what leads to the position of the outer-most part of the second arm (the grey ball) at any given time.\nNow imagine trying to answer a question of the form “what is the effect of the first arm’s mass on the grey ball’s position?” This kind of question is one that it’s simply not meaningful to even ask. It’s the complex interaction between all components of the system that jointly determines the ball’s position, and attempting to decompose the causal effect of any one variable in the system is simply not a fruitful way of trying to understand the system as a whole.\nThis does not mean, however, that we cannot develop a useful understanding of the double pendulum. We know, for example, that the ball cannot be further than the sum of the length of the two arms from the centre of the system. If we were thinking about placing another object near the double pendulum, for example, this would help us work out how far apart from the pendulum we should place it. Also, if one of the arms is much longer or more massive than the other, then maybe we could approximate it with a simple pendulum too. Additionally, all double pendulums tend to behave in similar ways during their initial fall. But the nature of this kind of complex system also means some types of causal question are beyond the realm of being answerable.\nThe double pendulum, for me, is an object lesson on the importance of epistemic humility. My overall concern relating to causal inference applies nearly equally to Rubinites and Pearlites alike, and is that excessive engagement with or enthusiasm for any kind of method or framework can lead to us believing we know more than we really know more about how one thing affects another. This can potentially lead both to errors of judgement - such as not planning sufficiently for eventualities our models suggest cannot happen - and potentially to intolerance towards those who ‘join the dots’ in a different way to ourselves. 9\nIn short: stay methodologically engaged, but also stay epistemically modest."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html",
    "href": "pages/extra-courses/time-series/index.html",
    "title": "Time Series",
    "section": "",
    "text": "In the main course I’ve returned many times to the same ‘mother formula’ for a generalised linear model:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nSo, we have a system that takes a series of inputs, \\(X\\), and returns an output, \\(Y\\), and we have two sets of parameters, \\(\\beta\\) in \\(g(.)\\) and \\(\\alpha\\) in \\(f(.)\\), which are calibrated based on the discrepancy between what the model predicted output \\(Y\\) and the observed output \\(y\\).\nThere are two important things to note: Firstly, that the choice of parts of the data go into the inputs \\(X\\) and the output(s) \\(Y\\) is ultimately our own. A statistical model won’t ‘know’ when we’re trying to predict the cause of something based on its effect, for example. Secondly, that although the choice of input and output for the model are ultimately arbitrary, they cannot be the same. i.e., we cannot do this:\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\n\\[\n\\theta_i = g(Y_i, \\beta)\n\\]\nThis would be the model calibration equivalent of telling a dog to chase, or a snake to eat, its own tail. It doesn’t make sense, and so the parameter values involved cannot be calculated.\nFor time series data, however, this might appear to be a fundamental problem, given our observations may comprise only of ‘outcomes’, which look like they should be in the output slot of the formulae, rather than determinants, which look like they should be in the input slot of the formulae. i.e. we might have data that looks as follows:\n\n\n\n\\(i\\)\n\\(Y_{T-2}\\)\n\\(Y_{T-1}\\)\n\\(Y_{T}\\)\n\n\n\n\n1\n4.8\n5.0\n4.9\n\n\n2\n3.7\n4.1\n4.3\n\n\n3\n4.3\n4.1\n4.3\n\n\n\nWhere \\(T\\) indicates an index time period, and \\(T-k\\) a fixed difference in time ahead of or behind the index time period. For example, \\(T\\) might be 2019, \\(T-1\\) might be 2018, \\(T-2\\) might be 2017, and so on.\nAdditionally, for some time series data, the dataset will be much more wide than long, perhaps with just a single observed unit, observed at many different time points:\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n\\(Y_{T-5}\\)\n\\(Y_{T-4}\\)\n\\(Y_{T-3}\\)\n\\(Y_{T-2}\\)\n\\(Y_{T-1}\\)\n\\(Y_{T}\\)\n\n\n\n\n1\n3.9\n5.1\n4.6\n4.8\n5.0\n4.9\n\n\n\nGiven all values are ‘outcomes’, where’s the candidate for an ‘input’ to the model, i.e. something we should consider putting into \\(X\\)?\nDoesn’t the lack of an \\(X\\) mean time series is an exception to the ‘rule’ about what a statistical model looks like, and so everything we’ve learned so far is no longer relevant?\nThe answer to the second question is no. Let’s look at why."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#introduction",
    "href": "pages/extra-courses/time-series/index.html#introduction",
    "title": "Time Series",
    "section": "",
    "text": "A few weeks ago, I polled both LinkedIn and (what’s left of) Twitter for statistical topics to cover next in this series. By a small to moderate margin, time series came out on top. So, after a longer-than-usual delay, here’s an introduction to time series modelling."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#time-series-an-exception-that-proves-the-rule",
    "href": "pages/extra-courses/time-series/index.html#time-series-an-exception-that-proves-the-rule",
    "title": "Time Series",
    "section": "Time Series: An exception that proves the rule?",
    "text": "Time Series: An exception that proves the rule?\nThroughout this series I’ve returned many times to the same ‘mother formula’ for a generalised linear model:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nSo, we have a system that takes a series of inputs, \\(X\\), and returns an output, \\(Y\\), and we have two sets of parameters, \\(\\beta\\) in \\(g(.)\\) and \\(\\alpha\\) in \\(f(.)\\), which are calibrated based on the discrepancy between what the model predicted output \\(Y\\) and the observed output \\(y\\).\nThere are two important things to note: Firstly, that the choice of parts of the data go into the inputs \\(X\\) and the output(s) \\(Y\\) is ultimately our own. A statistical model won’t ‘know’ when we’re trying to predict the cause of something based on its effect, for example. Secondly, that although the choice of input and output for the model are ultimately arbitrary, they cannot be the same. i.e., we cannot do this:\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\n\\[\n\\theta_i = g(Y_i, \\beta)\n\\]\nThis would be the model calibration equivalent of telling a dog to chase, or a snake to eat, its own tail. It doesn’t make sense, and so the parameter values involved cannot be calculated.\nFor time series data, however, this might appear to be a fundamental problem, given our observations may comprise only of ‘outcomes’, which look like they should be in the output slot of the formulae, rather than determinants, which look like they should be in the input slot of the formulae. i.e. we might have data that looks as follows:\n\n\n\n\\(i\\)\n\\(Y_{T-2}\\)\n\\(Y_{T-1}\\)\n\\(Y_{T}\\)\n\n\n\n\n1\n4.8\n5.0\n4.9\n\n\n2\n3.7\n4.1\n4.3\n\n\n3\n4.3\n4.1\n4.3\n\n\n\nWhere \\(T\\) indicates an index time period, and \\(T-k\\) a fixed difference in time ahead of or behind the index time period. For example, \\(T\\) might be 2019, \\(T-1\\) might be 2018, \\(T-2\\) might be 2017, and so on.\nAdditionally, for some time series data, the dataset will be much more wide than long, perhaps with just a single observed unit, observed at many different time points:\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n\\(Y_{T-5}\\)\n\\(Y_{T-4}\\)\n\\(Y_{T-3}\\)\n\\(Y_{T-2}\\)\n\\(Y_{T-1}\\)\n\\(Y_{T}\\)\n\n\n\n\n1\n3.9\n5.1\n4.6\n4.8\n5.0\n4.9\n\n\n\nGiven all values are ‘outcomes’, where’s the candidate for an ‘input’ to the model, i.e. something we should consider putting into \\(X\\)?\nDoesn’t the lack of an \\(X\\) mean time series is an exception to the ‘rule’ about what a statistical model looks like, and so everything we’ve learned so far is no longer relevant?\nThe answer to the second question is no. Let’s look at why."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#autoregression",
    "href": "pages/extra-courses/time-series/index.html#autoregression",
    "title": "Time Series",
    "section": "Autoregression",
    "text": "Autoregression\nInstead of looking at the data in the wide format above, let’s instead rearrange it in long format, so the time variable is indexed in its own column:\n\n\nCode\nlibrary(tidyverse)\n\ndf &lt;- tribble(\n    ~i, ~t, ~y,\n    1, 2008, 3.9,\n    1, 2009, 5.1,\n    1, 2010, 4.6,\n    1, 2011, 4.8,\n    1, 2012, 5.0,\n    1, 2013, 4.9\n)\ndf\n\n\n# A tibble: 6 × 3\n      i     t     y\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1  2008   3.9\n2     1  2009   5.1\n3     1  2010   4.6\n4     1  2011   4.8\n5     1  2012   5  \n6     1  2013   4.9\n\n\nAs there’s only one type of observational unit, \\(i=1\\) in all cases, there’s no variation in this variable, so it can’t provide any information in a model. Let’s look at the series and think some more:\n\n\nCode\ndf |&gt; \n    ggplot(aes(t, y)) + \n    geom_line() + \n    geom_point()\n\n\n\n\n\nWe could, of course, regress the outcome against time:\n\n\nCode\ndf |&gt;\n    ggplot(aes(t, y)) + \n    geom_line() + \n    geom_point() + \n    stat_smooth(method = \"lm\")\n\n\n\n\n\nIs this reasonable? It depends on the context. We obviously don’t have that many observations, but the regression slope appears to have a positive gradient, meaning values projected into the future will likely be higher than the observed values, and values projected into the past will likely have lower than the observed values.\n\n\nCode\ndf |&gt;\n    ggplot(aes(t, y)) + \n    geom_line() + \n    geom_point() + \n    scale_x_continuous(limits = c(2000, 2020)) + \n    stat_smooth(method = \"lm\", fullrange = TRUE)\n\n\n\n\n\nMaybe this kind of extrapolation is reasonable. Maybe it’s not. As usual it depends on context. Although it’s a model including time as a predictor, it’s not actually a time series model. Here’s an example of a time series model:\n\n\nCode\nlm_ts_ar0 &lt;- lm(y ~ 1, data = df)\n\nsummary(lm_ts_ar0)\n\n\n\nCall:\nlm(formula = y ~ 1, data = df)\n\nResiduals:\n       1        2        3        4        5        6 \n-0.81667  0.38333 -0.11667  0.08333  0.28333  0.18333 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.7167     0.1778   26.53 1.42e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4355 on 5 degrees of freedom\n\n\nThis is an example of a time series model, even though it doesn’t have a time component on the predictor side. In fact, it doesn’t have anything as a predictor. Its formula is y ~ 1, meaning there’s just an intercept term. It’s saying “assume new values are just like old values: all just drawn from the same normal distribution”.\nI called this model ar0. Why? Well, let’s look at the following:\n\n\nCode\nlm_ts_ar1 &lt;- df |&gt;\n    arrange(t) |&gt;\n    mutate(y_lag1 = lag(y, 1)) %&gt;%\n    lm(y ~ y_lag1, data = .)\n\nsummary(lm_ts_ar1)\n\n\n\nCall:\nlm(formula = y ~ y_lag1, data = .)\n\nResiduals:\n        2         3         4         5         6 \n-0.005066 -0.158811 -0.103084  0.154626  0.112335 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   6.2304     0.7661   8.132  0.00389 **\ny_lag1       -0.2885     0.1630  -1.770  0.17489   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1554 on 3 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.5108,    Adjusted R-squared:  0.3477 \nF-statistic: 3.133 on 1 and 3 DF,  p-value: 0.1749\n\n\nFor this model, I included one new predictor: y_lag1. For this, I created a new column in the dataset:\n\n\nCode\ndf |&gt;\n    arrange(t) |&gt;\n    mutate(y_lag1 = lag(y, 1))\n\n\n# A tibble: 6 × 4\n      i     t     y y_lag1\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1  2008   3.9   NA  \n2     1  2009   5.1    3.9\n3     1  2010   4.6    5.1\n4     1  2011   4.8    4.6\n5     1  2012   5      4.8\n6     1  2013   4.9    5  \n\n\nThe lag() operator takes a column and a lag term parameter, in this case 1 1. For each row, the value of y_lag1 is the value of y one row above. 2\nThe first simple model was called AR0, and the second AR1. So what does AR stand for?\nIf you’re paying attention to the section title, you already have the answer. It means autoregressive. The AR1 model is the simplest type of autoregressive model, by which I don’t mean this is a type of regression model that formulates itself, but do mean it includes its own past states (at time T-1) as predictors of its current state (at time T).\nAnd what does \\(T\\) and \\(T-1\\) refer to in the above? Well, for row 6 \\(T\\) refers to \\(t\\), which is 2013; and so \\(T-1\\) refers to 2012. But for row 5 \\(T\\) refers to 2012, so \\(T-1\\) refers to 2011. This continues back to row 2, where \\(T\\) is 2009 so \\(T-1\\) must be 2008. (The first row doesn’t have a value for y_lag1, so can’t be included in the regression).\nIsn’t this a bit weird, however? After all, we only have one real observational unit, \\(i\\), but for the AR1 model we’re using values from this unit five times. Doesn’t this violate some kind of rule or expectation required for model outputs to be legitimate?\nWell, it might. A common shorthand when describing the assumptions that we make when applying statistical models is \\(IID\\), which stands for ‘independent and identically distributed’. As with many unimaginative discussions of statistics, we can illustrate something that satifies both of these properties, independent and identically distributed by looking at some coin flips:\n\n\nCode\nset.seed(7)\n\nuniform_values &lt;- runif(10)\ncoin_df &lt;- tibble(\n    flip_number = 1:10, \n    is_head = uniform_values &gt;= 0.5\n)\n\ncoin_df\n\n\n# A tibble: 10 × 2\n   flip_number is_head\n         &lt;int&gt; &lt;lgl&gt;  \n 1           1 TRUE   \n 2           2 FALSE  \n 3           3 FALSE  \n 4           4 FALSE  \n 5           5 FALSE  \n 6           6 TRUE   \n 7           7 FALSE  \n 8           8 TRUE   \n 9           9 FALSE  \n10          10 FALSE  \n\n\nThe above series of coin flips is identically distributed because the order in which the flips occur doesn’t matter to the value generated. The dataframe could be permutated in any order and it wouldn’t matter to the data generation process at all. The series is independent because the probability of getting, say, a sequence of three heads is just the product of getting one head, three times, i.e. i.e. \\(\\frac{1}{2} \\frac{1}{2} \\frac{1}{2}\\) or \\([\\frac{1}{2}]^{3}\\). Without going into too much detail, both of these assumptions are necessary to make in order for likelihood estimation, which relies on multiplying sequences of numbers 3 to ‘work’.\nThe central assumption and hope with an autoregressive model specification is that, conditional on the autoregressive terms being included on the predictor side of the model, the data can assumed to have been generated from an IID data generating process (DGP).\nThe intuition behind this is something like the following: say you wanted to know if I’ll have the flu tomorrow. It would obviously be useful to know if I have the flu today, because symptoms don’t change very quickly. 4 Maybe it would also be good to know if I had the flu yesterday too, maybe even two days ago as well. But would it be good to know if I had the flu two weeks ago, or five weeks ago? Probably not. At some point, i.e. some number of lag terms from a given time, more historical data stops being informative. i.e., beyond a certain number of lag periods, the data series can be assumed to be IID (hopefully).\nWhen building autoregressive models, it is common to look at a range of specifications, each including different numbers of lag terms. I.e. we can build a series of AR specification models as follows:\n\nAR(0): \\(Y_T \\sim 1\\)\nAR(1): \\(Y_T \\sim Y_{T-1}\\)\nAR(2): \\(Y_T \\sim Y_{T-1} + Y_{T-2}\\)\nAR(3): \\(Y_T \\sim Y_{T-1} + Y_{T-2} + Y_{T-3}\\)\n\nAnd so on. Each successive AR(.) model contains more terms than the last, so is a more complicated and data hungry model than the previous one. We should already by this point in the series be familiar with standard approaches for trying to find the best trade off between model complexity and model fit. The above models are in a sense nested, so for example F-tests can be used to compare these models. Another approach, which ‘works’ for both nested and non-nested model specifications, is AIC, and indeed this is commonly used to select the ‘best’ number of autoregressive terms to include.\nFor future reference, the number of AR terms is commonly denoted with the letter ‘p’, meaning that if p is 3, for example, then we are talking about an AR(3) model specification.\n\nSumming up\nThe other important thing to note is that autoregression is a way of fitting time series data within the two component ‘mother formulae’ at the start of this post (and many others), by operating on the systematic component of the model framework, \\(g(.)\\). At this stage, nothing unusual is happening with the stochastic component of the model framework \\(f(.)\\).\nWith autoregression, denoted by the formula shorthand \\(AR(.)\\) and the parameter shorthand \\(p\\), we now have one of the three main tools in the modeller’s toolkit for handling time series data."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#summing-up",
    "href": "pages/extra-courses/time-series/index.html#summing-up",
    "title": "Time Series",
    "section": "Summing up",
    "text": "Summing up\nThe other important thing to note is that autoregression is a way of fitting time series data within the two component ‘mother formulae’ at the start of this post (and many others), by operating on the systematic component of the model framework, \\(g(.)\\). At this stage, nothing unusual is happening with the stochastic component of the model framework \\(f(.)\\).\nWith autoregression, denoted by the formula shorthand \\(AR(.)\\) and the parameter shorthand \\(p\\), we now have one of the three main tools in the modeller’s toolkit for handling time series data."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#coming-up",
    "href": "pages/extra-courses/time-series/index.html#coming-up",
    "title": "Time Series",
    "section": "Coming up",
    "text": "Coming up\nIn the next post in this mini-series, we’ll start to look at the other two main components of time series modelling, integration and moving averages, before looking at how they’re combined and applied in a general model specification called ARIMA."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#recap",
    "href": "pages/extra-courses/time-series/index.html#recap",
    "title": "Time Series",
    "section": "Recap",
    "text": "Recap\nIn the last part of this series, I discussed why time series data are both a bit dissimilar to many other types of data we try to model, and also ‘one weird trick’ - autoregression - which allows the standard generalised linear model ‘chasis’ - that two part equation - to be used with time series data.\nWithin the last part, I said autoregression was just one of three common tools used for working with time series data, with the other two being integration and moving averages. Let’s now cover those two remaining tools:"
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#integration",
    "href": "pages/extra-courses/time-series/index.html#integration",
    "title": "Time Series",
    "section": "Integration",
    "text": "Integration\nConsider the following time series data:\n\n\nCode\nlibrary(tidyverse)\nset.seed(8)\nt &lt;- 0:30\n\nintercept &lt;- 2.35\nslope &lt;- 0.15\n\ny &lt;- intercept + slope * t + rnorm(31, mean = 0, sd = 0.2)\n\ndf &lt;- tibble(\n    t = t,\n    y = y\n)\n\ndf |&gt;\n    ggplot(aes(t, y)) + \n    geom_point() + \n    geom_line() + \n    expand_limits(y = 0)\n\n\n\n\n\nThis time series data is an example of a non-stationary time series. This term means that its value drifts in a particular direction over time. In this case, upwards, meaning values towards the end of the series tend to be higher than values towards the start of the series.\nWhat this drift means is that the order of the observations matters, i.e. if we looked at the same observations, but in a random order, we wouldn’t see something that looks similar to what we’re seeing here.\n\n\nCode\ndf |&gt;\n    mutate(rand_selection = sample(0:30)) |&gt;\n    ggplot(aes(rand_selection, y)) + \n    geom_point() + \n    geom_line() + \n    expand_limits(y = 0)\n\n\n\n\n\nAs it’s clear the order of the sequence matters, the standard simplifying assumptions for statistical models of IID (independent and identically distributed) does not hold, so the extent to which observations from the same time series dataset can be treated like new pieces of information for the model is doubtful. We need a way of making the observations that go into the model (though not necessarily what we do with the model after fitting it) more similar to each other, so these observations can be treated as IID. How do we do this?\nThe answer is something that’s blindingly obvious in retrospect. We can transform the data that goes into the model by taking the differences between consecutive values. So, if the first ten values of our dataset look like this:\n\n\nCode\nhead(df, n=10 )\n\n\n# A tibble: 10 × 2\n       t     y\n   &lt;int&gt; &lt;dbl&gt;\n 1     0  2.33\n 2     1  2.67\n 3     2  2.56\n 4     3  2.69\n 5     4  3.10\n 6     5  3.08\n 7     6  3.22\n 8     7  3.18\n 9     8  2.95\n10     9  3.58\n\n\nThen we can take the differences between consecutive values and get the following:\n\n\nCode\ndf |&gt; \n    arrange(t) |&gt;\n    mutate(diff_y = y - lag(y, 1)) |&gt;\n    head(n=11)\n\n\n# A tibble: 11 × 3\n       t     y  diff_y\n   &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1     0  2.33 NA     \n 2     1  2.67  0.335 \n 3     2  2.56 -0.111 \n 4     3  2.69  0.133 \n 5     4  3.10  0.407 \n 6     5  3.08 -0.0188\n 7     6  3.22  0.138 \n 8     7  3.18 -0.0336\n 9     8  2.95 -0.235 \n10     9  3.58  0.634 \n11    10  3.70  0.117 \n\n\nSo, as with autoregression (AR), with integration we’ve arranged the data in order, then used the lag operator. The difference between the use of lagging as a tool for AR, and lagging as a tool for integration (I), is that, whereas for autoregression, we’re using lagging to construct one or more variables to use as predictor terms for the model, within integration we’re using lagging to construct new variables for use in either the response or the predictor sides of the model equation.\nWhat does our differenced data look like?\n\n\nCode\ndf |&gt; \n    arrange(t) |&gt;\n    mutate(diff_y = y - lag(y, 1)) |&gt;\n    ggplot(aes(t, diff_y)) + \n    geom_hline(yintercept = 0) + \n    geom_point() + \n    geom_line()\n\n\n\n\n\nIn the above I’ve added a reference line at y=0. Note that the average of this series appears to be above the zero line. Let’s check this assumption:\n\n\nCode\ndy &lt;- df |&gt;\n    arrange(t) |&gt;\n    mutate(diff_y = y - lag(y, 1)) |&gt;\n    pull(diff_y) \n\nprint(paste0(\"The mean dy is \", mean(dy, na.rm = TRUE) |&gt; round(2)))\n\n\n[1] \"The mean dy is 0.16\"\n\n\nCode\nprint(paste0(\"The corresponding SE is \", (sd(dy, na.rm=TRUE) / sqrt(length(dy)-1)) |&gt; round(2)))\n\n\n[1] \"The corresponding SE is 0.06\"\n\n\nThis mean value of the differences values, \\(dy\\), is about 0.16. This is the intercept of the differenced data. As we made up the original data, we also know that its slope is 0.15, i.e. except for estimation uncertainty, the intercept of the differenced data is the slope of the original data. 5\nImportantly, when it comes to time series, whereas our original data were not stationary, our differenced data are. This means they are more likely to meet the IID conditions, including that the order of observations no longer really matters as to its value.\nOne way of demonstrating this is with a statistical identity parade.\nHere are nine versions of the undifferenced data, eight of which have been randomly shuffled. Can you tell which is the original, unshuffled data?\n\n\nCode\nset.seed(9)\n\npermute_randomly &lt;- function(id, df){\n    df |&gt; \n        mutate(y = sample(y))\n}\n\ndf_parade &lt;- tibble(\n    id = LETTERS[1:9]\n) |&gt;\n    mutate(data = map(id, permute_randomly, df = df))\n\ndf_parade$data[[5]] &lt;- df\n\ndf_parade |&gt;\n    unnest(data) |&gt; \n    ggplot(aes(t, y)) + \n    geom_point() + \n    geom_line() + \n    facet_wrap(~id)\n\n\n\n\n\nHere it seems fairly obvious which dataset is the original, unshuffled version of the data, again illustrating that the original time series are not IID, and not a stationary series.\nBy contrast, let’s repeat the same exercise with the differenced data:\n\n\nCode\nd_df &lt;- df |&gt; \n    arrange(t) |&gt;\n    mutate(diff_y = y - lag(y, 1)) |&gt;\n    select(t, y = diff_y) %&gt;%\n    filter(complete.cases(.))\n\ndiff_df_parade &lt;- tibble(\n    id = LETTERS[1:9]\n) |&gt;\n    mutate(data = map(id, permute_randomly, df = d_df))\n\ndiff_df_parade$data[[5]] &lt;- d_df\n\ndiff_df_parade |&gt;\n    unnest(data) |&gt; \n    ggplot(aes(t, y)) + \n    geom_point() + \n    geom_line() + \n    facet_wrap(~id)\n\n\n\n\n\nHere it’s much less obvious which of the series is the original series, rather than a permuted/shuffled version of the same series. This should give some reassurance that, after differencing, the data are now IID.\n\nWhy is integration called integration not differencing?\nIn the above we have performed what in time series parlance would be called an I(1) operation, differencing the data once. But why is this referred to as integration, when we’re doing the opposite?\nWell, when it comes to transforming the time series data into something with IID properties, we are differentiating rather than integrating. But the flip side of this is that, if using model outputs based on differenced data for forecasting, we have to sum up (i.e. integrate) the values we generate in the order in which we generate them. So, the model works on the differenced data, but model forecasts work by integrating the random variables generated by the model working on the differenced data.\nLet’s explore what this means in practice. Let’s generate 10 new values from a model calibrated on the mean and standard deviation of the differenced data:\n\n\nCode\nnew_draws &lt;- rnorm(\n    10, \n    mean = mean(d_df$y, na.rm = TRUE),\n    sd = sd(d_df$y, na.rm = TRUE)\n)\n\nnew_draws\n\n\n [1]  0.053763359  0.638791031 -0.106726422  0.470132841 -0.084156880\n [6] -0.041865610  0.002496487  0.481433544 -0.082119365  0.086125038\n\n\nWe can append these to the end of our differenced data to see how this forecast series compared with the observed series:\n\n\nCode\nmax_t &lt;- max(d_df$t)\n\nforecast_df &lt;- tibble(\n    t = seq(from = max_t+1, to = max_t + length(new_draws)),\n    y = new_draws,\n    type = \"forecast\"\n)\n\nobs_forecast_df &lt;- bind_rows(\n    d_df |&gt; mutate(type = 'observed'),\n    forecast_df\n)\n\nobs_forecast_df |&gt; \n    ggplot(aes(t, y)) + \n    geom_point(aes(shape = type, colour = type)) +\n    geom_line(aes(linetype = type)) + \n    scale_linetype_manual(values = c(\"observed\" = 'solid', 'forecast' = 'dashed'))\n\n\n\n\n\nSo we can see that the forecast sequence of values looks quite similar to the differenced observations before it.\nIn order to use this for forecasting values, rather than differences, we therefore have to take the last observed value, and keep adding the consecutive forecast values.\n\n\nCode\nlast_obs_y &lt;- df |&gt; filter(t == max(t)) |&gt; pull(y)\n\naccumulated_new_draws &lt;- cumsum(new_draws)\n\naccumulated_new_draws\n\n\n [1] 0.05376336 0.69255439 0.58582797 1.05596081 0.97180393 0.92993832\n [7] 0.93243481 1.41386835 1.33174898 1.41787402\n\n\nCode\nforecast_values &lt;- last_obs_y + accumulated_new_draws\n\nforecast_df &lt;- tibble(\n    t = seq(from = max_t+1, to = max_t + length(new_draws)),\n    y = forecast_values,\n    type = \"forecast\"\n)\n\nobs_forecast_df &lt;- bind_rows(\n    df |&gt; mutate(type = 'observed'),\n    forecast_df\n)\n\nobs_forecast_df |&gt; \n    ggplot(aes(t, y)) + \n    geom_point(aes(shape = type, colour = type)) +\n    geom_line(aes(linetype = type)) + \n    scale_linetype_manual(values = c(\"observed\" = 'solid', 'forecast' = 'dashed'))\n\n\n\n\n\nSo, after integrating (accumulating or summing up) the modelled differenced values, we now see the forecast values continuing the upwards trend observed in the original data.\nOf course, there’s nothing special about the specific sequence of draws generated from the model. We could run the same exercise multiple times and each time get a different sequence of model draws, and so a different forecast path. Let’s see ten draws, for example:\n\n\nCode\nmake_multiple_paths &lt;- function(path_length, n_reps, start_value, mu, sigma, start_t){\n\n    make_path &lt;- function(start_t, mu, sigma, path_length, start_value) {\n        draws &lt;- rnorm(path_length, mean = mu, sd = sigma)\n\n        summed_values &lt;- cumsum(draws)\n        forecast_values &lt;- summed_values + start_value\n\n        out &lt;- tibble(\n            t = seq(from = start_t, to = start_t + path_length - 1),\n            y = forecast_values\n        )\n        return(out)\n    }\n\n    paths &lt;- replicate(\n        n_reps, \n        make_path(\n                    start_t = start_t, \n                    mu = mu, sigma = sigma, path_length = path_length, start_value = start_value\n            )\n    ) |&gt; \n        apply(2, as.data.frame)\n\n    out &lt;- tibble(\n        rep_num = 1:n_reps,\n        data = paths\n    ) |&gt;\n      unnest(data)\n\n    return(out)\n}\n\n\npaths &lt;- make_multiple_paths(\n    path_length = 10, \n    n_reps = 10, \n    mu = mean(d_df$y, na.rm = TRUE),\n    sigma = sd(d_df$y, na.rm = TRUE),\n    start_value = last_obs_y, \n    start_t = max(d_df$t) + 1\n)\n\ndf |&gt;\n    ggplot(aes(t, y)) + \n    geom_point() + geom_line() +\n    geom_line(aes(t, y, group = rep_num), inherit.aes = FALSE, data = paths, alpha = 0.5, colour = \"blue\")\n\n\n\n\n\nThis gives a sense of the kinds of upwards-drifting walks are compatible with the amount of variation observed in the original data series. If we ran the experiment another 10 times, we would get another ten paths.\nIn fact, we could generate a much larger number of simulations, say 10,000, and then report the range of values within which (say) 50% or 90% of the values for each time period are contained:\n\n\nCode\nmany_paths &lt;- make_multiple_paths(\n    path_length = 10, \n    n_reps = 10000, \n    mu = mean(d_df$y, na.rm = TRUE),\n    sigma = sd(d_df$y, na.rm = TRUE),\n    start_value = last_obs_y, \n    start_t = max(d_df$t) + 1\n)\n\nmany_paths_summary &lt;- many_paths |&gt;\n    group_by(t) |&gt; \n    summarise(\n        med = median(y), \n        lq = quantile(y, probs = 0.25), uq = quantile(y, probs = 0.75), l5 = quantile(y, probs = 0.05), u5 = quantile(y, probs = 0.95)) |&gt; \n    ungroup()\n\ndf |&gt;\n    ggplot(aes(t, y)) + \n    geom_point() + geom_line() +\n    geom_line(\n        aes(t, med), inherit.aes = FALSE, data = many_paths_summary, colour = \"blue\", linewidth = 1.2\n    ) +\n    geom_ribbon(\n        aes(t, ymin = lq, ymax = uq), \n        inherit.aes = FALSE, data = many_paths_summary,\n        colour = NA, alpha = 0.25\n    ) + \n    geom_ribbon(\n        aes(t, ymin = l5, ymax = u5),\n        inherit.aes = FALSE, data = many_paths_summary,\n        colour = NA, alpha = 0.25\n    )\n\n\n\n\n\nThese produce the kinds of ‘fans of uncertainty’ we might be used to seeing from a time series forecast. Because of the large numbers of simulations run, the shape of the fans appear quite smooth, and close to the likely analytical solution.\n\n\nSumming up\nIn this post we’ve explored the second of the three main tools in the most common time series analytical toolkit: Integration. We’ve differenced our data once, which in time series parlance is represented by the shorthand d=1. Then we’ve integrated estimates we’ve produced from a model after differencing to represent random paths projecting forward from the observed data into a more uncertain future. Doing this multiple times has allowed us to represent uncertainty about these projections, and the ways that uncertainty increases the further we move from the observed data."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#why-is-integration-called-integration-not-differencing",
    "href": "pages/extra-courses/time-series/index.html#why-is-integration-called-integration-not-differencing",
    "title": "Time Series",
    "section": "Why is integration called integration not differencing?",
    "text": "Why is integration called integration not differencing?\nIn the above we have performed what in time series parlance would be called an I(1) operation, differencing the data once. But why is this referred to as integration, when we’re doing the opposite?\nWell, when it comes to transforming the time series data into something with IID properties, we are differentiating rather than integrating. But the flip side of this is that, if using model outputs based on differenced data for forecasting, we have to sum up (i.e. integrate) the values we generate in the order in which we generate them. So, the model works on the differenced data, but model forecasts work by integrating the random variables generated by the model working on the differenced data.\nLet’s explore what this means in practice. Let’s generate 10 new values from a model calibrated on the mean and standard deviation of the differenced data:\n\n\nCode\nnew_draws &lt;- rnorm(\n    10, \n    mean = mean(d_df$y, na.rm = TRUE),\n    sd = sd(d_df$y, na.rm = TRUE)\n)\n\nnew_draws\n\n\n [1]  0.053763359  0.638791031 -0.106726422  0.470132841 -0.084156880\n [6] -0.041865610  0.002496487  0.481433544 -0.082119365  0.086125038\n\n\nWe can append these to the end of our differenced data to see how this forecast series compared with the observed series:\n\n\nCode\nmax_t &lt;- max(d_df$t)\n\nforecast_df &lt;- tibble(\n    t = seq(from = max_t+1, to = max_t + length(new_draws)),\n    y = new_draws,\n    type = \"forecast\"\n)\n\nobs_forecast_df &lt;- bind_rows(\n    d_df |&gt; mutate(type = 'observed'),\n    forecast_df\n)\n\nobs_forecast_df |&gt; \n    ggplot(aes(t, y)) + \n    geom_point(aes(shape = type, colour = type)) +\n    geom_line(aes(linetype = type)) + \n    scale_linetype_manual(values = c(\"observed\" = 'solid', 'forecast' = 'dashed'))\n\n\n\n\n\nSo we can see that the forecast sequence of values looks quite similar to the differenced observations before it.\nIn order to use this for forecasting values, rather than differences, we therefore have to take the last observed value, and keep adding the consecutive forecast values.\n\n\nCode\nlast_obs_y &lt;- df |&gt; filter(t == max(t)) |&gt; pull(y)\n\naccumulated_new_draws &lt;- cumsum(new_draws)\n\naccumulated_new_draws\n\n\n [1] 0.05376336 0.69255439 0.58582797 1.05596081 0.97180393 0.92993832\n [7] 0.93243481 1.41386835 1.33174898 1.41787402\n\n\nCode\nforecast_values &lt;- last_obs_y + accumulated_new_draws\n\nforecast_df &lt;- tibble(\n    t = seq(from = max_t+1, to = max_t + length(new_draws)),\n    y = forecast_values,\n    type = \"forecast\"\n)\n\nobs_forecast_df &lt;- bind_rows(\n    df |&gt; mutate(type = 'observed'),\n    forecast_df\n)\n\nobs_forecast_df |&gt; \n    ggplot(aes(t, y)) + \n    geom_point(aes(shape = type, colour = type)) +\n    geom_line(aes(linetype = type)) + \n    scale_linetype_manual(values = c(\"observed\" = 'solid', 'forecast' = 'dashed'))\n\n\n\n\n\nSo, after integrating (accumulating or summing up) the modelled differenced values, we now see the forecast values continuing the upwards trend observed in the original data.\nOf course, there’s nothing special about the specific sequence of draws generated from the model. We could run the same exercise multiple times and each time get a different sequence of model draws, and so a different forecast path. Let’s see ten draws, for example:\n\n\nCode\nmake_multiple_paths &lt;- function(path_length, n_reps, start_value, mu, sigma, start_t){\n\n    make_path &lt;- function(start_t, mu, sigma, path_length, start_value) {\n        draws &lt;- rnorm(path_length, mean = mu, sd = sigma)\n\n        summed_values &lt;- cumsum(draws)\n        forecast_values &lt;- summed_values + start_value\n\n        out &lt;- tibble(\n            t = seq(from = start_t, to = start_t + path_length - 1),\n            y = forecast_values\n        )\n        return(out)\n    }\n\n    paths &lt;- replicate(\n        n_reps, \n        make_path(\n                    start_t = start_t, \n                    mu = mu, sigma = sigma, path_length = path_length, start_value = start_value\n            )\n    ) |&gt; \n        apply(2, as.data.frame)\n\n    out &lt;- tibble(\n        rep_num = 1:n_reps,\n        data = paths\n    ) |&gt;\n      unnest(data)\n\n    return(out)\n}\n\n\npaths &lt;- make_multiple_paths(\n    path_length = 10, \n    n_reps = 10, \n    mu = mean(d_df$y, na.rm = TRUE),\n    sigma = sd(d_df$y, na.rm = TRUE),\n    start_value = last_obs_y, \n    start_t = max(d_df$t) + 1\n)\n\ndf |&gt;\n    ggplot(aes(t, y)) + \n    geom_point() + geom_line() +\n    geom_line(aes(t, y, group = rep_num), inherit.aes = FALSE, data = paths, alpha = 0.5, colour = \"blue\")\n\n\n\n\n\nThis gives a sense of the kinds of upwards-drifting walks are compatible with the amount of variation observed in the original data series. If we ran the experiment another 10 times, we would get another ten paths.\nIn fact, we could generate a much larger number of simulations, say 10,000, and then report the range of values within which (say) 50% or 90% of the values for each time period are contained:\n\n\nCode\nmany_paths &lt;- make_multiple_paths(\n    path_length = 10, \n    n_reps = 10000, \n    mu = mean(d_df$y, na.rm = TRUE),\n    sigma = sd(d_df$y, na.rm = TRUE),\n    start_value = last_obs_y, \n    start_t = max(d_df$t) + 1\n)\n\nmany_paths_summary &lt;- many_paths |&gt;\n    group_by(t) |&gt; \n    summarise(\n        med = median(y), \n        lq = quantile(y, probs = 0.25), uq = quantile(y, probs = 0.75), l5 = quantile(y, probs = 0.05), u5 = quantile(y, probs = 0.95)) |&gt; \n    ungroup()\n\ndf |&gt;\n    ggplot(aes(t, y)) + \n    geom_point() + geom_line() +\n    geom_line(\n        aes(t, med), inherit.aes = FALSE, data = many_paths_summary, colour = \"blue\", linewidth = 1.2\n    ) +\n    geom_ribbon(\n        aes(t, ymin = lq, ymax = uq), \n        inherit.aes = FALSE, data = many_paths_summary,\n        colour = NA, alpha = 0.25\n    ) + \n    geom_ribbon(\n        aes(t, ymin = l5, ymax = u5),\n        inherit.aes = FALSE, data = many_paths_summary,\n        colour = NA, alpha = 0.25\n    )\n\n\n\n\n\nThese produce the kinds of ‘fans of uncertainty’ we might be used to seeing from a time series forecast. Because of the large numbers of simulations run, the shape of the fans appear quite smooth, and close to the likely analytical solution."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#summing-up-1",
    "href": "pages/extra-courses/time-series/index.html#summing-up-1",
    "title": "Time Series",
    "section": "Summing up",
    "text": "Summing up\nIn this post we’ve explored the second of the three main tools in the most common time series analytical toolkit: Integration. We’ve differenced our data once, which in time series parlance is represented by the shorthand d=1. Then we’ve integrated estimates we’ve produced from a model after differencing to represent random paths projecting forward from the observed data into a more uncertain future. Doing this multiple times has allowed us to represent uncertainty about these projections, and the ways that uncertainty increases the further we move from the observed data."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#coming-up-1",
    "href": "pages/extra-courses/time-series/index.html#coming-up-1",
    "title": "Time Series",
    "section": "Coming up",
    "text": "Coming up\nIn the next post, we will look at the final of the three tools in the standard time series toolkit: the moving average."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#recap-1",
    "href": "pages/extra-courses/time-series/index.html#recap-1",
    "title": "Time Series",
    "section": "Recap",
    "text": "Recap\nIn the last couple of posts, we looked first at autoregression (the AR(p) model), then integration (represented by the term d), as part of a more general strategy for modelling time series data. In this post, we’ll complete the trilogy, by looking at the Moving Average (MA(q)) model."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#the-moving-average-model-in-context-of-arima",
    "href": "pages/extra-courses/time-series/index.html#the-moving-average-model-in-context-of-arima",
    "title": "Time Series",
    "section": "The Moving Average Model in context of ARIMA",
    "text": "The Moving Average Model in context of ARIMA\nAs with the post on autoregression, it’s worth returning to what I’ve been calling The Mother Model (a general way of thinking about statistical models), and how AR and I relate to it:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nTo which we might also imagine adding a transformation or preprocessing step, \\(h(.)\\) on the dataset \\(D\\)\n\\[\nZ = h(D)\n\\]\nThe process of differencing the data is the transformation step employed most commonly in time series modelling. This changes the types of values that go into both the data input slot of the mother model, \\(X\\), and the output slot of the mother model, \\(Y\\). The type of data transformer, however, is deterministic, hence the use of the \\(=\\) symbol. This means an inverse transform function, \\(h^{-1}(.)\\) can be applied to the transformed data to convert it back to the original data:\n\\[\nD = h^{-1}(Z)\n\\]\nThe process of integrating the differences, including a series of forecast values from the time series models, constitutes this inverse transform function in the context of time series modelling, as we saw in the last post, on integration.\nAutoregression is a technique for working with either the untransformed data, \\(D\\), or the transformed data \\(Z\\), which operates on the systematic component of the mother model. For example, an AR(3) autoregressive model, working on data which have been differenced once (\\(Z_t = Y_t - Y_{t-1}\\)), may look as follows:\n\\[\nZ_t \\sim N(\\mu, \\sigma^2)\n\\] \\[\n\\mu = \\beta_0 + \\beta_1 Z_{t-1} + \\beta_2 Z_{t-2} + \\beta_3 Z_{t-3}\n\\]\nWhich more commonly will look like something like:\n\\[\nZ_t =  \\mu + \\beta_1 Z_{t-1} + \\beta_2 Z_{t-2} + \\beta_3 Z_{t-3} + \\epsilon\n\\]\nNote that these two approaches, AR and I, have involved operating on the systematic component and the preprocessing step, respectively. This gives us a clue about how the Moving Average (MA) modelling strategy is fundamentally different. Whereas AR models work on the systematic component (\\(g(.)\\)), MA models work on the stochastic component (\\(f(.)\\)). The following table summarises the distinct roles each technique plays in the general time series modelling strategy:\n\nAR, I, and MA in the context of ‘the Mother Model’\n\n\n\n\n\n\n\nTechnique\nWorks on…\nARIMA letter shorthand\n\n\n\n\nIntegration (I)\nData Preprocessing \\(h(.)\\)\nd\n\n\nAutoregression (AR)\nSystematic Component \\(g(.)\\)\np\n\n\nMoving Average (MA)\nStochastic Component \\(f(.)\\)\nq\n\n\n\nIn the above, I’ve spoiled the ending! The Autoregressive (AR), Integration (I), and Moving Average (MA) strategies are commonly combined into a single model framework, called ARIMA. ARIMA is a framework for specifying a family of models, rather than a single model, which differ by the amount of differencing (d), or autoregression terms (p), or moving average terms (q) which the model contains.\nAlthough in the table above, I’ve listed integration/differencing first, as it’s the data preprocessing step, the more conventional way of specifying an ARIMA model is in the order indicated in the acronym:\n\nAR: p\nI: d\nMA: q\n\nThis means ARIMA models are usually specified with a three value shorthand ARIMA(p, d, q). For example:\n\nARIMA(1, 1, 0): AR(1) with I(1) and MA(0)\nARIMA(0, 2, 2): AR(0) with I(2) and MA(2)\nARIMA(1, 1, 1): AR(1) with I(1) and MA(1)\n\nEach of these models is fundamentally different. But each is a type of ARIMA model.\nWith this broader context, about how MA models fit into the broader ARIMA framework, let’s now look at the Moving Average model:\n\nThe sound of a Moving Average\nThe intuition of a MA(q) model is in some ways easier to develop by starting not with the model equations, but with the following image:\n\n\n\nTibetan Singing Bowl\n\n\nThis is a Tibetan Singing Bowl, available from all good stockists (and Amazon), whose product description includes:\n\nErgonomic Design: The 3 inch singing bowl comes with a wooden striker and hand-sewn cushion which flawlessly fits in your hand. The portability of this bowl makes it possible to carry it everywhere you go.\nHolistic Healing : Our singing bowls inherently produces a deep tone with rich quality. The resonance emanated from the bowl revitalizes and rejuvenates all the body, mind and spirit. It acts as a holistic healing tool for overall well-being.\n\nNotwithstanding the claims about health benefits and ergonomics, the bowl is something meant to be hit by a wooden striker, and once hit makes a sound. This sound sustains over time (‘sings’), but as time goes on the intensity decays. As a sound wave, this might look something like the following:\n\n\nCode\nlibrary(tidyverse)\n\nA0 = 5\ndecay_rate = 1/12\nperiodicity &lt;- 5\ndelay &lt;- 7\n\ntibble(\n    t = seq(0, 100, by = 0.001)\n) |&gt;\n    mutate(\n        H_t = ifelse(t &lt; delay, 0, 1),\n        A_t = A0 * exp(-(decay_rate * (t - delay))),\n        c_t = cos((1/periodicity) * 2 * pi * (t - delay)),\n        f_t = H_t * A_t * c_t\n    ) |&gt;\n    ggplot(aes(t, f_t)) + \n    geom_line() + \n    labs(\n        y = \"f(t)\",\n        x = \"t\",\n        title = \"Intensity over time\"\n    ) + \n    geom_vline(xintercept = delay, linetype = \"dashed\", colour = \"red\")\n\n\n\n\n\nIn this figure, the red dashed line indicates when the wooden striker strikes the bowl. Before this time, the bowl makes no sound. Directly after the strike, the bowl is loudest, and over time the intensity of the sound waves emanating from the bowl decays. The striker can to some extent determine the maximum amplitude of the bowl, whereas it’s largely likely to be the properties of the bowl itself which determines how quickly or slowly the sound decays over time.\nHow does this relate to the Moving Average model? Well, if we look at the Interpretation section of the moving average model wikipedia page, we see the cryptic statement “The moving-average model is essentially a finite impulse response filter applied to white noise”. And if we then delve into the finite impulse response page we get the definition “a finite impulse response (FIR) filter is a filter whose impulse response (or response to any finite length input) is of finite duration, because it settles to zero in finite time”. Finally, if we go one level deeper into the wikirabbit hole, and enter the impulse response page, we get the following definition:\n\n[The] impulse response, or impulse response function (IRF), of a dynamic system is its output when presented with a brief input signal, called an impulse.\n\nIn the singing bowl example, the striker is the impulse, and the ‘singing’ of the bowl is its response."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#the-sound-of-a-moving-average",
    "href": "pages/extra-courses/time-series/index.html#the-sound-of-a-moving-average",
    "title": "Time Series",
    "section": "The sound of a Moving Average",
    "text": "The sound of a Moving Average\nThe intuition of a MA(q) model is in some ways easier to develop by starting not with the model equations, but with the following image:\n\n\n\nTibetan Singing Bowl\n\n\nThis is a Tibetan Singing Bowl, available from all good stockists (and Amazon), whose product description includes:\n\nErgonomic Design: The 3 inch singing bowl comes with a wooden striker and hand-sewn cushion which flawlessly fits in your hand. The portability of this bowl makes it possible to carry it everywhere you go.\nHolistic Healing : Our singing bowls inherently produces a deep tone with rich quality. The resonance emanated from the bowl revitalizes and rejuvenates all the body, mind and spirit. It acts as a holistic healing tool for overall well-being.\n\nNotwithstanding the claims about health benefits and ergonomics, the bowl is something meant to be hit by a wooden striker, and once hit makes a sound. This sound sustains over time (‘sings’), but as time goes on the intensity decays. As a sound wave, this might look something like the following:\n\n\nCode\nlibrary(tidyverse)\n\nA0 = 5\ndecay_rate = 1/12\nperiodicity &lt;- 5\ndelay &lt;- 7\n\ntibble(\n    t = seq(0, 100, by = 0.001)\n) |&gt;\n    mutate(\n        H_t = ifelse(t &lt; delay, 0, 1),\n        A_t = A0 * exp(-(decay_rate * (t - delay))),\n        c_t = cos((1/periodicity) * 2 * pi * (t - delay)),\n        f_t = H_t * A_t * c_t\n    ) |&gt;\n    ggplot(aes(t, f_t)) + \n    geom_line() + \n    labs(\n        y = \"f(t)\",\n        x = \"t\",\n        title = \"Intensity over time\"\n    ) + \n    geom_vline(xintercept = delay, linetype = \"dashed\", colour = \"red\")\n\n\n\n\n\nIn this figure, the red dashed line indicates when the wooden striker strikes the bowl. Before this time, the bowl makes no sound. Directly after the strike, the bowl is loudest, and over time the intensity of the sound waves emanating from the bowl decays. The striker can to some extent determine the maximum amplitude of the bowl, whereas it’s largely likely to be the properties of the bowl itself which determines how quickly or slowly the sound decays over time.\nHow does this relate to the Moving Average model? Well, if we look at the Interpretation section of the moving average model wikipedia page, we see the cryptic statement “The moving-average model is essentially a finite impulse response filter applied to white noise”. And if we then delve into the finite impulse response page we get the definition “a finite impulse response (FIR) filter is a filter whose impulse response (or response to any finite length input) is of finite duration, because it settles to zero in finite time”. Finally, if we go one level deeper into the wikirabbit hole, and enter the impulse response page, we get the following definition:\n\n[The] impulse response, or impulse response function (IRF), of a dynamic system is its output when presented with a brief input signal, called an impulse.\n\nIn the singing bowl example, the striker is the impulse, and the ‘singing’ of the bowl is its response."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#the-moving-average-equation",
    "href": "pages/extra-courses/time-series/index.html#the-moving-average-equation",
    "title": "Time Series",
    "section": "The moving average equation",
    "text": "The moving average equation\nNow, finally, let’s look at the general equation for a moving average model:\n\\[\nX_t = \\mu + \\sum_{i=1}^{q} \\theta_i \\epsilon_{t-i} + \\epsilon_t\n\\]\nFor a MA(q=2) model, for example, this would look like:\n\\[\nX_t = \\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2}\n\\]\nHere is something like the fundamental value or quality of the time series system. For the singing bowl, \\(\\mu = 0\\), but it can take any value. \\(\\epsilon_t\\) is intended to capture something like a ‘shock’ or impulse now which would cause its manifested value to differ from its fundamental, \\(\\epsilon_{t-1}\\) a ‘shock’ or impulse one time unit ago, and \\(\\epsilon_{t-2}\\) a ‘shock’ or impulse two time units ago.\nThe values \\(\\theta_i\\) are similar to the way the intensity of the singing bowl’s sound decays over time. They are intended to represent how much influence past ‘shocks’, from various recent points in history, have on the present value manifested. Larger values of \\(\\theta\\) indicate past ‘shocks’ that have larger influence on the present value, and smaller \\(\\theta\\) values less influence.\n\nAutoregression and moving average: the long and the short of it\nAutoregressive and moving average models are intended to be complementary in their function in describing a time series system: whereas Autoregressive models allow for long term influence of history, which can change the fundamentals of the system, Moving Average models are intended to represent transient, short term disturbances to the system. For an AR model, the system evolves in response to its past states, and so to itself. For a MA model, the system, fundamentally, never changes. It’s just constantly being ‘shocked’ by external events.\n\n\nConcluding remarks\nSo, that’s the basic intuition and idea of the Moving Average model. A system fundamentally never changes, but external things keep ‘happening’ to it, meaning it’s almost always different to its true, fundamental value. A boat at sea will fundamentally have a height of sea level. But locally sea level is always changing, waves from every direction, and various intensities, buffetting the boat up and down - above and below the fundamental sea level average at every moment.\nAnd the ‘sound’ of a moving average model is almost invariably likely to be less sonorous than that of a singing bowl. Instead of a neat sine wave, each shock is a random draw from a noisemaker (typically the Normal distribution). In this sense a more accurate analogy might not be a singing bowl, but a guitar amplifier, with a constant hum, but also with dodgy connections, constantly getting moved and adjusted, with each adjustment causing a short belt of white noise to be emanated from the speaker. A moving average model is noise layered upon noise."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#autoregression-and-moving-average-the-long-and-the-short-of-it",
    "href": "pages/extra-courses/time-series/index.html#autoregression-and-moving-average-the-long-and-the-short-of-it",
    "title": "Time Series",
    "section": "Autoregression and moving average: the long and the short of it",
    "text": "Autoregression and moving average: the long and the short of it\nAutoregressive and moving average models are intended to be complementary in their function in describing a time series system: whereas Autoregressive models allow for long term influence of history, which can change the fundamentals of the system, Moving Average models are intended to represent transient, short term disturbances to the system. For an AR model, the system evolves in response to its past states, and so to itself. For a MA model, the system, fundamentally, never changes. It’s just constantly being ‘shocked’ by external events."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#concluding-remarks",
    "href": "pages/extra-courses/time-series/index.html#concluding-remarks",
    "title": "Time Series",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nSo, that’s the basic intuition and idea of the Moving Average model. A system fundamentally never changes, but external things keep ‘happening’ to it, meaning it’s almost always different to its true, fundamental value. A boat at sea will fundamentally have a height of sea level. But locally sea level is always changing, waves from every direction, and various intensities, buffetting the boat up and down - above and below the fundamental sea level average at every moment.\nAnd the ‘sound’ of a moving average model is almost invariably likely to be less sonorous than that of a singing bowl. Instead of a neat sine wave, each shock is a random draw from a noisemaker (typically the Normal distribution). In this sense a more accurate analogy might not be a singing bowl, but a guitar amplifier, with a constant hum, but also with dodgy connections, constantly getting moved and adjusted, with each adjustment causing a short belt of white noise to be emanated from the speaker. A moving average model is noise layered upon noise."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#recap-and-purpose-of-this-post",
    "href": "pages/extra-courses/time-series/index.html#recap-and-purpose-of-this-post",
    "title": "Time Series",
    "section": "Recap and purpose of this post",
    "text": "Recap and purpose of this post\nThe last three posts have covered three of the main techniques - autoregression, integration, and moving average modelling - which combine to form the ARIMA model framework for time series analysis.\nThe purpose of this post is to look at an example (or maybe two) showing how ARIMA models are fit and employed in practice."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#setup",
    "href": "pages/extra-courses/time-series/index.html#setup",
    "title": "Time Series",
    "section": "Setup",
    "text": "Setup\nFor this post I’ll make use of R’s forecast package.\n\n\nCode\nlibrary(tidyverse)\nlibrary(forecast)"
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#dataset-used-airmiles",
    "href": "pages/extra-courses/time-series/index.html#dataset-used-airmiles",
    "title": "Time Series",
    "section": "Dataset used: Airmiles",
    "text": "Dataset used: Airmiles\nThe dataset I’m going to use is airmiles, an example dataset from the datasets package, which is included in most R sessions by default\n\n\nCode\nairmiles\n\n\nTime Series:\nStart = 1937 \nEnd = 1960 \nFrequency = 1 \n [1]   412   480   683  1052  1385  1418  1634  2178  3362  5948  6109  5981\n[13]  6753  8003 10566 12528 14760 16769 19819 22362 25340 25343 29269 30514\n\n\nThe first thing we notice with this dataset is that it is not in the kind of tabular format we may be used to. Let’s see what class the dataset is:\n\n\nCode\nclass(airmiles)\n\n\n[1] \"ts\"\n\n\nThe dataset is of class ts, which stands for time series. A ts data object is basically a numeric vector with various additional pieces of metadata attached. We can see these metadata fields are start date, end date, and frequency. The documentation for ts indicates that if frequency is 1, then the data are annual. As the series are at fixed intervals, with the start date and frequency specified, along with the length of the numeric vector, the time period associated with each value in the series can be inferred.6\n\nVisual inspection of airmiles\nWe can look at the data using the base graphics plot function:\n\n\nCode\nplot(airmiles)\n\n\n\n\n\nWe can see this dataset is far from stationary, being much higher towards the end of the series than at the start. This implies we should consider differencing the data to make it stationery. We can use the diff() function for this:\n\n\nCode\nplot(diff(airmiles))\n\n\n\n\n\nThis differenced series still doesn’t look like IID data. Remember that differencing is just one of many kinds of transformation (data pre-processing) we could consider. Also we can difference more than once.\nAs there cannot be negative airmiles, and the series looks exponential since the start of the series, we can can consider using a log transform:\n\n\nCode\nplot(log(airmiles))\n\n\n\n\n\nHere the data look closer to a straight line. Differencing the data now should help us get to something closer to stationary:\n\n\nCode\nplot(diff(log(airmiles)))\n\n\n\n\n\nMaybe we should also look at differencing the data twice:\n\n\nCode\nplot(diff(log(airmiles), differences = 2))\n\n\n\n\n\nMaybe this is closer to the kind of stationary series that ARIMA works best with?\n\n\nARIMA fitting for airmiles\nThe visual inspection above suggested the dataset definitely needs at least one differencing term applied to it, and might need two; and might also benefit from being pre-transformed by being logged. With the forecast package, we can pass the series to the auto.arima() function, which will use an algorithm to attempt to identify the best combination of p, d and q terms to use. We can start by asking auto.arima() to determine the best ARIMA specification if the only transformation allowed is that of differencing the data, setting the trace argument to TRUE to learn more about which model specifications the algorithm has considered:\n\n\nCode\nbest_arima_nolambda &lt;- auto.arima(\n    y = airmiles, \n    trace = TRUE\n)\n\n\n\n ARIMA(2,2,2)                    : Inf\n ARIMA(0,2,0)                    : 384.231\n ARIMA(1,2,0)                    : 375.735\n ARIMA(0,2,1)                    : 375.3\n ARIMA(1,2,1)                    : 376.9756\n ARIMA(0,2,2)                    : 377.1793\n ARIMA(1,2,2)                    : Inf\n\n Best model: ARIMA(0,2,1)                    \n\n\nCode\nsummary(best_arima_nolambda)\n\n\nSeries: airmiles \nARIMA(0,2,1) \n\nCoefficients:\n          ma1\n      -0.7031\ns.e.   0.1273\n\nsigma^2 = 1234546:  log likelihood = -185.33\nAIC=374.67   AICc=375.3   BIC=376.85\n\nTraining set error measures:\n                   ME    RMSE      MAE      MPE     MAPE      MASE       ACF1\nTraining set 268.7263 1039.34 758.5374 4.777142 10.02628 0.5746874 -0.2848601\n\n\nWe can see from the trace that a range of ARIMA specifications were considered, starting with the ARIMA(2,2,2). The selection algorithm used is detailed here, and employs a variation of AIC, called ‘corrected AIC’ or AICc, in order to compare the model specifications.\nThe algorithm arrives at ARIMA(0, 2, 1) as the preferred specification. That is: no autorgression (p=0), twice differenced (d=2), and with one moving average term (MA=1).\nThe Forecasting book linked to above also has a recommended modelling procedure for ARIMA specifications, and cautions that the auto.arima() function only performs part of this proceudure. In particular, it recommends looking at the residuals\n\n\nCode\ncheckresiduals(best_arima_nolambda)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,2,1)\nQ* = 4.7529, df = 4, p-value = 0.3136\n\nModel df: 1.   Total lags used: 5\n\n\nThe three plots show the model residuals as a function of time (top), the distribution of residuals (bottom right), and the auto-correlation function, ACF (bottom-left), which indicates how the errors at different lags are correlated with each other. It also returns a test score, where high P-values (substantially above 0.05) should be considered evidence that the residuals appear like white noise, and so (something like) no further substantial systematic information in the data exists to be represented in the model.\nIn this case, the test statistic p-value is 0.31, which should be reassuring as to the appropriateness of the model specification identified.\nFinally, we can use this model to forecast a given number of periods ahead. Let’s take this data to the 1990s, even though this is a dangerously long projection.\n\n\nCode\nbest_arima_nolambda |&gt; forecast(h=35) |&gt; autoplot()\n\n\n\n\n\nThe central projection (dark blue line) is almost linear, but the projection intervals are wide and growing, and include projection scenarios where the number of flights in the 1990s are somewhat lower than those in the 1960s. These wide intervals should be considered a feature rather than a bug with the approach, as the further into the future we project, the more uncertain we should become.\n\n\nARIMA modelling with an additional transformation.\nAnother option to consider within the auto.arima() function is to allow another parameter to be estimated. This is known as the lambda parameter and represents an additional possible transformation of the data before the differencing step. This lambda parameter is used as part of a Box-Cox Transformation, intended to stabilise the variance of the series. If the lambda parameter is 0, then this becomes equivalent to logging the data. We can allow auto.arima to select a Box-Cox Transformation by setting the parameter lambda = \"auto\"\n\n\nCode\nbest_arima_lambda &lt;- auto.arima(\n    y = airmiles, \n    trace = TRUE,\n    lambda = \"auto\"\n)\n\n\n\n ARIMA(2,1,2) with drift         : Inf\n ARIMA(0,1,0) with drift         : 190.0459\n ARIMA(1,1,0) with drift         : 192.1875\n ARIMA(0,1,1) with drift         : 192.1483\n ARIMA(0,1,0)                    : 212.0759\n ARIMA(1,1,1) with drift         : 195.1062\n\n Best model: ARIMA(0,1,0) with drift         \n\n\nCode\nsummary(best_arima_lambda)\n\n\nSeries: airmiles \nARIMA(0,1,0) with drift \nBox Cox transformation: lambda= 0.5375432 \n\nCoefficients:\n        drift\n      18.7614\ns.e.   2.8427\n\nsigma^2 = 194.3:  log likelihood = -92.72\nAIC=189.45   AICc=190.05   BIC=191.72\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE      MASE      ACF1\nTraining set 123.1317 934.6956 724.6794 -5.484572 12.91378 0.5490357 -0.169863\n\n\nIn this case, a lambda value of about 0.54 has been identified, and a different ARIMA model specification selected. This specification is listed as ARIMA(0,1,0) with drift. This with drift term means the series are recognised as non-stationary, but where (after transformation) there is an average (in this case) constant amount upwards drift in the values as we progress through the series. 7 Let’s check the residuals for this model:\n\n\nCode\ncheckresiduals(best_arima_lambda)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,0) with drift\nQ* = 3.9064, df = 5, p-value = 0.563\n\nModel df: 0.   Total lags used: 5\n\n\nThe test P-value is even higher in this case, suggesting the remaining residuals appear to behave even more like white noise than in the previous specification.\nNow to look at projections from the model into the 1990s:\n\n\nCode\nbest_arima_lambda |&gt; forecast(h=35) |&gt; autoplot()\n\n\n\n\n\nUsing this specification we get a qualitatively different long-term projection with, on the identity scale of the data itself, a much narrower long-term projection interval.\n\n\nComparing model specifications\nSo, the two different ARIMA specifications arrived at - one with additional pre-transformation of the data before differencing; the other without - lead to qualitatively different long-term projections. Do we have any reason to presume one specification is better than the other?\nI guess we could look at the AIC and BIC of the two models:\n\n\nCode\nAIC(best_arima_nolambda, best_arima_lambda)\n\n\n                    df      AIC\nbest_arima_nolambda  2 374.6684\nbest_arima_lambda    2 189.4459\n\n\nCode\nBIC(best_arima_nolambda, best_arima_lambda)\n\n\n                    df      BIC\nbest_arima_nolambda  2 376.8505\nbest_arima_lambda    2 191.7169\n\n\nHere the lower scores for the model with a Box-Cox transformation suggest it should be preferred. However, as both functions warn, the number of observations differ between the two specifications. This is likely because the no-lambda version differences the data twice, whereas the with-lambda specification differences the data once, and so the no-lambda version should have one fewer observation. Let’s check this:\n\n\nCode\nn_obs_nolambda &lt;- summary(best_arima_nolambda)$nobs\n\nn_obs_lambda &lt;- summary(best_arima_lambda)$nobs\n\nprint(paste(\"Observations for no lambda:\", n_obs_nolambda))\n\n\n[1] \"Observations for no lambda: 22\"\n\n\nCode\nprint(paste(\"Observations for with-lambda:\", n_obs_lambda))\n\n\n[1] \"Observations for with-lambda: 23\"\n\n\nYes. This seems to be the cause of the difference.\nAnother way of comparing the models is by using the accuracy() function, which reports a range of accuracy measures:\n\n\nCode\nprint(\"No lambda specification: \")\n\n\n[1] \"No lambda specification: \"\n\n\nCode\naccuracy(best_arima_nolambda)\n\n\n                   ME    RMSE      MAE      MPE     MAPE      MASE       ACF1\nTraining set 268.7263 1039.34 758.5374 4.777142 10.02628 0.5746874 -0.2848601\n\n\nCode\nprint(\"With-lambda specification: \")\n\n\n[1] \"With-lambda specification: \"\n\n\nCode\naccuracy(best_arima_lambda)\n\n\n                   ME     RMSE      MAE       MPE     MAPE      MASE      ACF1\nTraining set 123.1317 934.6956 724.6794 -5.484572 12.91378 0.5490357 -0.169863\n\n\nWhat’s returned by accuracy() comprises one row (labelled Training set) and seven columns, each for a different accuracy metric. A common (and relatively easy-to-understand) accuracy measure is RMSE, which stands for (square) root mean squared error. According to this measure, the Box-Cox transformed ARIMA model outperforms the untransformed (by double-differenced) ARIMA model, so perhaps it should be preferred.\nHowever, as the act of transforming the data in effect changes (by design) the units of the data, perhaps RMSE is not appropriate to use for comparison. Instead, there is a measure called MAPE, which stands for “mean absolute percentage error”, that might be more appropriate to use because of the differences in scales. According to this measure, the Box-Cox transformed specification has a higher error score than the no-lambda specification (around 13% instead of around 10%), suggesting instead the no-lambda specification should be preferred instead.\nSo what to do? Once again, the ‘solution’ is probably just to employ some degree of informed subjective judgement, along with a lot of epistemic humility. The measures above can help inform our modelling decisions, but they cannot make these decisions for us."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#discussion-and-coming-up",
    "href": "pages/extra-courses/time-series/index.html#discussion-and-coming-up",
    "title": "Time Series",
    "section": "Discussion and coming up",
    "text": "Discussion and coming up\nFor the first three posts in this time-series miniseries, we looked mainly at the theory of the three components of the ARIMA time series modelling approach. This is the first approach where we’ve used ARIMA in practice. Hopefully you got a sense of two different things:\n\nThat because of packages like forecast, getting R to produce and forecast from an ARIMA model is relatively quick and straightforward to do in practice.\nThat even in this brief applied example of applied time series, we started to learn about a range of concepts - such as the auto-correlation function (ACF), the Box-Cox transformation, and alternative measures of accuracy - which were not mentioned in the previous three posts on ARIMA.\n\nIndeed, if you review the main book associated with the forecasting package, you can see that ARIMA comprises just a small part of the overall time series toolkit. There’s a lot more that can be covered, including some methods that are simpler to ARIMA, some methods (in particular SARIMA) which are further extensions of ARIMA, some methods that are alternatives to ARIMA, and some methods that are decidedly more complicated than ARIMA. By focusing on the theory of ARIMA in the last three posts, I’ve aimed to cover something in the middle-ground of the overall toolbox.\nComing up: to be determined!\ntroduction\nIn previous posts on time series, we decomposed then applied a common general purpose modelling strategy for working with time series data called ARIMA. ARIMA model can involve autoregressive components (AR(p)), integration/differencing components (I(d)), and moving average components (MA(q)). As we saw, the time series data can also be pre-transformed, in ways other than just differencing; the example of this we saw was the application of the Box-Cox transformation for regularising the variance of the outcome, and includes logging of values as one possible transformation within the framework.\nThe data we used previous was annual data, showing the numbers of airmiles travelled in the USA by year up to the 1960s. Of course, however, many types of time series data are sub-annual, reported not just by year, but by quarter, or month, or day as well. Data disaggregated into sub-annual units often exhibit seasonal variation, patterns that repeat themselves at regular intervals within a 12 month cycle. 8\nIn this post we will look at some seasonal data, and consider two strategies for working with this data: STL decomposition; and Seasonal ARIMA (SARIMA)."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#an-example-dataset",
    "href": "pages/extra-courses/time-series/index.html#an-example-dataset",
    "title": "Time Series",
    "section": "An example dataset",
    "text": "An example dataset\nLet’s continue to use the examples and convenience functions from the forecast package used in the previous post, and for which the excellent book Forecasting: Principles and Practice is available freely online.\nFirst some packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(forecast)\nlibrary(fable)\n\n\nNow some seasonal data\n\n\nCode\n# Using this example dataset: https://otexts.com/fpp3/components.html\ndata(us_employment)\nus_retail_employment &lt;- us_employment |&gt;\n  filter(Title == \"Retail Trade\")\n\nus_retail_employment\n\n\n# A tsibble: 969 x 4 [1M]\n# Key:       Series_ID [1]\n      Month Series_ID     Title        Employed\n      &lt;mth&gt; &lt;chr&gt;         &lt;chr&gt;           &lt;dbl&gt;\n 1 1939 Jan CEU4200000001 Retail Trade    3009 \n 2 1939 Feb CEU4200000001 Retail Trade    3002.\n 3 1939 Mar CEU4200000001 Retail Trade    3052.\n 4 1939 Apr CEU4200000001 Retail Trade    3098.\n 5 1939 May CEU4200000001 Retail Trade    3123 \n 6 1939 Jun CEU4200000001 Retail Trade    3141.\n 7 1939 Jul CEU4200000001 Retail Trade    3100 \n 8 1939 Aug CEU4200000001 Retail Trade    3092.\n 9 1939 Sep CEU4200000001 Retail Trade    3191.\n10 1939 Oct CEU4200000001 Retail Trade    3242.\n# ℹ 959 more rows\n\n\nThere are two differences we can see with this dataset compared with previous time series data we’ve looked at.\nFirstly, the data looks like a data.frame object, or more specifically a tibble() (due to the additional metadata at the top). In fact they are of a special type of tibble called a tsibble, which is basically a modified version of a tibble optimised to work with time series data. We can check this by interrogating the class attributes of us_employment:\n\n\nCode\nclass(us_retail_employment)\n\n\n[1] \"tbl_ts\"     \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThese class attributes go broadly from the most specific type of object class: tbl_ts (the tsibble); to the most general type of object class: the data.frame.\nSecondly, we can see that the data are disaggregated not by year as in the last post’s example, but also by month. So, what does this monthly data actually look like?\n\n\nCode\nautoplot(us_retail_employment, Employed) +\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\n\n\n\n\nThis data looks… spikey. There’s clearly both a long-term trend - including periods of faster and slower growth, and occasionally some falls - but there’s also what looks like a series of near-vertical spikes along this trend, at what may be regular intervals. What happens if we zoom into a smaller part of the time series?\n\n\nCode\nautoplot(\n    us_retail_employment |&gt;\n        filter(year(Month) &gt;=1990), \n    Employed) +\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\n\n\n\n\nHere we can start to see there’s not just a single repeating ‘vertical spike’, but a pattern that appears to repeat within each year, for each year. Let’s zoom in even further, for just three years:\n\n\nCode\nautoplot(\n    us_retail_employment |&gt;\n        filter(between(year(Month), 1994, 1996)), \n    Employed) +\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\n\n\n\n\nAlthough each of these three years is different in terms of the average number of persons employed in retail, they are similar in terms of having a spike in employment towards the end of the year, then a drop off at the start of the year, then a relative plateau for the middle of the year.\nThis is an example of a seasonal pattern, information that gets revealed about a time series when we use a sub-annual resolution that might not be apparent it we used only annual data. How do we handle this kind of data?"
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#approach-one-reannualise",
    "href": "pages/extra-courses/time-series/index.html#approach-one-reannualise",
    "title": "Time Series",
    "section": "Approach one: reannualise",
    "text": "Approach one: reannualise\nOf course we could simply reaggregate the data to an annual series:\n\n\nCode\nus_retail_employment |&gt;\n    mutate(\n        year = year(Month)\n    ) |&gt;\n    ungroup() |&gt;\n    index_by(year) |&gt;\n    summarise(\n        Employed = sum(Employed)\n    ) %&gt;%\n    autoplot(., Employed)\n\n\n\n\n\nOne thing we can notice with this is that there appears to be a big drop in total employment for the last year. This is likely because the last year is incomplete, so whereas previous years are summing up 12 months’ observations, for the last year a smaller number of months are being summed up. We could then drop the last year:\n\n\nCode\nus_retail_employment |&gt;\n    mutate(\n        year = year(Month)\n    ) |&gt;\n    ungroup() |&gt;\n    index_by(year) |&gt;\n    summarise(\n        Employed = sum(Employed)\n    ) |&gt;\n    filter(year != max(year)) %&gt;%\n    autoplot(., Employed)\n\n\n\n\n\nBut then we are losing some data that we really have. Even if we don’t have the full year, we might be able to get a sense from just the first few months worth of data whether the overall values for the last year are likely to be up or down compared to the same month in the previous years. We could even turn this single annual time series into 12 separate series: comparing Januaries with Januaries, Februaries with Februaries, and so on.\n\n\nCode\nus_retail_employment |&gt;\n    mutate(\n        year = year(Month), \n        month = month(Month, label = TRUE )\n    ) |&gt;\n    ggplot(\n        aes(year, Employed)\n    ) + \n    facet_wrap(~month) + \n    geom_line()\n\n\n\n\n\nHere we can see that comparing annual month-by-month shows a very similar trend overall. It’s as if each month’s values could be thought of as part of an annual ‘signal’ (an underlying long-term trend) plus a seasonal adjustment up or down: compared with the annual trend, Novembers and Decembers are likely to be high, and Januaries and Februaries to be low; and so on.\nIt’s this intuition - That we have a trend component, and a seasonal component - which leads us to our second strategy: decomposition."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#approach-two-seasonal-composition",
    "href": "pages/extra-courses/time-series/index.html#approach-two-seasonal-composition",
    "title": "Time Series",
    "section": "Approach Two: Seasonal Composition",
    "text": "Approach Two: Seasonal Composition\nThe basic intuition of decomposition is to break sub-annual data into a series of parts: The underling long term trend component; and repeating (usually) annual seasonal component.\nA common method for performing this kind of decomposition is known as STL. This actually stands for Seasonal and Trend Decomposition using Loess (Where Loess is itself another acronym). However it’s heuristically easier to imagine it stands for Season-Trend-Leftover, as it tends to generate three outputs from a single time-series input that correspond to these three components. Let’s regenerate the example in the forecasting book and then consider the outputs further:\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n  model(\n    STL(Employed ~ trend(window = 7) +\n                   season(window = \"periodic\"),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nThe plotted output contain four rows. These are, respectively:\n\nTop Row: The input data from the dataset\nSecond Row: The trend component from STL decomposition\nThird Row: The seasonal component from the STL decomposition\nBottom Row: The remainder (or leftover) component from the STL decomposition.\n\nSo, what’s going on?\nSTL uses an algorithm to find a repeated sequence (the seasonal component) in the data that, once subtracted from a long term trend, leaves a remainder (set of errors or deviations from observations) that is minimised in some way, and ideally random like white noise.\nIf you expanded the code chunk above, you will see two parameters as part of the STL model: the window argument for a trend() function; and the window argument for a season() function. This implies there are ways of setting up STL differently, and these would produce different output components. What happens if we change the window argument to 1 (which I think is its smallest allowable value)?\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n    filter(year(Month) &lt;= 2017) |&gt;\n  model(\n    STL(Employed ~ trend(window = 1) +\n                   season(window = \"periodic\"),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nHere the trend component becomes, for want of a better term, ‘wigglier’. And the remainder term, except for a strange data artefact at the end, appears much smaller. So what does the window argument do?\nConceptually, what the window argument to trend() does is adjust the stiffness of the curve that the trendline uses to fit to the data. A longer window, indicated by a higher argument value, makes the curve stiffer, and a shorter window, indicated by a lower argument value, makes the curve less stiff. We’ve adjusted from the default window length of 7 to a much shorter length of 1, making it much less stiff.9 Let’s look at the effect of increasing the window length instead:\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n  model(\n    STL(Employed ~ trend(window = 31) +\n                   season(window = \"periodic\"),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nHere we can see that, as well as the trend term being somewhat smoother than when a size 7 window length was used, the remainder term, though looking quite noisy, doesn’t really look random anymore. In particular, there seems to be a fairly big jump in the remainder component in the late 2000s. The remainder series also does not particularly stationary, lurching up and down at particular points in the series.\nIn effect, the higher stiffness of the trend component means it is not able to capture and represent enough signal in the data, and so some of that ‘signal’ is still present in the remainder term, when it should be extracted instead.\nNow what happens if we adjust the window argument in the season() function instead?\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n  model(\n    STL(Employed ~ trend(window = 7) +\n                   season(window = 5),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nIn the above I’ve reduced the season window size (by default it’s infinite). Whereas before this seasonal pattern was forced to be constant for the whole time period, this time we an see that it changes, or ‘evolves’, over the course of the time series. We can also see that the remainder component, though looking quite random, now looks especially ‘spiky’, suggesting that the kinds of residuals left are somewhat further from Guassian white noise than in the first example.\n\nSection concluding thoughts\nSTL decomposition is one of a number of strategies for decomposition available to us. Other examples are described here. However the aims and principles of decomposition are somewhat similar no matter what approach is used.\nHaving performed a decomposition on time series data, we could potentially apply something like an ARIMA model to the trend component of the data alone for purposes of projection. If using a constant seasonal component, we could then add this component onto forecast values from the trend component, along with noise consistent with the properties of the remainder component. However, there is a variant of the ARIMA model specification that can work with this kind of seasonal data directly. Let’s look at that now"
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#approach-three-sarima",
    "href": "pages/extra-courses/time-series/index.html#approach-three-sarima",
    "title": "Time Series",
    "section": "Approach Three: SARIMA",
    "text": "Approach Three: SARIMA\nSARIMA stands for ‘Seasonal ARIMA’ (where of course ARIMA stands for Autoregressive-Integrated-Moving Average). Whereas an ARIMA model has a specification shorthand ARIMA(p, d, q), a SARIMA model has an extended specification: SARIMA(p, d, q) (P, D, Q)_S. This means that whereas ARIMA has three parameters to specify, a SARIMA model has seven. This might appear like a big jump in model complexity, but the gap from ARIMA to SARIMA is smaller than it first appears.\nTo see this it’s first noticing that, as well as terms p, d and q, there are also terms P, D and Q. This would suggest that whatever Autoregressive (p), integration (d) and moving average (q) processes are involved in standard ARIMA are also involved in another capacity in SARIMA. And what’s this other capacity? The clue to this is in the S term.\nS 10 stands for the seasonal component of the model, and specifies the number of observations that are expected to include a repeating seasonal cycle. As most seasonal cycles are annual, this means S will be 12 if the data are monthly, 4 if the data are quarterly, and so on.\nThe UPPERCASE P, D and Q terms then specify which standard ARIMA processes should be modelled as occurring every S steps in the data series. Although algebraically this means SARIMA models may look a lot more complicated than standard ARIMA models, it’s really the same process, and the same intuition, applied twice: to characterising the seasonal ‘signals’ in the time series, and to characteristing the non-seasonal ‘signals’ in the time series.\nAlthough there are important diagnostic charts and heuristics to use when determining and judging which SARIMA specification may be most appropriate for modelling seasonal data, such as the PACF and ACF, we can still use the auto.arima() function to see if the best SARIMA specification can be identified algorithmically:\n\n\nCode\nbest_sarima_model &lt;- auto.arima(as.ts(us_retail_employment, \"Employed\"))\nbest_sarima_model\n\n\nSeries: as.ts(us_retail_employment, \"Employed\") \nARIMA(1,1,2)(2,1,2)[12] \n\nCoefficients:\n         ar1      ma1     ma2     sar1     sar2    sma1     sma2\n      0.8784  -0.8428  0.1028  -0.6962  -0.0673  0.2117  -0.3873\ns.e.  0.0374   0.0481  0.0332   0.0977   0.0691  0.0937   0.0776\n\nsigma^2 = 1442:  log likelihood = -4832.08\nAIC=9680.16   AICc=9680.31   BIC=9719.06\n\n\nHere auto.arima() produced an ARIMA(1, 1, 2) (2, 1, 2)_12 specification, meaning p=1, d=1, q=2 for the non-seasonal part; and P=2, D=1, Q=2 for the seasonal part.\nWhat kind of forecasts does this produce?\n\n\nCode\nbest_sarima_model |&gt; \n  forecast(h=48) |&gt;\n  autoplot()\n\n\n\n\n\nWe can see the forecasts tend to repeat the seasonal pattern apparent throughout the observed data, and also widen in the usual way the further we move from the observed data.\n\nSumming up\nIn this post we have looked at three approaches for working with seasonal data: aggregating seasonality away; decomposition; and SARIMA. These are far from an exhaustive list, but hopefully illustrate some common strategies for working with this kind of data."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#summing-up-2",
    "href": "pages/extra-courses/time-series/index.html#summing-up-2",
    "title": "Time Series",
    "section": "Summing up",
    "text": "Summing up\nIn this post we have looked at three approaches for working with seasonal data: aggregating seasonality away; decomposition; and SARIMA. These are far from an exhaustive list, but hopefully illustrate some common strategies for working with this kind of data."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#time-series-recap",
    "href": "pages/extra-courses/time-series/index.html#time-series-recap",
    "title": "Time Series",
    "section": "Time Series recap",
    "text": "Time Series recap\nSo far in this short series on time series, we’ve looked at time series modelling from some first principles, learning how the types of data and challenge in time series analysis both are similar and different from those of statistical modelling more generally. We started by looking at the concept of auto-regression, then differentiation and integration, and then the moving average model specification, before combining these three components - AR, I, and MA - to produce the ARIMA model specification common in time series analysis. Afterwards, we then extended the ARIMA specification slightly to deal with seasonally varying data, the ARIMA specification begetting the Seasonal ARIMA modelling framework, or SARIMA. As part of the post on Seasonality, we also looked at time series decomposition, using the STL decomposition framework."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#aim-of-this-post",
    "href": "pages/extra-courses/time-series/index.html#aim-of-this-post",
    "title": "Time Series",
    "section": "Aim of this post",
    "text": "Aim of this post\nIn this post, we’ll take time series in a different direction, to show an application of multivariate regression common in time series, called vector autoregression (VAR). VAR is both simpler in some ways, and more complex in other ways, than SARIMA modelling. It’s simpler in that, as the name suggests, moving average (MA) terms tend to not be part of VAR models; we’ll also not be considering seasonality either. But it’s more complicated in the sense that we are jointly modelling two outcomes at the same time."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#model-family-tree",
    "href": "pages/extra-courses/time-series/index.html#model-family-tree",
    "title": "Time Series",
    "section": "Model family tree",
    "text": "Model family tree\nThe following figure aims to show the family resemblances between model specifications and challenges:\n\n\n\n\nflowchart TB \n    uvm(univariate models)\n    mvm(multivariate models)\n\n    ar(\"AR(p)\")\n    i(\"I(d)\")\n    ma(\"MA(q)\")\n    arima(\"ARIMA(p, d, q)\")\n    sarima(\"ARIMA(p, d, q)[P, D, Q]_s\")\n    var(VAR)\n\n    ar --&gt; var\n    mvm --&gt; var\n\n    i -.-&gt; var\n\n    uvm -- autoregression --&gt; ar\n    uvm -- differencing --&gt; i\n    uvm -- moving average --&gt; ma\n    ar & i & ma --&gt; arima\n\n    arima -- seasonality --&gt;  sarima\n\n\n\n\n\n\nSo, the VAR model is an extension of the autoregressive component of a standard, univariate AR(p) specification models to multivariate models. It can also include both predictor and response variables that are differenced, hence the the dashed line from I(d) to VAR."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#so-what-is-a-multivariate-model",
    "href": "pages/extra-courses/time-series/index.html#so-what-is-a-multivariate-model",
    "title": "Time Series",
    "section": "So what is a multivariate model?",
    "text": "So what is a multivariate model?\nYou might have seen the term multivariate model before, and think you’re familiar with what it means.\nIn particular, you might have been taught that whereas a univariate regression model looks something like this:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\epsilon\n\\]\nA multivariate regression model looks more like this:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n\\]\ni.e. You might have been taught that, if the predictors include one term for the intercept (the \\(\\beta_0\\) term) and one term for the slope (the \\(\\beta_1\\) term), then this is a univariate model. But if there are two or more terms that can claim to be ‘the slope’ then this is a multivariate model.\nHowever, this isn’t the real distinction between a univariate model and a multivariate model. To see this distinction we have to return, for the umpeenth time, to the ‘grandmother model’ specification first introduced at the start of the very first post:\nStochastic Component\n\\[\nY \\sim f(\\theta, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta = g(X, \\beta)\n\\]\nNow, both the response data, \\(Y\\), and the predictor data, \\(X\\), are both taken from the same rectangular dataset, \\(D\\). Let’s say this dataset, \\(D\\), has six rows and five columns. As a matrix it would look something like this:\n\\[\nD =\n\\begin{pmatrix}\nd_{1,1} & d_{1,2} & d_{1,3} & d_{1, 4} & d_{1,5} \\\\\nd_{2,1} & d_{2,2} & d_{2,3} & d_{2, 4} & d_{2,5} \\\\\nd_{3,1} & d_{3,2} & d_{3,3} & d_{3, 4} & d_{3,5} \\\\\nd_{4,1} & d_{4,2} & d_{4,3} & d_{4, 4} & d_{4,5} \\\\\nd_{5,1} & d_{5,2} & d_{5,3} & d_{5, 4} & d_{5,5} \\\\\nd_{6,1} & d_{6,2} & d_{6,3} & d_{6, 4} & d_{6,5}\n\\end{pmatrix}\n\\]\nHere the dataset \\(D\\) is made up of a whole series of elements \\(d_{i,j}\\), where the first subset value indicates the row number \\(i\\) and the second subset value indicates the column number \\(j\\). So, for example, \\(d_{5, 2}\\) indicates the value of the 5th row and 2nd column, whereas \\(d_{2, 5}\\) indicates the value of the 2nd row and 5th column.\nFundamentally, the first challenge in building a model is deciding which columns from \\(D\\) we put in the predictor matrix \\(X\\), and which parts we put into the response matrix \\(Y\\). For example, if we wanted to predict the third column \\(j=3\\) given the fifth column \\(j=5\\) our predictor and response matrices would look as follows:\n\n\n\\[\nY = \\begin{pmatrix}\nd_{1,3} \\\\\nd_{2,3} \\\\\nd_{3,3} \\\\\nd_{4,3} \\\\\nd_{5,3} \\\\\nd_{6,3}  \n\\end{pmatrix}\n\\]\n\n\\[\nX = \\begin{pmatrix}\n1 & d_{1,5} \\\\\n1 & d_{2,5} \\\\\n1 & d_{3,5} \\\\\n1 & d_{4,5} \\\\\n1 & d_{5,5} \\\\\n1 & d_{6,5}  \n\\end{pmatrix}\n\\]\n\n\nWhere does the column of 1s come from? This is how we specify, in matrix notation, that we want an intercept term to be calculated. Models don’t have to have intercept terms, but in almost all cases we’re likely to be familiar with, they tend to.\nLet’s say we now want to include two columns, 2 and 5, from \\(D\\) in the predictor matrix, leading to what’s commonly (and wrongly) called a ‘multivariate regression’. This means that \\(Y\\) stays the same, but X is now as follows:\n\\[\nX = \\begin{pmatrix}\n1 & d_{1,2}  & d_{1,5}\\\\\n1 & d_{2,2}  & d_{2,5}\\\\\n1 & d_{3,2}  & d_{3,5}\\\\\n1 & d_{4,2}  & d_{4,5}\\\\\n1 & d_{5,2}  & d_{5,5}\\\\\n1 & d_{6,2}  & d_{6,5}\n\\end{pmatrix}\n\\]\nNo matter now many columns we include in the predictor matrix, X, however, we still don’t have a real multivariate regression model specification. Even if X had a hundred columns, or a thousand, it would still not be a multivariate regression in the more technical sense of the term.\nInstead, here’s an example of a multivariate regression model:\n\n\n\\[\nY = \\begin{pmatrix}\nd_{1,1} & d_{1,3} \\\\\nd_{2,1} & d_{2,3} \\\\\nd_{3,1} & d_{3,3} \\\\\nd_{4,1} & d_{4,3} \\\\\nd_{5,1} & d_{5,3} \\\\\nd_{6,1} & d_{6,3}  \n\\end{pmatrix}\n\\]\n\n\\[\nX = \\begin{pmatrix}\n1 & d_{1,5} \\\\\n1 & d_{2,5} \\\\\n1 & d_{3,5} \\\\\n1 & d_{4,5} \\\\\n1 & d_{5,5} \\\\\n1 & d_{6,5}  \n\\end{pmatrix}\n\\]\n\n\nThis is an example of a multivariate regression model. We encountered it before when we used the multivariate normal distribution in post 12, and when we draw from the posterior distribution of Bayesian models in post 13, but this is the first time we’ve considered multivariate modelling in the context of trying to represent something we suspect to be true about the world, rather than our uncertainty about the world. And it’s the first example of multivariate regression we’ve encountered in this series. For every previous model, no matter how apparently disparate, complicated or exotic they may appear, they’ve been univariate regression models in the sense that the response component \\(Y\\) has always only contained one column only.\nSo, with this definition of multivariate regression, let’s now look at VAR as a particular application of multivariate regression used in time series."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#vector-autoregression",
    "href": "pages/extra-courses/time-series/index.html#vector-autoregression",
    "title": "Time Series",
    "section": "Vector Autoregression",
    "text": "Vector Autoregression\nLet’s start with a semi-technical definition:\n\nIn vector autoregression (VAR) the values of two or more outcomes, \\(\\{Y_1(T), Y_2(T)\\}\\), are predicted based on previous values of those same outcomes \\(\\{Y_1(T-k), Y_2(T-k)\\}\\), for various lag periods \\(k\\).\n\nWhere \\(Y\\) has two columns, and an AR(1) specification (i.e. k is just 1), how is this different from simply running two separate AR(1) regression models, one for \\(Y_1\\), and the other for \\(Y_2\\)?\nWell, graphically, two separate AR(1) models proposes the following paths of influence:\n\n\n\n\nflowchart LR\nY1_T[\"Y1(T)\"]\nY2_T[\"Y2(T)\"]\n\nY1_T1[\"Y1(T-1)\"]\nY2_T1[\"Y2(T-1)\"]\n\nY1_T1 --&gt; Y1_T\nY2_T1 --&gt; Y2_T\n\n\n\n\n\nBy contrast, the paths implied and allowed in the corresponding VAR(1) model look more like the following:\n\n\n\n\nflowchart LR\nY1_T[\"Y1(T)\"]\nY2_T[\"Y2(T)\"]\n\nY1_T1[\"Y1(T-1)\"]\nY2_T1[\"Y2(T-1)\"]\n\nY1_T1 & Y2_T1 --&gt; Y1_T & Y2_T\n\n\n\n\n\nSo, each of the two outcomes at time T is influenced both by its own previous value, but also by the previous value of the other outcome. This other outcome influence is what is represented in the figure above by the diagonal lines: from Y2(T-1) to Y1(T), and from Y1(T-1) to Y2(T).\nExpressed verbally, if we imagine two entities - self and other - tracked through time, self is influenced both by self’s history, but also by other’s history too."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#example-and-application-in-r",
    "href": "pages/extra-courses/time-series/index.html#example-and-application-in-r",
    "title": "Time Series",
    "section": "Example and application in R",
    "text": "Example and application in R\nIn a more substantivelly focused post, I discussed how I suspect economic growth and longevity growth trends are correlated. What I proposed doesn’t exactly lend itself to the simplest kind of VAR(1) model specification, because I suggested a longer lag between the influence of economic growth on longevity growth, and a change in the fundamentals of growth in both cases. However, as an example of VAR I will ignore these complexities, and use the data I prepared for that post:\n\n\nCode\nlibrary(tidyverse)\n\ngdp_growth_pct_series &lt;- read_csv(\"still-the-economy-both-series.csv\") \n\ngdp_growth_pct_series\n\n\n# A tibble: 147 × 5\n    ...1  year series            pct_change period\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt; \n 1     1  1949 1. Per Capita GDP     NA     Old   \n 2     2  1950 1. Per Capita GDP      2.20  Old   \n 3     3  1951 1. Per Capita GDP      2.92  Old   \n 4     4  1952 1. Per Capita GDP      1.36  Old   \n 5     5  1953 1. Per Capita GDP      5.07  Old   \n 6     6  1954 1. Per Capita GDP      3.87  Old   \n 7     7  1955 1. Per Capita GDP      3.55  Old   \n 8     8  1956 1. Per Capita GDP      1.28  Old   \n 9     9  1957 1. Per Capita GDP      1.49  Old   \n10    10  1958 1. Per Capita GDP      0.815 Old   \n# ℹ 137 more rows\n\n\nWe need to do a certain amount of reformatting to bring this into a useful format:\n\n\nCode\nwide_ts_series &lt;- \ngdp_growth_pct_series |&gt;\n    select(-c(`...1`, period)) |&gt;\n    mutate(\n        short_series = case_when(\n            series == \"1. Per Capita GDP\" ~ 'gdp',\n            series == \"2. Life Expectancy at Birth\" ~ 'e0',\n            TRUE ~ NA_character_\n        )\n    ) |&gt;\n    select(-series) |&gt;\n    pivot_wider(names_from = short_series, values_from = pct_change) |&gt;\n    arrange(year) |&gt;\n    mutate(\n        lag_gdp = lag(gdp),\n        lag_e0 = lag(e0)\n    ) %&gt;%\n    filter(complete.cases(.))\n\n\nwide_ts_series\n\n\n# A tibble: 71 × 5\n    year   gdp      e0 lag_gdp  lag_e0\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1  1951 2.92  -0.568    2.20   0.763 \n 2  1952 1.36   1.89     2.92  -0.568 \n 3  1953 5.07   0.388    1.36   1.89  \n 4  1954 3.87   0.530    5.07   0.388 \n 5  1955 3.55  -0.0570   3.87   0.530 \n 6  1956 1.28   0.385    3.55  -0.0570\n 7  1957 1.49   0.170    1.28   0.385 \n 8  1958 0.815  0.255    1.49   0.170 \n 9  1959 3.70   0.184    0.815  0.255 \n10  1960 5.72   0.268    3.70   0.184 \n# ℹ 61 more rows\n\n\nSo, we can map columns to parts of the VAR specification as follows:\n\nY1: gdp\nY2: e0 (life expectancy at birth)\nperiod T: gdp and e0\nperiod T-1: lag_gdp and lag_e0\n\nTo include two or more variables as the response part, \\(Y\\), of a linear model we can use the cbind() function to combine more than one variable to the left hand side of the linear regression formula for lm or glm:\n\n\nCode\nvar_model &lt;- lm(\n    cbind(gdp, e0) ~ lag_gdp + lag_e0,\n    data = wide_ts_series\n)\n\nvar_model\n\n\n\nCall:\nlm(formula = cbind(gdp, e0) ~ lag_gdp + lag_e0, data = wide_ts_series)\n\nCoefficients:\n             gdp       e0      \n(Intercept)   1.99440   0.28108\nlag_gdp       0.06173   0.01179\nlag_e0       -0.48525  -0.33063\n\n\nWe can see here that the model reports a small matrix of coefficients: three rows (one for each coefficient term) and two columns: one for each of the response variables. This is as we should expect.\nBack in part 12 of the series, we saw we could extract the coefficients, variance-covariance matrix, and error terms of a linear regression model using the functions coefficients, vcov, and sigma respectively.11 Let’s use those functions here too:\nFirst the coefficients\n\n\nCode\ncoefficients(var_model)\n\n\n                    gdp          e0\n(Intercept)  1.99439587  0.28108028\nlag_gdp      0.06172721  0.01178781\nlag_e0      -0.48524808 -0.33062640\n\n\nAnd now the variance-covariance matrix:\n\n\nCode\nvcov(var_model)\n\n\n                gdp:(Intercept)   gdp:lag_gdp    gdp:lag_e0 e0:(Intercept)\ngdp:(Intercept)    0.1804533467 -0.0272190933 -0.1129646084   0.0039007790\ngdp:lag_gdp       -0.0272190933  0.0164590434 -0.0180517467  -0.0005883829\ngdp:lag_e0        -0.1129646084 -0.0180517467  0.6290771233  -0.0024419053\ne0:(Intercept)     0.0039007790 -0.0005883829 -0.0024419053   0.0037904364\ne0:lag_gdp        -0.0005883829  0.0003557878 -0.0003902165  -0.0005717391\ne0:lag_e0         -0.0024419053 -0.0003902165  0.0135984779  -0.0023728303\n                   e0:lag_gdp     e0:lag_e0\ngdp:(Intercept) -0.0005883829 -0.0024419053\ngdp:lag_gdp      0.0003557878 -0.0003902165\ngdp:lag_e0      -0.0003902165  0.0135984779\ne0:(Intercept)  -0.0005717391 -0.0023728303\ne0:lag_gdp       0.0003457235 -0.0003791783\ne0:lag_e0       -0.0003791783  0.0132138133\n\n\nAnd finally the error terms\n\n\nCode\nsigma(var_model)\n\n\n      gdp        e0 \n2.6906051 0.3899529 \n\n\nThe coefficients returns the same kind of 3x2 matrix we saw previously: two models run simultaneously. The error terms is now a vector of length 2: one for each of these models. The variance-covariance matrix is a square matrix of dimension 6: i.e. 6 rows and 6 columns. This is the number of predictor coefficients in each model (the number of columns of \\(X\\), i.e. 3) times the number of models simultaneously run, i.e. 2.\n\\(6^2\\) is 36, which is the number of elements in the variance-covariance matrix of this VAR model. By contrast, if we had run two independent models - one for gdp and the other for e0 - we would have two 3x3 variance-variance matrices, producing a total of 18 12 terms. This should provide some reassurance that, when we run a multivariate regression model of two outcomes, we’re not just doing the equivalent of running separate regression models for each outcome, but in slightly fewer lines.\nNow, let’s look at the model summary:\n\n\nCode\nsummary(var_model)\n\n\nResponse gdp :\n\nCall:\nlm(formula = gdp ~ lag_gdp + lag_e0, data = wide_ts_series)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.679  -1.061   0.143   1.483   6.478 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.99440    0.42480   4.695 1.34e-05 ***\nlag_gdp      0.06173    0.12829   0.481    0.632    \nlag_e0      -0.48525    0.79314  -0.612    0.543    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.691 on 68 degrees of freedom\nMultiple R-squared:  0.007555,  Adjusted R-squared:  -0.02163 \nF-statistic: 0.2588 on 2 and 68 DF,  p-value: 0.7727\n\n\nResponse e0 :\n\nCall:\nlm(formula = e0 ~ lag_gdp + lag_e0, data = wide_ts_series)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.45680 -0.19230 -0.00385  0.24379  1.38706 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.28108    0.06157   4.565 2.15e-05 ***\nlag_gdp      0.01179    0.01859   0.634  0.52823    \nlag_e0      -0.33063    0.11495  -2.876  0.00537 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.39 on 68 degrees of freedom\nMultiple R-squared:  0.1086,    Adjusted R-squared:  0.08243 \nF-statistic: 4.144 on 2 and 68 DF,  p-value: 0.02003\n\n\nThe summary is now reported for each of the two outcomes: first gdp, then e0.\nRemember that the outcome is percentage annual change in the outcome of interest from the previous year. i.e. both series have already been ‘differenced’ to produce approximately stationary series. It also means that the intercept terms are especially important, as they indicate the long-term trends observed in each series.\nIn this case the intercepts for both series are positive and statistically significant: over the long term, GDP has grown on average around 2% each year, and life expectancy by around 0.28%. As the post this relates to makes clear, however, these long-term trends may no longer apply.\nOf the four lag (AR(1)) terms in the model(s), three are not statistically significant; not even close. The exception is the lag_e0 term for the e0 response model, which is statistically significant and negative. Its coefficient is also of similar magnitude to the intercept too.\nWhat does this mean in practice? In effect, that annual mortality improvement trends have a tendency to oscillate: a better-than-average year tends to be followed by a worse-than-average year, and a worse-than-average year to be followed by a better-than-average year, in both cases at higher-than-chance rates.\nWhat could be the cause of this oscillatory phenomenon? When it comes to longevity, the phenomenon is somewhat well understood (though perhaps not widely enough), and referred to as either ‘forward mortality displacement’ or, more chillingly, ‘harvesting’. This outcome likely comes about because, if there were an exceptionally bad year in terms of (say) influenza mortality, the most frail and vulnerable are likely to be those who die disproportionately from this additional mortality event. This means that the ‘stock’ of people remaining the following year have been selected, on average, to be slightly less frail and vulnerable than those who started the previous year. Similarly, an exceptionally ‘good’ year can mean that the average ‘stock’ of the population in the following year is slightly more frail than in an average year, so more susceptible to mortality. And so, by this means, comparatively-bad-years tend to be followed by comparatively-good-years, and comparatively-good-years by comparatively-bad-years.\nThough this general process is not pleasant to think about or reason through, statistical signals such as the negative AR(1) coefficient identified here tend to keep appearing, whether we are looking for them or not."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#conclusion",
    "href": "pages/extra-courses/time-series/index.html#conclusion",
    "title": "Time Series",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post we’ve both concluded the time series subseries, and returned to and expanded on a few posts earlier in the series. This includes the very first post, where we were first introduced to the grandmother formulae, the posts on statistical modelling using both frequentist and Bayesian methods, and a substantive post linking life expectancy with economic growth.\nAlthough we’ve now first encountered multivariate regression models in the context of time series, they are a much more general phenomenon. Pretty much any type of model we can think of and apply in a univariate fashion - where \\(Y\\) has just a single column - can conceivably be expanded to two mor more columns, leading to their more complicated multiple regression variants."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#recap-2",
    "href": "pages/extra-courses/time-series/index.html#recap-2",
    "title": "Time Series",
    "section": "Recap",
    "text": "Recap\nIn the last few posts I’ve walked through some of the key concepts in time series, but focused on a particular modelling framework: the ARIMA model specification. Let’s recap what we’ve covered:\n\nIn post one, we discussed autoregression, and more generally the way that data including repeated measures of a single observation, or just a few observations, can still be treated largely like other types of data suitable for statistical modelling, through the inclusion of enough previous values as predictors that, once included, the observations can be considered as approximately independent of each other.\nIn post two, we discussed differencing and integration: an operation for trying to make non-stationary data stationary; and a reverse operation for starting to build forecasts based on such differenced data.\nIn post three, we discussed the intuition behind the moving average model: a way of thinking about time series as something analogous to a noisy singing bowl: a system that ‘wants’ to return to a position of either rest or a fundamental ‘tone’, but which is forever being subjected both to contemporary disturbances, and the influence of past disturbances.\nIn post four, we integrated the components of the first three posts - autoregression AR(p), integration I(d), and moving averages MA(q) - to produce the basic ARIMA(p, d, q) model specification, and saw some examples of trying to run and use this specification in practice.\nIn post five, we covered the topic of seasonality: repeated patterns over time that repeat in predictable ways over known and predictable time periods. We looked at both seasonal decomposition of such data into seasonal, trend, and (heuristically) ‘leftover’ components using the STL approach; and extending the ARIMA model specification to incorporate seasonality using the Seasonal ARIMA, or SARIMA, modelling framework.\nIn post six, we took some of the ideas covered in posts one and two and extended then in another way, to build the intuitions behind a vector autoregressive (or VAR) model specification. This post can be considered both an extension of some of the aspects covered elsewhere in the time series series, but also an extension of the discussions in our series on generalised modelling in general, and using models for prediction and simulation, as it was the first time we encountered multivariate models, i.e. models in which we aim to fit more than one outcome or response at a single time."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#whats-missing",
    "href": "pages/extra-courses/time-series/index.html#whats-missing",
    "title": "Time Series",
    "section": "What’s missing?",
    "text": "What’s missing?\nIn choosing to focus on the ARIMA modelling specification, we necessarily didn’t cover some other approaches to time series. This is similar to our series on causal inference, which stuck largely to one of the two main frameworks for thinking about the problems of causal inference - Rubin’s Missing Data framework - and only gave some brief coverage and attempt at consiliation of the other framework - the Pearlean graph-based framework - in the final post.\nSo, like the final post of the Causal Inference series, let’s discuss a couple of key areas that I’ve not covered in the series so far: state space models; and demographic models.\n\nState Space Models and ETS\nBorrowing concepts from control theory - literally rocket science! - and requiring an unhealthy level of knowledge of linear algebra to properly understand and articulate, state space models represent a 1990s statistical formalisation of - mainly - a series of model specifications first developed in the 1950s. In this sense they seem to bookend ARIMA model specifications, which were first developed in the 1970s. The 1950s models were based around the concept of exponential smoothing: the past influences the present, but the recent past influences the present more than the distant past. 13\nState space models involve solving (or getting a computer to solve) a series of simultaneous equations that link observed values over time \\(y_t\\) to a series of largely unobserved and unobservable model parameters. The key conceptual link between state space models and control theory is that these observed parameters are allowed to change/evolve over time, in response to the degree of error between predicted and observed values at different points in time.\nTo conceptualise what’s going on, think of a rocket trying to hit another moving target: the target the rocket’s guidance system is tracking keeps changing, as does the position of the rocket relative to its target. So, the targetting parameters used by the rocket to ensure it keeps track of the target need to keep getting updated too. And more recent observations of the target’s location are likely to be more important to determining where the rocket should move, and so how its parameters should be updated, than older observations. Also, the older observations are already in a sense incorporated into the system, as they influenced past decisions about the rocket’s parameters, and so its trajectory in the past, and so its current position. So, given the rocket’s current position, the most recent target position may be the only information needed, now, to decide how much to update the parameters.\nTo add 14 to the analogy (which in some use-cases may have not been an analogy), we could imagine having some parameters which determine how quickly or slowly other parameters get updated. Think of a dial that affects the stiffness of another dial: when this first dial is turned down, the second dial can be moved clockwise or counterclockwise very quickly in response to the moving target; when the first dial is turned up, the second dial is more resistant to change, so takes longer to turn: this is another way of thinking about what exponential smoothing means in practice. There’s likely to be a sweet spot when it comes to this first type of dial: too stiff is bad, as it means the system takes a long time to adjust to the moving target; but too responsive is bad too, as it means the system could become very unstable very quickly. Both excess stiffness and excess responsiveness can contribute to the system (i.e. our model predictions) getting further and further away from its target, and so to greater error.\nIn practice, with the excellent packages associated with Hyndman and Athanasopoulos’s excellent Forecasting book, we can largely ignore some of the more technical aspects of using state space models for time series forecasting with exponential smoothing, and just think of such models by analogy with ARIMA models, as model frameworks with a number of parameters to either select, or use heuristic or algorithmic methods to select for us. With ARIMA with have three parameters: for autoregression (p), differencing (d), and moving average (q); and with Seasonal ARIMA each of these receives a seasonal pair: p pairs with its seasonal analogue, P; d pairs with its seasonal analogue D; and q with its seasonal analogue Q.\nThe exponential smoothing analogue of the ARIMA framework is known as ETS, which stands for error-trend-season. Just as ARIMA model frameworks take three ‘slots’ - an AR() slot, an I() slot, and a MA() slot - ETS models also have three slots to be filled. However, these three slots don’t take numeric values, but the following arguments:\n\nThe error slot E(): takes N for ‘none’, A for ‘additive’, or M for ‘multiplicative’\nThe trend slot T(): takes N for ‘none’, A for ‘additive’, or A_d for ‘additive-damped’\nThe seasonal slot S(): takes N for ‘none’, A for ‘additive’, or M for ‘multiplicative’.\n\nSo, regardless of their different backgrounds, we can use ETS models as an alternative to, and in a very similar way to, ARIMA and SARIMA models.15\n\n\nDemographic forecasting models\nAnother (more niche) type of forecasting model I’ve not covered in this series are those used in demographic forecasting, such as the Lee-Carter model specification and its derivatives. When forecasting life expectancy, we could simply model life expectancy directly… or we could model each of its components together: the mortality rates at each specific year of age, and then derive future life expectancies from what the forecast age specific life expectancies at different ages imply the resultant life expectancy should be (i.e. perform a life table calculation on the projected/forecast values for a given future year, much as we do for observed data). This is our second example of multivariate regression, which we were first introduced to earlier. However, Lee-Carter style models can involve projecting forward dozens, if not over a hundred, response values at the same time, whereas in the VAR model example we just had two response values.\nLee-Carter style models represent an intersection between forecasting and factor analysis, due to the way the observed values of many different age-specific mortality rates are assumed to be influenced by, and provide information that can inform, an underlying latent variable known as the drift parameter. Once (something like) factor analysis is used to determine this drift parameter, the (logarithm of the) mortality rates at each specific age are assumed to follow this drift approximately, subject to some random variation, meaning the time series specification they follow is random-walk-with-drift (RWD), which (I think) is an ARIMA(0, 1, 0) specification. This assumption of a single underlying latent drift parameter influencing all ages has been criticised as perhaps too strong a structural assumption, leading both to the suggestion that each age-specific mortality rate should be forecast independently with RWD, or that the Lee-Carter assumptions implicit in its specification be relaxed in a more piecemeal fashion, leading to some of the alternative longevity forecasting models summarised and evaluated in this paper, and this paper.\nThe overlap between general time series and demographic forecasting is less tenuous than it might first appear, when you consider that one of the authors of the first of the two papers above is none other than Rob Hyndman, whose forecasting book I’ve already referenced and made use of many times in this series. Hyndman is also the maintainer of R’s demography package. So, much of what can be learned about time series can be readily applied to demography too.\nFinally, demographic forecasting models in which age-specific mortality over time is modelled open up another way of thinking about the problem: that of spatial statistics. Remember that with exponential smoothing models the assumed information value of models declines exponentially with time? As mentioned in an earlier footnote this is largely what’s known as Tobler’s First Law of Geography, but applied to just a single dimension (time) rather than two spatial dimensions such as latitude and longitude. Well, with spatial models there are two spatial dimensions, both of which have the same units (say, metres, kilometres, etc). Two points can be equally far apart, but along different spatial dimensions. Point B could be 2km east of point A, or 2km north of point A, or 2km north-east of point A.16 In each case, the distance between points is the same, and so the amount of downweighting of the information value of point B as to the true value of point A would be the same.\nWell, with the kind of data used by Lee-Carter style models, we have mortality rates that are double indexed: \\(m_{x,t}\\), where \\(x\\) indexes the age in years, and \\(t\\) indexes the time in years. Note the phrase in years: both age and time are in the same unit, much as latitude and longitude are in the same unit. So it makes sense to think of two points on the surface - \\(m_{a,b}\\) and \\(m_{c,d}\\) - as being a known distance apart from each other,17 and so for closer observations to be more similar or informative about each other than more distant values. This kind of as-if-spatial reasoning leads to thinking about how a surface of mortality rates might be smoothed appropriately, with estimates for each point being a function of the observed value of that point, plus some kind of exponentially weighted average of more and less proximately neighbouring points. (See, for example, Girosi & King’s framework)\n\n\nSumming up\nIn this last post, we’ve scratched the surface on a couple of areas related to time series that the main post series didn’t cover: state space modelling and demographic forecasting models. Although hopefully we can agree that they’re both interesting topics, they’ve also helped to illustrate why the main post series has been anchored around the ARIMA modelling framework. Time series is a complex area, and so by sticking with ARIMA, we’ve managed to avoid getting too far adrift."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#state-space-models-and-ets",
    "href": "pages/extra-courses/time-series/index.html#state-space-models-and-ets",
    "title": "Time Series",
    "section": "State Space Models and ETS",
    "text": "State Space Models and ETS\nBorrowing concepts from control theory - literally rocket science! - and requiring an unhealthy level of knowledge of linear algebra to properly understand and articulate, state space models represent a 1990s statistical formalisation of - mainly - a series of model specifications first developed in the 1950s. In this sense they seem to bookend ARIMA model specifications, which were first developed in the 1970s. The 1950s models were based around the concept of exponential smoothing: the past influences the present, but the recent past influences the present more than the distant past. 13\nState space models involve solving (or getting a computer to solve) a series of simultaneous equations that link observed values over time \\(y_t\\) to a series of largely unobserved and unobservable model parameters. The key conceptual link between state space models and control theory is that these observed parameters are allowed to change/evolve over time, in response to the degree of error between predicted and observed values at different points in time.\nTo conceptualise what’s going on, think of a rocket trying to hit another moving target: the target the rocket’s guidance system is tracking keeps changing, as does the position of the rocket relative to its target. So, the targetting parameters used by the rocket to ensure it keeps track of the target need to keep getting updated too. And more recent observations of the target’s location are likely to be more important to determining where the rocket should move, and so how its parameters should be updated, than older observations. Also, the older observations are already in a sense incorporated into the system, as they influenced past decisions about the rocket’s parameters, and so its trajectory in the past, and so its current position. So, given the rocket’s current position, the most recent target position may be the only information needed, now, to decide how much to update the parameters.\nTo add 14 to the analogy (which in some use-cases may have not been an analogy), we could imagine having some parameters which determine how quickly or slowly other parameters get updated. Think of a dial that affects the stiffness of another dial: when this first dial is turned down, the second dial can be moved clockwise or counterclockwise very quickly in response to the moving target; when the first dial is turned up, the second dial is more resistant to change, so takes longer to turn: this is another way of thinking about what exponential smoothing means in practice. There’s likely to be a sweet spot when it comes to this first type of dial: too stiff is bad, as it means the system takes a long time to adjust to the moving target; but too responsive is bad too, as it means the system could become very unstable very quickly. Both excess stiffness and excess responsiveness can contribute to the system (i.e. our model predictions) getting further and further away from its target, and so to greater error.\nIn practice, with the excellent packages associated with Hyndman and Athanasopoulos’s excellent Forecasting book, we can largely ignore some of the more technical aspects of using state space models for time series forecasting with exponential smoothing, and just think of such models by analogy with ARIMA models, as model frameworks with a number of parameters to either select, or use heuristic or algorithmic methods to select for us. With ARIMA with have three parameters: for autoregression (p), differencing (d), and moving average (q); and with Seasonal ARIMA each of these receives a seasonal pair: p pairs with its seasonal analogue, P; d pairs with its seasonal analogue D; and q with its seasonal analogue Q.\nThe exponential smoothing analogue of the ARIMA framework is known as ETS, which stands for error-trend-season. Just as ARIMA model frameworks take three ‘slots’ - an AR() slot, an I() slot, and a MA() slot - ETS models also have three slots to be filled. However, these three slots don’t take numeric values, but the following arguments:\n\nThe error slot E(): takes N for ‘none’, A for ‘additive’, or M for ‘multiplicative’\nThe trend slot T(): takes N for ‘none’, A for ‘additive’, or A_d for ‘additive-damped’\nThe seasonal slot S(): takes N for ‘none’, A for ‘additive’, or M for ‘multiplicative’.\n\nSo, regardless of their different backgrounds, we can use ETS models as an alternative to, and in a very similar way to, ARIMA and SARIMA models.15"
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#demographic-forecasting-models",
    "href": "pages/extra-courses/time-series/index.html#demographic-forecasting-models",
    "title": "Time Series",
    "section": "Demographic forecasting models",
    "text": "Demographic forecasting models\nAnother (more niche) type of forecasting model I’ve not covered in this series are those used in demographic forecasting, such as the Lee-Carter model specification and its derivatives. When forecasting life expectancy, we could simply model life expectancy directly… or we could model each of its components together: the mortality rates at each specific year of age, and then derive future life expectancies from what the forecast age specific life expectancies at different ages imply the resultant life expectancy should be (i.e. perform a life table calculation on the projected/forecast values for a given future year, much as we do for observed data). This is our second example of multivariate regression, which we were first introduced to in the post on vector autoregression VAR. However, Lee-Carter style models can involve projecting forward dozens, if not over a hundred, response values at the same time, whereas in the VAR model example we just had two response values.\nLee-Carter style models represent an intersection between forecasting and factor analysis, due to the way the observed values of many different age-specific mortality rates are assumed to be influenced by, and provide information that can inform, an underlying latent variable known as the drift parameter. Once (something like) factor analysis is used to determine this drift parameter, the (logarithm of the) mortality rates at each specific age are assumed to follow this drift approximately, subject to some random variation, meaning the time series specification they follow is random-walk-with-drift (RWD), which (I think) is an ARIMA(0, 1, 0) specification. This assumption of a single underlying latent drift parameter influencing all ages has been criticised as perhaps too strong a structural assumption, leading both to the suggestion that each age-specific mortality rate should be forecast independently with RWD, or that the Lee-Carter assumptions implicit in its specification be relaxed in a more piecemeal fashion, leading to some of the alternative longevity forecasting models summarised and evaluated in this paper, and this paper.\nThe overlap between general time series and demographic forecasting is less tenuous than it might first appear, when you consider that one of the authors of the first of the two papers above is none other than Rob Hyndman, whose forecasting book I’ve already referenced and made use of many times in this series. Hyndman is also the maintainer of R’s demography package. So, much of what can be learned about time series can be readily applied to demography too.\nFinally, demographic forecasting models in which age-specific mortality over time is modelled open up another way of thinking about the problem: that of spatial statistics. Remember that with exponential smoothing models the assumed information value of models declines exponentially with time? As mentioned in an earlier footnote this is largely what’s known as Tobler’s First Law of Geography, but applied to just a single dimension (time) rather than two spatial dimensions such as latitude and longitude. Well, with spatial models there are two spatial dimensions, both of which have the same units (say, metres, kilometres, etc). Two points can be equally far apart, but along different spatial dimensions. Point B could be 2km east of point A, or 2km north of point A, or 2km north-east of point A.16 In each case, the distance between points is the same, and so the amount of downweighting of the information value of point B as to the true value of point A would be the same.\nWell, with the kind of data used by Lee-Carter style models, we have mortality rates that are double indexed: \\(m_{x,t}\\), where \\(x\\) indexes the age in years, and \\(t\\) indexes the time in years. Note the phrase in years: both age and time are in the same unit, much as latitude and longitude are in the same unit. So it makes sense to think of two points on the surface - \\(m_{a,b}\\) and \\(m_{c,d}\\) - as being a known distance apart from each other,17 and so for closer observations to be more similar or informative about each other than more distant values. This kind of as-if-spatial reasoning leads to thinking about how a surface of mortality rates might be smoothed appropriately, with estimates for each point being a function of the observed value of that point, plus some kind of exponentially weighted average of more and less proximately neighbouring points. (See, for example, Girosi & King’s framework)"
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#summing-up-3",
    "href": "pages/extra-courses/time-series/index.html#summing-up-3",
    "title": "Time Series",
    "section": "Summing up",
    "text": "Summing up\nIn this last post, we’ve scratched the surface on a couple of areas related to time series that the main post series didn’t cover: state space modelling and demographic forecasting models. Although hopefully we can agree that they’re both interesting topics, they’ve also helped to illustrate why the main post series has been anchored around the ARIMA modelling framework. Time series is a complex area, and so by sticking with ARIMA, we’ve managed to avoid getting too far adrift."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#footnotes",
    "href": "pages/extra-courses/time-series/index.html#footnotes",
    "title": "Time Series",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhich is also the default, so not strictly necessary this time.↩︎\nTwo further things to note: Firstly, that if there were more than one observational unit i then the data frame should be grouped by the observational unit, then arranged by time. Secondly, that because the first time unit has no time unit, its lagged value is necessarily missing, hence NA.↩︎\nOr equivalently and more commonly summing up the log of these numbers↩︎\nThis is in some ways a bad example, as influenza is of course highly seasonal, and one flu episode may be negatively predictive of another flu episode in the same season. But I’m going with it for now…↩︎\nHurray! We’ve demonstrated something we should know from school, namely that if \\(y = \\alpha + \\beta x\\), then \\(\\frac{\\partial y}{\\partial x} = \\beta\\).↩︎\nThe information appears to be ‘overdeterimined’, as one of the metadata fields should be inferrable given the other pieces of information. I suspect this works as something like a ‘checksum’ test, to ensure the data are as intended.↩︎\nAutoregressive terms p can be included as part of non-stationary series, and there can be an arbitrary number of differencing operations d, but the moving average term q is only suitable for stationary series. So, for example, ARIMA(1,0,0) with drift can be possible, as can ARIMA(1,1,0) with drift, but ARIMA(0,0, 1) with drift is not a legitimate specification.↩︎\nOccasionally, we might also see repeated patterns over non-annual timescales. For example, we might see the apparent population size of a country shifting abruptly every 10 years, due to information from national censuses run every decade being incorporated into the population estimates. Or if we track sales by day we might see a weekly cycle, because trade during the weekends tends to be different than during the weekdays.↩︎\nHow this works is due to the acronym-in-the-acronym: LOESS, meaning local estimation. Effectively for each data point a local regression slope is calculated based on values a certain number of observations ahead and behind the value in question. The number of values ahead and behind considered is the ‘window’ size.↩︎\nSometimes m is used instead of S.↩︎\nA primary aim extracting these components from a linear regression in this way is to allow something approximating a Bayesian posterior distribution of coefficients to be generated, using a multivariate normal distribution (the first place we actually encountered a multivariate regression), without using a Bayesian modelling approach. This allows for the estimating and propagation of ‘honest uncertainty’ in predicted and expected outcomes. However, as we saw in the main course, it can sometimes be as or more straightforward to just use a Bayesian modelling approach.↩︎\ni.e. two times three squared.↩︎\nIn some ways, this idea seems equivalent to Tobler’s First Law of Geography, but applied to temporal rather than spatial distance.↩︎\nPossibly confusion↩︎\nA complicating coda to this is that state space modelling approaches can also be applied to ARIMA models.↩︎\nUsing a little trigonometry, this would be about 1.41 km east of point A, and 1.41km north of point A.↩︎\nAgain, a little trigomometry tells us that the Cartesian distance between two points \\(m_{a,b}\\) and \\(m_{c,d}\\) should be \\(d = \\sqrt{(c-a)^2 + (d-b)^2}\\). In practice with spatial statistics two elements are often encoded in terms of adjacency: 1 if two elements are contiguous (next to each other); 0 if they are not.↩︎"
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#is-time-series-an-exception-that-proves-the-rule",
    "href": "pages/extra-courses/time-series/index.html#is-time-series-an-exception-that-proves-the-rule",
    "title": "Time Series",
    "section": "",
    "text": "In the main course I’ve returned many times to the same ‘mother formula’ for a generalised linear model:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nSo, we have a system that takes a series of inputs, \\(X\\), and returns an output, \\(Y\\), and we have two sets of parameters, \\(\\beta\\) in \\(g(.)\\) and \\(\\alpha\\) in \\(f(.)\\), which are calibrated based on the discrepancy between what the model predicted output \\(Y\\) and the observed output \\(y\\).\nThere are two important things to note: Firstly, that the choice of parts of the data go into the inputs \\(X\\) and the output(s) \\(Y\\) is ultimately our own. A statistical model won’t ‘know’ when we’re trying to predict the cause of something based on its effect, for example. Secondly, that although the choice of input and output for the model are ultimately arbitrary, they cannot be the same. i.e., we cannot do this:\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\n\\[\n\\theta_i = g(Y_i, \\beta)\n\\]\nThis would be the model calibration equivalent of telling a dog to chase, or a snake to eat, its own tail. It doesn’t make sense, and so the parameter values involved cannot be calculated.\nFor time series data, however, this might appear to be a fundamental problem, given our observations may comprise only of ‘outcomes’, which look like they should be in the output slot of the formulae, rather than determinants, which look like they should be in the input slot of the formulae. i.e. we might have data that looks as follows:\n\n\n\n\\(i\\)\n\\(Y_{T-2}\\)\n\\(Y_{T-1}\\)\n\\(Y_{T}\\)\n\n\n\n\n1\n4.8\n5.0\n4.9\n\n\n2\n3.7\n4.1\n4.3\n\n\n3\n4.3\n4.1\n4.3\n\n\n\nWhere \\(T\\) indicates an index time period, and \\(T-k\\) a fixed difference in time ahead of or behind the index time period. For example, \\(T\\) might be 2019, \\(T-1\\) might be 2018, \\(T-2\\) might be 2017, and so on.\nAdditionally, for some time series data, the dataset will be much more wide than long, perhaps with just a single observed unit, observed at many different time points:\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n\\(Y_{T-5}\\)\n\\(Y_{T-4}\\)\n\\(Y_{T-3}\\)\n\\(Y_{T-2}\\)\n\\(Y_{T-1}\\)\n\\(Y_{T}\\)\n\n\n\n\n1\n3.9\n5.1\n4.6\n4.8\n5.0\n4.9\n\n\n\nGiven all values are ‘outcomes’, where’s the candidate for an ‘input’ to the model, i.e. something we should consider putting into \\(X\\)?\nDoesn’t the lack of an \\(X\\) mean time series is an exception to the ‘rule’ about what a statistical model looks like, and so everything we’ve learned so far is no longer relevant?\nThe answer to the second question is no. Let’s look at why."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#full-arima-examples",
    "href": "pages/extra-courses/time-series/index.html#full-arima-examples",
    "title": "Time Series",
    "section": "Full ARIMA Examples",
    "text": "Full ARIMA Examples\nThe last three posts have covered three of the main techniques - autoregression, integration, and moving average modelling - which combine to form the ARIMA model framework for time series analysis.\nWe’ll now look at an example (or maybe two) showing how ARIMA models - including AR, I, and MA components - are fit and employed in practice.\n\nSetup\nFor this post I’ll make use of R’s forecast package.\n\n\nCode\nlibrary(tidyverse)\nlibrary(forecast)\n\n\n\n\nDataset used: Airmiles\nThe dataset I’m going to use is airmiles, an example dataset from the datasets package, which is included in most R sessions by default\n\n\nCode\nairmiles\n\n\nTime Series:\nStart = 1937 \nEnd = 1960 \nFrequency = 1 \n [1]   412   480   683  1052  1385  1418  1634  2178  3362  5948  6109  5981\n[13]  6753  8003 10566 12528 14760 16769 19819 22362 25340 25343 29269 30514\n\n\nThe first thing we notice with this dataset is that it is not in the kind of tabular format we may be used to. Let’s see what class the dataset is:\n\n\nCode\nclass(airmiles)\n\n\n[1] \"ts\"\n\n\nThe dataset is of class ts, which stands for time series. A ts data object is basically a numeric vector with various additional pieces of metadata attached. We can see these metadata fields are start date, end date, and frequency. The documentation for ts indicates that if frequency is 1, then the data are annual. As the series are at fixed intervals, with the start date and frequency specified, along with the length of the numeric vector, the time period associated with each value in the series can be inferred.6\n\n\nVisual inspection of airmiles\nWe can look at the data using the base graphics plot function:\n\n\nCode\nplot(airmiles)\n\n\n\n\n\nWe can see this dataset is far from stationary, being much higher towards the end of the series than at the start. This implies we should consider differencing the data to make it stationery. We can use the diff() function for this:\n\n\nCode\nplot(diff(airmiles))\n\n\n\n\n\nThis differenced series still doesn’t look like IID data. Remember that differencing is just one of many kinds of transformation (data pre-processing) we could consider. Also we can difference more than once.\nAs there cannot be negative airmiles, and the series looks exponential since the start of the series, we can can consider using a log transform:\n\n\nCode\nplot(log(airmiles))\n\n\n\n\n\nHere the data look closer to a straight line. Differencing the data now should help us get to something closer to stationary:\n\n\nCode\nplot(diff(log(airmiles)))\n\n\n\n\n\nMaybe we should also look at differencing the data twice:\n\n\nCode\nplot(diff(log(airmiles), differences = 2))\n\n\n\n\n\nMaybe this is closer to the kind of stationary series that ARIMA works best with?\n\n\nARIMA fitting for airmiles\nThe visual inspection above suggested the dataset definitely needs at least one differencing term applied to it, and might need two; and might also benefit from being pre-transformed by being logged. With the forecast package, we can pass the series to the auto.arima() function, which will use an algorithm to attempt to identify the best combination of p, d and q terms to use. We can start by asking auto.arima() to determine the best ARIMA specification if the only transformation allowed is that of differencing the data, setting the trace argument to TRUE to learn more about which model specifications the algorithm has considered:\n\n\nCode\nbest_arima_nolambda &lt;- auto.arima(\n    y = airmiles, \n    trace = TRUE\n)\n\n\n\n ARIMA(2,2,2)                    : Inf\n ARIMA(0,2,0)                    : 384.231\n ARIMA(1,2,0)                    : 375.735\n ARIMA(0,2,1)                    : 375.3\n ARIMA(1,2,1)                    : 376.9756\n ARIMA(0,2,2)                    : 377.1793\n ARIMA(1,2,2)                    : Inf\n\n Best model: ARIMA(0,2,1)                    \n\n\nCode\nsummary(best_arima_nolambda)\n\n\nSeries: airmiles \nARIMA(0,2,1) \n\nCoefficients:\n          ma1\n      -0.7031\ns.e.   0.1273\n\nsigma^2 = 1234546:  log likelihood = -185.33\nAIC=374.67   AICc=375.3   BIC=376.85\n\nTraining set error measures:\n                   ME    RMSE      MAE      MPE     MAPE      MASE       ACF1\nTraining set 268.7263 1039.34 758.5374 4.777142 10.02628 0.5746874 -0.2848601\n\n\nWe can see from the trace that a range of ARIMA specifications were considered, starting with the ARIMA(2,2,2). The selection algorithm used is detailed here, and employs a variation of AIC, called ‘corrected AIC’ or AICc, in order to compare the model specifications.\nThe algorithm arrives at ARIMA(0, 2, 1) as the preferred specification. That is: no autorgression (p=0), twice differenced (d=2), and with one moving average term (MA=1).\nThe Forecasting book linked to above also has a recommended modelling procedure for ARIMA specifications, and cautions that the auto.arima() function only performs part of this proceudure. In particular, it recommends looking at the residuals\n\n\nCode\ncheckresiduals(best_arima_nolambda)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,2,1)\nQ* = 4.7529, df = 4, p-value = 0.3136\n\nModel df: 1.   Total lags used: 5\n\n\nThe three plots show the model residuals as a function of time (top), the distribution of residuals (bottom right), and the auto-correlation function, ACF (bottom-left), which indicates how the errors at different lags are correlated with each other. It also returns a test score, where high P-values (substantially above 0.05) should be considered evidence that the residuals appear like white noise, and so (something like) no further substantial systematic information in the data exists to be represented in the model.\nIn this case, the test statistic p-value is 0.31, which should be reassuring as to the appropriateness of the model specification identified.\nFinally, we can use this model to forecast a given number of periods ahead. Let’s take this data to the 1990s, even though this is a dangerously long projection.\n\n\nCode\nbest_arima_nolambda |&gt; forecast(h=35) |&gt; autoplot()\n\n\n\n\n\nThe central projection (dark blue line) is almost linear, but the projection intervals are wide and growing, and include projection scenarios where the number of flights in the 1990s are somewhat lower than those in the 1960s. These wide intervals should be considered a feature rather than a bug with the approach, as the further into the future we project, the more uncertain we should become.\n\n\nARIMA modelling with an additional transformation.\nAnother option to consider within the auto.arima() function is to allow another parameter to be estimated. This is known as the lambda parameter and represents an additional possible transformation of the data before the differencing step. This lambda parameter is used as part of a Box-Cox Transformation, intended to stabilise the variance of the series. If the lambda parameter is 0, then this becomes equivalent to logging the data. We can allow auto.arima to select a Box-Cox Transformation by setting the parameter lambda = \"auto\"\n\n\nCode\nbest_arima_lambda &lt;- auto.arima(\n    y = airmiles, \n    trace = TRUE,\n    lambda = \"auto\"\n)\n\n\n\n ARIMA(2,1,2) with drift         : Inf\n ARIMA(0,1,0) with drift         : 190.0459\n ARIMA(1,1,0) with drift         : 192.1875\n ARIMA(0,1,1) with drift         : 192.1483\n ARIMA(0,1,0)                    : 212.0759\n ARIMA(1,1,1) with drift         : 195.1062\n\n Best model: ARIMA(0,1,0) with drift         \n\n\nCode\nsummary(best_arima_lambda)\n\n\nSeries: airmiles \nARIMA(0,1,0) with drift \nBox Cox transformation: lambda= 0.5375432 \n\nCoefficients:\n        drift\n      18.7614\ns.e.   2.8427\n\nsigma^2 = 194.3:  log likelihood = -92.72\nAIC=189.45   AICc=190.05   BIC=191.72\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE      MASE      ACF1\nTraining set 123.1317 934.6956 724.6794 -5.484572 12.91378 0.5490357 -0.169863\n\n\nIn this case, a lambda value of about 0.54 has been identified, and a different ARIMA model specification selected. This specification is listed as ARIMA(0,1,0) with drift. This with drift term means the series are recognised as non-stationary, but where (after transformation) there is an average (in this case) constant amount upwards drift in the values as we progress through the series. 7 Let’s check the residuals for this model:\n\n\nCode\ncheckresiduals(best_arima_lambda)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,0) with drift\nQ* = 3.9064, df = 5, p-value = 0.563\n\nModel df: 0.   Total lags used: 5\n\n\nThe test P-value is even higher in this case, suggesting the remaining residuals appear to behave even more like white noise than in the previous specification.\nNow to look at projections from the model into the 1990s:\n\n\nCode\nbest_arima_lambda |&gt; forecast(h=35) |&gt; autoplot()\n\n\n\n\n\nUsing this specification we get a qualitatively different long-term projection with, on the identity scale of the data itself, a much narrower long-term projection interval.\n\n\nComparing model specifications\nSo, the two different ARIMA specifications arrived at - one with additional pre-transformation of the data before differencing; the other without - lead to qualitatively different long-term projections. Do we have any reason to presume one specification is better than the other?\nI guess we could look at the AIC and BIC of the two models:\n\n\nCode\nAIC(best_arima_nolambda, best_arima_lambda)\n\n\n                    df      AIC\nbest_arima_nolambda  2 374.6684\nbest_arima_lambda    2 189.4459\n\n\nCode\nBIC(best_arima_nolambda, best_arima_lambda)\n\n\n                    df      BIC\nbest_arima_nolambda  2 376.8505\nbest_arima_lambda    2 191.7169\n\n\nHere the lower scores for the model with a Box-Cox transformation suggest it should be preferred. However, as both functions warn, the number of observations differ between the two specifications. This is likely because the no-lambda version differences the data twice, whereas the with-lambda specification differences the data once, and so the no-lambda version should have one fewer observation. Let’s check this:\n\n\nCode\nn_obs_nolambda &lt;- summary(best_arima_nolambda)$nobs\n\nn_obs_lambda &lt;- summary(best_arima_lambda)$nobs\n\nprint(paste(\"Observations for no lambda:\", n_obs_nolambda))\n\n\n[1] \"Observations for no lambda: 22\"\n\n\nCode\nprint(paste(\"Observations for with-lambda:\", n_obs_lambda))\n\n\n[1] \"Observations for with-lambda: 23\"\n\n\nYes. This seems to be the cause of the difference.\nAnother way of comparing the models is by using the accuracy() function, which reports a range of accuracy measures:\n\n\nCode\nprint(\"No lambda specification: \")\n\n\n[1] \"No lambda specification: \"\n\n\nCode\naccuracy(best_arima_nolambda)\n\n\n                   ME    RMSE      MAE      MPE     MAPE      MASE       ACF1\nTraining set 268.7263 1039.34 758.5374 4.777142 10.02628 0.5746874 -0.2848601\n\n\nCode\nprint(\"With-lambda specification: \")\n\n\n[1] \"With-lambda specification: \"\n\n\nCode\naccuracy(best_arima_lambda)\n\n\n                   ME     RMSE      MAE       MPE     MAPE      MASE      ACF1\nTraining set 123.1317 934.6956 724.6794 -5.484572 12.91378 0.5490357 -0.169863\n\n\nWhat’s returned by accuracy() comprises one row (labelled Training set) and seven columns, each for a different accuracy metric. A common (and relatively easy-to-understand) accuracy measure is RMSE, which stands for (square) root mean squared error. According to this measure, the Box-Cox transformed ARIMA model outperforms the untransformed (by double-differenced) ARIMA model, so perhaps it should be preferred.\nHowever, as the act of transforming the data in effect changes (by design) the units of the data, perhaps RMSE is not appropriate to use for comparison. Instead, there is a measure called MAPE, which stands for “mean absolute percentage error”, that might be more appropriate to use because of the differences in scales. According to this measure, the Box-Cox transformed specification has a higher error score than the no-lambda specification (around 13% instead of around 10%), suggesting instead the no-lambda specification should be preferred instead.\nSo what to do? Once again, the ‘solution’ is probably just to employ some degree of informed subjective judgement, along with a lot of epistemic humility. The measures above can help inform our modelling decisions, but they cannot make these decisions for us.\n\n\nDiscussion and coming up\nFor the first three posts in this time-series miniseries, we looked mainly at the theory of the three components of the ARIMA time series modelling approach. This is the first approach where we’ve used ARIMA in practice. Hopefully you got a sense of two different things:\n\nThat because of packages like forecast, getting R to produce and forecast from an ARIMA model is relatively quick and straightforward to do in practice.\nThat even in this brief applied example of applied time series, we started to learn about a range of concepts - such as the auto-correlation function (ACF), the Box-Cox transformation, and alternative measures of accuracy - which were not mentioned in the previous three posts on ARIMA.\n\nIndeed, if you review the main book associated with the forecasting package, you can see that ARIMA comprises just a small part of the overall time series toolkit. There’s a lot more that can be covered, including some methods that are simpler to ARIMA, some methods (in particular SARIMA) which are further extensions of ARIMA, some methods that are alternatives to ARIMA, and some methods that are decidedly more complicated than ARIMA. By focusing on the theory of ARIMA in the last three posts, I’ve aimed to cover something in the middle-ground of the overall toolbox."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#seasonal-arima-sarima",
    "href": "pages/extra-courses/time-series/index.html#seasonal-arima-sarima",
    "title": "Time Series",
    "section": "Seasonal ARIMA (SARIMA)",
    "text": "Seasonal ARIMA (SARIMA)\nIn previous posts on time series, we decomposed then applied a common general purpose modelling strategy for working with time series data called ARIMA. ARIMA model can involve autoregressive components (AR(p)), integration/differencing components I(d), and moving average components (MA(q)). As we saw, the time series data can also be pre-transformed, in ways other than just differencing; the example of this we saw was the application of the Box-Cox transformation for regularising the variance of the outcome, and includes logging of values as one possible transformation within the framework.\nThe data we used previous was annual data, showing the numbers of airmiles travelled in the USA by year up to the 1960s. Of course, however, many types of time series data are sub-annual, reported not just by year, but by quarter, or month, or day as well. Data disaggregated into sub-annual units often exhibit seasonal variation, patterns that repeat themselves at regular intervals within a 12 month cycle. 8\nIn this post we will look at some seasonal data, and consider two strategies for working with this data: STL decomposition; and Seasonal ARIMA (SARIMA).\n\nAn example dataset\nLet’s continue to use the examples and convenience functions from the forecast package used in the previous post, and for which the excellent book Forecasting: Principles and Practice is available freely online.\nFirst some packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(forecast)\nlibrary(fable)\n\n\nNow some seasonal data\n\n\nCode\n# Using this example dataset: https://otexts.com/fpp3/components.html\ndata(us_employment)\nus_retail_employment &lt;- us_employment |&gt;\n  filter(Title == \"Retail Trade\")\n\nus_retail_employment\n\n\n# A tsibble: 969 x 4 [1M]\n# Key:       Series_ID [1]\n      Month Series_ID     Title        Employed\n      &lt;mth&gt; &lt;chr&gt;         &lt;chr&gt;           &lt;dbl&gt;\n 1 1939 Jan CEU4200000001 Retail Trade    3009 \n 2 1939 Feb CEU4200000001 Retail Trade    3002.\n 3 1939 Mar CEU4200000001 Retail Trade    3052.\n 4 1939 Apr CEU4200000001 Retail Trade    3098.\n 5 1939 May CEU4200000001 Retail Trade    3123 \n 6 1939 Jun CEU4200000001 Retail Trade    3141.\n 7 1939 Jul CEU4200000001 Retail Trade    3100 \n 8 1939 Aug CEU4200000001 Retail Trade    3092.\n 9 1939 Sep CEU4200000001 Retail Trade    3191.\n10 1939 Oct CEU4200000001 Retail Trade    3242.\n# ℹ 959 more rows\n\n\nThere are two differences we can see with this dataset compared with previous time series data we’ve looked at.\nFirstly, the data looks like a data.frame object, or more specifically a tibble() (due to the additional metadata at the top). In fact they are of a special type of tibble called a tsibble, which is basically a modified version of a tibble optimised to work with time series data. We can check this by interrogating the class attributes of us_employment:\n\n\nCode\nclass(us_retail_employment)\n\n\n[1] \"tbl_ts\"     \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThese class attributes go broadly from the most specific type of object class: tbl_ts (the tsibble); to the most general type of object class: the data.frame.\nSecondly, we can see that the data are disaggregated not by year as in the last post’s example, but also by month. So, what does this monthly data actually look like?\n\n\nCode\nautoplot(us_retail_employment, Employed) +\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\n\n\n\n\nThis data looks… spikey. There’s clearly both a long-term trend - including periods of faster and slower growth, and occasionally some falls - but there’s also what looks like a series of near-vertical spikes along this trend, at what may be regular intervals. What happens if we zoom into a smaller part of the time series?\n\n\nCode\nautoplot(\n    us_retail_employment |&gt;\n        filter(year(Month) &gt;=1990), \n    Employed) +\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\n\n\n\n\nHere we can start to see there’s not just a single repeating ‘vertical spike’, but a pattern that appears to repeat within each year, for each year. Let’s zoom in even further, for just three years:\n\n\nCode\nautoplot(\n    us_retail_employment |&gt;\n        filter(between(year(Month), 1994, 1996)), \n    Employed) +\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\n\n\n\n\nAlthough each of these three years is different in terms of the average number of persons employed in retail, they are similar in terms of having a spike in employment towards the end of the year, then a drop off at the start of the year, then a relative plateau for the middle of the year.\nThis is an example of a seasonal pattern, information that gets revealed about a time series when we use a sub-annual resolution that might not be apparent it we used only annual data. How do we handle this kind of data?\n\n\nApproach one: reannualise\nOf course we could simply reaggregate the data to an annual series:\n\n\nCode\nus_retail_employment |&gt;\n    mutate(\n        year = year(Month)\n    ) |&gt;\n    ungroup() |&gt;\n    index_by(year) |&gt;\n    summarise(\n        Employed = sum(Employed)\n    ) %&gt;%\n    autoplot(., Employed)\n\n\n\n\n\nOne thing we can notice with this is that there appears to be a big drop in total employment for the last year. This is likely because the last year is incomplete, so whereas previous years are summing up 12 months’ observations, for the last year a smaller number of months are being summed up. We could then drop the last year:\n\n\nCode\nus_retail_employment |&gt;\n    mutate(\n        year = year(Month)\n    ) |&gt;\n    ungroup() |&gt;\n    index_by(year) |&gt;\n    summarise(\n        Employed = sum(Employed)\n    ) |&gt;\n    filter(year != max(year)) %&gt;%\n    autoplot(., Employed)\n\n\n\n\n\nBut then we are losing some data that we really have. Even if we don’t have the full year, we might be able to get a sense from just the first few months worth of data whether the overall values for the last year are likely to be up or down compared to the same month in the previous years. We could even turn this single annual time series into 12 separate series: comparing Januaries with Januaries, Februaries with Februaries, and so on.\n\n\nCode\nus_retail_employment |&gt;\n    mutate(\n        year = year(Month), \n        month = month(Month, label = TRUE )\n    ) |&gt;\n    ggplot(\n        aes(year, Employed)\n    ) + \n    facet_wrap(~month) + \n    geom_line()\n\n\n\n\n\nHere we can see that comparing annual month-by-month shows a very similar trend overall. It’s as if each month’s values could be thought of as part of an annual ‘signal’ (an underlying long-term trend) plus a seasonal adjustment up or down: compared with the annual trend, Novembers and Decembers are likely to be high, and Januaries and Februaries to be low; and so on.\nIt’s this intuition - That we have a trend component, and a seasonal component - which leads us to our second strategy: decomposition.\n\n\nApproach Two: Seasonal Composition\nThe basic intuition of decomposition is to break sub-annual data into a series of parts: The underling long term trend component; and repeating (usually) annual seasonal component.\nA common method for performing this kind of decomposition is known as STL. This actually stands for Seasonal and Trend Decomposition using Loess (Where Loess is itself another acronym). However it’s heuristically easier to imagine it stands for Season-Trend-Leftover, as it tends to generate three outputs from a single time-series input that correspond to these three components. Let’s regenerate the example in the forecasting book and then consider the outputs further:\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n  model(\n    STL(Employed ~ trend(window = 7) +\n                   season(window = \"periodic\"),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nThe plotted output contain four rows. These are, respectively:\n\nTop Row: The input data from the dataset\nSecond Row: The trend component from STL decomposition\nThird Row: The seasonal component from the STL decomposition\nBottom Row: The remainder (or leftover) component from the STL decomposition.\n\nSo, what’s going on?\nSTL uses an algorithm to find a repeated sequence (the seasonal component) in the data that, once subtracted from a long term trend, leaves a remainder (set of errors or deviations from observations) that is minimised in some way, and ideally random like white noise.\nIf you expanded the code chunk above, you will see two parameters as part of the STL model: the window argument for a trend() function; and the window argument for a season() function. This implies there are ways of setting up STL differently, and these would produce different output components. What happens if we change the window argument to 1 (which I think is its smallest allowable value)?\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n    filter(year(Month) &lt;= 2017) |&gt;\n  model(\n    STL(Employed ~ trend(window = 1) +\n                   season(window = \"periodic\"),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nHere the trend component becomes, for want of a better term, ‘wigglier’. And the remainder term, except for a strange data artefact at the end, appears much smaller. So what does the window argument do?\nConceptually, what the window argument to trend() does is adjust the stiffness of the curve that the trendline uses to fit to the data. A longer window, indicated by a higher argument value, makes the curve stiffer, and a shorter window, indicated by a lower argument value, makes the curve less stiff. We’ve adjusted from the default window length of 7 to a much shorter length of 1, making it much less stiff.9 Let’s look at the effect of increasing the window length instead:\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n  model(\n    STL(Employed ~ trend(window = 31) +\n                   season(window = \"periodic\"),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nHere we can see that, as well as the trend term being somewhat smoother than when a size 7 window length was used, the remainder term, though looking quite noisy, doesn’t really look random anymore. In particular, there seems to be a fairly big jump in the remainder component in the late 2000s. The remainder series also does not particularly stationary, lurching up and down at particular points in the series.\nIn effect, the higher stiffness of the trend component means it is not able to capture and represent enough signal in the data, and so some of that ‘signal’ is still present in the remainder term, when it should be extracted instead.\nNow what happens if we adjust the window argument in the season() function instead?\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n  model(\n    STL(Employed ~ trend(window = 7) +\n                   season(window = 5),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nIn the above I’ve reduced the season window size (by default it’s infinite). Whereas before this seasonal pattern was forced to be constant for the whole time period, this time we an see that it changes, or ‘evolves’, over the course of the time series. We can also see that the remainder component, though looking quite random, now looks especially ‘spiky’, suggesting that the kinds of residuals left are somewhat further from Guassian white noise than in the first example.\n\nSection concluding thoughts\nSTL decomposition is one of a number of strategies for decomposition available to us. Other examples are described here. However the aims and principles of decomposition are somewhat similar no matter what approach is used.\nHaving performed a decomposition on time series data, we could potentially apply something like an ARIMA model to the trend component of the data alone for purposes of projection. If using a constant seasonal component, we could then add this component onto forecast values from the trend component, along with noise consistent with the properties of the remainder component. However, there is a variant of the ARIMA model specification that can work with this kind of seasonal data directly. Let’s look at that now"
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#vector-autoregression-var",
    "href": "pages/extra-courses/time-series/index.html#vector-autoregression-var",
    "title": "Time Series",
    "section": "Vector Autoregression (VAR)",
    "text": "Vector Autoregression (VAR)\nIn this post, we’ll take time series in a different direction, to show an application of multivariate regression common in time series, called vector autoregression (VAR). VAR is both simpler in some ways, and more complex in other ways, than SARIMA modelling. It’s simpler in that, as the name suggests, moving average (MA) terms tend to not be part of VAR models; we’ll also not be considering seasonality either. But it’s more complicated in the sense that we are jointly modelling two outcomes at the same time.\n\nModel family tree\nThe following figure aims to show the family resemblances between model specifications and challenges:\n\n\n\n\nflowchart TB \n    uvm(univariate models)\n    mvm(multivariate models)\n\n    ar(\"AR(p)\")\n    i(\"I(d)\")\n    ma(\"MA(q)\")\n    arima(\"ARIMA(p, d, q)\")\n    sarima(\"ARIMA(p, d, q)[P, D, Q]_s\")\n    var(VAR)\n\n    ar --&gt; var\n    mvm --&gt; var\n\n    i -.-&gt; var\n\n    uvm -- autoregression --&gt; ar\n    uvm -- differencing --&gt; i\n    uvm -- moving average --&gt; ma\n    ar & i & ma --&gt; arima\n\n    arima -- seasonality --&gt;  sarima\n\n\n\n\n\n\nSo, the VAR model is an extension of the autoregressive component of a standard, univariate AR(p) specification models to multivariate models. It can also include both predictor and response variables that are differenced, hence the the dashed line from I(d) to VAR.\n\n\nSo what is a multivariate model?\nYou might have seen the term multivariate model before, and think you’re familiar with what it means.\nIn particular, you might have been taught that whereas a univariate regression model looks something like this:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\epsilon\n\\]\nA multivariate regression model looks more like this:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n\\]\ni.e. You might have been taught that, if the predictors include one term for the intercept (the \\(\\beta_0\\) term) and one term for the slope (the \\(\\beta_1\\) term), then this is a univariate model. But if there are two or more terms that can claim to be ‘the slope’ then this is a multivariate model.\nHowever, this isn’t the real distinction between a univariate model and a multivariate model. To see this distinction we have to return, for the umpeenth time, to the ‘grandmother model’ specification first introduced at the start of the main course:\nStochastic Component\n\\[\nY \\sim f(\\theta, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta = g(X, \\beta)\n\\]\nNow, both the response data, \\(Y\\), and the predictor data, \\(X\\), are both taken from the same rectangular dataset, \\(D\\). Let’s say this dataset, \\(D\\), has six rows and five columns. As a matrix it would look something like this:\n\\[\nD =\n\\begin{pmatrix}\nd_{1,1} & d_{1,2} & d_{1,3} & d_{1, 4} & d_{1,5} \\\\\nd_{2,1} & d_{2,2} & d_{2,3} & d_{2, 4} & d_{2,5} \\\\\nd_{3,1} & d_{3,2} & d_{3,3} & d_{3, 4} & d_{3,5} \\\\\nd_{4,1} & d_{4,2} & d_{4,3} & d_{4, 4} & d_{4,5} \\\\\nd_{5,1} & d_{5,2} & d_{5,3} & d_{5, 4} & d_{5,5} \\\\\nd_{6,1} & d_{6,2} & d_{6,3} & d_{6, 4} & d_{6,5}\n\\end{pmatrix}\n\\]\nHere the dataset \\(D\\) is made up of a whole series of elements \\(d_{i,j}\\), where the first subset value indicates the row number \\(i\\) and the second subset value indicates the column number \\(j\\). So, for example, \\(d_{5, 2}\\) indicates the value of the 5th row and 2nd column, whereas \\(d_{2, 5}\\) indicates the value of the 2nd row and 5th column.\nFundamentally, the first challenge in building a model is deciding which columns from \\(D\\) we put in the predictor matrix \\(X\\), and which parts we put into the response matrix \\(Y\\). For example, if we wanted to predict the third column \\(j=3\\) given the fifth column \\(j=5\\) our predictor and response matrices would look as follows:\n\n\n\\[\nY = \\begin{pmatrix}\nd_{1,3} \\\\\nd_{2,3} \\\\\nd_{3,3} \\\\\nd_{4,3} \\\\\nd_{5,3} \\\\\nd_{6,3}  \n\\end{pmatrix}\n\\]\n\n\\[\nX = \\begin{pmatrix}\n1 & d_{1,5} \\\\\n1 & d_{2,5} \\\\\n1 & d_{3,5} \\\\\n1 & d_{4,5} \\\\\n1 & d_{5,5} \\\\\n1 & d_{6,5}  \n\\end{pmatrix}\n\\]\n\n\nWhere does the column of 1s come from? This is how we specify, in matrix notation, that we want an intercept term to be calculated. Models don’t have to have intercept terms, but in almost all cases we’re likely to be familiar with, they tend to.\nLet’s say we now want to include two columns, 2 and 5, from \\(D\\) in the predictor matrix, leading to what’s commonly (and wrongly) called a ‘multivariate regression’. This means that \\(Y\\) stays the same, but X is now as follows:\n\\[\nX = \\begin{pmatrix}\n1 & d_{1,2}  & d_{1,5}\\\\\n1 & d_{2,2}  & d_{2,5}\\\\\n1 & d_{3,2}  & d_{3,5}\\\\\n1 & d_{4,2}  & d_{4,5}\\\\\n1 & d_{5,2}  & d_{5,5}\\\\\n1 & d_{6,2}  & d_{6,5}\n\\end{pmatrix}\n\\]\nNo matter now many columns we include in the predictor matrix, X, however, we still don’t have a real multivariate regression model specification. Even if X had a hundred columns, or a thousand, it would still not be a multivariate regression in the more technical sense of the term.\nInstead, here’s an example of a multivariate regression model:\n\n\n\\[\nY = \\begin{pmatrix}\nd_{1,1} & d_{1,3} \\\\\nd_{2,1} & d_{2,3} \\\\\nd_{3,1} & d_{3,3} \\\\\nd_{4,1} & d_{4,3} \\\\\nd_{5,1} & d_{5,3} \\\\\nd_{6,1} & d_{6,3}  \n\\end{pmatrix}\n\\]\n\n\\[\nX = \\begin{pmatrix}\n1 & d_{1,5} \\\\\n1 & d_{2,5} \\\\\n1 & d_{3,5} \\\\\n1 & d_{4,5} \\\\\n1 & d_{5,5} \\\\\n1 & d_{6,5}  \n\\end{pmatrix}\n\\]\n\n\nThis is an example of a multivariate regression model. We encountered it before when we used the multivariate normal distribution here, and when we draw from the posterior distribution of Bayesian models here, but this is the first time we’ve considered multivariate modelling in the context of trying to represent something we suspect to be true about the world, rather than our uncertainty about the world. And it’s the first example of multivariate regression we’ve encountered in this series. For every previous model, no matter how apparently disparate, complicated or exotic they may appear, they’ve been univariate regression models in the sense that the response component \\(Y\\) has always only contained one column only.\nSo, with this definition of multivariate regression, let’s now look at VAR as a particular application of multivariate regression used in time series.\n\n\nVector Autoregression\nLet’s start with a semi-technical definition:\n\nIn vector autoregression (VAR) the values of two or more outcomes, \\(\\{Y_1(T), Y_2(T)\\}\\), are predicted based on previous values of those same outcomes \\(\\{Y_1(T-k), Y_2(T-k)\\}\\), for various lag periods \\(k\\).\n\nWhere \\(Y\\) has two columns, and an AR(1) specification (i.e. k is just 1), how is this different from simply running two separate AR(1) regression models, one for \\(Y_1\\), and the other for \\(Y_2\\)?\nWell, graphically, two separate AR(1) models proposes the following paths of influence:\n\n\n\n\nflowchart LR\nY1_T[\"Y1(T)\"]\nY2_T[\"Y2(T)\"]\n\nY1_T1[\"Y1(T-1)\"]\nY2_T1[\"Y2(T-1)\"]\n\nY1_T1 --&gt; Y1_T\nY2_T1 --&gt; Y2_T\n\n\n\n\n\nBy contrast, the paths implied and allowed in the corresponding VAR(1) model look more like the following:\n\n\n\n\nflowchart LR\nY1_T[\"Y1(T)\"]\nY2_T[\"Y2(T)\"]\n\nY1_T1[\"Y1(T-1)\"]\nY2_T1[\"Y2(T-1)\"]\n\nY1_T1 & Y2_T1 --&gt; Y1_T & Y2_T\n\n\n\n\n\nSo, each of the two outcomes at time T is influenced both by its own previous value, but also by the previous value of the other outcome. This other outcome influence is what is represented in the figure above by the diagonal lines: from Y2(T-1) to Y1(T), and from Y1(T-1) to Y2(T).\nExpressed verbally, if we imagine two entities - self and other - tracked through time, self is influenced both by self’s history, but also by other’s history too.\n\n\nExample and application in R\nIn a blog post, I discussed how I suspect economic growth and longevity growth trends are correlated. What I proposed doesn’t exactly lend itself to the simplest kind of VAR(1) model specification, because I suggested a longer lag between the influence of economic growth on longevity growth, and a change in the fundamentals of growth in both cases. However, as an example of VAR I will ignore these complexities, and use the data I prepared for that post: I also discussed how I suspect economic growth and longevity growth trends are correlated. What I proposed doesn’t exactly lend itself to the simplest kind of VAR(1) model specification, because I suggested a longer lag between the influence of economic growth on longevity growth, and a change in the fundamentals of growth in both cases. However, as an example of VAR I will ignore these complexities, and use the data I prepared for that post:\n\n\nCode\nlibrary(tidyverse)\n\ngdp_growth_pct_series &lt;- read_csv(\"still-the-economy-both-series.csv\") \n\ngdp_growth_pct_series\n\n\n# A tibble: 147 × 5\n    ...1  year series            pct_change period\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt; \n 1     1  1949 1. Per Capita GDP     NA     Old   \n 2     2  1950 1. Per Capita GDP      2.20  Old   \n 3     3  1951 1. Per Capita GDP      2.92  Old   \n 4     4  1952 1. Per Capita GDP      1.36  Old   \n 5     5  1953 1. Per Capita GDP      5.07  Old   \n 6     6  1954 1. Per Capita GDP      3.87  Old   \n 7     7  1955 1. Per Capita GDP      3.55  Old   \n 8     8  1956 1. Per Capita GDP      1.28  Old   \n 9     9  1957 1. Per Capita GDP      1.49  Old   \n10    10  1958 1. Per Capita GDP      0.815 Old   \n# ℹ 137 more rows\n\n\nWe need to do a certain amount of reformatting to bring this into a useful format:\n\n\nCode\nwide_ts_series &lt;- \ngdp_growth_pct_series |&gt;\n    select(-c(`...1`, period)) |&gt;\n    mutate(\n        short_series = case_when(\n            series == \"1. Per Capita GDP\" ~ 'gdp',\n            series == \"2. Life Expectancy at Birth\" ~ 'e0',\n            TRUE ~ NA_character_\n        )\n    ) |&gt;\n    select(-series) |&gt;\n    pivot_wider(names_from = short_series, values_from = pct_change) |&gt;\n    arrange(year) |&gt;\n    mutate(\n        lag_gdp = lag(gdp),\n        lag_e0 = lag(e0)\n    ) %&gt;%\n    filter(complete.cases(.))\n\n\nwide_ts_series\n\n\n# A tibble: 71 × 5\n    year   gdp      e0 lag_gdp  lag_e0\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1  1951 2.92  -0.568    2.20   0.763 \n 2  1952 1.36   1.89     2.92  -0.568 \n 3  1953 5.07   0.388    1.36   1.89  \n 4  1954 3.87   0.530    5.07   0.388 \n 5  1955 3.55  -0.0570   3.87   0.530 \n 6  1956 1.28   0.385    3.55  -0.0570\n 7  1957 1.49   0.170    1.28   0.385 \n 8  1958 0.815  0.255    1.49   0.170 \n 9  1959 3.70   0.184    0.815  0.255 \n10  1960 5.72   0.268    3.70   0.184 \n# ℹ 61 more rows\n\n\nSo, we can map columns to parts of the VAR specification as follows:\n\nY1: gdp\nY2: e0 (life expectancy at birth)\nperiod T: gdp and e0\nperiod T-1: lag_gdp and lag_e0\n\nTo include two or more variables as the response part, \\(Y\\), of a linear model we can use the cbind() function to combine more than one variable to the left hand side of the linear regression formula for lm or glm:\n\n\nCode\nvar_model &lt;- lm(\n    cbind(gdp, e0) ~ lag_gdp + lag_e0,\n    data = wide_ts_series\n)\n\nvar_model\n\n\n\nCall:\nlm(formula = cbind(gdp, e0) ~ lag_gdp + lag_e0, data = wide_ts_series)\n\nCoefficients:\n             gdp       e0      \n(Intercept)   1.99440   0.28108\nlag_gdp       0.06173   0.01179\nlag_e0       -0.48525  -0.33063\n\n\nWe can see here that the model reports a small matrix of coefficients: three rows (one for each coefficient term) and two columns: one for each of the response variables. This is as we should expect.\nBack in the main course, we saw we could extract the coefficients, variance-covariance matrix, and error terms of a linear regression model using the functions coefficients, vcov, and sigma respectively.11 Let’s use those functions here too:\nFirst the coefficients\n\n\nCode\ncoefficients(var_model)\n\n\n                    gdp          e0\n(Intercept)  1.99439587  0.28108028\nlag_gdp      0.06172721  0.01178781\nlag_e0      -0.48524808 -0.33062640\n\n\nAnd now the variance-covariance matrix:\n\n\nCode\nvcov(var_model)\n\n\n                gdp:(Intercept)   gdp:lag_gdp    gdp:lag_e0 e0:(Intercept)\ngdp:(Intercept)    0.1804533467 -0.0272190933 -0.1129646084   0.0039007790\ngdp:lag_gdp       -0.0272190933  0.0164590434 -0.0180517467  -0.0005883829\ngdp:lag_e0        -0.1129646084 -0.0180517467  0.6290771233  -0.0024419053\ne0:(Intercept)     0.0039007790 -0.0005883829 -0.0024419053   0.0037904364\ne0:lag_gdp        -0.0005883829  0.0003557878 -0.0003902165  -0.0005717391\ne0:lag_e0         -0.0024419053 -0.0003902165  0.0135984779  -0.0023728303\n                   e0:lag_gdp     e0:lag_e0\ngdp:(Intercept) -0.0005883829 -0.0024419053\ngdp:lag_gdp      0.0003557878 -0.0003902165\ngdp:lag_e0      -0.0003902165  0.0135984779\ne0:(Intercept)  -0.0005717391 -0.0023728303\ne0:lag_gdp       0.0003457235 -0.0003791783\ne0:lag_e0       -0.0003791783  0.0132138133\n\n\nAnd finally the error terms\n\n\nCode\nsigma(var_model)\n\n\n      gdp        e0 \n2.6906051 0.3899529 \n\n\nThe coefficients returns the same kind of 3x2 matrix we saw previously: two models run simultaneously. The error terms is now a vector of length 2: one for each of these models. The variance-covariance matrix is a square matrix of dimension 6: i.e. 6 rows and 6 columns. This is the number of predictor coefficients in each model (the number of columns of \\(X\\), i.e. 3) times the number of models simultaneously run, i.e. 2.\n\\(6^2\\) is 36, which is the number of elements in the variance-covariance matrix of this VAR model. By contrast, if we had run two independent models - one for gdp and the other for e0 - we would have two 3x3 variance-variance matrices, producing a total of 18 12 terms. This should provide some reassurance that, when we run a multivariate regression model of two outcomes, we’re not just doing the equivalent of running separate regression models for each outcome, but in slightly fewer lines.\nNow, let’s look at the model summary:\n\n\nCode\nsummary(var_model)\n\n\nResponse gdp :\n\nCall:\nlm(formula = gdp ~ lag_gdp + lag_e0, data = wide_ts_series)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.679  -1.061   0.143   1.483   6.478 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.99440    0.42480   4.695 1.34e-05 ***\nlag_gdp      0.06173    0.12829   0.481    0.632    \nlag_e0      -0.48525    0.79314  -0.612    0.543    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.691 on 68 degrees of freedom\nMultiple R-squared:  0.007555,  Adjusted R-squared:  -0.02163 \nF-statistic: 0.2588 on 2 and 68 DF,  p-value: 0.7727\n\n\nResponse e0 :\n\nCall:\nlm(formula = e0 ~ lag_gdp + lag_e0, data = wide_ts_series)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.45680 -0.19230 -0.00385  0.24379  1.38706 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.28108    0.06157   4.565 2.15e-05 ***\nlag_gdp      0.01179    0.01859   0.634  0.52823    \nlag_e0      -0.33063    0.11495  -2.876  0.00537 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.39 on 68 degrees of freedom\nMultiple R-squared:  0.1086,    Adjusted R-squared:  0.08243 \nF-statistic: 4.144 on 2 and 68 DF,  p-value: 0.02003\n\n\nThe summary is now reported for each of the two outcomes: first gdp, then e0.\nRemember that the outcome is percentage annual change in the outcome of interest from the previous year. i.e. both series have already been ‘differenced’ to produce approximately stationary series. It also means that the intercept terms are especially important, as they indicate the long-term trends observed in each series.\nIn this case the intercepts for both series are positive and statistically significant: over the long term, GDP has grown on average around 2% each year, and life expectancy by around 0.28%. As the post this relates to makes clear, however, these long-term trends may no longer apply.\nOf the four lag (AR(1)) terms in the model(s), three are not statistically significant; not even close. The exception is the lag_e0 term for the e0 response model, which is statistically significant and negative. Its coefficient is also of similar magnitude to the intercept too.\nWhat does this mean in practice? In effect, that annual mortality improvement trends have a tendency to oscillate: a better-than-average year tends to be followed by a worse-than-average year, and a worse-than-average year to be followed by a better-than-average year, in both cases at higher-than-chance rates.\nWhat could be the cause of this oscillatory phenomenon? When it comes to longevity, the phenomenon is somewhat well understood (though perhaps not widely enough), and referred to as either ‘forward mortality displacement’ or, more chillingly, ‘harvesting’. This outcome likely comes about because, if there were an exceptionally bad year in terms of (say) influenza mortality, the most frail and vulnerable are likely to be those who die disproportionately from this additional mortality event. This means that the ‘stock’ of people remaining the following year have been selected, on average, to be slightly less frail and vulnerable than those who started the previous year. Similarly, an exceptionally ‘good’ year can mean that the average ‘stock’ of the population in the following year is slightly more frail than in an average year, so more susceptible to mortality. And so, by this means, comparatively-bad-years tend to be followed by comparatively-good-years, and comparatively-good-years by comparatively-bad-years.\nThough this general process is not pleasant to think about or reason through, statistical signals such as the negative AR(1) coefficient identified here tend to keep appearing, whether we are looking for them or not.\n\n\nConclusion\nIn this post we’ve both concluded the time series subseries, and returned to and expanded on a few posts earlier in the series. This includes the very first post, where we were first introduced to the grandmother formulae, the posts on statistical modelling using both frequentist and Bayesian methods, and a substantive post linking life expectancy with economic growth.\nAlthough we’ve now first encountered multivariate regression models in the context of time series, they are a much more general phenomenon. Pretty much any type of model we can think of and apply in a univariate fashion - where \\(Y\\) has just a single column - can conceivably be expanded to two mor more columns, leading to their more complicated multiple regression variants."
  },
  {
    "objectID": "pages/extra-courses/time-series/index.html#seasonal-time-series",
    "href": "pages/extra-courses/time-series/index.html#seasonal-time-series",
    "title": "Time Series",
    "section": "Seasonal Time Series",
    "text": "Seasonal Time Series\nIn previous posts on time series, we decomposed then applied a common general purpose modelling strategy for working with time series data called ARIMA. ARIMA model can involve autoregressive components (AR(p)), integration/differencing components I(d), and moving average components (MA(q)). As we saw, the time series data can also be pre-transformed, in ways other than just differencing; the example of this we saw was the application of the Box-Cox transformation for regularising the variance of the outcome, and includes logging of values as one possible transformation within the framework.\nThe data we used previous was annual data, showing the numbers of airmiles travelled in the USA by year up to the 1960s. Of course, however, many types of time series data are sub-annual, reported not just by year, but by quarter, or month, or day as well. Data disaggregated into sub-annual units often exhibit seasonal variation, patterns that repeat themselves at regular intervals within a 12 month cycle. 8\nIn this post we will look at some seasonal data, and consider two strategies for working with this data: STL decomposition; and Seasonal ARIMA (SARIMA).\n\nAn example dataset\nLet’s continue to use the examples and convenience functions from the forecast package used in the previous post, and for which the excellent book Forecasting: Principles and Practice is available freely online.\nFirst some packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(forecast)\nlibrary(fable)\n\n\nNow some seasonal data\n\n\nCode\n# Using this example dataset: https://otexts.com/fpp3/components.html\ndata(us_employment)\nus_retail_employment &lt;- us_employment |&gt;\n  filter(Title == \"Retail Trade\")\n\nus_retail_employment\n\n\n# A tsibble: 969 x 4 [1M]\n# Key:       Series_ID [1]\n      Month Series_ID     Title        Employed\n      &lt;mth&gt; &lt;chr&gt;         &lt;chr&gt;           &lt;dbl&gt;\n 1 1939 Jan CEU4200000001 Retail Trade    3009 \n 2 1939 Feb CEU4200000001 Retail Trade    3002.\n 3 1939 Mar CEU4200000001 Retail Trade    3052.\n 4 1939 Apr CEU4200000001 Retail Trade    3098.\n 5 1939 May CEU4200000001 Retail Trade    3123 \n 6 1939 Jun CEU4200000001 Retail Trade    3141.\n 7 1939 Jul CEU4200000001 Retail Trade    3100 \n 8 1939 Aug CEU4200000001 Retail Trade    3092.\n 9 1939 Sep CEU4200000001 Retail Trade    3191.\n10 1939 Oct CEU4200000001 Retail Trade    3242.\n# ℹ 959 more rows\n\n\nThere are two differences we can see with this dataset compared with previous time series data we’ve looked at.\nFirstly, the data looks like a data.frame object, or more specifically a tibble() (due to the additional metadata at the top). In fact they are of a special type of tibble called a tsibble, which is basically a modified version of a tibble optimised to work with time series data. We can check this by interrogating the class attributes of us_employment:\n\n\nCode\nclass(us_retail_employment)\n\n\n[1] \"tbl_ts\"     \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThese class attributes go broadly from the most specific type of object class: tbl_ts (the tsibble); to the most general type of object class: the data.frame.\nSecondly, we can see that the data are disaggregated not by year as in the last post’s example, but also by month. So, what does this monthly data actually look like?\n\n\nCode\nautoplot(us_retail_employment, Employed) +\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\n\n\n\n\nThis data looks… spikey. There’s clearly both a long-term trend - including periods of faster and slower growth, and occasionally some falls - but there’s also what looks like a series of near-vertical spikes along this trend, at what may be regular intervals. What happens if we zoom into a smaller part of the time series?\n\n\nCode\nautoplot(\n    us_retail_employment |&gt;\n        filter(year(Month) &gt;=1990), \n    Employed) +\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\n\n\n\n\nHere we can start to see there’s not just a single repeating ‘vertical spike’, but a pattern that appears to repeat within each year, for each year. Let’s zoom in even further, for just three years:\n\n\nCode\nautoplot(\n    us_retail_employment |&gt;\n        filter(between(year(Month), 1994, 1996)), \n    Employed) +\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\n\n\n\n\nAlthough each of these three years is different in terms of the average number of persons employed in retail, they are similar in terms of having a spike in employment towards the end of the year, then a drop off at the start of the year, then a relative plateau for the middle of the year.\nThis is an example of a seasonal pattern, information that gets revealed about a time series when we use a sub-annual resolution that might not be apparent it we used only annual data. How do we handle this kind of data?\n\n\nApproach one: reannualise\nOf course we could simply reaggregate the data to an annual series:\n\n\nCode\nus_retail_employment |&gt;\n    mutate(\n        year = year(Month)\n    ) |&gt;\n    ungroup() |&gt;\n    index_by(year) |&gt;\n    summarise(\n        Employed = sum(Employed)\n    ) %&gt;%\n    autoplot(., Employed)\n\n\n\n\n\nOne thing we can notice with this is that there appears to be a big drop in total employment for the last year. This is likely because the last year is incomplete, so whereas previous years are summing up 12 months’ observations, for the last year a smaller number of months are being summed up. We could then drop the last year:\n\n\nCode\nus_retail_employment |&gt;\n    mutate(\n        year = year(Month)\n    ) |&gt;\n    ungroup() |&gt;\n    index_by(year) |&gt;\n    summarise(\n        Employed = sum(Employed)\n    ) |&gt;\n    filter(year != max(year)) %&gt;%\n    autoplot(., Employed)\n\n\n\n\n\nBut then we are losing some data that we really have. Even if we don’t have the full year, we might be able to get a sense from just the first few months worth of data whether the overall values for the last year are likely to be up or down compared to the same month in the previous years. We could even turn this single annual time series into 12 separate series: comparing Januaries with Januaries, Februaries with Februaries, and so on.\n\n\nCode\nus_retail_employment |&gt;\n    mutate(\n        year = year(Month), \n        month = month(Month, label = TRUE )\n    ) |&gt;\n    ggplot(\n        aes(year, Employed)\n    ) + \n    facet_wrap(~month) + \n    geom_line()\n\n\n\n\n\nHere we can see that comparing annual month-by-month shows a very similar trend overall. It’s as if each month’s values could be thought of as part of an annual ‘signal’ (an underlying long-term trend) plus a seasonal adjustment up or down: compared with the annual trend, Novembers and Decembers are likely to be high, and Januaries and Februaries to be low; and so on.\nIt’s this intuition - That we have a trend component, and a seasonal component - which leads us to our second strategy: decomposition.\n\n\nApproach Two: Seasonal Composition\nThe basic intuition of decomposition is to break sub-annual data into a series of parts: The underling long term trend component; and repeating (usually) annual seasonal component.\nA common method for performing this kind of decomposition is known as STL. This actually stands for Seasonal and Trend Decomposition using Loess (Where Loess is itself another acronym). However it’s heuristically easier to imagine it stands for Season-Trend-Leftover, as it tends to generate three outputs from a single time-series input that correspond to these three components. Let’s regenerate the example in the forecasting book and then consider the outputs further:\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n  model(\n    STL(Employed ~ trend(window = 7) +\n                   season(window = \"periodic\"),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nThe plotted output contain four rows. These are, respectively:\n\nTop Row: The input data from the dataset\nSecond Row: The trend component from STL decomposition\nThird Row: The seasonal component from the STL decomposition\nBottom Row: The remainder (or leftover) component from the STL decomposition.\n\nSo, what’s going on?\nSTL uses an algorithm to find a repeated sequence (the seasonal component) in the data that, once subtracted from a long term trend, leaves a remainder (set of errors or deviations from observations) that is minimised in some way, and ideally random like white noise.\nIf you expanded the code chunk above, you will see two parameters as part of the STL model: the window argument for a trend() function; and the window argument for a season() function. This implies there are ways of setting up STL differently, and these would produce different output components. What happens if we change the window argument to 1 (which I think is its smallest allowable value)?\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n    filter(year(Month) &lt;= 2017) |&gt;\n  model(\n    STL(Employed ~ trend(window = 1) +\n                   season(window = \"periodic\"),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nHere the trend component becomes, for want of a better term, ‘wigglier’. And the remainder term, except for a strange data artefact at the end, appears much smaller. So what does the window argument do?\nConceptually, what the window argument to trend() does is adjust the stiffness of the curve that the trendline uses to fit to the data. A longer window, indicated by a higher argument value, makes the curve stiffer, and a shorter window, indicated by a lower argument value, makes the curve less stiff. We’ve adjusted from the default window length of 7 to a much shorter length of 1, making it much less stiff.9 Let’s look at the effect of increasing the window length instead:\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n  model(\n    STL(Employed ~ trend(window = 31) +\n                   season(window = \"periodic\"),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nHere we can see that, as well as the trend term being somewhat smoother than when a size 7 window length was used, the remainder term, though looking quite noisy, doesn’t really look random anymore. In particular, there seems to be a fairly big jump in the remainder component in the late 2000s. The remainder series also does not particularly stationary, lurching up and down at particular points in the series.\nIn effect, the higher stiffness of the trend component means it is not able to capture and represent enough signal in the data, and so some of that ‘signal’ is still present in the remainder term, when it should be extracted instead.\nNow what happens if we adjust the window argument in the season() function instead?\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n  model(\n    STL(Employed ~ trend(window = 7) +\n                   season(window = 5),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nIn the above I’ve reduced the season window size (by default it’s infinite). Whereas before this seasonal pattern was forced to be constant for the whole time period, this time we an see that it changes, or ‘evolves’, over the course of the time series. We can also see that the remainder component, though looking quite random, now looks especially ‘spiky’, suggesting that the kinds of residuals left are somewhat further from Guassian white noise than in the first example.\n\nSection concluding thoughts\nSTL decomposition is one of a number of strategies for decomposition available to us. Other examples are described here. However the aims and principles of decomposition are somewhat similar no matter what approach is used.\nHaving performed a decomposition on time series data, we could potentially apply something like an ARIMA model to the trend component of the data alone for purposes of projection. If using a constant seasonal component, we could then add this component onto forecast values from the trend component, along with noise consistent with the properties of the remainder component. However, there is a variant of the ARIMA model specification that can work with this kind of seasonal data directly. Let’s look at that now\n\n\n\nApproach Three: SARIMA\nSARIMA stands for ‘Seasonal ARIMA’ (where of course ARIMA stands for Autoregressive-Integrated-Moving Average). Whereas an ARIMA model has a specification shorthand ARIMA(p, d, q), a SARIMA model has an extended specification: SARIMA(p, d, q) (P, D, Q)_S. This means that whereas ARIMA has three parameters to specify, a SARIMA model has seven. This might appear like a big jump in model complexity, but the gap from ARIMA to SARIMA is smaller than it first appears.\nTo see this it’s first noticing that, as well as terms p, d and q, there are also terms P, D and Q. This would suggest that whatever Autoregressive (p), integration (d) and moving average (q) processes are involved in standard ARIMA are also involved in another capacity in SARIMA. And what’s this other capacity? The clue to this is in the S term.\nS 10 stands for the seasonal component of the model, and specifies the number of observations that are expected to include a repeating seasonal cycle. As most seasonal cycles are annual, this means S will be 12 if the data are monthly, 4 if the data are quarterly, and so on.\nThe UPPERCASE P, D and Q terms then specify which standard ARIMA processes should be modelled as occurring every S steps in the data series. Although algebraically this means SARIMA models may look a lot more complicated than standard ARIMA models, it’s really the same process, and the same intuition, applied twice: to characterising the seasonal ‘signals’ in the time series, and to characteristing the non-seasonal ‘signals’ in the time series.\nAlthough there are important diagnostic charts and heuristics to use when determining and judging which SARIMA specification may be most appropriate for modelling seasonal data, such as the PACF and ACF, we can still use the auto.arima() function to see if the best SARIMA specification can be identified algorithmically:\n\n\nCode\nbest_sarima_model &lt;- auto.arima(as.ts(us_retail_employment, \"Employed\"))\nbest_sarima_model\n\n\nSeries: as.ts(us_retail_employment, \"Employed\") \nARIMA(1,1,2)(2,1,2)[12] \n\nCoefficients:\n         ar1      ma1     ma2     sar1     sar2    sma1     sma2\n      0.8784  -0.8428  0.1028  -0.6962  -0.0673  0.2117  -0.3873\ns.e.  0.0374   0.0481  0.0332   0.0977   0.0691  0.0937   0.0776\n\nsigma^2 = 1442:  log likelihood = -4832.08\nAIC=9680.16   AICc=9680.31   BIC=9719.06\n\n\nHere auto.arima() produced an ARIMA(1, 1, 2) (2, 1, 2)_12 specification, meaning p=1, d=1, q=2 for the non-seasonal part; and P=2, D=1, Q=2 for the seasonal part.\nWhat kind of forecasts does this produce?\n\n\nCode\nbest_sarima_model |&gt; \n  forecast(h=48) |&gt;\n  autoplot()\n\n\n\n\n\nWe can see the forecasts tend to repeat the seasonal pattern apparent throughout the observed data, and also widen in the usual way the further we move from the observed data.\n\n\nSumming up\nIn this post we have looked at three approaches for working with seasonal data: aggregating seasonality away; decomposition; and SARIMA. These are far from an exhaustive list, but hopefully illustrate some common strategies for working with this kind of data."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/index.html",
    "href": "pages/extra-courses/hacker-stats/index.html",
    "title": "Hacker Stats",
    "section": "",
    "text": "This page discusses resampling approaches to statistical inference. Resampling approaches are a powerful and highly adaptable set of approaches for trying to get ‘good enough’ estimates of how statistically significant some observed value or summary of observed values is likely to be, or equivalently how likely what one’s observed is to have been observed by chance. They can also be extended and applied to performing post-stratification, which allows samples of the population with known biases to be adjusted in ways that aim to mitigate such biases, and so produce summary estimates more representative of the population of interest.\n\n\n\nResampling methods are sometimes called Hacker Stats, which might be a slightly derogatory term, but is also an informative one. Broadly, Resampling Methods:\n\nSubstitute meat brain effort (deriving and recalling analytic solutions) for silicon brain effort (i.e. they’re computationally intensive rather than human knowledge and reasoning intensive).\nAre theoretically and methodologically thin rather than theoretically and methodologically fat.\nAre approximate, stochastic and general; rather than precise, deterministic and specialist.\n\nPut another way, Hacker Stats are methods that data scientists and more casual users of statistics can use to get good enough approximations of the kinds of careful, analytic solutions and tests that, with many years of specialist training and memorisation, a degree in statistics would provide. They’re a good example of the 80:20 Principle: part of the 20% of stats know-how that’s used for 80% of the tasks."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/index.html#overview",
    "href": "pages/extra-courses/hacker-stats/index.html#overview",
    "title": "Hacker Stats",
    "section": "",
    "text": "This page discusses resampling approaches to statistical inference. Resampling approaches are a powerful and highly adaptable set of approaches for trying to get ‘good enough’ estimates of how statistically significant some observed value or summary of observed values is likely to be, or equivalently how likely what one’s observed is to have been observed by chance. They can also be extended and applied to performing post-stratification, which allows samples of the population with known biases to be adjusted in ways that aim to mitigate such biases, and so produce summary estimates more representative of the population of interest.\n\n\n\nResampling methods are sometimes called Hacker Stats, which might be a slightly derogatory term, but is also an informative one. Broadly, Resampling Methods:\n\nSubstitute meat brain effort (deriving and recalling analytic solutions) for silicon brain effort (i.e. they’re computationally intensive rather than human knowledge and reasoning intensive).\nAre theoretically and methodologically thin rather than theoretically and methodologically fat.\nAre approximate, stochastic and general; rather than precise, deterministic and specialist.\n\nPut another way, Hacker Stats are methods that data scientists and more casual users of statistics can use to get good enough approximations of the kinds of careful, analytic solutions and tests that, with many years of specialist training and memorisation, a degree in statistics would provide. They’re a good example of the 80:20 Principle: part of the 20% of stats know-how that’s used for 80% of the tasks."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/index.html#types-of-permutation-method",
    "href": "pages/extra-courses/hacker-stats/index.html#types-of-permutation-method",
    "title": "Hacker Stats",
    "section": "Types of permutation method",
    "text": "Types of permutation method\nThe following flowchart shows the ‘family tree’ of types of resampling method:\n\n\n\n\nflowchart TB\n    sd[Sample Data]\n    us(Uniform Sampling)\n    nus(Non-Uniform Sampling)\n    pt[Permutation Testing]\n    bs[Bootstrapping]\n    ps[Post-Stratification]\n\n    pw[Population Weights]\n\n    dec1{Equal Probability?}\n    dec2{With Replacement?}\n\n    sd --sampling--&gt; dec1\n\n    us --&gt; dec2\n\n    dec1 --Yes--&gt; us\n    dec1 --No--&gt; nus\n    nus --&gt; ps\n\n    dec2 --Yes--&gt; bs\n    dec2 --No--&gt; pt\n\n    pw --&gt; nus\n\n\n\n\n\n\nn.b. Bootstrapping and permutation testing can be applied to post-stratified data too!\n\nThe thin-but-deep theories\nBoth Bootstrapping, which is resampling with replacement, and Permutation Testing, which is resampling without replacement, use computation to explore the implications of two distinct, simple, and important theories about the sample data, and any observations we may think we’ve observed within it. Let’s try to talk through these two thin-but-deep theories:\n\nBootstrapping\n\nDefinition and history\nAccording to Wikipedia:\n\nBootstrapping is any test or metric that uses random sampling with replacement (e.g. mimicking the sampling process), and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates.[1][2] This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.[3][4]\n\n\nBootstrapping estimates the properties of an estimand (such as its variance) by measuring those properties when sampling from an approximating distribution. One standard choice for an approximating distribution is the empirical distribution function of the observed data. In the case where a set of observations can be assumed to be from an independent and identically distributed population, this can be implemented by constructing a number of resamples with replacement, of the observed data set (and of equal size to the observed data set).\n\n\nIt may also be used for constructing hypothesis tests.[5] It is often used as an alternative to statistical inference based on the assumption of a parametric model when that assumption is in doubt, or where parametric inference is impossible or requires complicated formulas for the calculation of standard errors.\n\nn.b. The same page (In History) also states: “Other names … suggested for the ‘bootstrap’ method were: Swiss Army Knife, Meat Axe, Swan-Dive, Jack-Rabbit, and Shotgun.” So, there might not be good reasons to fear statistics, but given this list of suggestions there might be good reasons to fear some statisticians! Of these alternative names, perhaps Swiss Army Knife is the most appropriate, as it’s a very widely applicable approach!\n\n\nThe key assumption of Bootstrapping\nBootstrapping starts and ends with something like the following assumption:\n\nEvery observation in our dataset is equally likely.\n\nWhy is this?\n\nBecause each specific observation in our dataset has been observed the same number of times.\n\nWhy do you say that?\n\nBecause each observation in the dataset has been observed exactly one time, and 1=1!\n\nAnd why does this matter?\n\nBecause, if we can accept the above, we can say that another dataset, made up by resampling the real sample data, so that each observation (row) is as likely to be picked as every other one, is as likely as the dataset we actually observed. And so long as this other dataset has the same number of observations as the original dataset, then it’s also as precise as the original dataset.\n\nIt’s this line of reasoning - and the two conditions for another dataset: equally likely; and equally precise - which lead to the justification, in bootstrapping, for resampling with replacement.\n\n\n\nPermutation Tests\nHere’s a more informal description of what permutation tests do, followed by a slightly more formal coverage of the same.\n\nInformal Explanation\nLet’s try to understand the intuition of permutation tests using a (rather boring) story:\n\nImagine you have two types of index cards: red cards and blue cards.\nSay there are 12 red cards and 8 blue cards, so a total of 20 cards.\nOn each of the cards is a value. Let’s say it’s a binary value: 1 (maybe for a ‘success’) or 0 (a ‘failure’).\nLet’s say the values from the red card came from flipping a specific coin, Coin A, 12 times, and writing a 1 on a blank red index card if the coin came up heads, and 0 on a blank red index card if the coin came up tails.\nThe values on the blue cards came from flipping a different coin, Coin B, 8 times, and doing the same thing, but with blue cards instead of red cards.\n\nWhat you want to know is whether Coin A or Coin B are different, i.e. one has a different probability of producing heads than the other one. However, you don’t have access either Coin A or Coin B. The only information you have to go on is the 20 index cards: 12 red, 8 blue.\nHow do you go about determining if the two coins are different, when you don’t have access to either coin, and all you have are the 20 index cards?\nOne approach is to perform permutation tests. This is a way of using computation to produce a Null Distribution, meaning a distribution of some kind of summary statistic that you would expect to observe if there were really no difference between Coin A and Coin B. This Null Distribution is a distribution of summary values that you would expect to observe if the Null Hypothesis were true, where the Null Hypothesis is that Coin A and Coin B behave in exactly the same way. You then compare the corresponding summary statistic from the observed data against this Null Distribution. If the observed summary statistic is far from the range of summary statistics, then you have more reason to Reject the Null Hypothesis, which generally corresponds to evidence for the Alternative Hypothesis, which in this case is that Coin A and Coin B are different.\nThe way you would manually perform a permutation test (without computers) in this example is as follows:\n\nYou get a big box of only red index cards, and a big box of blue index cards, all of which are blank.\nFrom the big box of red index cards, you take 12 cards, and put them into a little pile.\nFrom the big box of blue index cards, you take 8 cards, and put them into the same pile containing the 12 red index cards.\nYou then randomly shuffle the 20 cards with values written on them (your data), and place this randomly shuffled pile face down.\nYou take the top card from the data pile, turn it over, and write its value on the first card in the small pile of 20 blank cards you’ve just made. You then take this now-not-blank card from the small pile, and place it next to the pile of now 19 blank cards.\nYou then repeat with the next card in the data pile, and the next card in the small blank card pile, until all cards in the blank card pile have had a value (1 or 0) written onto them.\nYou then repeat steps 2 through 6 a large number of times: say another 999 times. At the end of this, you now have one real dataset, comprising 20 index cards - 12 red, 8 blue - and 1000 ‘fake datasets’, i.e. 1000 piles of 20 index cards each - 12 red, 8 blue - which also each have 1 or 0 written on them.\nAfter you have done this, you calculate a summary statistic for both the one real dataset, and the 1000 ‘fake datasets’. Say this is the difference in the proportions of 1 in the red subset of cards, and the blue subset in cards. You calculate this for the real dataset, and call it the observed statistic. And you also calculate it for each of the 1000 fake datasets, which provides your Null distribution for this same statistic.\nFinally, you compare the observed statistic (from the real dataset), with the Null distribution of summary statistics. If the observed statistic is somewhere in the middle of the Null distribution, there’s little reason to reject the Null Hypothesis; if it’s quite far from the Null distribution, there’s much more reason to reject the Null Hypothesis.\n\nAs you can tell from the description above, this would be quite a slow approach to making a Null distribution if we were to follow the steps manually. This is why historically many of the approaches for producing Null distributions that you might be familiar with involve algebra-based theoretical distributions. In the example above a classic way of calculating the Null distribution would be using the Chi-Squared distribution. Historically, it was much quicker for one person to figure out the algebra once, and perform calculation based on the algebraic solution, than to perform a permutation test. These days, even if we have an algebraic solution, it can still be as quick or quicker to perform a permutation test.\n\n\nFormal Explanation\nSay we have a sample dataset, \\(D\\), which is a big rectangle of data with rows (observations) and columns (variables). To simplify, imagine \\(D\\) comprises five observations and two variables, so it looks like this:\n\\[\nD =\n\\begin{pmatrix}\nd_{1,1} & d_{1,2} \\\\\nd_{2,1} & d_{2,2} \\\\\nd_{3,1} & d_{3,2} \\\\\nd_{4,1} & d_{4,2} \\\\\nd_{5,1} & d_{5,2}  \n\\end{pmatrix}\n\\]\nThere are a number of different ways of describing and thinking about this kind of data, which is really just a structured collection of elements. One approach is to think about from the perspective of observations, which leads to a row-wise interpretation of the dataset:\n\\[\nD =\n\\begin{pmatrix}\nd_{1} = \\{d_{1,1} , d_{1,2}\\} \\\\\nd_{2} = \\{d_{2,1} , d_{2,2}\\} \\\\\nd_{3} = \\{d_{3,1} , d_{3,2}\\} \\\\\nd_{4} = \\{d_{4,1} , d_{4,2}\\} \\\\\nd_{5} = \\{d_{5,1} , d_{5,2}\\}  \n\\end{pmatrix}\n\\]\nAnd another way of thinking about the data is from the perspective of variables, which leads to a column-wise interpretation of the data:\n\\[\nD = \\{X, Y\\}\n\\]\n\\[\nX = \\{d_{1,1}, d_{2,1}, d_{3, 1}, d_{4, 1}, d_{5, 1}\\}\n\\]\n\\[\nY = \\{d_{1,2}, d_{2,2}, d_{3, 2}, d_{4, 2}, d_{5, 2}\\}\n\\]\nNow, imagine we’ve looked at our dataset, and we think there’s an association between the two variables \\(X\\) and \\(Y\\). What would be a very generalisable way of testing for whether we’re correct in assuming this association?\nThe key piece of reasoning behind resampling without replacement for permutation testing is as follows:\n\nIf there is a real association between the variables then the way values are paired up as observations matters, and should be preserved. If there’s no real association between the variables then the pairing up of values into observations doesn’t matter, so we can break this pairing and still get outcomes similar to what we actually observed.\n\nThere’s another term for resampling with replacement: shuffling. We can break-up the observational pairing seen in the dataset by shuffling one or both of the variables, then putting back the data into the same kind of rectangular structure it was before.\nFor instance, say we shuffle variable \\(Y\\), and end up with the following new vector of observations:\n\\[\nY^{shuffled} = \\{ d_{2,2}, d_{5, 2}, d_{3, 2}, d_{1,2}, d_{4, 2} \\}\n\\]\nWe could then make a new fake dataset, with all the same values as in the original dataset, but not necessarily in the same order:\n\\[\nX = \\{d_{1,1}, d_{2,1}, d_{3, 1}, d_{4, 1}, d_{5, 1}\\}\n\\]\n\\[\nY^{shuffled} = \\{d_{4,2}, d_{2,2}, d_{1, 2}, d_{3, 2}, d_{5, 2}\\}\n\\]\n\\[\nD^{fake} = \\{X, Y^{shuffled}\\}\n\\]\n\\[\nD^{fake} =\n\\begin{pmatrix}\nd_{1}^{fake} = \\{d_{1,1} , d_{4,2}\\} \\\\\nd_{2}^{fake} = \\{d_{2,1} , d_{2,2}\\} \\\\\nd_{3}^{fake} = \\{d_{3,1} , d_{1,2}\\} \\\\\nd_{4}^{fake} = \\{d_{4,1} , d_{3,2}\\} \\\\\nd_{5}^{fake} = \\{d_{5,1} , d_{5,2}\\}  \n\\end{pmatrix}\n\\]\nSo, in \\(D^{fake}\\) the observed (row-wise) association between each \\(X\\) and corresponding \\(Y\\) value has broken, even though the same values \\(d_{i,j}\\) are present.\nHowever, if the assumption/‘hunch’ about there being an association between \\(X\\) and \\(Y\\) from the real dataset \\(D\\) was justified through some kind of summary statistic, such as a correlation coefficient, \\(r(X, Y)\\), then we calculate the same summary statistic for the fake dataset too, \\(r(X, Y^{fake})\\).\nIn fact (and in practice) we can repeat the fakery, permuting the values again and again, and each time calculating the summary statistic of interest. This produces a distribution of values for this summary statistic, against which we can compare the observed value of this summary statistic.\nThis distribution of summary statistics produced from a large selection of permutated (fake) datasets is the distribution we would expect to see under the Null Hypothesis, which is that the apparent association is illusionary, and that no real association exists: the appearance of association comes from chance alone.\n\n\n\n\nPost-stratification\nResampling methods can also be used as a method for post-stratification, reweighting sample data to try to make it more representative of the population of interest. Consider two scenarios where this might be important:\n\nIntentional Oversampling: Say we know that 95% of people working in a particular occupation tend to be female, and 5% male. We are interested both in the typical characteristics of people who work in this occupation, but also in properly understanding the characteristics of males and females separately, and the differences between males and females within the occupation. And we know that, if we take a purely random sample of the population, we’ll only get, on average, 5% of the sample being males, which won’t give us enough precision/resolution to properly understand males in the population. So, we intentionally oversample from the male population, meaning our sample contains 20% males and 80% females, even though this isn’t representative of the population as a whole.\n\n\nUnintentional Undersampling: Say we are interested in political party voting intentions at an upcoming election. However for reasons of convenience we decide only to poll people who play console games, by asking someone about to play a game if they’re more likely to vote for the Blue Party or the Red Party. We know that our sample has very different characteristics to the population at large. However we also know so many people play console games that we have a reasonably large (and so sufficiently precise) set of estimates for each of the main demographic stratas of interest to us. So what do we do to convert the very biased sample data into unbiased population estimates? 1\n\nIn either case resampling methods can be applied. Just go from equal probability sampling to weighted probability sampling, in which samples from our dataset is more likely to be selected if they are under-represented in the sample dataset compared with the population, and less likely to be selected if they are under-represented in the sample dataset compared with the population.\n\n\nSummary\nIn this post we’ve discussed the key ideas behind resampling methods, AKA Hacker Stats. These approaches are computationally intensive as compared with analytical solutions, which would have been a big barrier to their use until, perhaps, the mid 1980s. However computationally intensive these days might just mean it takes five seconds to perform many times, whereas the analytic solution takes five microseconds: still a large relative difference in computing time, but practically both kinds of approaches are similarly fast to perform.\nThese days, whether you know an analytic approximation for performing the test or calculation of interest, or whether you don’t, the Hacker Stats approach is still worth trying out. Even at their slowest, the worst case scenario with Hacker Stats is your computer might whirr a bit more loudly than usual, and you’ll finally have a good excuse to get that much-deserved tea- or coffee-break!"
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/index.html#footnotes",
    "href": "pages/extra-courses/hacker-stats/index.html#footnotes",
    "title": "Hacker Stats",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis isn’t a made-up example, but broadly the approach used by Wang et al 2014 to produce pretty accurate estimates of a then-upcoming US election↩︎\nTechnically, the Chi-Squared test here is two sided, looking for much smaller and much higher values than the Null distribution, whereas in the example below where we used a one-sided test.↩︎"
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/index.html#bootstrapping-example",
    "href": "pages/extra-courses/hacker-stats/index.html#bootstrapping-example",
    "title": "Hacker Stats",
    "section": "Bootstrapping example",
    "text": "Bootstrapping example\nI’m not going to look for length-of-stay data; instead I’m going to look at length-of-teeth, and the hamster experiment dataset I used in the main course.\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nLet’s say, instead of building a statistical model, I’m just interested in the following question:\n\nWhere the dose is 1mg, is using the OJ supplement instead of the VC supplement associated with a significant and detectable difference in the median tooth length?\n\nWe can do this in at least a couple of ways:\n\nCalculate the median of OJ at 1mg tooth lengths, and compare it to a bootstrapped distribution of medians from VC at 1mg.\nBootstrap both the OJ and VC (both at 1mg) populations, get the medians for each bootstrapped population, and record the difference in the medians.\n\nThese are asking slightly different questions, but both ways of using bootstrapping to address the general type of question framed above.\n\nApproach One\n\n\nCode\nNreps &lt;- 10000 # Number of bootstrap replicates\n\nbs_med_vc &lt;- vector(mode = 'numeric',  length = Nreps) #Vector for holding bootstrapped medians\n\ndta_vc &lt;- df |&gt;\n    filter(supp == \"VC\", dose == 1.0) # The equivalent of our 'control' population\n\ncontrol_toothlengths &lt;- dta_vc |&gt; pull(len) # Literally pulling teeth!\n\nNcontrol &lt;- length(control_toothlengths) #Length of 'control' population sample\n\nfor (i in 1:Nreps){\n    bs_c_length &lt;- sample(\n        control_toothlengths, \n        size = Ncontrol,\n        replace = TRUE\n    ) # resampling to the same length as the 'control' population\n\n    this_bs_control_median &lt;- median(bs_c_length)\n    bs_med_vc[i] &lt;- this_bs_control_median\n}\n\n\nWe’ve now done the bootstrapping on the ‘control’ population. Let’s look at this bootstrapped distribution of medians in comparison with the observed median from the ‘treatment’ group.\n\n\nCode\ntreatment_toothlengths &lt;- df |&gt;\n    filter(supp == \"OJ\", dose == 1.0) |&gt;\n    pull(len) # pulling teeth for the 'treatment' population\n\nobs_med_oj &lt;- median(treatment_toothlengths)\n\ntibble(bs_control_median = bs_med_vc) |&gt;\n    ggplot(aes(x=bs_control_median)) +\n    geom_histogram() +\n    geom_vline(xintercept = obs_med_oj, colour = \"red\", linetype = \"dashed\") + \n    geom_vline(xintercept = median(control_toothlengths), colour = \"blue\", linetype = \"dashed\") + \n    labs(\n       x = \"Median toothlength\",\n       y = \"Number of bootstraps\",\n       title = \"Bootstrapping approach One\",\n       subtitle = \"Red line: Observed median toothlength in 'treatment' arm. Blue line: Observed median in 'control' arm\"\n    )\n\n\n\n\n\nWe can see here that the red line, which is the observed median in the ‘treatment’ arm, is higher than all of the bootstrapped medians from the ‘control’ arm. The blue line shows the equivalent in the observed median in the ‘control’ arm.\nSo, without even performing a calculation, we can feel more confident that the OJ supplement is associated with larger tooth length, even though both arms comprise just ten observations.\n\n\nApproach Two\nLet’s now use bootstrapping to produce a distributions of differences in medians between the two arms. So, this time we repeatedly resample from both the control and the treatment arm.\n\n\nCode\nNreps &lt;- 10000 # Number of bootstrap replicates\n\nbs_diff_meds &lt;- vector(mode = 'numeric',  length = Nreps) #Vector for holding differences in bootstrapped medians\n\ndta_vc &lt;- df |&gt;\n    filter(supp == \"VC\", dose == 1.0) # The equivalent of our 'control' population\n\ncontrol_toothlengths &lt;- dta_vc |&gt; pull(len) # Literally pulling teeth!\n\nNcontrol &lt;- length(control_toothlengths) #Length of 'control' population sample\n\ndta_oj &lt;- df |&gt;\n    filter(supp == \"OJ\", dose == 1.0) # The equivalent of our 'treamtnet' population\n\ntreatment_toothlengths &lt;- dta_oj |&gt; pull(len) # Literally pulling teeth!\n\nNtreatment &lt;- length(treatment_toothlengths) #Length of 'treatment' population sample\n\n\nfor (i in 1:Nreps){\n    bs_c_length &lt;- sample(\n        control_toothlengths, \n        size = Ncontrol,\n        replace = TRUE\n    ) # resampling to the same length as the 'control' population\n\n    this_bs_control_median &lt;- median(bs_c_length)\n\n    bs_t_length &lt;- sample(\n        treatment_toothlengths, \n        size = Ntreatment,\n        replace = TRUE\n    ) # resampling to the same length as the 'control' population\n\n    this_bs_treat_median &lt;- median(bs_t_length)\n\n    bs_diff_meds[i] &lt;- this_bs_treat_median - this_bs_control_median\n}\n\n\nWe now have a bootstrapped distribution of differences, each time subtracting the bootstrapped control median from the bootstrapped treat median. So, values above 0 indicate the treatment is more effective (at lengthening teeth) than the control.\nLet’s look at this distribution\n\n\nCode\ntibble(bs_diffs_median = bs_diff_meds) |&gt;\n    ggplot(aes(x=bs_diffs_median)) +\n    geom_histogram() +\n    geom_vline(xintercept = 0) + \n    geom_vline(\n        xintercept = median(treatment_toothlengths) - median(control_toothlengths), linetype = \"dashed\", colour = \"green\"         \n        ) + \n    labs(\n       x = \"Differences in medians\",\n       y = \"Number of bootstraps\",\n       title = \"Bootstrapping approach Two\",\n       subtitle = \"Values above 0: medians are higher in treatment group\"\n    )\n\n\n\n\n\nI’ve added the observed difference in medians as a vertical green line. This corresponds with the highest peak in bootstrapped differences in medians, as we might expect.\nAlmost all bootstrapped differences in medians are above 0, which again suggests we don’t even need to calculate the proportion above 0 to work out if there is likely to be a difference in medians between the two groups.\nHowever if we wanted to get this empirical p-value, we could do it as follows:\n\n\nCode\nsum(bs_diff_meds &lt; 0) / Nreps\n\n\n[1] 4e-04\n\n\nTiny!"
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/index.html#permutation-testing-with-base-r",
    "href": "pages/extra-courses/hacker-stats/index.html#permutation-testing-with-base-r",
    "title": "Hacker Stats",
    "section": "Permutation Testing with Base R",
    "text": "Permutation Testing with Base R\nIn this post I’ll cover the intuition of permutation tests through a little toy example. In a follow-up post I’ll discuss how this intuition can be implemented (and made a bit easier) using the infer package.\nLet’s actually make the dataset I’ve described above (using a random number seed so the answers don’t change). Let’s say in our example the true proportion for Coin A is 0.55, and for Coin B it’s 0.50. (Something we’d never know in practice.)\n\n\nCode\nset.seed(7) # Random number set.seed\n\ndraws_A &lt;- rbinom(n=12, size=1, prob=0.55)\ndraws_B &lt;- rbinom(n=8, size=1, prob=0.50)\n\ncard_colour &lt;- c(\n    rep(\"red\", 12),\n    rep(\"blue\", 8)\n)\n\nreal_data &lt;- data.frame(\n    card_colour = card_colour,\n    outcome = c(draws_A, draws_B)\n)\n\nreal_data\n\n\n   card_colour outcome\n1          red       0\n2          red       1\n3          red       1\n4          red       1\n5          red       1\n6          red       0\n7          red       1\n8          red       0\n9          red       1\n10         red       1\n11         red       1\n12         red       1\n13        blue       1\n14        blue       0\n15        blue       0\n16        blue       0\n17        blue       1\n18        blue       0\n19        blue       1\n20        blue       0\n\n\nIn this example, what is the proportion of 1s in the red card subgroup, and the blue card subgroup?\n\n\nCode\nprop_in_red &lt;- real_data$outcome[real_data$card_colour == \"red\"] |&gt;\n    mean()\n\nprop_in_blue &lt;- real_data$outcome[real_data$card_colour == \"blue\"] |&gt;\n    mean()\n\ndiff_in_props &lt;- prop_in_red - prop_in_blue\n\ndiff_in_props\n\n\n[1] 0.375\n\n\nIn this example the proportion ‘heads’ in the red subgroup (from coin A) is 0.750, and in the blue subgroup (from coin B) happens to be exactly 0.375. This means the difference in proportions is 0.375.\nHow would we use a permutation test to produce a Null distribution of differences in proportions between the two groups?\nHere’s one approach:\n\n\nCode\nnReps &lt;- 1000 # We'll perform 1000 replications/resamples\n\nnullVector &lt;- vector(mode = \"numeric\", length = 1000)\n\n\noutcomes &lt;- real_data$outcome\nlabels &lt;- real_data$card_colour\n\nnObs &lt;- length(outcomes)\n\nfor (i in 1:nReps){\n\n    random_draw_of_outcomes &lt;- sample(outcomes, size = nObs, replace = FALSE)\n\n    fake_prop_red &lt;- mean(\n        random_draw_of_outcomes[labels == \"red\"]\n    )\n\n    fake_prop_blue &lt;- mean(\n        random_draw_of_outcomes[labels == \"blue\"]\n    )\n\n    fake_diff_outcomes &lt;- fake_prop_red - fake_prop_blue\n\n    nullVector[i] &lt;- fake_diff_outcomes\n}\n\nhead(nullVector)\n\n\n[1] -0.25000000 -0.04166667  0.16666667 -0.04166667 -0.25000000 -0.04166667\n\n\nWhat does the distribution of differences look like?\n\n\nCode\nhist(nullVector)\n\n\n\n\n\nHere we can see quite a wide range of differences in proportions are generated by the permutation-based Null distribution. We can use the quantile function to get a sense of the range:\n\n\nCode\nquantile(nullVector, prob = c(0.025, 0.050, 0.25, 0.50, 0.75, 0.95, 0.975))\n\n\n       2.5%          5%         25%         50%         75%         95% \n-0.45833333 -0.45833333 -0.25000000 -0.04166667  0.16666667  0.37500000 \n      97.5% \n 0.37500000 \n\n\nHere the median value of the proportion of differences is -0.042. Half of the values are between -0.025 and 0.0167; 90% of the values are between -0.458 and 0.375, and 95% of values are between -0.458 and 0.375.\nFor reference, the real observed difference in proportions is 0.375. This seems to be at the far right end of the Null distribution. We can calculate what is in effect a p-value, of the probability of seeing a value as or more extreme than the observed value from the Null distribution, by counting up the proportion of Null distribution values that were as or more extreme than the observed value:\n\n\nCode\nsum(nullVector &gt;= diff_in_props) / length(nullVector)\n\n\n[1] 0.103\n\n\nSo, the proportion of times the Null distribution generates a value as great or greater than the observed value is about 10%. This wouldn’t meet conventional thresholds of statistical significance, which would be less than 5% of values being this or more extreme. However it does seem from the data that it’s more likely than not the two coins may be different. (And we know, as a fact, the two coins are different, because we made them to be!)\nFinally, let’s use the Chi-squared test to try to answer the same sort of question2:\nFirst we make a cross-tab out of the real data:\n\n\nCode\nxtab &lt;- xtabs(~card_colour + outcome, data = real_data)\nxtab\n\n\n           outcome\ncard_colour 0 1\n       blue 5 3\n       red  3 9\n\n\nAnd then we pass the cross-tab to the function chisq.test:\n\n\nCode\nchisq.test(xtab)\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  xtab\nX-squared = 1.467, df = 1, p-value = 0.2258\n\n\nHere the function produces a p-value that’s even larger than the approximately 0.10 value from the permutation approach, giving even less confidence that there may be a difference between the two groups. However it also gives a warnings that the assumptions made in producing this p-value may not be appropriate. In particular, two of the four cells (so 50% of the cells) in the cross-tab have values less than 5, whereas a rule-of-thumb when calculating a Chi-squared statistic is that no more than 20% of cells should have values less than 5.\nAn alternative to the Chi-Square test, when there are small sample sizes, is the Fisher Exact test. This is more computationally intensive than the Chi-Square test, but can be more appropriate when there are small sample sizes. Unlike with the Chi-Square test, we can perform one sided as well as two sided tests using this method, with the default being two sided. Let’s see what this produces:\n\n\nCode\nfisher.test(xtab)\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  xtab\np-value = 0.1675\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.5182846 52.4512095\nsample estimates:\nodds ratio \n  4.564976 \n\n\nHere the p-value is slightly smaller than for the Chi-squared test, but slightly larger than for the (one-sided) permutation based p-value. Let’s see what the corresponding p-value is if we specify we want a one-sided test, by setting the alternative argument to \"greater\":\n\n\nCode\nfisher.test(xtab, alternative = \"greater\")\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  xtab\np-value = 0.1132\nalternative hypothesis: true odds ratio is greater than 1\n95 percent confidence interval:\n 0.6835727       Inf\nsample estimates:\nodds ratio \n  4.564976 \n\n\nThis time, we get a p value of 0.113, which is much closer to the permutation-based one-sided p-value of 0.103 we derived previously.\n\nSummary\nIn this post we’ve used only Base R functions to understand the intuition and implementation of permutation based tests for trying to either reject or not reject the Null hypothesis. Permutation methods, like bootstrapping, fall under the broader umbrella of resampling methods, and are immensely versatile and applicable to a great many types of data and question.\nApproaches like these are sometimes referred to as ‘Hacker Stats’, as being able to implement them correctly depends much more on having some computer science knowledge - such as for loops or equivalent - than much knowledge of statistical methods and tests. In this example I happened to know of a couple of classic conventional statistical tests that were broadly appropriate to the type of question we were trying to answer, but a reasonable programmer, once they understand the intuition behind the approach, would be able to produce a p-value and Null distribution in the way I did, and get to roughly the right answer even without knowing or implementing either of the classical statistical methods shown here.\nFrom my perspective, I don’t think it’s a case of either-or when it comes to which kind of approach we use - Hacker Stats or ‘Proper’ Stats. Indeed, I think it’s from these simulation based examples, where we can run a little experiment and see what happens, that we can develop the kind of deep intuition about the Null hypothesis - and so p-values, statistical significance, and the bread-and-butter of a lot of conventional statistical learning - that we need to be effective statisticians. It’s likely only by historical accident, in my view, that Hacker Stats are often only taught later in courses, and classical approaches taught first. Resampling methods can be both the Alpha of statistics, because they help to develop the deep intuitions through clear examples that don’t rely on much algebra, and also the Omega of statistics, because some quantities of interest just aren’t easy (and in some cases may be impossible) to derive analytic solutions to."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/index.html#hacker-stats-and-hypothesis-testing-with-the-infer-package",
    "href": "pages/extra-courses/hacker-stats/index.html#hacker-stats-and-hypothesis-testing-with-the-infer-package",
    "title": "Hacker Stats",
    "section": "Hacker Stats and Hypothesis Testing with the Infer package",
    "text": "Hacker Stats and Hypothesis Testing with the Infer package\n\nIntroduction\nIn this post, I show how the infer package, can be used to perform both bootstrapping and permutation testing in a way that’s slightly easier, and more declarative in the context of a general hypothesis testing framework.\n\n\nSetting up\nLet’s install the infer packge and try a couple of examples from the documentation.\n\n\nCode\n# install.packages(\"infer\") # First time around\nlibrary(tidyverse)\nlibrary(infer)\n\n\n\n\nThe infer package\nFrom the vignette page we can see that infer’s workflow is framed around four verbs:\n\nspecify() allows you to specify the variable, or relationship between variables, that you’re interested in.\nhypothesize() allows you to declare the null hypothesis.\ngenerate() allows you to generate data reflecting the null hypothesis.\ncalculate() allows you to calculate a distribution of statistics from the generated data to form the null distribution.\n\nThe package describes the problem of hypothesis testing as being somewhat generic, regardless of the specific test, hypothesis, or dataset being used:\n\nRegardless of which hypothesis test we’re using, we’re still asking the same kind of question: is the effect/difference in our observed data real, or due to chance? To answer this question, we start by assuming that the observed data came from some world where “nothing is going on” (i.e. the observed effect was simply due to random chance), and call this assumption our null hypothesis. (In reality, we might not believe in the null hypothesis at all—the null hypothesis is in opposition to the alternate hypothesis, which supposes that the effect present in the observed data is actually due to the fact that “something is going on.”) We then calculate a test statistic from our data that describes the observed effect. We can use this test statistic to calculate a p-value, giving the probability that our observed data could come about if the null hypothesis was true. If this probability is below some pre-defined significance level \\(\\alpha\\), then we can reject our null hypothesis.\n\n\n\nThe gss dataset\nLet’s look through - and in some places adapt - the examples used. These mainly make use of the gss dataset.\n\n\nCode\ndata(gss)\n\n\n\n\nCode\nglimpse(gss)\n\n\nRows: 500\nColumns: 11\n$ year    &lt;dbl&gt; 2014, 1994, 1998, 1996, 1994, 1996, 1990, 2016, 2000, 1998, 20…\n$ age     &lt;dbl&gt; 36, 34, 24, 42, 31, 32, 48, 36, 30, 33, 21, 30, 38, 49, 25, 56…\n$ sex     &lt;fct&gt; male, female, male, male, male, female, female, female, female…\n$ college &lt;fct&gt; degree, no degree, degree, no degree, degree, no degree, no de…\n$ partyid &lt;fct&gt; ind, rep, ind, ind, rep, rep, dem, ind, rep, dem, dem, ind, de…\n$ hompop  &lt;dbl&gt; 3, 4, 1, 4, 2, 4, 2, 1, 5, 2, 4, 3, 4, 4, 2, 2, 3, 2, 1, 2, 5,…\n$ hours   &lt;dbl&gt; 50, 31, 40, 40, 40, 53, 32, 20, 40, 40, 23, 52, 38, 72, 48, 40…\n$ income  &lt;ord&gt; $25000 or more, $20000 - 24999, $25000 or more, $25000 or more…\n$ class   &lt;fct&gt; middle class, working class, working class, working class, mid…\n$ finrela &lt;fct&gt; below average, below average, below average, above average, ab…\n$ weight  &lt;dbl&gt; 0.8960034, 1.0825000, 0.5501000, 1.0864000, 1.0825000, 1.08640…\n\n\n\n\nExample 1: Categorical Predictor; Continuous Response\nLet’s go slightly off piste and say we are interested in seeing if there is a relationship between age, a cardinal variable, and sex, a categorical variable. We can start by stating our null and alternative hypotheses explicitly:\n\nNull hypothesis: There is no difference between age and sex\nAlt hypothesis: There is a difference between age and sex\n\nLet’s see if we can start by just looking at the data to see if, informally, it looks like it might better fit the Null or Alt hypothesis.\n\n\nCode\ngss |&gt; \n    ggplot(aes(x=age, group = sex, colour = sex)) + \n    geom_density()\n\n\n\n\n\nIt looks like the densities of age distributions are similar for both sexes. However, they’re not identical. Are the differences more likely to be due to chance, or are they more structural?\nWe can start by calculating, say, the differences in average ages between males and females:\n\n\nCode\ngss |&gt;\n    group_by(sex) |&gt;\n    summarise(n = n(), mean_age = mean(age))\n\n\n# A tibble: 2 × 3\n  sex        n mean_age\n  &lt;fct&gt;  &lt;int&gt;    &lt;dbl&gt;\n1 male     263     40.6\n2 female   237     39.9\n\n\n\nOur first testable hypothesis (using permutation testing/sampling without replacement)\nThe mean age is 40.6 for males and 39.9 for females, a difference of about 0.7 years of age. Could this have occurred by chance?\nThere are 263 male observations, and 237 female observations, in the dataset. Imagine that the ages are values, and the sexes are labels that are added to these values.\nOne approach to operationalising the concept of the Null Hypothesis is to ask: If we shifted around the labels assigned to the values, so there were still as many male and female labels, but they were randomly reassigned, what would the difference in mean age between these two groups be? What would happen if we did this many times?\nThis is the essence of building a Null distribution using a permutation test, which is similar to a bootstrap except it involves resampling with replacement rather than without replacement.\nWe can perform this permutation test using the infer package as follows:\n\n\nCode\nmodel &lt;- gss |&gt;\n    specify(age ~ sex) |&gt;\n    hypothesize(null = 'independence') |&gt;\n    generate(reps = 10000, type = 'permute')\n\nmodel\n\n\nResponse: age (numeric)\nExplanatory: sex (factor)\nNull Hypothesis: independence\n# A tibble: 5,000,000 × 3\n# Groups:   replicate [10,000]\n     age sex    replicate\n   &lt;dbl&gt; &lt;fct&gt;      &lt;int&gt;\n 1    57 male           1\n 2    49 female         1\n 3    61 male           1\n 4    23 male           1\n 5    20 male           1\n 6    36 female         1\n 7    42 female         1\n 8    28 female         1\n 9    51 female         1\n10    32 female         1\n# ℹ 4,999,990 more rows\n\n\nThe infer package has now arbitrarily shifted around the labels assigned to the age values 10000 times. Each time is labelled with a different replicate number. Let’s take the first nine replicates and show what the densities by sex look like:\n\n\nCode\nmodel |&gt;\n    filter(replicate &lt;= 9) |&gt;\n    ggplot(aes(x=age, group = sex, colour = sex)) + \n    geom_density() + \n    facet_wrap(~replicate)\n\n\n\n\n\nWhat if we now look at the differences in means apparent in each of these permutations\n\n\nCode\nmodel |&gt;\n    calculate(stat = \"diff in means\", order = c(\"male\", \"female\")) |&gt;\n    visualize()\n\n\n\n\n\nHere we can see the distribution of differences in means follows broadly a normal distribution, which appears to be centred on 0.\nLet’s now calculate and save the observed difference in means.\n\n\nCode\ntmp &lt;- gss |&gt;\n    group_by(sex) |&gt;\n    summarise(mean_age = mean(age))\n\ntmp \n\n\n# A tibble: 2 × 2\n  sex    mean_age\n  &lt;fct&gt;     &lt;dbl&gt;\n1 male       40.6\n2 female     39.9\n\n\nCode\ndiff_means &lt;- tmp$mean_age[tmp$sex == \"male\"] - tmp$mean_age[tmp$sex == \"female\"]\n\ndiff_means\n\n\n[1] 0.7463541\n\n\n\n\nA two-sided hypothesis\nLet’s now show where the observed difference in means falls along the distribution of differences in means generated by this permutation-based Null distribution:\n\n\nCode\nmodel |&gt;\n    calculate(stat = \"diff in means\", order = c(\"male\", \"female\")) |&gt;\n    visualize() +\n    shade_p_value(obs_stat = diff_means, direction = \"two-sided\")\n\n\n\n\n\nThe observed difference in means appears to be quite close to the centre of mass for the distribution of differences in means generated by the Null distribution. So it appears very likely that this observed difference could be generated from a data generating process in which there’s no real difference in mean ages between the two groups. We can formalise this slightly by calcuating a p-value:\n\n\nCode\nmodel |&gt;\n    calculate(stat = \"diff in means\", order = c(\"male\", \"female\")) |&gt;\n    get_p_value(obs_stat = diff_means, direction = \"two-sided\")\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.527\n\n\nThe p value is much, much greater than 0.05, suggesting there’s little evidence to reject the Null hypothesis, that in this dataset age is not influenced by sex.\n\n\n\nExample 2: Categorical Predictor; Categorical Response\nNow let’s look at the two variables college and partyid:\n\ncollege: Can be degree or no degree\npartyid: Can be ind rep, dem, other\n\nThe simplest type of hypothesis to state is probably something like:\n\nNull Hypothesis: There is no relationship between partyid and college\nAlt Hypothesis: There is a relationship between partyid and college\n\nWe can then consider more specific and targetted hypotheses at a later date.\nLet’s see how we could use infer to help decide between these hypotheses, using a permutation test:\n\n\nCode\nmodel &lt;- gss |&gt;\n    specify(partyid ~ college) |&gt;\n    hypothesize(null = 'independence') |&gt;\n    generate(reps = 10000, type = 'permute')\n\nmodel\n\n\nResponse: partyid (factor)\nExplanatory: college (factor)\nNull Hypothesis: independence\n# A tibble: 5,000,000 × 3\n# Groups:   replicate [10,000]\n   partyid college   replicate\n   &lt;fct&gt;   &lt;fct&gt;         &lt;int&gt;\n 1 rep     degree            1\n 2 rep     no degree         1\n 3 dem     degree            1\n 4 rep     no degree         1\n 5 other   degree            1\n 6 ind     no degree         1\n 7 dem     no degree         1\n 8 rep     degree            1\n 9 rep     degree            1\n10 ind     no degree         1\n# ℹ 4,999,990 more rows\n\n\nLet’s visualise the relationship between partyid and college in the first nine replicates:\n\n\nCode\nmodel |&gt;\n    filter(replicate &lt;= 9) |&gt;\n    ggplot(aes(x = college, fill = partyid)) + \n    geom_bar(position = \"fill\") + \n    facet_wrap(~replicate) +\n    labs(title = \"Permuted (fake) datasets\")\n\n\n\n\n\nAnd how does this compare with the observed dataset?\n\n\nCode\ngss |&gt;\n    ggplot(aes(x = college, fill = partyid)) + \n    geom_bar(position = \"fill\") + \n    labs(title = \"Relationship in real dataset\")\n\n\n\n\n\nBut what summary statistic can we use for comparing the observed level of extremeness of any apparent association between the two variables, with summary statistics under the Null hypothesis (i.e. using permutation testing)? The standard answer is to calculate the Chi-squared statistic, as detailed here.\nFirst, what’s the Chi-squared value we get from the observed data?\n\n\nCode\nChisq_obs &lt;- gss |&gt;\n    specify(partyid ~ college) |&gt;\n    hypothesize(null = \"independence\") |&gt;\n    calculate(stat = \"Chisq\")\n\nChisq_obs\n\n\nResponse: partyid (factor)\nExplanatory: college (factor)\nNull Hypothesis: independence\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  4.15\n\n\nSo, the value is 4.15. Is this a big or a small value?\nTo answer that let’s calculate the same statistic from the Null distribution\n\n\nCode\nchi_dist_null &lt;- model |&gt;\n    calculate(stat = \"Chisq\")\n\nchi_dist_null\n\n\nResponse: partyid (factor)\nExplanatory: college (factor)\nNull Hypothesis: independence\n# A tibble: 10,000 × 2\n   replicate  stat\n       &lt;int&gt; &lt;dbl&gt;\n 1         1 2.08 \n 2         2 1.29 \n 3         3 0.508\n 4         4 3.71 \n 5         5 0.438\n 6         6 2.35 \n 7         7 7.17 \n 8         8 6.03 \n 9         9 2.67 \n10        10 0.834\n# ℹ 9,990 more rows\n\n\nSo, is the observed value something that could have been plausibly generated from the Null distribution? We can answer this by seeing how extreme the observed Chi-squared value is compared with the distribution of values under the Null:\n\n\nCode\nvisualise(chi_dist_null) +\n    shade_p_value(obs_stat = Chisq_obs, direction = \"greater\")\n\n\n\n\n\nSo, it looks like it’s fairly likely that the value we observed could have been observed under the Null, a scenario in which there’s no true relationship between the variables. But how likely?\n\n\nCode\nchi_dist_null |&gt;\n    get_p_value(obs_stat = Chisq_obs, direction = \"greater\")\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.245\n\n\nAround a quarter of Chi-squared values under the Null are as greater or greater than that observed in the real dataset. So there’s not great evidence of there being a relationship between having a degree and distribution of party affiliations.\nInfer makes it fairly straightforward to calculate the extremeness of our observed test statistic using the analytic/theoretical approach too, using the assume() verb:\n\n\nCode\nnull_dist_theory &lt;- gss %&gt;%\n    specify(partyid ~ college) |&gt;\n    assume(distribution = \"Chisq\")\n\nvisualize(null_dist_theory) +\n  shade_p_value(obs_stat = Chisq_obs, direction = \"greater\")\n\n\n\n\n\n\n\nCode\nnull_dist_theory |&gt;\n    get_p_value(obs_stat = Chisq_obs, direction = \"greater\")\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.386\n\n\nHere the theoretical distribution suggests the observed value is even more likely to have been observed by chance under the Null, than using the permutation-based approach.\nAnd we can show both approaches together:\n\n\nCode\nchi_dist_null |&gt;\n    visualise(method = \"both\") +\n    shade_p_value(obs_stat = Chisq_obs, direction = \"greater\")\n\n\n\n\n\nHere we can see the resampling-based distribution (the histogram) has more values lower than the observed value, and fewer values higher than the observed value, than the theoretical distribution (the density line), which helps to explain the difference in p-values calculated.\n\n\nSumming up\nSo, that’s a brief introduction to the infer package. It provides a clear and opinionated way of thinking about and constructing hypothesis tests using a small series of verbs, and as part of this handles a lot of the code for performing permutation tests, visualising data, and comparing resampling-based estimates of the Null distribution with theoretical estimates of the same quantities. And, though both of the examples I’ve shown above are about permutation testing, it also allows for bootstrapped calculations to be performed too.\nIn some ways, infer seems largely intended as a pedagogic/teaching tool, for understanding the intuition behind the concept of the Null hypothesis and distribution, and so what a p-value actually means. However you can see that it does abstract away some of the computational complexity involved in producing Null distributions using both resampling and ‘traditional’ approaches. In previous posts we showed that it’s not necessarily too difficult to produce resampled distributions without this, but there’s still potentially some quality-of-life benefits to using it."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/index.html#post-stratification-1",
    "href": "pages/extra-courses/hacker-stats/index.html#post-stratification-1",
    "title": "Hacker Stats",
    "section": "Post Stratification",
    "text": "Post Stratification\n\nIntroduction\nAt the top of this page on Hacker Stats, I mentioned that resampling methods can be used to perform post-stratification, meaning reweighting of observations from a sample in such a way as to make them more representative of the population of interest to us. Let’s look at this using a variation of the red coin/blue coin example from a couple of posts ago.\n\n\nRed Coin/Blue Coin\nImagine we have a population of two types of coin:\n\nRed Coins, which come up heads 65% of the time\nBlue Coins, which come up heads 47% of the time\n\nWithin our population, we know 75% of the coins are Blue coins, and 25 of the coins are Red Coins.\nHowever, our sample contains 20 red coins, and 20 blue coins. i.e. the distribution of coin types in our sample is different to that in our population.\nLet’s first create this sample dataset:\n\n\nCode\nlibrary(tidyverse)\n\nset.seed(9)\n\ndraws_red &lt;- rbinom(n=20, size = 1, prob = 0.65)\ndraws_blue &lt;- rbinom(n=20, size = 1, prob = 0.47)\n\ncoin_colour &lt;- c(\n    rep(\"red\", 20),\n    rep(\"blue\", 20)\n)\n\nreal_sample_data &lt;- data.frame(\n    coin_colour = coin_colour, \n    outcome = c(draws_red, draws_blue)\n)\n\nrm(draws_red, draws_blue, coin_colour)\n\nhead(real_sample_data)\n\n\n  coin_colour outcome\n1         red       1\n2         red       1\n3         red       1\n4         red       1\n5         red       1\n6         red       1\n\n\nWhat’s the expected probability of heads in the sample?\n\n\nCode\nmean(real_sample_data$outcome)\n\n\n[1] 0.65\n\n\nCode\nreal_sample_data |&gt;\n    group_by(coin_colour) |&gt;\n    summarise(prop = mean(outcome))\n\n\n# A tibble: 2 × 2\n  coin_colour  prop\n  &lt;chr&gt;       &lt;dbl&gt;\n1 blue          0.5\n2 red           0.8\n\n\nOverall, 65% of the sample - 20 reds, 20 blues - are heads. The proportion of blues is 50%, and of reds is 80%. So, it so happens that, with this random number seed, the proportions in the sample of both reds and blues are higher than the theoretical average (the prob value arguments in the code above).\nLet’s now try to use bootstrapping to calculate a distribution around the sample mean:\n\n\nCode\nbootstrap_means &lt;- function(x, nReps = 10000){\n    out &lt;- vector(\"numeric\", nReps) \n\n    for (i in 1:nReps){\n        this_resample &lt;- sample(\n            x=x, \n            size = length(x), \n            replace = TRUE # This is what makes it bootstrapping\n        )\n        out[i] &lt;- mean(this_resample)\n    }\n    out\n}\n\nbootstrapped_means &lt;- bootstrap_means(real_sample_data$outcome)\n\nhead(bootstrapped_means)\n\n\n[1] 0.750 0.625 0.700 0.775 0.800 0.700\n\n\nWhat does this look like as a histogram?\n\n\nCode\ntibble(value = bootstrapped_means) |&gt;\n    ggplot(aes(x = value)) + \n    geom_histogram(bins = 50)\n\n\n\n\n\nWe can see the familiar bell-shaped distribution of values here. What about for blues and reds separately?\n\n\nCode\nbootstrapped_means_reds &lt;- bootstrap_means(\n    real_sample_data |&gt;\n        filter(coin_colour == \"red\") |&gt;\n        pull('outcome')  \n    )\n\nbootstrapped_means_blues &lt;- bootstrap_means(\n    real_sample_data |&gt;\n        filter(coin_colour == \"blue\") |&gt;\n        pull('outcome')  \n    )\n\n\n\n\nhead(bootstrapped_means_reds)\n\n\n[1] 0.65 0.70 0.85 0.85 1.00 0.60\n\n\nCode\nhead(bootstrapped_means_blues)\n\n\n[1] 0.45 0.60 0.50 0.45 0.70 0.55\n\n\nAnd what do these two distributions look like?\n\n\nCode\ntibble(\n    rep = 1:length(bootstrapped_means_reds),\n    red = bootstrapped_means_reds,\n    blue = bootstrapped_means_blues\n) |&gt;\n    pivot_longer(\n        cols = c(red, blue),\n        names_to = \"colour\",\n        values_to = \"value\"\n    ) |&gt;\n    ggplot(aes(x = value, fill = colour)) + \n    geom_histogram(bins = 50, position = \"dodge\")\n\n\n\n\n\nSo it’s clear the distributions for mean values of the two different coin types are different, even though there’s some overlap.\nLet’s now look at doing some post-stratification, where we sample from the two groups in proportion to the relative probabilities of encountering observations from the two groups in the population as compared with the sample. Let’s think through what this means:\n\nProportions by group in sample and population\n\n\nGroup\nSample\nPopulation\nRatio\n\n\n\n\nBlue\n0.5\n0.75\n\\(3/2\\)\n\n\nRed\n0.5\n0.25\n\\(1/2\\)\n\n\nColumn Sum\n1.00\n1.00\n\n\n\n\nIn this table, the ratio is the row-wise ratio of the population value divided by the sample value. Note that the ratios have a common denominator, 2, which we can drop in defining the probability weights, leaving us with 3 for blue and 1 for red.\nWe can adapt the standard bootstrapping approach by using the prob argument in the sample() function, using these weights:\n\n\nCode\nsample_weights &lt;- \n    tibble(\n        coin_colour = c(\"blue\", \"red\"),\n        wt = c(3, 1)\n    )\n\nreal_sample_data_wt &lt;- \n    left_join(\n        real_sample_data, sample_weights\n    )\n\nreal_sample_data_wt\n\n\n   coin_colour outcome wt\n1          red       1  1\n2          red       1  1\n3          red       1  1\n4          red       1  1\n5          red       1  1\n6          red       1  1\n7          red       1  1\n8          red       1  1\n9          red       0  1\n10         red       0  1\n11         red       1  1\n12         red       1  1\n13         red       0  1\n14         red       1  1\n15         red       1  1\n16         red       1  1\n17         red       1  1\n18         red       0  1\n19         red       1  1\n20         red       1  1\n21        blue       1  3\n22        blue       0  3\n23        blue       0  3\n24        blue       0  3\n25        blue       0  3\n26        blue       1  3\n27        blue       0  3\n28        blue       0  3\n29        blue       1  3\n30        blue       1  3\n31        blue       0  3\n32        blue       1  3\n33        blue       0  3\n34        blue       1  3\n35        blue       0  3\n36        blue       1  3\n37        blue       1  3\n38        blue       1  3\n39        blue       0  3\n40        blue       1  3\n\n\nAnd now a slightly modified version of the bootstrapping function:\n\n\nCode\nbootstrap_means_wt &lt;- function(x, wt, nReps = 10000){ #wt is the weighting\n    out &lt;- vector(\"numeric\", nReps) \n\n    for (i in 1:nReps){\n        this_resample &lt;- sample(\n            x=x, \n            size = length(x), \n            prob = wt, # This is the new argument\n            replace = TRUE # This is what makes it bootstrapping\n        )\n        out[i] &lt;- mean(this_resample)\n    }\n    out\n}\n\n\nAnd to run:\n\n\nCode\nbootstrapped_means_poststratified &lt;- bootstrap_means_wt(\n    x = real_sample_data_wt$outcome,\n    wt = real_sample_data_wt$wt\n)\n\nhead(bootstrapped_means_poststratified)\n\n\n[1] 0.750 0.550 0.625 0.525 0.575 0.600\n\n\nNow, analytically, we can calculate what the mean of the population should be given the proportion of blues and reds, and the proportion of blues that are heads, and proportion of reds that are heads:\n\n\nCode\nheads_if_blue &lt;- 0.47\nheads_if_red &lt;- 0.65\n\nexpected_pop_prop_heads &lt;- (3/4) * heads_if_blue + (1/4) * heads_if_red\n\nexpected_pop_prop_heads\n\n\n[1] 0.515\n\n\nSo within the population we would expect 51.5% of coins to come up heads.\nLet’s now look at the bootstrapped and reweighted distribution to see where 0.515 fits within this distribution:\n\n\nCode\nggplot() + \n    geom_histogram(aes(x = bootstrapped_means_poststratified), bins=50) + \n    geom_vline(aes(xintercept = expected_pop_prop_heads), linewidth = 1.2, colour = \"purple\")\n\n\n\n\n\nSo we can see that the true population mean falls within the reweighted bootstrapped distribution of the values of the mean estimated. How about if we had not performed reweighting on the sample?\n\n\nCode\ntibble(value = bootstrapped_means) |&gt;\n    ggplot() + \n    geom_histogram(aes(x = value), bins=50) + \n    geom_vline(aes(xintercept = expected_pop_prop_heads), linewidth = 1.2, colour = \"purple\")\n\n\n\n\n\nSo, although on this occasion, the true population value is also within the range of the un-reweighted bootstrapped distribution, it is further from the centre of this distribution’s mass.\nLet’s give some numbers to the above. What proportion of the bootstrapped values are below the true population value?\nFirst without reweighting:\n\n\nCode\nmean(bootstrapped_means &lt; expected_pop_prop_heads)\n\n\n[1] 0.0343\n\n\nOnly about 3.4% of the means from the unweighted bootstrapping were more extreme than the true population value.\nAnd now with reweighting:\n\n\nCode\nmean(bootstrapped_means_poststratified &lt; expected_pop_prop_heads)\n\n\n[1] 0.2102\n\n\nNow 22.4% of values of the means from the reweighted/post-stratified bootstrapped distribution are below the true value. This is the difference between the true value being in the 90% central interval or not.\n\n\nSummary\nIn this post we’ve illustrated the importance of post-stratifying data were we know a sample is biased in terms of the relative weight given to the strata it contains as compared with the population. We’ve also shown, using Base R functions alone, how to perform this post-stratification using just two additional changes: a vector of weights, which was fairly straightforward to calculate; and the passing of this vector of weights to the prob argument in the sample() function. In this section we’ve focused on a hypothetical example, and built the requisite functions and code from scratch."
  },
  {
    "objectID": "pages/extra-courses/hacker-stats/index.html#overall-summary",
    "href": "pages/extra-courses/hacker-stats/index.html#overall-summary",
    "title": "Hacker Stats",
    "section": "Overall summary",
    "text": "Overall summary\nThis page was originally a series of posts to my blog, with material originally a bit out of order. This means I covered some of the same topics in multiple ways. Some of what remains on this page might still be a bit disjointed as a result of this.\nAfter running through a number of examples in base R, building bootstrapping and permutation functions from scratch. I then moved onto the infer package, which makes resampling a bit more straightforward.\nIn practice the infer package is mainly intended for teaching the intuitions of hypothesis testing more generally. For using bootstrapping and other Hacker Stats in practice it can still be helpful either to build the requisite functions in Base R, or to use packages like like survey and svrep for post-stratification, and boot for bootstrapping and permutation testing."
  },
  {
    "objectID": "pages/main-course/statistics-as-circuits/index.html",
    "href": "pages/main-course/statistics-as-circuits/index.html",
    "title": "Statistics as Circuit Boards",
    "section": "",
    "text": "The general approach I advocate for thinking about statistics is, for me, predicated on a series of related mental models for thinking about statistical inference and what we can do with statistical models. And these mental models are both graphical, and have some similarity with circuit board schematics, or more generally graphical representations of complex systems. To the extent these mental models have been useful for me, I hope they’ll be useful to others as well.\nIn the handwritten notes below I’ll try to show some of these mental models, and how they can help demystify some of the processes and opportunities involved in statistical inference"
  },
  {
    "objectID": "pages/main-course/statistics-as-circuits/index.html#introduction",
    "href": "pages/main-course/statistics-as-circuits/index.html#introduction",
    "title": "Statistics as Circuit Boards",
    "section": "",
    "text": "The general approach I advocate for thinking about statistics is, for me, predicated on a series of related mental models for thinking about statistical inference and what we can do with statistical models. And these mental models are both graphical, and have some similarity with circuit board schematics, or more generally graphical representations of complex systems. To the extent these mental models have been useful for me, I hope they’ll be useful to others as well.\nIn the handwritten notes below I’ll try to show some of these mental models, and how they can help demystify some of the processes and opportunities involved in statistical inference"
  },
  {
    "objectID": "pages/main-course/statistics-as-circuits/index.html#terminology",
    "href": "pages/main-course/statistics-as-circuits/index.html#terminology",
    "title": "Statistics as Circuit Boards",
    "section": "Terminology",
    "text": "Terminology\n\nData for the model\nAs I recently discussed in a post relating to multivariate models, ultimately almost all models work with a big rectangle of data: each row an observation, each column a variable. We can call this big rectangle \\(D\\). Then, we need to imagine a way of splitting out this rectangle into two pieces: the model inputs (or predictor matrix), which we call \\(X\\), and the model outputs (or response matrix) which we call \\(y\\).\nTo try to represent this, I first thought about a big piece of square-lined paper, with a vertical perforation on it. Tear along this vertical perforation, and one of the pieces of paper is the output \\(y\\), and the other the input \\(X\\).\nHowever, I then realised the \\(X\\)/\\(y\\) distinction is probably clearer to express symbolically as two complementary shapes: the input \\(X\\) being basically a rectangle with a right-facing chevron, and the output \\(y\\) being a rectangle with a chevron-shaped section missing from it on the left.\n\n\n\nSymbols\n\n\n\n\nModel components\nAt a high enough level of generalisation, there are basically two component types that statistical models contain: stochastic components, denoted \\(f(.)\\), and deterministic components, denoted \\(g(.)\\). Within the figure, I’m referring to the deterministic components as transformers and the stochastic components as noisemakers. I decided to draw the transformers as triangles, and the noisemakers as ribbons."
  },
  {
    "objectID": "pages/main-course/statistics-as-circuits/index.html#model-fittingcalibration",
    "href": "pages/main-course/statistics-as-circuits/index.html#model-fittingcalibration",
    "title": "Statistics as Circuit Boards",
    "section": "Model fitting/calibration",
    "text": "Model fitting/calibration\nBoth transformers and noisemakers require specific parameters. Imagine these as a series of dials on two separate panels. The parameters for the transformers are usually referred to as \\(\\beta\\) (‘beta’), which can be either (and rarely) a single value, or a vector of values. And the parameters for the noisemakers are usually referred to as \\(\\alpha\\) (‘alpha’). There are many possible values of \\(\\beta\\) and \\(\\alpha\\) that a model can accept - many different ways the dials on the two panels can be set - and the main challenge of fitting a statistical model is to decide on the best configuration of \\(\\beta\\) and \\(\\alpha\\) to set the model to.\nAnd what does ‘best’ mean? Broadly, that the discrepancy between what comes out of the model, \\(Y\\), and the corresponding outcome values in the dataset, \\(y\\), is minimised in some way. Essentially, this discrepancy, \\(\\delta\\), is calculated with the current configuration of \\(\\beta\\) and \\(\\alpha\\), and then some kind of algorithm is applied to make a decision about how to adjust the dials. With the dials now adjusted, new model predictions \\(Y\\) are produced, leading to a new discrepancy value \\(\\delta\\). If necessary, then the calibration algorithm is applied once again, so the \\(\\alpha\\) and \\(\\beta\\) parameters adjusted, and so the parameter operationalisation loop is repeated, until some kind of condition is met defining when the parameters identified are good enough.\n\n\n\nModel fitting\n\n\nThere are a number of possible ways of arriving at the ‘best’ parameter configuration. One approach is to employ an analytical solution, such as with the least-squares or generalised least-squares methods. In these cases some kind of algebraic ‘magic’ is performed and - poof! - the parameters just drop out instantly from the solution, meaning no repeated interation. In all other cases, however, it’s likely the predict-compare-calibrate cycle will be repeated many times, either until the error is deemed small enough, or until some kind of resource-based stopping condition - such as “stop after 10,000 tries” - has been reached.\nThis iterative cyclic quality of model fitting applies regardless of whether frequentist models - using maximum likelihood estimation - or Bayesian models - employing something like Hamiltonian Monte-Carlo estimation - have been employed. Both involve trying to minimise a loss function, i.e. the error \\(\\delta\\) though updating the current best estimate of the parameter set \\(\\beta\\) and \\(\\alpha\\). 1"
  },
  {
    "objectID": "pages/main-course/statistics-as-circuits/index.html#model-simulation",
    "href": "pages/main-course/statistics-as-circuits/index.html#model-simulation",
    "title": "Statistics as Circuit Boards",
    "section": "Model simulation",
    "text": "Model simulation\n\nSimple simulation\nOnce the model \\(M\\) has been calibrated, i.e. the best possible set of parameters \\(\\beta\\) (for the transformer, \\(g(.)\\)) and \\(\\alpha\\) (for the noisemaker, \\(f(.)\\)) have been identified in the calibration set, the model can now be used for prediction, projection, interpolation, extrapolation, and simulation more generally.\nThe challenges are two-fold: knowing how to ‘ask the model questions’; and knowing how to interpret the answers the model gives.\n\n\n\nModel prediction\n\n\nTo ‘ask the model questions’, we need to specify some input data, \\(X\\), to put into the model. This input data could be taken from the same dataset used to calibrate the model in the first place. But it doesn’t have to be. We could ask the model to produce a prediction for configurations of input the model has never seen before. In this part of the complete example, we asked the model to predict levels of tooth growth where the dosage was between the dosage values in the dataset; this is an example of interpolation. In the same dataset we also saw examples of extrapolation, including dosage levels that were predicted/projected to lead to negative tooth growth, i.e. impossible values. So, a key challenge in asking questions of the model, through proposing an input predictor matrix \\(X\\), is to know which questions are and are not sensible to ask.\nKnowing how to interpret the answers from the model is the second part of the challenge. If we run the model in its entirety, the values from the systematic component (transformer) are passed to the stochastic component (noisemaker), meaning we’ll get different answers each time.2 For some types of model, we can just extract the results of the transformer part alone, and so produce expected values. If we pass the values from the transformer to the noisemaker, however, we’ll end up with a distribution of values from the model, even though the calibration parameters \\(\\beta\\) and \\(\\alpha\\), and input data \\(X\\) are non-varying. So, we may need to choose a way to summarise this distribution. For example, we may want to know the proportion of occasions/draws that exceed a particular threshold value \\(\\tau\\). Or may want to calculate the median and a prediction interval.\n\n\nSimulating ‘trials’\nOnce the fundamentals of simulation using statistical model simulations are understood, it’s just a small step to producing hypothetical simulation-based ‘trials’. Just apply two different input datasets \\(X_0\\) and \\(X_1\\) to the same model \\(M\\). These two datasets should differ only in terms of a specific exposure variable of interest \\(Z\\), with all other inputs kept the same. This is equivalent to magicking up the hypothetical ‘platinum standard’3 discussed in the course on causal inference: imagine exactly the same individual being observed in two different worlds, where only one thing (exposed/not exposed, or treated/not treated) is different.\n\n\n\nSimulated ‘trial’\n\n\nWhen it comes to intepretating the outputs, the job is now to compare between the outputs generated when \\(X_0\\) is passed to \\(M\\), and when \\(X_1\\) is passed to \\(M\\). Call these outputs \\(Y_1\\) and \\(Y_0\\) respectively; our treatment or exposure effect estimate is therefore the difference: \\(Y_1 - Y_0\\).\n\n\nSimulating with honest uncertainty\nOnce you’re familiar with the last couple of steps - how to ‘ask models questions’, and how to ‘perform simulated trials’, the last challenge is how to do so honestly.\nBy honestly, I mean with appropriate acknowledgement of the effect that parameter uncertainty has on uncertainty in model outputs. We don’t really know the ‘true’ values of the parameter values \\(\\beta\\) and \\(\\alpha\\); we’ve just estimated them. And because they’re estimated, we’re not certain of their true value.\n\n\n\nSimulation with uncertainty\n\n\nSo, in order to represent the effect this parameter uncertainty has on the model outputs, there needs to be a way of generating and passing lots of ‘plausible parameter values’, \\(\\theta = \\{ \\beta, \\alpha \\}\\), to the model. This means there’s a collection, or ensemble, of parameter values for the model, \\(\\tilde{\\theta}\\), and so an ensemble of models - each with a slightly different parameter configuration - that the predictor matrix \\(X\\) goes into.\nAnd this then means that there’s an ensemble of model outputs, and so again a need to think about to summarise the distribution of outputs. Note that, because the variation in outputs comes about because of variation in the model parameters, a distribution of \\(Y\\) is generated even if the noisemakers (\\(f(.)\\)) are turned off, i.e. even if estimating for expected values rather than predicted values.\nAnd how is the ensemble of parameter values produced? There are basically three approaches:\n\nAnalytical approximate solutions using something called the Delta Method: Not discussed here\nSimulation methods involving normal approximations for frequentist-based models\nUse the converged Bayesian posterior distribution.\n\nIn the figure, the Bayesian approach is shown on the right, and the simulation approach is shown on the left. A Bayesian converged posterior distribution is a distribution of plausible parameter values after the calibration process has been run enough times. It’s ideal for doing simulation with honest uncertainty, and was discussed back in the marbles-and-jumping-beans post. The downside is Bayesian models can take longer to run, and require more specialist software and algorithms to be installed/used.\nThe simulation approach for frequentist statistics was covered in the complete simulation example of the series, with its rationale developed over a few earlier posts. The basic aim of this approach is to generate an approximate analogue to the Bayesian posterior distribution, and this usually involves using a model to make inputs to feed into another model. It’s not quite models-all-the-way-down, but is a bit more meta than it may first appear!"
  },
  {
    "objectID": "pages/main-course/statistics-as-circuits/index.html#conclusion",
    "href": "pages/main-course/statistics-as-circuits/index.html#conclusion",
    "title": "Statistics as Circuit Boards",
    "section": "Conclusion",
    "text": "Conclusion\nThis post aimed to re-introduce many of the key intuitions I’ve tried to develop in my main stats post series, but with a focus on the graphical intuition and concepts involved in statistical simulation, rather than just the algebra and R code examples. I hope this provides a useful complementary set of materials for thinking about statistical inference and statistical simulation. As mentioned at the start of the post, this is largely how I tend to think about statistical modelling, and so hopefully this way of thinking is useful for others who want to use statistical models more effectively too."
  },
  {
    "objectID": "pages/main-course/statistics-as-circuits/index.html#footnotes",
    "href": "pages/main-course/statistics-as-circuits/index.html#footnotes",
    "title": "Statistics as Circuit Boards",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOne crucial difference between the frequentist and Bayesian approaches is that, in the frequentist approach, a final set of parameter estimates is identified, equivalent to the dials on the panels being set a particular way and then never touched again. By contrast with the Bayesian approach the parameter set never quite stops changing, though it does tend to change less than it did at the start. The Bayesian approach is like a music producer who’s never quite satisfied with his desk, always tweaking this and that dial, though usually not by much. The technical definition is that frequentist parameter estimation converges to a point (hopefully), whereas Bayesian parameter estimation converges to a distribution (hopefully). This is what I was trying to express through the marble/jumping bean distinction here. The marble finds a position of rest; the jumping bean does not.↩︎\nThough we can set a random number seed to make sure the different answers are the same each time.↩︎\nIf you’re a fan of the niche genre of sci-fi-rom-coms, you could also think of these as “sliding door moments”.↩︎"
  }
]