[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This is my Statistical Methods website.\nHere you can find collated ‘courses’ on a variety of statistical topics, based largely around generalised linear models.\nThe website was built using Quarto in VSCode."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my statistical methods website.\nThis websites contains materials originally developed as part of my blog. The blog should be considered something of a staging post for new material. Once this material has reached enough maturity, it will be ‘promoted’ to this dedicated statisics site."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-15/index.html",
    "href": "pages/causal-inference/lms-are-glms-part-15/index.html",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "",
    "text": "This is the second post on a short mini-series on causal inference. The previous post provided a non-technical introduction to the core challenge of causal inference, namely that the counterfactual is always unobserved, meaning at least half of the data required to really know the causal effect of something is always missing. In the previous post different historians made different assumptions about what the counterfactual would have looked like - what would have happened if something that did happen, hadn’t happened - and based on this came to very different judgements about the effect that Henry Dundas, an 18th century Scottish politician, had on the transatlantic slave trade.\nThis post is more technical, aiming to show: how awkward phrases like “What would have happened if something that did happen, hadn’t happened” are expressed algebraically; how the core problem of causal inference is expressed in this framework; the technical impossibility of addressing the question of causal inference from the Platinum Standard of estimating causal effects on individuals; and describe the reason why randomised controlled trials (RCTs) provide the Gold Standard for trying to estimate these effects for populations."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-15/index.html#introduction",
    "href": "pages/causal-inference/lms-are-glms-part-15/index.html#introduction",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "",
    "text": "This is the second post on a short mini-series on causal inference. The previous post provided a non-technical introduction to the core challenge of causal inference, namely that the counterfactual is always unobserved, meaning at least half of the data required to really know the causal effect of something is always missing. In the previous post different historians made different assumptions about what the counterfactual would have looked like - what would have happened if something that did happen, hadn’t happened - and based on this came to very different judgements about the effect that Henry Dundas, an 18th century Scottish politician, had on the transatlantic slave trade.\nThis post is more technical, aiming to show: how awkward phrases like “What would have happened if something that did happen, hadn’t happened” are expressed algebraically; how the core problem of causal inference is expressed in this framework; the technical impossibility of addressing the question of causal inference from the Platinum Standard of estimating causal effects on individuals; and describe the reason why randomised controlled trials (RCTs) provide the Gold Standard for trying to estimate these effects for populations."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-15/index.html#models-dont-care-about-causality-but-we-do",
    "href": "pages/causal-inference/lms-are-glms-part-15/index.html#models-dont-care-about-causality-but-we-do",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Models don’t care about causality… but we do",
    "text": "Models don’t care about causality… but we do\nThe first stage when using a statistical model is to take a big rectangle of data, \\(D\\), and split the columns of the data into two types:\n\nPredictor variables, usually denoted \\(X\\)\nResponse variables, usually denoted \\(y\\)\n\nWith the predictor variables and the response variables defined, the challenge of model fitting is then to find some combination of model parameters \\(\\theta\\) that minimises in some way the gap between the observed response values \\(y\\), and the predicted response values from the model \\(Y\\).\nThe first point to note is that, from the perspective of the model, it does not matter which variable or variables from \\(D\\) we choose to put in the predictor side \\(X\\) or the response side \\(y\\). Even if we put a variable from the future in as a predictor of something in the past, the optimisation algorithms will still work in exactly the same way, working to minimise the gap between observed and predicted responses. The only problem is such a model would make no sense from a causal perspective.\nThe model also does not ‘care’ about how we think about and go about defining any of the variables that go into the predictor side of the equation, \\(X\\). But again, we do. In particular, when thinking about causality it can be immensely helpful to imagine splitting the predictor columns up into some conceptually different types. This will be helpful for thinking about causal inference using some algebra."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-15/index.html#the-impossible-platinum-standard",
    "href": "pages/causal-inference/lms-are-glms-part-15/index.html#the-impossible-platinum-standard",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "The (Impossible) Platinum Standard",
    "text": "The (Impossible) Platinum Standard\nIn some previous expressions of the data, \\(D\\), we used the subscript \\(i\\) to indicate the rows of the data which go into the model. Each of these rows is, by convention, a different observation. So, instead of saying the purpose of the model is to predict \\(y\\) on \\(X\\), it’s more precisely to predict \\(y_i\\) on \\(X_i\\), for all \\(i\\) in the data (i.e. all rows in \\(D\\)).\nNow let’s do some predictor variable fission and say, for our purposes, that:\n\\[\nX_i = \\{X_i^*, Z_i\\}\n\\]\nHere \\(Z_i\\) is an assignment variable, and takes either a value of 1, meaning ‘is assigned’, or 0, meaning ‘is not assigned’. The variable \\(X_i^*\\), by contrast, means ‘all other predictor variables’.\nFor individual observations \\(D_i\\) where \\(Z_i = 1\\), the individual is exposed (or treated) to something. And for individual observations \\(D_i\\) where \\(Z_i = 0\\), the individual is not exposed (or not treated) to that thing.\nThe causal effect of assignment, or treatment, for any individual observation is:\n\\[\nTE_i = y_i|(X_i^*, Z = 1) - y_i| (X_i^*, Z = 0)\n\\]\nThe fundamental problem of causal inference, however, is that for any individual observation \\(i\\), one of the two parts of this expression is always missing. If an individual \\(i\\) had been assigned, then \\(y_i|(X_i^*, Z=1)\\) is observed, but \\(y_i|(X_i^*, Z=0)\\) is unobserved. By contrast, if an individual \\(i\\) had not been assigned, then \\(y_i|(X_i^*, Z=0)\\) is observed, but \\(y_i|(X_i^*, Z=1)\\) is unobserved.\nAnother way to think about this is as a table, where the treatment effect for an individual involves comparing the outcomes reported in two columns of the same row, but the cells in one of these two columns is always missing:\n\n\n\n\n\n\n\n\n\nindividual\noutcome if treated\noutcome if not treated\ntreatment effect\n\n\n\n\n1\n4.8\n??\n??\n\n\n2\n3.7\n??\n??\n\n\n3\n??\n2.3\n??\n\n\n4\n3.1\n??\n??\n\n\n5\n??\n3.4\n??\n\n\n6\n??\n2.9\n??\n\n\n\nThe Platinum Standard of causal effect estimation would therefore be if the missing cells in the outcome columns could be accurately filled in, allowing the treatment effect for each individual to be calculated.\nHowever, this isn’t possible. It’s social science fiction, as we can’t split the universe and compare parallel realities: one in which what happened didn’t happen, and the other in which what didn’t happen happened.\nSo, what can be done?"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-15/index.html#the-everyday-fools-gold-standard",
    "href": "pages/causal-inference/lms-are-glms-part-15/index.html#the-everyday-fools-gold-standard",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "The Everyday Fool’s Gold Standard",
    "text": "The Everyday Fool’s Gold Standard\nThere’s one thing you might be tempted to do with the kind of data shown in the table above: compare the average outcome in the treated group with the average outcome in the untreated group, i.e.:\n\\[\nATE = E(y | Z = 1) - E(y | Z = 0)\n\\]\nLet’s do this with the example above:\n\n\nCode\ne_y_z1 &lt;- mean(c(4.8, 3.7, 3.1))\ne_y_z0 &lt;- mean(c(2.3, 3.4, 2.9))\n\n\n# And the difference?\ne_y_z1 - e_y_z0\n\n\n[1] 1\n\n\nIn this example, the difference in the averages between the two groups is 1.0.1 Based on this, we might imagine the first individual, who was treated, would have had a score of 3.8 rather than 4.8, and the third individual, who was not treated, would have received a score of 3.3 rather than 2.3 if they had been treated.\nSo, what’s the problem with just comparing the averages in this way? Potentially, nothing. But potentially, a lot. It depends on the data and the problem. More specifically, it depends on the relationship between the assignment variable, \\(Z\\), and the other characteristics of the individual, which includes but is not usually entirely captured by the known additional characteristics of the individual, \\(X_i^*\\).\nLet’s give a specific example: What if I were to tell you that the outcomes \\(y_i\\) were waiting times at public toilets/bathrooms, and the assignment variable, \\(Z\\), takes the value 1 if the individual has been assigned to a facility containing urinals, and 0 if the individual has been assigned to a facility containing no urinals? Would it be right to infer that the difference in the average is the average causal effect of urinals in public toilets/bathrooms?\nI’d suggest not, because there are characteristics of the individual which govern assignment to bathroom type. What this means is that \\(Z_i\\) and \\(X_i^*\\) are coupled or related to each other in some way. So, any difference in the average outcome between those assigned to (or ‘treated with’) urinals could be due to the urinals themselves; or could be due to other ways that ‘the treated’ and ‘the untreated’ differ from each other systematically. We may be able to observe a difference, and to report that it’s statistically significant. But we don’t know how much, if any, of that difference is due to the exposure or treatment of primary interest to us, and how much is due to other ways in the ‘treated’ and ‘untreated’ groups differ.\nSo, we need some way of breaking the link between \\(Z\\) and \\(X^*\\). How do we do this?"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-15/index.html#why-randomised-controlled-trials-are-the-real-gold-standard",
    "href": "pages/causal-inference/lms-are-glms-part-15/index.html#why-randomised-controlled-trials-are-the-real-gold-standard",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Why Randomised Controlled Trials are the real Gold Standard",
    "text": "Why Randomised Controlled Trials are the real Gold Standard\nThe clue’s in the subheading. Randomised Controlled Trials (RCTs) are known as the Gold Standard for scientific evaluation of effects for a reason, and the reason is this: they’re explicitly designed to break the link between \\(Z\\) and \\(X^*\\). And not just \\(X^*\\), but any unobserved or unincluded characteristics of the individuals, \\(W^*\\), which might also otherwise influence assignment or selection to \\(Z\\) but we either couldn’t measure or didn’t choose to include.\nThe key idea of an RCT is that assignment to either a treated or untreated group, or to any additional arms of the trial, has nothing to do with the characteristics of any individual in the trial. Instead, the allocation is random, determined by a figurature (or historically occasionally literal) coin toss. 2\nWhat this random assignment means is that assignment \\(Z\\) should be unrelated to the known characteristics \\(X^*\\), as well as unknown characteristics \\(W^*\\). The technical term for this (if I remember correctly) is that assignment is orthogonal to other characteristics, represented algebraically as \\(Z \\perp X^*\\) and \\(Z \\perp W^*\\).\nThis doesn’t mean that, for any particular trial, there will be zero correlation between \\(Z\\) and other characteristics. Nor does it mean that the characteristics of participants will be the same across trial arms. Because of random variation there are always going to be differences between arms in any specific RCT. However, we know that, because we are aware of the mechanism used to allocate participants to treated or non-treated groups (or more generally to trial arms), the expected difference in characteristics will be zero across many RCTs. Along with increased observations, this is the reason why, in principle, a meta-analysis of methodologically identical RCTs should offer even greater precision as to the causal effect of a treatment than just relying on a single RCT. 3"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-15/index.html#summing-up-and-coming-up",
    "href": "pages/causal-inference/lms-are-glms-part-15/index.html#summing-up-and-coming-up",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Summing up and coming up",
    "text": "Summing up and coming up\nA key point to note is that, when analysing a properly conducted RCT to estimate a treatment effect, the ATE formula shown above, which is naive and likely to be biased when working with observational data, is likely to produce an unbiased estimate of the treatment effect. Because the trial design is sophisticated in the way it breaks the link between \\(Z\\) and everything else, the statistical analysis does not have to be sophisticated.\nThe flip side of this, however, is that when the data are observational, and it would be naive (as with the urinals and waiting times example) to assume that \\(Z\\) is unlinked to everything else known (\\(X^*\\)) and unknown (\\(W^*\\)), then more careful and bespoke statistical modelling approaches are likely to be required to recover non-biased causal effects. Such modelling approaches need to be mindful of both the platinum and gold standards presented above, and rely on modelling and other assumptions to try to simulate what the treatment effects would be if these unobtainable (platinum) and unobtained (gold) standards had been obtained.\nThe next post will start to delve into some of these approaches."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-15/index.html#footnotes",
    "href": "pages/causal-inference/lms-are-glms-part-15/index.html#footnotes",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is pure fluke. I didn’t choose the values to get a difference of exactly 1, but there we go…↩︎\nIn the gold-plated gold standard of the double-blind RCT, not even the people running the trial and interacting with participants would be aware of which treatment a participant has been assigned. They would simply be given a participant ID, find a pack containing the participant’s treatment, and give this pack to the participant. Only a statistician, who has access to a random number cypher, would know which participants are assigned to which treatment, and they might not know until the trial has concluded. The idea of all of these layers of secrecy in assignment is to reduce the possibility that those running the experiment could intentionally or unintentially inform participants about which treatment they’re receiving, and so create expectations in participants about the effectiveness or otherwise of the treatments, which could have an additional effect on the outcomes.↩︎\nIn practice, issues like methodological variation, and publication bias, mean that meta-analyses of RCTs are unlikely to provide as accurate and unbiased an estimate of treatment effect as we would hope for.↩︎"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-16/index.html",
    "href": "pages/causal-inference/lms-are-glms-part-16/index.html",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "",
    "text": "This is the third post in a short mini-series on causal inference, which extends a much longer series on statistical theory and practice. After introducing the fundamental issue of causal inference, namely that the counterfactual is unobserved, through description alone in part 14, part 15 provided a more technical treatment of the same issues. We described the Platinum 1 Standard of data required for causal inference as involving observing the same individuals in two different scenarios - treated2 and untreated3 - which is not possible; and the Gold Standard as being a randomised controlled trial (RCT), which is sometimes possible, but tends to be time and resource intensive. The RCT is a mechanism for breaking the association between assignment to treatment \\(Z_i\\) and both known/included covariates \\(X^*_i\\) and unknown/unincluded characteristics \\(W_i\\); this link-breaking is described as orthogonality and represented algebraically as \\(Z_i \\perp X_i^*\\) and \\(Z_i \\perp W^*_i\\).\nThe purpose of this post is to introduce some of the statistical approaches used when the only data available are observational, and so do not meet the special properties required for robust causal inference estimation of an RCT."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-16/index.html#introduction",
    "href": "pages/causal-inference/lms-are-glms-part-16/index.html#introduction",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "",
    "text": "This is the third post in a short mini-series on causal inference, which extends a much longer series on statistical theory and practice. After introducing the fundamental issue of causal inference, namely that the counterfactual is unobserved, through description alone in part 14, part 15 provided a more technical treatment of the same issues. We described the Platinum 1 Standard of data required for causal inference as involving observing the same individuals in two different scenarios - treated2 and untreated3 - which is not possible; and the Gold Standard as being a randomised controlled trial (RCT), which is sometimes possible, but tends to be time and resource intensive. The RCT is a mechanism for breaking the association between assignment to treatment \\(Z_i\\) and both known/included covariates \\(X^*_i\\) and unknown/unincluded characteristics \\(W_i\\); this link-breaking is described as orthogonality and represented algebraically as \\(Z_i \\perp X_i^*\\) and \\(Z_i \\perp W^*_i\\).\nThe purpose of this post is to introduce some of the statistical approaches used when the only data available are observational, and so do not meet the special properties required for robust causal inference estimation of an RCT."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-16/index.html#method-one-controlling-for-variables",
    "href": "pages/causal-inference/lms-are-glms-part-16/index.html#method-one-controlling-for-variables",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Method One: ‘Controlling for’ variables",
    "text": "Method One: ‘Controlling for’ variables\nThe most familiar approach for trying to estimate the causal effect of treatment \\(Z\\) on outcome \\(Y\\) is to construct a multivariate 4 regression model. Here we make sure to include those ‘nuisance parameters’ \\(X^*\\) on the predictor side of the model’s equation, along with our treatment parameter of interest \\(Z\\). For each individual \\(i\\) in the dataset \\(D\\) we can therefore use the model, calibrated on the data, to produce a prediction of the outcome \\(Y_i\\) under both the treated scenario \\(Z_i = 1\\) and the untreated scenario \\(Z_i = 0\\). As post four discussed, in the specific case of linear regression, but few other model specifications, this causal effect estimate of treatment \\(Y_i | Z=1 - Y_i | Z = 0\\) can be gleamed directly from the \\(\\beta\\) coefficient for \\(Z\\). As post four also makes clear, for other model specifications, the process for estimating causal effects can be more involved.\nIt is worth pointing out that, when using models in this way, we are really ‘just’ producing estimates of first differences, the quantity of interest which we focused on in posts 11, 12, and 13. The model prediction approach is not fundamentally any different to that discussed previously, except for two things: firstly, that we will usually be averaging across first differences for multiple observations rather a single scenario; and secondly, that we will be interpreting the first differences (or rather their aggregation) as being a causal effect estimate.\nThere are actually two types of causal effect estimate we can produce using this approach, the Average Treatment Effect (ATE), and the Average Treatment Effect on the Treated (ATT). 5 The difference between ATE and ATT is that, for ATE, the counterfactuals are simulated for all observations in the dataset \\(D\\), and that these counterfactuals will be both for individuals which were observed as treated \\(Z=1\\) and untreated \\(Z=0\\). By contrast, for ATT, only those observations in the data which were observed as treated \\(Z=1\\) are included in the causal effect estimation,6 meaning that the counterfactual being modelled will always be of the scenario \\(Z=0\\).\nSo, what are the potential problems with modelling in this way?\n\nUnobserved and unincluded covariates. Remember in the previous part we introduced the term \\(W_i^*\\)? This refers to those factors which could affect assignment \\(Z_i\\) but which are not included in our model. They could either be: i) covariates that exist in the dataset \\(D\\) but we chose not to include in the model \\(M\\); or ii) covariates that are simply not recorded in the dataset \\(D\\), so even if we wanted to, we couldn’t include them. In an RCT, the random allocation mechanism breaks both the \\(X^* \\rightarrow Z\\) and the \\(W \\rightarrow Z\\) causal paths; we don’t have to observe or even know what these factors \\(W\\) might be for an RCT to block their influence. But a regression model can only really operate to attempt to attenuate the \\(X^* \\rightarrow Z\\) pathway.\nInsufficient or improper controls. Returning to our hamster tooth growth example of post 11, recall we looked at a number of different model specifications. Our starter model specification included ‘controls for’ both dosage and supplement, and so did our final model specification. But does this mean either model is equally good at ‘controlling for’ these factors? I’d suggest they aren’t, as though our final model specification included the same covariates \\(X\\) as the initial model specification, it represented the relationship between the predictor and response variables in a qualitatively different way. For the final model specification, the dosage variable was transformed by logging it; additionally, an interaction term was included between (transformed) dosage and supplement. The reasons for this were justified by the observed relationships and by measures of penalised model fit, but we do not know if this represents the ‘best possible’ model specification. And the specification used, and the assumptions contained and represented by the model specification, will affect the predictions the model produces, including the first differences used to produce the ATE and ATT causal effect estimates.\n\nOverall, just remember that, when a researcher states in a paper that they have used a model to ‘control for’ various factors and characteristics, this can often be more a statement of what the researcher aspired to do with the model rather than managed to do. There are often a great many researcher degrees of freedom in terms of how a particular observational dataset can be used to produce modelled estimates of causal effects, and these can markedly affect the effect estimates produced.\nSo, what are some alternatives?"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-16/index.html#method-two-matching-methods",
    "href": "pages/causal-inference/lms-are-glms-part-16/index.html#method-two-matching-methods",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Method Two: Matching methods",
    "text": "Method Two: Matching methods\nRemember the Platinum Standard: For each individual, with their own personal characteristics (\\(X_i^*\\)), we known if they were treated \\(Z_i = 1\\) or untreated \\(Z_i = 0\\). In the sci-fi scenario of the genuine Platinum Standard, we are able to observe a clone of each of these individuals in the parallel universe of the unobserved counterfactual.\nObviously we can’t do that in reality. But maybe was can do something, with the data we have, which allows us to do something like the Platinum Standard, individual level pairwise comparison, \\(Y_i | Z_i = 1 - Y_i | Z_i = 0\\), even though we only precisely observe each individual \\(i\\) in one of the two scenarios \\(Z=1\\) or \\(Z=0\\).\nWe can do this by relaxing the requirement that the counterfactual be of a clone of the observed individual, and so identical in every way except for treatment status, and instead allow them to be compared to someone who’s merely similar to them.\nLet’s think through an example:\n\nBilly is 72 years old, male, overweight but not obese, works part time as a carpenter but is largely retired, married for five years but before that a widower for three, hypertensive; scores in the 85th percentile for conscentiousness, and 40th percentile for openness, in the Big Five Personality scale; owns his own home, worked in a factory in his twenties, likes baked beans with his biweekly fish suppers, enjoys war films but also musicals, liked holidaying in Spain back in the 1990s when his children were still children; owns a thirteen year old dog with advancing arthritis, who when younger used to take him on regular brisk walks, but now has to be cajoled to leave the house, especially when it’s cold and wet outside. He lives in the North East of England, and when that young woman - who seemed friendly but a bit nervous and had that weird piece of metal through the middle of her nose - from the survey company knocked on the door four months ago, and asked him to rate his level of agreement to the statement, “I am satisfied with my life” on a seven point scale, he answered with ‘6 - agree’, but pursed his lips and took five seconds to answer this question.\n\nObviously we have a lot of information about Billy. But that doesn’t mean the survey company, and thus our dataset \\(D\\), knows all that we now know. So, some of the information in the above is contained in \\(X^*_i\\), but others is part of \\(W_i\\).\nAnd what’s our treatment, and what’s our outcome? Let’s say the outcome is the response to the life satisfaction question, and the treatment is UK region, with the South East excluding London as the ‘control’ region.\nSo, how do matching methods work? Well, they can of course only work with the data available to them, \\(D\\). The basic approach is as follows:\n\nFor each person like Billy, who’s in the ‘treatment’ group \\(Z = 1\\) (‘treated’ to living in the North of England), we know various recorded characteristics about them \\(X_j^*\\), and so we want to look for one or more people on the ‘control’ group \\(Z=0\\) who are like the treated individual.\nSo, for Billy, we’re looking for someone in the part of the dataset where \\(Z=0\\) whose characteristics other than treatment assignment, i.e. \\(X^*\\) not \\(Z\\), are similar to Billy’s. Let’s say that, on paper, the person who’s most similar to Billy in the dataset is Mike, who’s 73 (just one year older), also owns his own home, also married, has a BMI of 26.3 (Billy’s is 26.1), and also diagnosed with hypertension. But, whereas Billy lives in the North of England, Mike lives in the South East.\nWe then compare the recorded response for Billy (6 - agree) with the recorded response for Mike (5 - mildly agree), to get an estimated treatment effect for Billy. 7\nWe then repeat the exercise for everyone else who, like Billy, is in the treatment/exposure group, trying to match them up with one or more individuals in the control group pool.\nOnce we’ve done that, we then average up the paired differences in responses - between each treated individual, and each person the’ve been paired up with - to produce an average treatment effect on the treated (ATT) estimate.\n\nHow do we go about about matchmaking Billy and other treated individuals? There are a variety of approaches, and as with using regression to ‘control for’ variables quite a lot of researcher degrees of freedom, different ways of matching, that can lead to different causal effect estimates. These include:\n\nExact matching: Find someone for all available characteristics other than assignment is exactly the same as the individual in the treated group to be matched. Obviously this is seldom possible, so an alternative is:\nCoarsened Exact Matching: Lump the characteristics into broader groups, such as 10 year age groups rather than age in single years, and match on someone who’s exactly roughly the same, i.e. matches the target within the more lumped/aggregated categories rather than exactly the same to the finest level of data resolution agailable.\nPropensity Score Matching: Use the known characteristics of individiduals to predict their probability of being in the treatment group, then use these predicted probabilities to try to balance the known characteristics of the populations in both treatment and control arms.\nSynthetic Controls: Combine and ‘mix’ observed characteristics from multiple untreated/unexposed individuals so that their average/admixed/combined characteristics is closely similar to those of individuals in the treated/exposed population.\n\nThese approaches are neither exhaustive nor mutually exclusive, and there are a great many ways that they could be applied in practice. One of the general aims of matching approaches is to reduce the extent to which ATT or ATE estimates depend on the specific modelling approach adopted, 8 and for Propensity Score Matching, it’s often to try to break the \\(X^* \\rightarrow Z\\) link, and so achieve orthogonality (\\(X^* \\perp Z\\)). However, it can’t necessarily do the same with unobserved characteristics (\\(W \\rightarrow Z\\))."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-16/index.html#method-three-utilise-natural-experiments",
    "href": "pages/causal-inference/lms-are-glms-part-16/index.html#method-three-utilise-natural-experiments",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Method Three: Utilise ‘natural experiments’",
    "text": "Method Three: Utilise ‘natural experiments’\nThe idea with a ‘natural experiment’ is that something happens in the world that just happens to break the links between individual characteristics and assignment to exposure/treatment. The world has therefore created a situation for us where the orthogonality assumptions \\(W \\perp Z\\) and \\(X^* \\perp Z\\) which are safe to assume when working with RCT data can also, probably, possibly, be made with certain types of observational data too. When such factors are proposed and used by economists, they tend to call them instrumental variables. Some examples include:\n\nLottery winnings to estimate the effect of money on happiness: A lottery win is an increase in money available to someone that ‘just happens’ (at least amongst lottery players). Do lottery winners’ subjective wellbeing scores increase following a win? If so for how long? Why is this preferable to just looking at the relationship between income/assets and happiness? Well, the causality could go the other way: perhaps happier people work harder, increasing their income. Or perhaps a common underlying personality factor - something like ‘conscientious stoicism’, which isn’t measured - affects both income and happiness. By utilising the randomness of a big win allocation to just a small minority of players, 9 such alternative explanations for why there are differences between populations being compared can be more safely discounted.\nComparing educational outcomes for pupils who only just got into, and only just got rejected from, selective schools and universities: Say a selective school runs its own standardised entry exam, for which a pass mark of 70 or higher is required to be accepted. An applicant who achieves a mark of 69 isn’t really that different in their aptitude than one who achieves a of 70, but this one point difference sadly appears to make the world of difference for the applicant with a 69, and gladly appears to make the world of difference for the applicant with a 70. For years afterwards, the 70-scoring applicant will have access to a fundemntally different educational environment than the 69-scoring applicant. And presumably both applicants 10 both applied because they thought the selective educational institution really would make a substantial and positive difference for their long-term educational outcomes. But does it really? By following the actual educational outcomes of pupils just north of the selection boundary, and of non-pupils just south of the selection boundaries, we have something like a treatment and control group, whose only main difference is that some are in the selective school and some are not.\n\nNote that neither of these examples are perfect substitutes for an RCT. Perhaps the people who win lotteries, or win big, are different enough from those who don’t that the winner/non-winner group’s aren’t similar in important ways. And perhaps the way people process and feel about money they get through lottery winnings isn’t the same as they they receive through earnings or social security, so the idea of there being a single money-to-happiness pathway isn’t valid. For the second example there are other concerns: of course applicants only one mark apart won’t be very different to each other, but there won’t be many of these, meaning the precision of the estimate will tend to be low. So how about expanding the ‘catchment’ to each arm, either side of the boundary line, to 2 marks, 3 marks, 5 marks? Now there should be more people in both the control and treatment arms, but they’ll also be more different to each other. 11\nAs you might expect, if using instrumental variables, the quality of the instrument matters a lot. But generally the quality of the instrument isn’t something that can be determined through any kind of formal or statistical test. It tends to be, for want of a better term, a matter of story telling. If the story the researcher can tell their audience, about the instrument and why it’s able to break the causal links it needs to break, is convincing to the audience, then the researcher and audience will both be more willing to assume that the estimates produced at the end of the analysis are causal."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-16/index.html#summing-up",
    "href": "pages/causal-inference/lms-are-glms-part-16/index.html#summing-up",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Summing up",
    "text": "Summing up\nSo, three methods for trying to do something technically impossible: using observational data to estimate causal effects. These methods aren’t mutually exclusive, nor are they likely to be exhaustive, and nor are any of them failsafe.\nIn the absence of being able to really know, to peak behind the veil and see the causal chains working their magic, a good pragmatic strategy tends to be to try multiple approaches. At its extreme, this can mean asking multiple teams of researchers the same question, and giving them access to the same dataset, and encouraging each team to not contact any other teams until they’ve finished their analysis, then compare the results they produce. If many different teams, with many different approaches, all tend to produce similar estimates, then maybe the estimates are really tapping into genuine causal effects, and not just reflecting some of the assumptions and biases built into the specific models and methods we’re using?"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-16/index.html#coming-up",
    "href": "pages/causal-inference/lms-are-glms-part-16/index.html#coming-up",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Coming up",
    "text": "Coming up\nThe next post attempts to apply matching methods to a relatively complex dataset on an economic intervention, using the MatchIt package. The post largely follows an introductory example from the package, but at some points goes ‘off piste’. I hope it does so, however, in ways that are interesting, useful, and help bridge the gaps between the theoretical discussions in this and previous posts, with the practical challenges involved in applying such theory."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-16/index.html#footnotes",
    "href": "pages/causal-inference/lms-are-glms-part-16/index.html#footnotes",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Footnotes",
    "text": "Footnotes\n\n\npronounced ‘unobtainium’↩︎\nAKA ‘exposed’↩︎\nAKA ‘unexposed’↩︎\nOr multivariable, if we wish to reserve the term multivariate to models with multiple response columns.↩︎\nLogically, we should assume there is also an Average Treatment Effect on the Untreated (ATU), but this is seldom discussed in practice.↩︎\nThis might be represented as something like \\(D^{(T)} \\subset D \\iff Z_i = 1\\), i.e. the data used are filtered based on the value of \\(Z\\) matching a condition.↩︎\nThis data is really ordinal, meaning we know ‘agree’ is higher than ‘mildly agree’, but don’t know how much higher, so should really be modelled as such, with something like an ordered logit or ordered probit model specification. However it’s often either treated as cardinal - 1, 2, 3, 4, 5, 6, 7 - with something like a linear regression, or collapsed into two categories (agree/ don’t agree) so standard logit or probit regression could be used.↩︎\nEven B-A is a modelling approach, to an extent.↩︎\nIt could be you. But it probably won’t be.↩︎\nOr their pushy parents…↩︎\nAn example of a bias/variance trade-off↩︎"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-17/index.html",
    "href": "pages/causal-inference/lms-are-glms-part-17/index.html",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "",
    "text": "The previous post (re)introduced three ways to try to allow causal effect estimation using observational data: i) ‘controlling for’ variables using multiple regression; ii) matching methods; iii) Identifying possible ‘natural experiments’ in observational datasets. The fundamental challenge of using observational data to estimate causal effects is that we cannot be sure either the observed (\\(X^*\\)) or unobserved (\\(W\\)) characteristics of observations do not influence allocation to exposure/treatment, i.e. cannot rule out \\(X^* \\rightarrow Z\\) or \\(W \\rightarrow Z\\), meaning that statistical estimates of the effect of Z on the outcome \\(Z \\rightarrow y_i\\) may be biased.\nThe first two approaches will, within limits, generally attenuate the link between \\(X^*\\) and \\(Z\\), but can do little to break the link between \\(W\\) and \\(Z\\), as \\(W\\) is by definition those features of observational units that are not contained in the dataset \\(D\\), and so any statistical method will be ‘blind’ to. The last approach, if the instrumental variable possesses the properties we expect and hope it will, should be able to break the \\(W \\rightarrow Z\\) link too. But unfortunately that can be a big if: the instrument may not have the properties we hope it does.\nThis post will go explore some application of the first two approaches: controlling for variables using multiple regression; and using matching methods. A fuller consideration of the issues is provided in Ho et al. (2007), and the main package and dataset used will be that of the associated MatchIt package Ho et al. (2011) and vignette using the lalonde dataset."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-17/index.html#recap-and-aim",
    "href": "pages/causal-inference/lms-are-glms-part-17/index.html#recap-and-aim",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "",
    "text": "The previous post (re)introduced three ways to try to allow causal effect estimation using observational data: i) ‘controlling for’ variables using multiple regression; ii) matching methods; iii) Identifying possible ‘natural experiments’ in observational datasets. The fundamental challenge of using observational data to estimate causal effects is that we cannot be sure either the observed (\\(X^*\\)) or unobserved (\\(W\\)) characteristics of observations do not influence allocation to exposure/treatment, i.e. cannot rule out \\(X^* \\rightarrow Z\\) or \\(W \\rightarrow Z\\), meaning that statistical estimates of the effect of Z on the outcome \\(Z \\rightarrow y_i\\) may be biased.\nThe first two approaches will, within limits, generally attenuate the link between \\(X^*\\) and \\(Z\\), but can do little to break the link between \\(W\\) and \\(Z\\), as \\(W\\) is by definition those features of observational units that are not contained in the dataset \\(D\\), and so any statistical method will be ‘blind’ to. The last approach, if the instrumental variable possesses the properties we expect and hope it will, should be able to break the \\(W \\rightarrow Z\\) link too. But unfortunately that can be a big if: the instrument may not have the properties we hope it does.\nThis post will go explore some application of the first two approaches: controlling for variables using multiple regression; and using matching methods. A fuller consideration of the issues is provided in Ho et al. (2007), and the main package and dataset used will be that of the associated MatchIt package Ho et al. (2011) and vignette using the lalonde dataset."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-17/index.html#getting-started",
    "href": "pages/causal-inference/lms-are-glms-part-17/index.html#getting-started",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Getting started",
    "text": "Getting started\nWe start by loading the Matchit package and exploring the lalonde dataset.\n\n\nCode\nlibrary(tidyverse)\nlibrary(MatchIt)\nunmatched_data &lt;- tibble(lalonde)\n\nunmatched_data\n\n\n# A tibble: 614 × 9\n   treat   age  educ race   married nodegree  re74  re75   re78\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1    37    11 black        1        1     0     0  9930.\n 2     1    22     9 hispan       0        1     0     0  3596.\n 3     1    30    12 black        0        0     0     0 24909.\n 4     1    27    11 black        0        1     0     0  7506.\n 5     1    33     8 black        0        1     0     0   290.\n 6     1    22     9 black        0        1     0     0  4056.\n 7     1    23    12 black        0        0     0     0     0 \n 8     1    32    11 black        0        1     0     0  8472.\n 9     1    22    16 black        0        0     0     0  2164.\n10     1    33    12 white        1        0     0     0 12418.\n# ℹ 604 more rows"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-17/index.html#data",
    "href": "pages/causal-inference/lms-are-glms-part-17/index.html#data",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Data",
    "text": "Data\nThe description of the lalonde dataset is as follows:\n\n\nCode\nhelp(lalonde)\n\n\n\nDescription\nThis is a subsample of the data from the treated group in the National Supported Work Demonstration (NSW) and the comparison sample from the Population Survey of Income Dynamics (PSID). This data was previously analyzed extensively by Lalonde (1986) and Dehejia and Wahba (1999).\nFormat\nA data frame with 614 observations (185 treated, 429 control). There are 9 variables measured for each individual.\n\n“treat” is the treatment assignment (1=treated, 0=control).\n“age” is age in years.\n“educ” is education in number of years of schooling.\n“race” is the individual’s race/ethnicity, (Black, Hispanic, or White). Note previous versions of this dataset used indicator variables black and hispan instead of a single race variable.\n“married” is an indicator for married (1=married, 0=not married).\n“nodegree” is an indicator for whether the individual has a high school degree (1=no degree, 0=degree).\n“re74” is income in 1974, in U.S. dollars.\n“re75” is income in 1975, in U.S. dollars.\n“re78” is income in 1978, in U.S. dollars.\n\n“treat” is the treatment variable, “re78” is the outcome, and the others are pre-treatment covariates.\n\nLet’s look at the data to get a sense of it:\n\n\nCode\nunmatched_data |&gt;\n    mutate(treat = as.factor(treat)) |&gt;\n    filter(re78 &lt; 25000) |&gt;\n    ggplot(aes(y = re78, x = re75, shape = treat, colour = treat)) + \ngeom_point() + \ngeom_abline(intercept = 0, slope = 1) +\ncoord_equal() + \nstat_smooth(se = FALSE, method = \"lm\")\n\n\n\n\n\nClearly this is quite complicated data, where the single implied control, wages in 1975 (re75) is not sufficient. There are also a great many observations where wages in either of both years were 0, hence the horizontal and vertical streaks apparent.\nThe two lines are the linear regression lines for the two treatment groups as a function of earlier wage. The lines are not fixed to have the same slope, so the differences in any crude treatment effect estimate vary by earlier wage, but for most previous wages the wages in 1978 appear to be lower in the treatment group (blue), than the control group (red). This would suggest either that the treatment may be harmful to wages… or that there is severe imbalance between the characteristics of persons in both treatment conditions.\nLet’s now start to use a simple linear regression to estimate an average treatment effect, before adding more covariates to see how these model-derived estimates change\n\n\nCode\n# Model of treatment assignment only\nmod_01 &lt;- lm(re78 ~ treat, unmatched_data)\nsummary(mod_01) \n\n\n\nCall:\nlm(formula = re78 ~ treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6984  -6349  -2048   4100  53959 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6984.2      360.7  19.362   &lt;2e-16 ***\ntreat         -635.0      657.1  -0.966    0.334    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7471 on 612 degrees of freedom\nMultiple R-squared:  0.001524,  Adjusted R-squared:  -0.0001079 \nF-statistic: 0.9338 on 1 and 612 DF,  p-value: 0.3342\n\n\nOn average the treated group had (annual?) wages $635 lower than the control group. However the difference is not statistically significant.\nNow let’s add previous wage from 1975\n\n\nCode\nmod_02 &lt;- lm(re78 ~ re75 + treat, unmatched_data)\nsummary(mod_02)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-15918  -5457  -2025   3824  54103 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5547.63718  412.84637  13.438  &lt; 2e-16 ***\nre75           0.58242    0.08937   6.517  1.5e-10 ***\ntreat        -90.79498  641.40291  -0.142    0.887    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7230 on 611 degrees of freedom\nMultiple R-squared:  0.06642,   Adjusted R-squared:  0.06336 \nF-statistic: 21.73 on 2 and 611 DF,  p-value: 7.611e-10\n\n\nPreviously observed wage is statistically significant and positive. The point estimate on treatment is smaller, and even less ‘starry’.\nNow let’s add all possible control variables and see what the treatment effect estimate produced is:\n\n\nCode\nmod_03 &lt;- lm(re78 ~ re75 + age + educ + race + married + nodegree + re74 + treat, unmatched_data)\nsummary(mod_03)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + age + educ + race + married + nodegree + \n    re74 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13595  -4894  -1662   3929  54570 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.174e+03  2.456e+03  -0.478   0.6328    \nre75         2.315e-01  1.046e-01   2.213   0.0273 *  \nage          1.298e+01  3.249e+01   0.399   0.6897    \neduc         4.039e+02  1.589e+02   2.542   0.0113 *  \nracehispan   1.740e+03  1.019e+03   1.708   0.0882 .  \nracewhite    1.241e+03  7.688e+02   1.614   0.1071    \nmarried      4.066e+02  6.955e+02   0.585   0.5590    \nnodegree     2.598e+02  8.474e+02   0.307   0.7593    \nre74         2.964e-01  5.827e-02   5.086 4.89e-07 ***\ntreat        1.548e+03  7.813e+02   1.982   0.0480 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6948 on 604 degrees of freedom\nMultiple R-squared:  0.1478,    Adjusted R-squared:  0.1351 \nF-statistic: 11.64 on 9 and 604 DF,  p-value: &lt; 2.2e-16\n\n\nWith all of these variables as controls, the effect of treatment is now statistically significant and positive, associated with on average an increase of $155 over the control group.\nHowever, we should probably be concerned about how dependent this estimate is on the specific model specification we used. For example, it is fairly common to try to ‘control for’ nonlinearities in age effects by adding a squared term. If modeller decisions like this don’t make much difference, then its addition shouldn’t affect the treatment effect estimate. Let’s have a look:\n\n\nCode\nmod_04 &lt;- lm(re78 ~ re75 + poly(age, 2) + educ + race + married + nodegree + re74 + treat, unmatched_data)\nsummary(mod_04)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + poly(age, 2) + educ + race + married + \n    nodegree + re74 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13692  -4891  -1514   3884  54313 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -5.395e+02  2.172e+03  -0.248   0.8039    \nre75           2.190e-01  1.057e-01   2.072   0.0387 *  \npoly(age, 2)1  3.895e+03  7.994e+03   0.487   0.6262    \npoly(age, 2)2 -6.787e+03  7.918e+03  -0.857   0.3917    \neduc           3.889e+02  1.599e+02   2.432   0.0153 *  \nracehispan     1.682e+03  1.021e+03   1.648   0.0999 .  \nracewhite      1.257e+03  7.692e+02   1.634   0.1028    \nmarried        2.264e+02  7.267e+02   0.312   0.7555    \nnodegree       3.185e+02  8.504e+02   0.375   0.7081    \nre74           2.948e-01  5.832e-02   5.055 5.73e-07 ***\ntreat          1.369e+03  8.090e+02   1.692   0.0911 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6949 on 603 degrees of freedom\nMultiple R-squared:  0.1488,    Adjusted R-squared:  0.1347 \nF-statistic: 10.54 on 10 and 603 DF,  p-value: &lt; 2.2e-16\n\n\nThe inclusion of the squared term to age has changed the point estimate of treatment from around $1550 to $1370. However it has also changed the statistical significance of the effect from p &lt; 0.05 to p &lt; 0.10, i.e. from ‘statistically significant’ to ‘not statistically significant’. If we were playing the stargazing game, this might be the difference between a publishable finding and an unpublishable finding.\nAnd what if we excluded age, because none of the terms are statistically significant at the standard level?\n\n\nCode\nmod_05 &lt;- lm(re78 ~ re75 + educ + race + married + nodegree + re74 + treat, unmatched_data)\nsummary(mod_05)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + educ + race + married + nodegree + \n    re74 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13681  -4912  -1652   3877  54648 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -676.43048 2115.37702  -0.320   0.7493    \nre75           0.22705    0.10395   2.184   0.0293 *  \neduc         389.00786  154.33865   2.520   0.0120 *  \nracehispan  1710.16654 1015.15590   1.685   0.0926 .  \nracewhite   1241.00510  768.22972   1.615   0.1067    \nmarried      478.55017  671.28910   0.713   0.4762    \nnodegree     201.04497  833.99164   0.241   0.8096    \nre74           0.30209    0.05645   5.351 1.24e-07 ***\ntreat       1564.68896  779.65173   2.007   0.0452 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6943 on 605 degrees of freedom\nMultiple R-squared:  0.1475,    Adjusted R-squared:  0.1363 \nF-statistic: 13.09 on 8 and 605 DF,  p-value: &lt; 2.2e-16\n\n\nNow the exclusion of this term, which the coefficient tables suggested wasn’t statistically significant, but intuitively we recognise as an important determinant of labour market activity, has led to yet another point estimate. It’s switched back to ‘statistically significant’ again, but now the point estimate is about $1565 more. Such estimates aren’t vastly different, but they definitely aren’t the same, and come from just a tiny same of the potentially hundreds of different model specifications we could have considered and decided to present to others."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-17/index.html#matching-with-matchit",
    "href": "pages/causal-inference/lms-are-glms-part-17/index.html#matching-with-matchit",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Matching with MatchIt",
    "text": "Matching with MatchIt\nAs the title of Ho et al. (2007) indicates, matching methods are presented as a way of preprocessing the data to reduce the kind of model dependence we’ve just started to explore. Let’s run the first example they present in the MatchIt vignette then discuss what it means:\n\n\nCode\nm.out0 &lt;- matchit(treat ~ age + educ + race + married + \n                   nodegree + re74 + re75, data = lalonde,\n                 method = NULL, distance = \"glm\")\nsummary(m.out0)\n\n\n\nCall:\nmatchit(formula = treat ~ age + educ + race + married + nodegree + \n    re74 + re75, data = lalonde, method = NULL, distance = \"glm\")\n\nSummary of Balance for All Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.1822          1.7941     0.9211    0.3774\nage              25.8162       28.0303         -0.3094     0.4400    0.0813\neduc             10.3459       10.2354          0.0550     0.4959    0.0347\nraceblack         0.8432        0.2028          1.7615          .    0.6404\nracehispan        0.0595        0.1422         -0.3498          .    0.0827\nracewhite         0.0973        0.6550         -1.8819          .    0.5577\nmarried           0.1892        0.5128         -0.8263          .    0.3236\nnodegree          0.7081        0.5967          0.2450          .    0.1114\nre74           2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75           1532.0553     2466.4844         -0.2903     0.9563    0.1342\n           eCDF Max\ndistance     0.6444\nage          0.1577\neduc         0.1114\nraceblack    0.6404\nracehispan   0.0827\nracewhite    0.5577\nmarried      0.3236\nnodegree     0.1114\nre74         0.4470\nre75         0.2876\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       429     185\nUnmatched       0       0\nDiscarded       0       0\n\n\nWith method = NULL, the matchit function presents some summary estimates of differences in characteristics between the Treatment and Control groups. For example, the treated group has an average age of around 25, compared with 28 in the control group, have a slightly higher education score, are more likely to be Black, less likely to be Hispanic, and much less likely to be White (all important differences in the USA context, especially perhaps of the 1970s). They are also less likely to be married, more likely to have no degree, and have substantially earlier wages in both 1974 and 1975. Clearly a straightforward comparision between average outcomes is far from a like-with-like comparisons between groups. The inclusion of other covariates (\\(X^*\\)) does seem to have made a difference, switching the reported direction of effect and its statistical significance, but if we could find a subsample of the control group whose characteristics better match those of the treatment groups, we would hopefully get a more precise and reliable estimate of the effect of the labour market programme.\nThe next part of the vignette shows MatchIt working with some fairly conventional settings:\n\n\nCode\nm.out1 &lt;- matchit(treat ~ age + educ + race + married + \n                   nodegree + re74 + re75, data = lalonde,\n                 method = \"nearest\", distance = \"glm\")\nm.out1\n\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score\n             - estimated with logistic regression\n - number of obs.: 614 (original), 370 (matched)\n - target estimand: ATT\n - covariates: age, educ, race, married, nodegree, re74, re75\n\n\nThe propensity score, i.e. the probability of being in the treatment group, has been predicted using the other covariates, and using logistic regression. For each individual in the treatment group, a ‘nearest neighbour’ in the control group has been identified with the most similar propensity score, which we hope also will also mean the characteristics of the treatment group, and matched pairs from the control group, will be more similar too.\nWe can start to see what this means in practice by looking at the summary of the above object\n\n\nCode\nsummary(m.out1)\n\n\n\nCall:\nmatchit(formula = treat ~ age + educ + race + married + nodegree + \n    re74 + re75, data = lalonde, method = \"nearest\", distance = \"glm\")\n\nSummary of Balance for All Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.1822          1.7941     0.9211    0.3774\nage              25.8162       28.0303         -0.3094     0.4400    0.0813\neduc             10.3459       10.2354          0.0550     0.4959    0.0347\nraceblack         0.8432        0.2028          1.7615          .    0.6404\nracehispan        0.0595        0.1422         -0.3498          .    0.0827\nracewhite         0.0973        0.6550         -1.8819          .    0.5577\nmarried           0.1892        0.5128         -0.8263          .    0.3236\nnodegree          0.7081        0.5967          0.2450          .    0.1114\nre74           2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75           1532.0553     2466.4844         -0.2903     0.9563    0.1342\n           eCDF Max\ndistance     0.6444\nage          0.1577\neduc         0.1114\nraceblack    0.6404\nracehispan   0.0827\nracewhite    0.5577\nmarried      0.3236\nnodegree     0.1114\nre74         0.4470\nre75         0.2876\n\nSummary of Balance for Matched Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.3629          0.9739     0.7566    0.1321\nage              25.8162       25.3027          0.0718     0.4568    0.0847\neduc             10.3459       10.6054         -0.1290     0.5721    0.0239\nraceblack         0.8432        0.4703          1.0259          .    0.3730\nracehispan        0.0595        0.2162         -0.6629          .    0.1568\nracewhite         0.0973        0.3135         -0.7296          .    0.2162\nmarried           0.1892        0.2108         -0.0552          .    0.0216\nnodegree          0.7081        0.6378          0.1546          .    0.0703\nre74           2095.5737     2342.1076         -0.0505     1.3289    0.0469\nre75           1532.0553     1614.7451         -0.0257     1.4956    0.0452\n           eCDF Max Std. Pair Dist.\ndistance     0.4216          0.9740\nage          0.2541          1.3938\neduc         0.0757          1.2474\nraceblack    0.3730          1.0259\nracehispan   0.1568          1.0743\nracewhite    0.2162          0.8390\nmarried      0.0216          0.8281\nnodegree     0.0703          1.0106\nre74         0.2757          0.7965\nre75         0.2054          0.7381\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       185     185\nUnmatched     244       0\nDiscarded       0       0\n\n\nPreviously, there were 185 people in the treatment group, and 429 people in the control group. After matching there are 185 people in the treatment group… and also 185 people in the control group. So, each of the 185 people in the treatment group has been matched up with a ‘data twin’ in the control group, so the ATT should involve more of a like-with-like comparison.\nThe summary presents covariate-wise differences between the Treatment and Control groups for All Data, then for Matched Data. We would hope that, in the Matched Data, the differences are smaller for each covariate, though this isn’t necessarily the case. After matching, for example, we can see that the Black proportion in the Control group is now 0.47 rather than 0.20, and that the earlier income levels are lower, in both cases bringing the values in the Control group closer to, but not identical to, those in the Treatment group. Another way of seeing how balancing has changed things is to look at density plots:\n\n\nCode\nplot(m.out1, type = \"density\", interactive = FALSE,\n     which.xs = ~age + married + re75+ race + nodegree + re74)\n\n\n\n\n\n\n\n\nIn these density charts, the darker lines indicate the Treatment group and the lighter lines the Control groups. The matched data are on the right hand side, with All data on the left. We are looking to see if, on the right hand side, the two sets of density lines are more similar than they are on the right. Indeed they do appear to be, though we can also tell they are far from identical."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-17/index.html#estimating-treatment-effect-sizes-after-matching",
    "href": "pages/causal-inference/lms-are-glms-part-17/index.html#estimating-treatment-effect-sizes-after-matching",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Estimating Treatment Effect Sizes after matching",
    "text": "Estimating Treatment Effect Sizes after matching\nHistorically, the MatchIt package was designed to work seamlessly with Zelig, which made it much easier to use a single library and framework to produce ‘quantities of interest’ using multiple model structures. However Zelig has since been deprecated, meaning the vignette now recommends using the marginaleffects package. We’ll follow their lead:\nFirst the vignette recommends extracting matched data from the matchit output:\n\n\nCode\nm.data &lt;- match.data(m.out1)\n\nm.data &lt;- as_tibble(m.data)\nm.data\n\n\n# A tibble: 370 × 12\n   treat   age  educ race   married nodegree  re74  re75   re78 distance weights\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1     1    37    11 black        1        1     0     0  9930.   0.639        1\n 2     1    22     9 hispan       0        1     0     0  3596.   0.225        1\n 3     1    30    12 black        0        0     0     0 24909.   0.678        1\n 4     1    27    11 black        0        1     0     0  7506.   0.776        1\n 5     1    33     8 black        0        1     0     0   290.   0.702        1\n 6     1    22     9 black        0        1     0     0  4056.   0.699        1\n 7     1    23    12 black        0        0     0     0     0    0.654        1\n 8     1    32    11 black        0        1     0     0  8472.   0.790        1\n 9     1    22    16 black        0        0     0     0  2164.   0.780        1\n10     1    33    12 white        1        0     0     0 12418.   0.0429       1\n# ℹ 360 more rows\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nWhereas the unmatched data contains 614 observations, the matched data contains 370 observations. Note that the Treatment group contained 185 observations, and that 370 is 185 times two. So, the matched data contains one person in the Control group for each person in the Treatment group.\nWe can also see that, in addition to the metrics originally included, the matched data contains three additional variables: ‘distance’, ‘weights’ and ‘subclass’. The ‘subclass’ field is perhaps especially useful for understanding the intuition of the approach, because it helps show which individual in the Control group has been paired with which individual in the Treatment group. Let’s look at the first three subgroups:\n\n\nCode\nm.data |&gt; filter(subclass == '1')\n\n\n# A tibble: 2 × 12\n  treat   age  educ race  married nodegree   re74  re75  re78 distance weights\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    37    11 black       1        1     0      0 9930.    0.639       1\n2     0    22     8 black       1        1 16961.     0  959.    0.203       1\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nSo, for the first subclass, a 37 year old married Black person with no degree has been matched to a 22 year old Black married person with no degree.\n\n\nCode\nm.data |&gt; filter(subclass == '2')\n\n\n# A tibble: 2 × 12\n  treat   age  educ race  married nodegree  re74  re75   re78 distance weights\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    33    12 white       1        0    0      0 12418.   0.0429       1\n2     0    39    12 white       1        0 1289.     0  1203.   0.0430       1\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nFor the second subclass a 33 year old married White person with a degree has been paired with a 39 year old White person with a degree.\n\n\nCode\nm.data |&gt; filter(subclass == '3')\n\n\n# A tibble: 2 × 12\n  treat   age  educ race   married nodegree  re74  re75   re78 distance weights\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    31     9 hispan       0        1     0    0  26818.    0.250       1\n2     0    16    10 white        0        1     0  190.  2137.    0.105       1\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nFor the third subclass, a 31 year old unmarried Hispanic person with no degree has been paired with a 16 year old White person with no degree.\nIn each case, we can see the pairings are similar in some ways but (as with the last example) quite dissimilar in others. The matching algorithm is trying to do the best it can with the data available, especially with the constraint1 that once a person in the Control group has been paired up once to someone in the Treatment group, they can’t be paired up again with someone else in the Treatment group.\nThe identification of these specific pairings suggests we can used a fairly crude strategy to produce an estimate of the ATT: namely just compare the outcome across each of these pairs. Let’s have a look at this:\n\n\nCode\ntrt_effects &lt;- \n    m.data |&gt;\n        group_by(subclass) |&gt;\n        summarise(\n            ind_treat_effect = re78[treat == 1] - re78[treat == 0]\n        ) |&gt; \n        ungroup()\n\ntrt_effects |&gt;\n    ggplot(aes(ind_treat_effect)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = mean(trt_effects$ind_treat_effect), colour = \"red\") + \n    geom_vline(xintercept = 0, colour = 'lightgray', linetype = 'dashed')\n\n\n\n\n\nThis crude paired comparison suggests an average difference that’s slightly positive, of $894.37.\nThis is not a particularly sophisticated or ‘kosher’ approach however. Instead the vignette suggests calculating the treatment effect estimate as follows:\n\n\nCode\nlibrary(\"marginaleffects\")\n\nfit &lt;- lm(re78 ~ treat * (age + educ + race + married + nodegree + \n             re74 + re75), data = m.data, weights = weights)\n\navg_comparisons(fit,\n                variables = \"treat\",\n                vcov = ~subclass,\n                newdata = subset(m.data, treat == 1),\n                wts = \"weights\")\n\n\n\n  Term          Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n treat mean(1) - mean(0)     1121        837 1.34    0.181 2.5  -520   2763\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\nUsing the recommended approach, the ATT estimate is now $1121. Not statistically significant at the conventional 95% threshold, but also more likely to be positive than negative."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-17/index.html#summary",
    "href": "pages/causal-inference/lms-are-glms-part-17/index.html#summary",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Summary",
    "text": "Summary\nIn this post we have largely followed along with the introductionary vignette from the MatchIt package, in order to go from the fairly cursory theoretical overview in the previous post, to showing how some of the ideas and methods relating to multiple regression and matching methods work in practice. There are a great many ways that both matching, and multiple regression, can be implemented in practice, and both are likely to affect any causal effect estimates we produce. However, the aspiration of using matching methods is to somewhat reduce the dependency that causal effect estimates have on the specific model specifications we used."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-17/index.html#coming-up",
    "href": "pages/causal-inference/lms-are-glms-part-17/index.html#coming-up",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Coming up",
    "text": "Coming up\nThe next post concludes this series on causal inference, by discussing in more detail a topic many users of causal inference will assume I should have started with: the Pearlean school of causal inference. In brief: the approach to causal inference I’m used to interprets the problem, fundamentally, as a missing data problem; whereas the Pearlean approach interprets it more as a modelling problem. I see value in both sides, as well as some points of overlap, but in general I’m both more used to, and more comfortable with, the missing data interpretation."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-17/index.html#footnotes",
    "href": "pages/causal-inference/lms-are-glms-part-17/index.html#footnotes",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI think this is implied by the use of method = \"nearest\", which is the default, meaning ‘greedy nearest neighbour matching’.↩︎"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-18/index.html",
    "href": "pages/causal-inference/lms-are-glms-part-18/index.html",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "",
    "text": "Over posts 14 through to 17 I’ve discussed causal inference. However, readers who’ve been involved and interested in the topic of causal inference over the last few years might be less surprised by what I have covered than by what I’ve not, namely the causal inference framework developed by Judea Pearl, and (somewhat) popularised by his co-authored book, The Book of Why: The New Science of Cause and Effect. (Pearl and Mackenzie (2018))\nThis ‘oversight’ in posts so far has been intentional, but in this post the Pearl framework will finally be discussed. I’ll aim to: i) give an overview of the two primary ways of thinking about causal inference: either as a missing data problem; or as a ‘do-logic’ problem; ii) discuss the concept of the omitted variable vs post treatment effect bias trade-off as offering something of a bridge between the two paradigms; iii) give some brief examples of directed acyclic graphs (DAGs) and do-logic, two important ideas from the Pearl framework, as described in Pearl and Mackenzie (2018); iv) make some suggestions about the benefits and uses of the Pearl framework; and finally v) advocate for epistemic humility when it comes to trying to draw causal inferences from observational data, even where a DAG has been clearly articulated and agreed upon within a research community. 1 Without further ado, let’s begin:"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-18/index.html#introduction-correcting-an-oversight-in-discussing-causality",
    "href": "pages/causal-inference/lms-are-glms-part-18/index.html#introduction-correcting-an-oversight-in-discussing-causality",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "",
    "text": "Over posts 14 through to 17 I’ve discussed causal inference. However, readers who’ve been involved and interested in the topic of causal inference over the last few years might be less surprised by what I have covered than by what I’ve not, namely the causal inference framework developed by Judea Pearl, and (somewhat) popularised by his co-authored book, The Book of Why: The New Science of Cause and Effect. (Pearl and Mackenzie (2018))\nThis ‘oversight’ in posts so far has been intentional, but in this post the Pearl framework will finally be discussed. I’ll aim to: i) give an overview of the two primary ways of thinking about causal inference: either as a missing data problem; or as a ‘do-logic’ problem; ii) discuss the concept of the omitted variable vs post treatment effect bias trade-off as offering something of a bridge between the two paradigms; iii) give some brief examples of directed acyclic graphs (DAGs) and do-logic, two important ideas from the Pearl framework, as described in Pearl and Mackenzie (2018); iv) make some suggestions about the benefits and uses of the Pearl framework; and finally v) advocate for epistemic humility when it comes to trying to draw causal inferences from observational data, even where a DAG has been clearly articulated and agreed upon within a research community. 1 Without further ado, let’s begin:"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-18/index.html#causal-inference-two-paradigms",
    "href": "pages/causal-inference/lms-are-glms-part-18/index.html#causal-inference-two-paradigms",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "Causal Inference: Two paradigms",
    "text": "Causal Inference: Two paradigms\nIn the posts so far, I’ve introduced and kept returning to the idea that the fundamental problem of causal inference is that at least half of the data is always missing. i.e., for each individual observation, who has either been treated or not treated, if they had been treated then we do not observe them in the untreated state, and if they had not been treated we do not observe them in the treated state. It’s this framing of the problem which\nIn introducing causal inference from this perspective, I’ve ‘taken a side’ in an ongoing debate, or battle, or even war, between two clans of applied epistemologists. Let’s call them the Rubinites, and the Pearlites. Put crudely, the Rubinites adopt a data-centred framing of the challenge of causal inference, whereas the Pearlites adopt a model-centred framing of the challenge of causal inference. For the Rubinites, the data-centred framing leads to an intepretation of causal inference as a missing data problem, for which the solution is therefore to perform some kind of data imputation. For the Pearlites, by contrast, the solution is focused on developing, describing and drawing out causal models, which describe how we believe one thing leads to another and the paths of effect and influence that one variable has on each other variable.\nIt is likely no accident that the broader backgrounds and interests of Rubin and Pearl align with type of solution each proposes. Rubin’s other main interests are in data imputation more generally, including methods of multiple imputation which allow ‘missing values’ to be filled in stochastically, rather than deterministically, to allow some representation of uncertainty and variation in the missing values to be indicated by the range of values that are generated for a missing hole in the data. Pearl worked as a computer scientist, whose key contribution to the field was the development of Bayesian networks, which share many similarities with neural networks. For both types of network, there are nodes, and there are directed links. The nodes have values, and these values can be influenced and altered by the values of other nodes that are connected to the node in question. This influence that each node has on other nodes, through the paths indicated in the directed links, is perhaps more likely to be described as updating from the perspective of a Bayesian network, and propagation from the perspective of a neural network. But in either case, it really is correct to say that one node really does cause another node’s value to change through the causal pathway of the directed link. The main graphical tool Pearl proposes for reasoning about causality in obervational data is the directed acyclic graph (DAG), and again it should be unsurprising that DAGs look much like Bayesian networks."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-18/index.html#the-omitted-variable-bias-vs-post-treatment-bias-trade-off-as-a-potential-bridge-between-the-two-paradigms",
    "href": "pages/causal-inference/lms-are-glms-part-18/index.html#the-omitted-variable-bias-vs-post-treatment-bias-trade-off-as-a-potential-bridge-between-the-two-paradigms",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "The Omitted Variable Bias vs Post Treatment Bias Trade-off as a potential bridge between the two paradigms",
    "text": "The Omitted Variable Bias vs Post Treatment Bias Trade-off as a potential bridge between the two paradigms\nThe school of inference I’m most familiar with is that of Gary King, a political scientist, methodologist and (in the hallowed halls of Harvard) populariser of statistical methods in the social sciences. In the crude paradigmatic split I’ve sketched out above, King is a Rubinite, and so I guess - mainly through historical accident but partly through conscious decision - I am too. However, I have read Pearl and Mackenzie (2018) (maybe not recently enough nor enough times to fully digest it), consider it valuable and insightful in many places, and think there’s at least one place where the epistemic gap between the two paradigms can be bridged.\nThe bridge point on the Rubinite side,2 I’d suggest, comes from thinking carefully about the sources of bias enumerated in section 3.2 of King and Zeng (2006), which posits that:\n\\[\nbias = \\Delta_o + \\Delta_p + \\Delta_i + \\Delta_e\n\\]\nThis section states:\n\nThese four terms denote exactly the four sources of bias in using observational data, with the subscripts being mnemonics for the components … . The bias components are due to, respectively, omitted variable bias (\\(\\Delta_o\\)), post-treatment bias (\\(\\Delta_p\\)), interpolation bias (\\(\\Delta_i\\)) and extrapolation bias (\\(\\Delta_e\\)). [Emphases added]\n\nOf the four sources of bias listed, it’s the first two which appear to offer a potential link between the two paradigms, and so suggest to Rubinites why some engagement with the Pearlite approach may be valuable. The section continues:\n\nBriefly, \\(\\Delta_o\\) is the bias due to omitting relevant variables such as common causes of both the treatment and the outcome variables [whereas] \\(\\Delta_p\\) is bias due to controlling for the consequences of the treatment. [Emphases added]\n\nFrom the Rubinite perspective, it seems that omitted variable bias and post-treatment bias are recognised, in combination, as constituting a wicked problem. This is because the inclusion of an specific variable can simultaneously affect both types of bias: reducing omitted variable bias, but also potentially increasing post treatment bias. You’re doomed if you do, but you’re also doomed if you don’t."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-18/index.html#with-apologies-to-economists-and-epidemiologists-alike",
    "href": "pages/causal-inference/lms-are-glms-part-18/index.html#with-apologies-to-economists-and-epidemiologists-alike",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "With apologies to economists and epidemiologists alike…",
    "text": "With apologies to economists and epidemiologists alike…\nOf the two sources of bias, omitted variable bias seems to be the more discussed. And historically, it seems different social and health science disciplines have placed a different weight of addressing these two sources of bias. In particular, at least in the UK context, it’s seemed that economists tend to be more concerned about omitted variable bias, leading to the inclusion of a large number of variables in their statistical models, whereas epidemiologists (though they might not be familiar with and use the term) tend to be more concerned about post-treatment bias, leading a statistical models with fewer variables.\nThe issue of post treatment bias is especially important to consider in the context of root or fundamental causes, which again is often something more of interest to epidemiologists than economists. And the importance of the issue comes into sharp relief if considering factors like sex or race. An economist/econometrician, if asked to estimate the effect of race on (say) the probability of a successful job application to an esteemed organisation, might be very liable to try to include many additional covariates, such as previous work experience and job qualifications, as ‘control variables’ in a statistical model in addition to race. From this, they might find that the covariate associated with race is neither statistically nor substantively, and from this conclude that there is no evidence of (say) racial discrimination in employment, because any disparities in outcomes between racial groups appear to be ‘explained by’ other factors like previous experience and job qualifications.\nTo this, a methodologically minded epidemiologist might counter - very reasonably - that the econometrician’s model is over-controlling, and that the inclusion of factors like educational outcomes and previous work experience in the model risks introducing post treatment bias. If there were discrimination on the basis of race, or sex, it would be unlikely to just affect the specific outcome on the response side of the model. Instead, discrimination (or other race-based factors) would also likely affect the kind of education available to people of different races, and the kinds of educational expectations placed on people of different racial groups. This would then affect the level of educational achievement by group as well. Similarly, both because of prior differences in educational achievement, and because of concurrent effects of discrimination, race might also be expected to affect job history too. Based on this, the epidemiologist might choose to omit both qualifications and job history from the model, because both are presumed to be causallly downstream of the key factor of interest, race.\nSo which type of model is correct? The epidemiologist’s more parsimonious model, which is mindful of post-treatment bias, or the economist’s more complicated model, which is mindful of omitted variable bias? The conclusion from the four-biases position laid out above is that we don’t know, but that all biases potentially exist in observational data, and neither model specification can claim to be free from bias. Perhaps both kinds of model can be run, and perhaps looking at the estimates from both models can give something like a plausible range of possible effects. But fundamentally, we don’t know, and can’t know, and ideally we should seek better quality data, run RCTs and so on.\nPearl and Mackenzie (2018) argues that Rubinites don’t see much (or any) value in causal diagrams, stating “The Rubin causal model treats counterfactuals as abstract mathematical objects that are managed by algebraic machinery but not derived from a model.” [p. 280] Though I think this characterisation is broadly consciously correct, the recognition within the Rubinite community that such things as post-treatment bias and omitted variables exist suggests to me that, unconsciously, even Rubinites employ something like path-diagram reasoning when considering which sources of bias are likely to affect their effect estimates. Put simply: I don’t see how claims of either omitted variable or post treatment bias could be made or believed but for the kind of graphical, path-like thinking at the centre of the Pearlite paradigm.\nLet’s draw the two types of statistical model implied in the discussion above. Firstly the economist’s model:\n\n\n\n\nflowchart LR\n\nrace(race)\nqual(qualifications)\nhist(job history)\naccept(job offer)\n\nrace --&gt;|Z| accept\nqual --&gt;|X*| accept\nhist --&gt;|X*| accept \n\n\n\n\n\n\nAnd now the epidemiologist’s model:\n\n\n\n\nflowchart LR \n\nrace(race)\naccept(job offer)\n\nrace --&gt;|Z| accept\n\n\n\n\n\n\nEmploying a DAG-like causal path diagram would at the very least allow both the economist and epidemiologist to discuss whether or not they agree that the underlying causal pathways are more likely to be something like the follows:\n\n\n\n\nflowchart LR\n\n\nrace(race)\nqual(qualifications)\nhist(job history)\naccept(job offer)\n\nrace --&gt; qual\nqual --&gt; hist\nhist --&gt; accept\n\nrace --&gt; hist\nqual --&gt; accept\nrace --&gt; accept\n\n\n\n\n\n\nIf, having drawn out their presumed causal pathways like this, the economist and epidemiologist end up with the same path diagram, then the Pearlian framework offers plenty of suggestions about how, subject to various assumptions about the types of effect each node has on each downstream node, statistical models based on observational data should be specified, and how the values of various coefficients in the statistical model should be combined in order to produce an overall estimate of the left-most node on the right-most node. Even a Rubinite who does not subscribe to some of these assumptions may still find this kind of graphical, path-based reasoning helpful for thinking through what their concerns are relating to both omitted variable and post-treatment biases are, and whether there’s anything they can do about it. In the path diagram above, for example, the importance of temporal sequence appears important: first there’s education and qualification; then there’s initial labour market experience; and then there’s contemporary labour market experience. This appreciation of the sequence of events might suggest that, perhaps, data employing a longitudinal research design might be preferred to one using only cross-sectional data; and/or that what appeared intially to be only a single research question, investigated through a single statistical model, is actually a series of linked, stepped research questions, each employing a different statistical model, breaking down the cause-effect question into a series of smaller steps."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-18/index.html#summary-thoughts-on-social-complexity-and-the-need-for-epistemic-humility",
    "href": "pages/causal-inference/lms-are-glms-part-18/index.html#summary-thoughts-on-social-complexity-and-the-need-for-epistemic-humility",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "Summary thoughts: on social complexity and the need for epistemic humility",
    "text": "Summary thoughts: on social complexity and the need for epistemic humility\nAs mentioned before, I probably lean somewhat more towards the Rubinite than the Pearlite framework. A lot of this is simply because this is the causal effect framework I was first introduced to, but some of it comes from more fundamental concerns I have about how some users and advocates of the Pearlite framework seem to think, or suggest, it can solve issues of causal inference from observational data that, fundamentally, I don’t think it may be possible to address.\nOne clue about what the Pearlite framework can and cannot do comes from the ‘A’ in DAG: ‘acyclic’. This means that causal pathways of the following form can be specified:\n\n\n\n\nflowchart LR\nA(A)\nB(B)\n\nA --&gt; B\n\n\n\n\n\nBut causal pathways of the following form cannot:\n\n\n\n\nflowchart LR\n\nA(A)\nB(B)\n\nA --&gt; B\nB --&gt; A\n\n\n\n\n\n\nUnfortunately, cyclic relationships between two or more factors, in which the pathways of influence go in both directions, are likely extremely common in social and economic systems, because such systems are complex rather than merely complicated. 3 One approach to trying to fit a representation of a complex coupled system into a DAG-like framework would be to use time to try to break the causal paths:\n\n\n\n\nflowchart LR\n\nc0(Chicken at T0)\ne1(Egg at T1)\nc2(Chicken at T2)\ne3(Egg at T3)\n\nc0 --&gt; e1\ne1 --&gt; c2\nc2 --&gt; e3\n\n\n\n\n\n\nBut another way of reasoning about such localised coupled complexity might be to use something like factor analysis to identify patterns of co-occurence of variables which may be consistent with this kind of localised complex coupling:\n\n\n\n\nflowchart LR\n\nce((ChickenEgg))\ne[egg]\nc[chicken]\n\nce --&gt; e\nce --&gt; c\n\n\n\n\n\n\nWithin the above diagram, based on structural equation modelling, the directed arrows have a different meaning. They’re not claims of causal effects, but instead of membership. The circle is an underlying proposed ‘latent variable’, the ChickenEgg, which is presumed to manifest through the two observed/manifest variables egg and chicken represented by the rectangles. In places with a lot of ChickenEgg, such as a hen house, we would expect to observe a lot of both chickens and eggs. The statistical model in the above case is a measurement model, rather than a causal model, but in this case is one which is informed by an implicit recognition of continual causal influence operating within members of a complex, paired, causal system.\nSo, I guess my first concern relating to DAGs is that, whereas they can be really useful in allowing researchers to express some form of causal thinking and assumptions about paths of influence between factors, their acyclic requirement can also lead researchers to disregard or underplay the role of complexity even when considering inherently complex systems. In summary, they offer the potential both to expand, but also to restrict, our ability to reason effectively about causal influence.\nMy second, related, concern about the potential over-use or over-reach of DAG-like thinking comes from conventional assumptions built into the paths of influence between nodes. We can get to the heart of this latter concern by looking at , and carefully considering the implications of, something called a double pendulum, a video of which is shown below:\n\n\nA double pendulum is not a complicated system, but it is a complex system, and also a chaotic system. The variables at play include two length variables, two mass variables, a gravity variable, and time. The chaotic complexity of the system comes from the way the length and mass of the first arm interact with the length and and mass of the second arm. This complex interaction is what leads to the position of the outer-most part of the second arm (the grey ball) at any given time.\nNow imagine trying to answer a question of the form “what is the effect of the first arm’s mass on the grey ball’s position?” This kind of question is one that it’s simply not meaningful to even ask. It’s the complex interaction between all components of the system that jointly determines the ball’s position, and attempting to decompose the causal effect of any one variable in the system is simply not a fruitful way of trying to understand the system as a whole.\nThis does not mean, however, that we cannot develop a useful understanding of the double pendulum. We know, for example, that the ball cannot be further than the sum of the length of the two arms from the centre of the system. If we were thinking about placing another object near the double pendulum, for example, this would help us work out how far apart from the pendulum we should place it. Also, if one of the arms is much longer or more massive than the other, then maybe we could approximate it with a simple pendulum too. Additionally, all double pendulums tend to behave in similar ways during their initial fall. But the nature of this kind of complex system also means some types of causal question are beyond the realm of being answerable.\nThe double pendulum, for me, is an object lesson on the importance of epistemic humility. My overall concern relating to causal inference applies nearly equally to Rubinites and Pearlites alike, and is that excessive engagement with or enthusiasm for any kind of method or framework can lead to us believing we know more than we really know more about how one thing affects another. This can potentially lead both to errors of judgement - such as not planning sufficiently for eventualities our models suggest cannot happen - and potentially to intolerance towards those who ‘join the dots’ in a different way to ourselves. 4\nIn short: stay methodologically engaged, but also stay epistemically modest."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-18/index.html#footnotes",
    "href": "pages/causal-inference/lms-are-glms-part-18/index.html#footnotes",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI might not cover these areas in the order listed above, and thinking about this further this might be too much territory for a single post. Let’s see how this post develops…↩︎\nThe bridge point on the Pearlite side might be a recognition of the apparent bloody obviousness of the fact that, if an observational unit was treated, we don’t observe untreated, and vice versa. The kind of table with missing cells, as shown in part fifteen, would appear to follow straightforwardly from conceding this point. However, Pearl and Mackenzie (2018) includes an example of this kind of table (table 8.1; p. 273), and argues forcefully against this particular framing.↩︎\nThe economist’s model is more complicated than the epidemiologist’s model, but both are equally complex, i.e. not complex at all, because they don’t involve any pathways going from right to left.↩︎\nA majority of political disagreement, for example, seems to occur when people agree on the facts, but disagree about the primary causal pathway.↩︎"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-14/index.html",
    "href": "pages/causal-inference/lms-are-glms-part-14/index.html",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "",
    "text": "Henry Dundas, as observed\n\n\n\n\n\n\n\nHenry Dundas, the unobserved good counterfactual\n\n\n\n\n\n\n\nHenry Dundas, the unobserved bad counterfactual"
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-14/index.html#high-level-notewarning",
    "href": "pages/causal-inference/lms-are-glms-part-14/index.html#high-level-notewarning",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "High level note/warning",
    "text": "High level note/warning\nThere are broadly two schools of thought when it comes to thinking about the problems of causal inference. One which interprets the challenge of causal inference mainly as a missing data problem; and another which interprets it mainly in terms of a modelling problem. The posts in this series are largely drawn from the missing data interpretation. If you want an overview of the two approaches (albeit subject to my own ignorance and biases), please skip briefly to the last post in this series before continuing."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-14/index.html#henry-dundas-hero-or-villain",
    "href": "pages/causal-inference/lms-are-glms-part-14/index.html#henry-dundas-hero-or-villain",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "Henry Dundas: Hero or Villain?",
    "text": "Henry Dundas: Hero or Villain?\nA few minutes’ walk from where I live is St Andrew Square. And in the middle of St Andrew Square is the Melville Monument, a 40 metre tall column, on which stands a statue of Henry Dundas, 1st Viscount Melville.\nThough the Melville Monument was constructed in the 19th century to commemorate and celebrate this 18th century figure, in 2020 the City of Edimburgh Council chose to add more context to Dundas’ legacy by unveiling a plaque with the following message::\n\nAt the top of this neoclassial column stands a statue of Hentry Dundas, 1st Viscount Melville (1742-1811). He was the Scottish Lord Advocate, an MP for Edinburgh and Midlothian, and the First Lord of the Admiralty. Dundas was a contentious figure, provoking controversies that resonate to this day. While Home Secretary in 1792, and first Secretary of State for War in 1796 he was instrumental in deferring the abolition of the Atlantic slave trade. Slave trading by British ships was not abolished until 1807. As a result of this delay, more than half a million enslaved Africans crossed the Atlantic.\n\nSo, the claim of the council plaque was that Dundas caused the enslavement of hundreds of thousands of Africans, by promoting a gradualist policy of abolition.\nThe descendents of Dundas contested these claims, however, instead arguing:\n\nThe claim that Henry Dundas caused the enslavement of more than half a million Africans is patently false. The truth is: Dundas was the first MP to advocate in Parliament for the emancipation of slaves in the British territories along with the abolition of the slave trade. Dundas’s efforts resulted in the House of Commons voting in favour of ending the Atlantic slave trade for the first time in its history.\n\nSo, the claim of the descendents was that Dundas prevented the enslavement of (at least) hundreds of thousands of Africans, by promoting a gradualist policy of abolition.\nHow can the same agreed-upon historical facts lead to such diametrically opposing interpretations of the effects of Dundas and his actions?\nThe answer to this question is at the heart of causal inference, and an example of why, when trying to estimate causal effects, at least half of the data are always missing."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-14/index.html#the-unobserved-counterfactual",
    "href": "pages/causal-inference/lms-are-glms-part-14/index.html#the-unobserved-counterfactual",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "The unobserved counterfactual",
    "text": "The unobserved counterfactual\nBoth parties in the Dundas debate have, as mentioned, access to the same historical facts. They agree on the same observed historical reality. And both are making bold claims about the impact of Dundas in relation to the Transatlantic slave trade. In doing this, they are both comparing this observed historical reality with something else: the unobserved counterfactual.\nThe unobserved counterfactual is the data that would have been observed if what had happened, hadn’t happened 1 However, what happened did happen, so this data isn’t observed. So, as it hasn’t been observed, it doesn’t exist in any historic facts. Instead, the unobserved counterfactual has to be imputed, or inferred… in effect, made up.\nCausal inference always involves some kind of comparison between an observed reality and an unobserved counterfactual. The issue at heart of the Dundas debate is that both parties have compared the observed reality with a different unobserved counterfactual, and from this different Dundas effects have been inferred.\nFor the council, the unobserved counterfactual appears to be something like the following:\n\nDundas doesn’t propose a gradualist amendment to a bill in parliament. The more radical and rapid version of the bill passes, and slavery is abolished earlier, leading to fewer people becoming enslaved.\n\nWhereas for the descendents, the unobserved counterfactual appears to be something like this:\n\nDundas doesn’t propose a gradualist amendment to a bill in parliament. Because of this, the more radical version of the bill doesn’t have enough support in parliament (perhaps because it would be acting too much against the financial interests of some parliamentarians and powerful business interests), and so is defeated. As a result of this, the abolition of slavery is delayed, leading to more people becoming enslaved.\n\nSo, by having the same observed historical facts, the observed Dundas, but radically different counterfactuals, the two parties have used the same methodology to derive near antithetical estimates of the ‘Dundas Effect’."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-14/index.html#coming-up",
    "href": "pages/causal-inference/lms-are-glms-part-14/index.html#coming-up",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "Coming up",
    "text": "Coming up\nThe next post offers more of a technical treatment of the key concept introduced here: namely that causal effect estimation depends on comparing observed with counterfactual data, and as the counterfactual is unobserved, causal effect estimation is fundamentally a missing data problem."
  },
  {
    "objectID": "pages/causal-inference/lms-are-glms-part-14/index.html#footnotes",
    "href": "pages/causal-inference/lms-are-glms-part-14/index.html#footnotes",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data that would have been observed if what hadn’t happened, had happened, is the other type of unobserved counterfactual.↩︎"
  },
  {
    "objectID": "pages/intro-to-glms/index.html",
    "href": "pages/intro-to-glms/index.html",
    "title": "Introduction to Generalised Linear Models",
    "section": "",
    "text": "The aims of this web page is to provide an overview of generalised linear models, and ways of thinking about modelling that go beyond ‘star-gazing’."
  },
  {
    "objectID": "pages/intro-to-glms/index.html#fundamentals-of-generalised-linear-models",
    "href": "pages/intro-to-glms/index.html#fundamentals-of-generalised-linear-models",
    "title": "JonStats",
    "section": "",
    "text": "This is some text"
  },
  {
    "objectID": "pages/intro-to-glms/index.html#tldr",
    "href": "pages/intro-to-glms/index.html#tldr",
    "title": "Part One: Model fitting as parameter calibration",
    "section": "tl;dr",
    "text": "tl;dr\nThis is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in @KinTomWit00 ."
  },
  {
    "objectID": "pages/intro-to-glms/index.html#part-1-what-are-statistical-models-and-how-are-they-fit",
    "href": "pages/intro-to-glms/index.html#part-1-what-are-statistical-models-and-how-are-they-fit",
    "title": "Part One: Model fitting as parameter calibration",
    "section": "Part 1: What are statistical models and how are they fit?",
    "text": "Part 1: What are statistical models and how are they fit?\nIt’s common for different statistical methods to be taught as if they’re completely different species or families. In particular, for standard linear regression to be taught first, then additional, more exotic models, like logistic or Poisson regression, to be introduced at a later stage, in an advanced course.\nThe disadvantage with this standard approach to teaching statistics is that it obscures the way that almost all statistical models are, fundamentally, trying to do something very similar, and work in very similar ways.\nSomething I’ve found immensely helpful over the years is the following pair of equations:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nIn words, the above is saying something like:\n\nThe predicted response \\(Y_i\\) for a set of predictors \\(X_i\\) is assumed to be drawn from (the \\(\\sim\\) symbol) a stochastic distribution (\\(f(.,.)\\))\nThe stochastic distribution contains both parameters we’re interested in, and which are determined by the data \\(\\theta_i\\), and parameters we’re not interested in and might just have to assume, \\(\\alpha\\).\nThe parameters we’re interested in determining from the data \\(\\theta_i\\) are themselves determined by a systematic component \\(g(.,.)\\) which take and transform two inputs: The observed predictor data \\(X_i\\), and a set of coefficients \\(\\beta\\)\n\nAnd graphically this looks something like:\n\n\n\n\nflowchart LR\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  \n  X --&gt; g\n  beta --&gt; g\n  g --&gt; theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt; Y\n\n\n\n\n\n\n\nTo understand how this fits into the ‘whole game’ of modelling, it’s worth introducing another term, \\(D\\), for the data we’re using, and to say that \\(D\\) is partitioned into observed predictors \\(X_i\\), and observed responses, \\(y_i\\).\nFor each observation, \\(i\\), we therefore have a predicted response, \\(Y_i\\), and an observed response, \\(y_i\\). We can compare \\(Y_i\\) with \\(y_i\\) to get the difference between the two, \\(\\delta_i\\).\nNow, obviously can’t change the data to make it fit our model better. But what we can do is calibrate the model a little better. How do we do this? Through adjusting the \\(\\beta\\) parameters that feed into the systematic component \\(g\\). Graphically, this process of comparison, adjustment, and calibration looks as follows:\n\n\n\n\nflowchart LR\n  D\n  y\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  diff\n  \n  D --&gt;|partition| X\n  D --&gt;|partition| y\n  X --&gt; g\n  beta --&gt;|rerun| g\n  g --&gt;|transform| theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt;|predict| Y\n  \n  Y --&gt;|compare| diff\n  y --&gt;|compare| diff\n  \n  diff --&gt;|adjust| beta\n  \n  \n  \n  linkStyle default stroke:blue, stroke-width:1px\n\n\n\n\n\n\nPretty much all statistical model fitting involves iterating along this \\(g \\to \\beta\\) and \\(\\beta \\to g\\) feedback loop until some kind of condition is met involving minimising \\(\\delta\\).\nI’ll expand on this idea further in part 2."
  },
  {
    "objectID": "pages/intro-to-glms/index.html#footnotes",
    "href": "pages/intro-to-glms/index.html#footnotes",
    "title": "Introduction to Generalised Linear Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote here I’m using \\(x_j\\), not \\(x_i\\), and that \\(X\\beta\\) is shorthand for \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) and so on. In using the \\(j\\) suffix, I’m referring to just one of the specific \\(x\\) values, \\(x_1\\), \\(x_2\\), \\(x_3\\), which is equivalent to selecting one of the columns in \\(X\\). By contrast \\(i\\) should be considered shorthand for selection of one of the rows of \\(X\\), i.e. one of the series of observations that goes into the dataset \\(D\\).↩︎\nNote here I’m using \\(x_j\\), not \\(x_i\\), and that \\(X\\beta\\) is shorthand for \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) and so on. In using the \\(j\\) suffix, I’m referring to just one of the specific \\(x\\) values, \\(x_1\\), \\(x_2\\), \\(x_3\\), which is equivalent to selecting one of the columns in \\(X\\). By contrast \\(i\\) should be considered shorthand for selection of one of the rows of \\(X\\), i.e. one of the series of observations that goes into the dataset \\(D\\).↩︎\n\\(E(.)\\) is the expectation operator, and \\(|\\) indicates a condition. So, the two terms mean, respectively, what is the expected value of the outcome if the variable of interest is ‘switched on’?, and what is the expected value of the outcome if the variable of interest is ‘switched off’?↩︎\nThe logistic function maps any real number z onto the value range 0 to 1. z is \\(X\\beta\\), which in non-matrix notation is equivalent to a sum of products \\(\\sum_{k=0}^{K}x_k\\beta_k\\) (where, usually, \\(x_0\\) is 1, i.e. the intercept term). Another way of expressing this would be something like \\(\\sum_{k \\in S}x_k\\beta_k\\) where by default \\(S = \\{0, 1, 2, ..., K\\}\\). We can instead imagine partitioning out \\(S = \\{S^{-J}, S^{J}\\}\\) where the superscript \\(J\\) indicates the Jth variable, and \\(-J\\) indicates everything in \\(S\\) apart from the Jth variable. Where J is a discrete variable, the effect of J on \\(P(Y=1)\\) is \\(logistic({\\sum_{k \\in S^{-J}}x_k\\beta_k + \\beta_J}) - logistic({\\sum_{k \\in S^{-J}}x_k\\beta_k})\\), where \\(logistic(z) = \\frac{1}{1 + e^{-z}}\\). The marginal effect of the \\(\\beta_J\\) coefficient thus depends on the other term \\(\\sum_{k \\in S^{-J}}x_k\\beta_k\\). Where this other term is set to 0 the marginal effect of \\(\\beta_J\\) becomes \\(logistic(\\beta_J) - logistic(0)\\). According to p.82 of this chapter by Gelman we can equivalently ask the question ‘what is the first derivative of the logistic regression with respect to \\(\\beta\\)?’. Asking more about this to Wolfram Alpha we get this page of information, and scrolling down to the section on the global minimum we indeed get an absolute value of \\(\\frac{1}{4}\\), so the maximum change in \\(P(Y=1)\\) given a unit change in \\(\\beta\\) is indeed one quarter of the value of \\(\\beta\\), hence why the ‘divide-by-four’ heuristic ‘works’. This isn’t quite a full derivation, but more explanation than I was planning for a footnote! In general, it’s better just to remember ‘divide-by-four’ than go down the rabbit warren of derivation each time! (As I’ve just learned, to my cost, writing this footnote!)↩︎\nWe should always expect the absolute value of a coefficient for a discrete variable to be less than four, for this reason.↩︎\nThe lower bound for the marginal effect of a discrete variable, or any variable, is zero. This is when the absolute value of the sum of the product of the other variables is infinite.↩︎\nOr the base R expand.grid function↩︎"
  },
  {
    "objectID": "pages/intro-to-glms/index.html#what-are-statistical-models-and-how-are-they-fit",
    "href": "pages/intro-to-glms/index.html#what-are-statistical-models-and-how-are-they-fit",
    "title": "Introduction to Generalised Linear Models",
    "section": "What are statistical models and how are they fit?",
    "text": "What are statistical models and how are they fit?\nIt’s common for different statistical methods to be taught as if they’re completely different species or families. In particular, for standard linear regression to be taught first, then additional, more exotic models, like logistic or Poisson regression, to be introduced at a later stage, in an advanced course.\nThe disadvantage with this standard approach to teaching statistics is that it obscures the way that almost all statistical models are, fundamentally, trying to do something very similar, and work in very similar ways.\nSomething I’ve found immensely helpful over the years is the following pair of equations:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nIn words, the above is saying something like:\n\nThe predicted response \\(Y_i\\) for a set of predictors \\(X_i\\) is assumed to be drawn from (the \\(\\sim\\) symbol) a stochastic distribution (\\(f(.,.)\\))\nThe stochastic distribution contains both parameters we’re interested in, and which are determined by the data \\(\\theta_i\\), and parameters we’re not interested in and might just have to assume, \\(\\alpha\\).\nThe parameters we’re interested in determining from the data \\(\\theta_i\\) are themselves determined by a systematic component \\(g(.,.)\\) which take and transform two inputs: The observed predictor data \\(X_i\\), and a set of coefficients \\(\\beta\\)\n\nAnd graphically this looks something like:\n\n\n\n\nflowchart LR\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  \n  X --&gt; g\n  beta --&gt; g\n  g --&gt; theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt; Y\n\n\n\n\n\n\nTo understand how this fits into the ‘whole game’ of modelling, it’s worth introducing another term, \\(D\\), for the data we’re using, and to say that \\(D\\) is partitioned into observed predictors \\(X_i\\), and observed responses, \\(y_i\\).\nFor each observation, \\(i\\), we therefore have a predicted response, \\(Y_i\\), and an observed response, \\(y_i\\). We can compare \\(Y_i\\) with \\(y_i\\) to get the difference between the two, \\(\\delta_i\\).\nNow, obviously can’t change the data to make it fit our model better. But what we can do is calibrate the model a little better. How do we do this? Through adjusting the \\(\\beta\\) parameters that feed into the systematic component \\(g\\). Graphically, this process of comparison, adjustment, and calibration looks as follows:\n\n\n\n\nflowchart LR\n  D\n  y\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  diff\n  \n  D --&gt;|partition| X\n  D --&gt;|partition| y\n  X --&gt; g\n  beta --&gt;|rerun| g\n  g --&gt;|transform| theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt;|predict| Y\n  \n  Y --&gt;|compare| diff\n  y --&gt;|compare| diff\n  \n  diff --&gt;|adjust| beta\n  \n  \n  \n  linkStyle default stroke:blue, stroke-width:1px\n\n\n\n\n\n\nPretty much all statistical model fitting involves iterating along this \\(g \\to \\beta\\) and \\(\\beta \\to g\\) feedback loop until some kind of condition is met involving minimising \\(\\delta\\)."
  },
  {
    "objectID": "pages/intro-to-glms/index.html#systematic-components-and-link-functions",
    "href": "pages/intro-to-glms/index.html#systematic-components-and-link-functions",
    "title": "Introduction to Generalised Linear Models",
    "section": "Systematic components and link functions",
    "text": "Systematic components and link functions\nThe two part equation shown above is too general and abstract to be implemented directly. Instead, specific choices about the \\(f(.)\\) and \\(g(.)\\) need to be made. King, Tomz, and Wittenberg (2000) gives the following examples:\nLogistic Regression\n\\[\nY_i \\sim Bernoulli(\\pi_i)\n\\]\n\\[\n\\pi_i = \\frac{1}{1 + e^{-X_i\\beta}}\n\\]\nLinear Regression\n\\[\nY_i \\sim N(\\mu_i, \\sigma^2)\n\\] \\[\n\\mu_i = X_i\\beta\n\\]\nSo, what’s so special about linear regression, in this framework?\nIn one sense, not so much. It’s got a systematic component, and it’s got a stochastic component. But so do other models. But in another sense, quite a lot. It’s a rare case where the systematic component, \\(g(.)\\), doesn’t transform its inputs in some weird and wonderful way. We can say that \\(g(.)\\) is the identity transform, \\(I(.)\\), which in words means take what you’re given, do nothing to it, and pass it on.\nBy contrast, the systematic component for logistic regression is known as the logistic function. \\(logistic(x) := \\frac{1}{1 + e^{-x}}\\) It transforms inputs that could be anywhere on the real number line to values that lay somewhere between 0 and 1. Why 0 to 1? Because what logistic regression models produce aren’t predicted values, but predicted probabilities, and nothing can be more probable than certain (1) or less probable than impossible (0).\nWe can compare the transformations used in linear and logistic regression as follows:1\n\n\nCode\n# Define transformations\nident &lt;- function(x) {x}\nlgt &lt;- function(x) {1 / (1 + exp(-x))}\n\n\n# Draw the associations\ncurve(ident, -6, 6,\n      xlab = \"x (before transform)\",\n      ylab = \"z (after transform)\",\n      main = \"The Identity 'Transformation'\"\n      )\ncurve(lgt, -6, 6, \n      xlab = \"x (before transform)\", \n      ylab = \"z (after transform)\",\n      main = \"The Logistic Transformation\"\n      )\n\n\n\n\n\n\n\nIdentity Transformation\n\n\n\n\n\n\n\nLogistic Transformation\n\n\n\n\n\n\nThe usual input to the transformation function \\(g(.)\\) is a sum of products. For three variables, for example, this could be \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\). In matrix algebra this generalises to \\(\\boldsymbol{X\\beta}\\) , where \\(\\boldsymbol{X}\\) is the predictor data whose rows are observations, columns are variables, and whose first column is a vector of 1s (for the intercept term). The \\(\\boldsymbol{\\beta}\\) term is a row-wise vector comprising each specific \\(\\beta\\) term, such as \\(\\boldsymbol{\\beta} = \\{ \\beta_0, \\beta_1, \\beta_2 \\}\\) in the three variable example above.\nWhat’s special about the identity transformation, and so linear regression, is that there is a fairly clear correspondence between a \\(\\beta_j\\) term and the estimated influence of changing a predictor variable \\(x_j\\) on the predicted outcome \\(Y\\), i.e. the ‘effect of \\(x_j\\) on \\(Y\\)’. For other transformations this tends to not be the case."
  },
  {
    "objectID": "pages/intro-to-glms/index.html#how-to-express-a-linear-model-as-a-generalised-linear-model",
    "href": "pages/intro-to-glms/index.html#how-to-express-a-linear-model-as-a-generalised-linear-model",
    "title": "Introduction to Generalised Linear Models",
    "section": "How to express a linear model as a generalised linear model",
    "text": "How to express a linear model as a generalised linear model\nIn R, there’s the lm function for linear models, and the glm function for generalised linear models.\nI’ve argued previously that the standard linear regression is just a specific type of generalised linear model, one that makes use of an identity transformation I(.) for its systematic component g(.). Let’s now demonstrate that by producing the same model specification using both lm and glm.\nWe can start by being painfully unimaginative and picking using one of R’s standard datasets\n\n\nCode\nlibrary(tidyverse)\n\niris |&gt; \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n\n\n\n\n\nIt looks like, where the petal length is over 2.5, the relationship with sepal length is fairly linear\n\n\nCode\niris |&gt; \n  filter(Petal.Length &gt; 2.5) |&gt; \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n\n\n\n\n\nSo, let’s make a linear regression just of this subset\n\n\nCode\niris_ss &lt;- \n  iris |&gt; \n  filter(Petal.Length &gt; 2.5) \n\n\nWe can produce the regression using lm as follows:\n\n\nCode\nmod_lm &lt;- lm(Sepal.Length ~ Petal.Length, data = iris_ss)\n\n\nAnd we can use the summary function (which checks the type of mod_lm and evokes summary.lm implicitly) to get the following:\n\n\nCode\nsummary(mod_lm)\n\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = iris_ss)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.09194 -0.26570  0.00761  0.21902  0.87502 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3731 on 98 degrees of freedom\nMultiple R-squared:  0.6864,    Adjusted R-squared:  0.6832 \nF-statistic: 214.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nWoohoo! Three stars next to the Petal.Length coefficient! Definitely publishable!\nTo do the same using glm.\n\n\nCode\nmod_glm &lt;- glm(Sepal.Length ~ Petal.Length, data = iris_ss)\n\n\nAnd we can use the summary function for this data too. In this case, summary evokes summary.glm because it knows the class of mod_glm contains glm.\n\n\nCode\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = Sepal.Length ~ Petal.Length, data = iris_ss)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1391962)\n\n    Null deviance: 43.496  on 99  degrees of freedom\nResidual deviance: 13.641  on 98  degrees of freedom\nAIC: 90.58\n\nNumber of Fisher Scoring iterations: 2\n\n\nSo, the coefficients are exactly the same. But there’s also some additional information in the summary, including on the type of ‘family’ used. Why is this?\nIf we look at the help for glm we can see that, by default, the family argument is set to gaussian.\nAnd if we delve a bit further into the help file, in the details about the family argument, it links to the family help page. The usage statement of the family help file is as follows:\nfamily(object, ...)\n\nbinomial(link = \"logit\")\ngaussian(link = \"identity\")\nGamma(link = \"inverse\")\ninverse.gaussian(link = \"1/mu^2\")\npoisson(link = \"log\")\nquasi(link = \"identity\", variance = \"constant\")\nquasibinomial(link = \"logit\")\nquasipoisson(link = \"log\")\nEach family has a default link argument, and for this gaussian family, this link is the identity function.\nWe can also see that, for both the binomial and quasibinomial family, the default link is logit, which transforms all predictors onto a 0-1 scale, as shown in the last post.\nSo, by using the default family, the Gaussian family is selected, and by using the default Gaussian family member, the identity link is selected.\nWe can confirm this by setting the family and link explicitly, showing that we get the same results\n\n\nCode\nmod_glm2 &lt;- glm(Sepal.Length ~ Petal.Length, family = gaussian(link = \"identity\"), data = iris_ss)\nsummary(mod_glm2)\n\n\n\nCall:\nglm(formula = Sepal.Length ~ Petal.Length, family = gaussian(link = \"identity\"), \n    data = iris_ss)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1391962)\n\n    Null deviance: 43.496  on 99  degrees of freedom\nResidual deviance: 13.641  on 98  degrees of freedom\nAIC: 90.58\n\nNumber of Fisher Scoring iterations: 2\n\n\nIt’s the same!\nHow do these terms used in the glm function, family and link, relate to the general framework in King, Tomz, and Wittenberg (2000)?\n\nfamily is the stochastic component, f(.)\nlink is the systematic component, g(.)\n\nThey’re different terms, but it’s the same broad framework.\nLinear models are just one type of general linear model!"
  },
  {
    "objectID": "pages/intro-to-glms/index.html#why-only-betas-look-at-betas",
    "href": "pages/intro-to-glms/index.html#why-only-betas-look-at-betas",
    "title": "Introduction to Generalised Linear Models",
    "section": "Why only betas look at betas",
    "text": "Why only betas look at betas\n\nWhy overuse of linear regression leads people to look at models in the wrong way\nThough it’s not always phrased this way, a motivating question behind the construction of most statistical models is, “What influence does a single input to the model, \\(x_j\\), have on the output, \\(Y\\)?”2 For a single variable \\(x_j\\) which is either present (1) or absent (0), this is in effect asking what is \\(E(Y | x_j = 1) - E(Y | x_j = 0)\\) ?3\nLet’s look at a linear regression case, then a logistic regression case.\n\n\nLinear Regression example\nUsing the iris dataset, let’s try to predict Sepal Width (a continuous variable) on Sepal Length (a continuous variable) and whether the species is setosa or not (a discrete variable). As a reminder, the data relating these three variables look as follows:\n\n\nCode\nlibrary(ggplot2)\n\niris |&gt;\n    ggplot(aes(Sepal.Length, Sepal.Width, group = Species, colour = Species, shape = Species)) + \n    geom_point()\n\n\n\n\n\nLet’s now build the model:\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- iris |&gt; mutate(is_setosa = Species == 'setosa')\n\nmod_lm &lt;- lm(Sepal.Width ~ Sepal.Length + is_setosa, data = df)\n\nmod_lm\n\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length + is_setosa, data = df)\n\nCoefficients:\n  (Intercept)   Sepal.Length  is_setosaTRUE  \n       0.7307         0.3420         0.9855  \n\n\nThe coefficients \\(\\boldsymbol{\\beta} = \\{\\beta_0, \\beta_1, \\beta_2\\}\\) are \\(\\{0.73, 0.34, 0.99\\}\\), and refer to the intercept, Sepal Length and is_setosa respectively.\nIf we assume a Sepel Length of 6, for example, then the expected Sepal Width (the thing we are predicting) is 0.73 + 6 * 0.34 + 0.99 or about 3.77 in the case where is_setosa is true, and 0.73 + 6 * 0.34 or about 2.78 where is_setosa is false.\nThe difference between these two values, 3.77 and 2.78, i.e. the ‘influence of setosa’ on the outcome, is 0.99, i.e. the \\(\\beta_2\\) coefficient shown before. In fact, for any conceivable (and non-conceivable, i.e. negative) value of Sepal Length, the difference is still 0.99.\nThis is the \\(\\beta_2\\) coefficient, and the reason why, for linear regression, and almost exclusively linear regression, looking at the coefficients themselves provides substantively meaningful information (something King, Tomz, and Wittenberg (2000) calls a ‘quantity of interest’) about the size of influence that a predictor has on a response.\n\n\nLogistic Regression example\nNow let’s look at an example using logistic regression. We will use another tiresomely familiar dataset, mtcars. We are interested in estimating the effect that having a straight engine (vs=1) has on the probability of the car having a manual transmission (am=1). Our model also tries to control for the miles-per-gallon (mpg). The model specification is shown, the model is run, and the coefficeints are all shown below:\n\n\nCode\nmod_logistic &lt;- glm(\n    am ~ mpg + vs,\n    data = mtcars, \n    family = binomial()\n    )\n\nmod_logistic\n\n\n\nCall:  glm(formula = am ~ mpg + vs, family = binomial(), data = mtcars)\n\nCoefficients:\n(Intercept)          mpg           vs  \n    -9.9183       0.5359      -2.7957  \n\nDegrees of Freedom: 31 Total (i.e. Null);  29 Residual\nNull Deviance:      43.23 \nResidual Deviance: 24.94    AIC: 30.94\n\n\nHere the coefficients \\(\\boldsymbol{\\beta} = \\{\\beta_0, \\beta_1, \\beta_2\\}\\) are \\(\\{-9.92, 0.54, -2.80\\}\\), and refer to the intercept, mpg, and vs respectively.\nBut what does this actually mean, substantively?\n\n\n(Don’t) Stargaze\nA very common approach to trying to answer this question is to look at the statistical significance of the coefficients, which we can do with the summary() function\n\n\nCode\nsummary(mod_logistic)\n\n\n\nCall:\nglm(formula = am ~ mpg + vs, family = binomial(), data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -9.9183     3.4942  -2.839  0.00453 **\nmpg           0.5359     0.1967   2.724  0.00644 **\nvs           -2.7957     1.4723  -1.899  0.05758 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 24.944  on 29  degrees of freedom\nAIC: 30.944\n\nNumber of Fisher Scoring iterations: 6\n\n\nA common practice in many social and health sciences is to offer something like a narrative summary of the above, something like:\n\nOur logistic regression model indicates that manualness is positively and significantly associated with our measure of fuel efficiency (p &lt; 0.01). There is also an indication of a negative association with straight engine, but this effect does not quite meet conventional thresholds for statistical significance (p &lt; 0.10).\n\nThis above practice is known as ‘star-gazing’, because summary tables like those above tend to have one or more * symbols in the final row, if the value of the Pr(&gt;|z|) is below 0.05, and narrative summaries like those just above tend to involve looking at the number of stars in each row, alongside whether the Estimate values have a minus sign in front of them.\nStar gazing is a very common practice. It’s also a terrible practice, which - ironically - turns the final presented output of a quantitative model into the crudest of qualitative summaries (positive, negative; significant, not significant). Star gazing is what researchers tend to default to when presented with model outputs from the above because, unlike in the linear regression example, the extent to which the \\(\\beta\\) coefficients answer substantive ‘how-much’-ness questions, like “How much does having a straight engine change the probability of manual transmission?, is not easily apparent from the coefficients themselves.\n\n\nStandardisation\nSo, how can we do better?\nOne approach is to standardise the data that goes into the model before passing them to the model. Standardisation means attempting to make the distribution and range of different variables more similar, and is especially useful when comparing between different continuous variables.\nTo give an example of this, let’s look at a specification with weight (wt) and horsepower (hp) in place of mpg, but keeping engine-type indicator (vs):\n\n\nCode\nmod_logistic_2 &lt;- glm(\n    am ~ vs + wt + hp,\n    data = mtcars, \n    family = binomial()\n    )\n\nsummary(mod_logistic_2)\n\n\n\nCall:\nglm(formula = am ~ vs + wt + hp, family = binomial(), data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) 25.35510   11.24613   2.255   0.0242 *\nvs          -3.12906    2.92958  -1.068   0.2855  \nwt          -9.64982    4.05528  -2.380   0.0173 *\nhp           0.03242    0.01959   1.655   0.0979 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.2297  on 31  degrees of freedom\nResidual deviance:  8.5012  on 28  degrees of freedom\nAIC: 16.501\n\nNumber of Fisher Scoring iterations: 8\n\n\nHere both wt and hp are continuous variables.\nA star gazing zombie might say something like\n\nmanualness is negatively and significantly associated with weight (p &lt; 0.05); there is a positive association with horsepower but this does not meet standard thresholds of statistical significance (0.05 &lt; p &lt; 0.10).\n\nA slightly better approach would be to standardise the variables wt and hp before passing to the model. Standardising means trying to set the variables to a common scale, and giving the variables more similar statistical characteristics.\n\n\nCode\nstandardise &lt;- function(x){\n  (x - mean(x)) / sd(x)\n}\n\nmtcars_z &lt;- mtcars\nmtcars_z$wt_z = standardise(mtcars$wt)\nmtcars_z$hp_z = standardise(mtcars$hp)\n\nmod_logistic_2_z &lt;- glm(\n    am ~ vs + wt_z + hp_z,\n    data = mtcars_z, \n    family = binomial()\n    )\n\nsummary(mod_logistic_2_z)\n\n\n\nCall:\nglm(formula = am ~ vs + wt_z + hp_z, family = binomial(), data = mtcars_z)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -0.9348     1.4500  -0.645   0.5191  \nvs           -3.1291     2.9296  -1.068   0.2855  \nwt_z         -9.4419     3.9679  -2.380   0.0173 *\nhp_z          2.2230     1.3431   1.655   0.0979 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.2297  on 31  degrees of freedom\nResidual deviance:  8.5012  on 28  degrees of freedom\nAIC: 16.501\n\nNumber of Fisher Scoring iterations: 8\n\n\nwt_z is the standardised version of wt, and hp_z is the standardised version of hp. By convention, whereas unstandardised coefficients are usually referred to as \\(\\beta\\) (‘beta’) coefficients, standardised coefficients are instead referred to as \\(b\\) coefficients. But really, it’s the same model.\nNote the p value of wt_z is the same as for wt, and the p value of hp_z is the same as that for hp. Note also the directions of effect are the same: the coefficients on wt and wt_z are both negative, and the coefficients of hp and hp_z are both positive.\nThis isn’t a coincidence. Of course standardising can’t really add any new information, can’t really change the relationship between a predictor and a response. It’s not really a new variable, it’s the same old variable, so the relationship between predictor and response that there used to be is still there now.\nSo why bother standardising?\nOne reason is it gives, subject to some assumptions and caveats, a way of gauging the relative importance of the two different continuous variables, by allowing a slightly more meaningful comparison between the two coefficients.\nIn this case, we have a standardised \\(b\\) coefficient of -9.44 for wt_z, and of 2.22 for hp_z. As with the unstandardised coefficients we can still assert that manualness is negatively associated with weight, and positively associated with horsepower. But now we can also compare the two numbers -9.44 and 2.22. The ratio of these two numbers is around 4.3. So, we might hazard to suggest something like:\n\na given increase in weight is around four times as important in negatively predicting manual transmission (i.e. in predicting an automatic transmission) as an equivalent increase in horsepower is in positively predicting manual transmission.\n\nThis isn’t a statement that’s easy to parse, but does at least allow slightly more information to be gleamed from the model. For example, it implies that, if a proposed change to a vehicle leads to similar relative (standardised) increases in both weight and horsepower then, as the weight effect is greater than the horsepower effect, the model will predict a decreased probability of manualness as a result.\nBut what about the motivating question, “What’s the effect of a straight engine (vs=1) on the probability of manual transmission (am=1)?”\nThe problem, unlike with the linear regression, is this is now a badly formulated question, based on an incorrect premise. The problem is with the word ‘the’, which implies there should be a single answer to this question, i.e. that the effect of vs on the probability of am=1 should always be the same. But, at least when it comes to absolute changes in the probability of am=1, this is no longer the case, as it depends on the values of the other variables in the model.\nInstead of assuming vs=1 has a single effect on P(am=1), we instead need to think about predictions of the marginal effects of vs on am in the context of other plausible values of the other predictors in the model, wt and hp. This involves asking the model a series of well formulated and specific questions.\n\n\nMaximum marginal effects: Divide-by-four\nBefore we do that, however, there’s a useful heuristic that can be employed when looking at discrete variables and using a logistic regression specification. The heuristic, which is based on the properties of the logistic function,4 is called divide-by-four. What this means is that, if we take the coefficient on vs of -3.13, and divide this value by four, we get a value of -0.78. Notice that the absolute value of -0.78 is between 0 and 1.5 What this value gives is the maximum possible effect that the discrete variable (the presence rather than absence of a straight engine) has on the probability of being a manual transmission. We can say, “a straight engine reduces the probability of a manual transmission by up to 78%”\nBut, as mentioned, this doesn’t quite answer the motivating question, it gives an upper bound to the answer, not the answer itself.6 We can instead start to get a sense of ‘the’ effect of the variable vs on P(am=1) by asking the model a series of questions.\n\n\nPredictions on a matrix\nWe can start by getting the range of observed values for the two continuous variables, hp and mpg:\n\n\nCode\nmin(mtcars$hp)\n\n\n[1] 52\n\n\nCode\nmax(mtcars$hp)\n\n\n[1] 335\n\n\nCode\nmin(mtcars$wt)\n\n\n[1] 1.513\n\n\nCode\nmax(mtcars$wt)\n\n\n[1] 5.424\n\n\nWe can then ask the model to make predictions of \\(P(am=1)\\) for a large number of values of hp and wt within the observed range, both in the condition in which vs=0 and in the condition in which vs=1. The expand_grid function7 can help us do this:\n\n\nCode\npredictors &lt;- expand_grid(\n  hp = seq(min(mtcars$hp), max(mtcars$hp), length.out = 100),\n  wt = seq(min(mtcars$wt), max(mtcars$wt), length.out = 100)\n)\n\npredictors_straight &lt;- predictors |&gt; \n  mutate(vs = 1)\n\npredictors_vshaped &lt;- predictors |&gt; \n  mutate(vs = 0)\n\n\nFor each of these permutations of inputs, we can use the model to get a conditional prediction. For convenience, we can also attach this as an additional column to the predictor data frame:\n\n\nCode\npredictions_predictors_straight &lt;- predictors_straight |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictors_straight)\n  )\n\npredictions_predictors_vshaped &lt;- predictors_vshaped |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictors_vshaped)\n  )\n\n\nWe can see how the predictions vary over hp and wt using a heat map or contour map:\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  ggplot(aes(x = hp, y = wt, z = p_manual)) + \n  geom_contour_filled() + \n  facet_wrap(~vs) +\n  labs(\n    title = \"Predicted probability of manual transmission by wt, hp, and vs\"\n  )\n\n\n\n\n\nWe can also produce a contour map of the differences between these two contour maps, i.e. the effect of a straight (vs=1) compared with v-shaped (vs=0) engine, which gets us a bit closer to the answer:\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  group_by(hp, wt) |&gt; \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |&gt; \n  ungroup() |&gt; \n  ggplot(\n    aes(x = hp, y = wt, z = diff_p_manual)\n  ) + \n  geom_contour_filled() + \n  labs(\n    title = \"Marginal effect of vs=1 given wt and hp on P(am=1)\"\n  )\n\n\n\n\n\nWe can see here that, for large ranges of wt and hp, the marginal effect of vs=1 is small. However, for particular combinations of hp and wt, such as where hp is around 200 and wt is slightly below 3, then the marginal effect of vs=1 becomes large, up to around a -70% reduction in the probability of manual transmission. (i.e. similar to the theoretical maximum marginal effect of around -78%).\nSo, what’s the effect of vs=1 on P(am=1)? i.e. how should we boil down all these 10,000 predicted effect sizes into a single effect size?\nI guess, if we have to try to answer this silly question, then we could take the average effect size…\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  group_by(hp, wt) |&gt; \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |&gt; \n  ungroup() |&gt; \n  summarise(\n    mean_diff_p_manual = mean(diff_p_manual)\n  )\n\n\n# A tibble: 1 × 1\n  mean_diff_p_manual\n               &lt;dbl&gt;\n1            -0.0821\n\n\nSo, we get an average difference of around -0.08, i.e. about an 8% reduction in probability of manual transmission.\n\n\nMarginal effects on observed data\nIs this a reasonable answer? Probably not, because although the permutations of wt and hp we looked at come from the observed range, most of these combinations are likely very ‘theoretical’. We can get a sense of this by plotting the observed values of wt and hp onto the above contour map:\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  group_by(hp, wt) |&gt; \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |&gt; \n  ungroup() |&gt; \n  ggplot(\n    aes(x = hp, y = wt, z = diff_p_manual)\n  ) + \n  geom_contour_filled(alpha = 0.2, show.legend = FALSE) + \n  labs(\n    title = \"Observations from mtcars on the predicted probability surface\"\n  ) +\n  geom_point(\n    aes(x = hp, y = wt), inherit.aes = FALSE,\n    data = mtcars\n  )\n\n\n\n\n\nPerhaps a better option, then, would be to calculate an average marginal effect using the observed values, but switching the observations for vs to 1 in one scenario, and 0 in another scenario:\n\n\nCode\npredictions_predictors_observed_straight &lt;- mtcars |&gt; \n  select(hp, wt) |&gt; \n  mutate(vs = 1)\n\npredictions_predictors_observed_straight &lt;- predictions_predictors_observed_straight |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictions_predictors_observed_straight)\n  )\n\npredictions_predictors_observed_vshaped &lt;- mtcars |&gt; \n  select(hp, wt) |&gt; \n  mutate(vs = 0) \n\npredictions_predictors_observed_vshaped &lt;- predictions_predictors_observed_vshaped |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictions_predictors_observed_vshaped)\n  )\n  \n\npredictions_predictors_observed &lt;- \n  bind_rows(\n    predictions_predictors_observed_straight,\n    predictions_predictors_observed_vshaped\n  )\n\npredictions_marginal &lt;- \n  predictions_predictors_observed |&gt; \n    group_by(hp, wt) |&gt; \n    summarise(\n      diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n    )\n\npredictions_marginal |&gt; \n  ggplot(aes(x = diff_p_manual)) + \n  geom_histogram() +\n  geom_vline(aes(xintercept = mean(diff_p_manual)), colour = \"red\") + \n  geom_vline(aes(xintercept = median(diff_p_manual)), colour = \"green\")\n\n\n\n\n\nIn the above the red line indicates the mean value of these marginal differences, which is -0.12, and the green line the median value of these differences, which is around -0.02. So, even with just these two measures of central tendency, there’s around a six-fold difference in the estimate of ‘the effect’. We can also see there’s a lot of variation, from around nothing (right hand side), to around a 65% reduction (left hand side).\nIf forced to give a simple answer (to this overly simplistic question), we might plump for the mean for theoretical reasons, and say something like “The effect of a straight engine is to reduce the probability of a manual transmission by around an eighth”. But I’m sure, having seen how much variation there is in these marginal effects, we can agree this ‘around an eighth’ answer, or any single number answer, is likely to be overly reductive.\nHopefully, however, it is more informative than ‘statistically significant and negative’, (the stargazing approach) or ‘up to around 78%’ (the divide-by-four approach).\n\n\nConclusion\nLinear regression tends to give a false impression about how straightforward it is to use a model to answer questions of the form “What is the effect of x on y?”. This is because, for linear regression, but few other model specifications, the answer to this question is in the \\(\\beta\\) coefficients themselves. For other model specifications, like the logistic regression example above, the correct-but-uninformative answer tends to be “it depends”, and potentially more informative answers tend to require a bit more work to derive and interpret."
  },
  {
    "objectID": "pages/intro-to-glms/index.html#conclusion",
    "href": "pages/intro-to-glms/index.html#conclusion",
    "title": "Part Four: why only betas just look at betas",
    "section": "Conclusion",
    "text": "Conclusion\nLinear regression tends to give a false impression about how straightforward it is to use a model to answer questions of the form “What is the effect of x on y?”. This is because, for linear regression, but few other model specifications, the answer to this question is in the \\(\\beta\\) coefficients themselves. For other model specifications, like the logistic regression example above, the correct-but-uninformative answer tends to be “it depends”, and potentially more informative answers tend to require a bit more work to derive and interpret."
  },
  {
    "objectID": "pages/intro-to-glms/index.html#coming-up",
    "href": "pages/intro-to-glms/index.html#coming-up",
    "title": "Part Four: why only betas just look at betas",
    "section": "Coming up",
    "text": "Coming up\nThis post concludes the first section of this blog series. We showed the importance of producing predictions from models, rather than just staring at tables of coefficients and producing qualitative ‘stargazing’ summaries of their statistical significance and direction of effect. Statistical significance of individual coefficients almost never answers questions of substantive significance, which instead come from model predictions.\nHowever in the predictions so far, we’ve accidentially pretended to know more than we do. For each prediction, despite imperfect knowledge, we’ve presented point estimates, a single prediction, implying our estimates are sometimes perfectly precise. To be more honest to the user, we should instead present a range of estimates that takes into account all the sources of uncertainty in our modelling which lead to uncertainty in our predictions.\nSection two of this blog series, starting with part five, takes us through the material necessary to go from presenting the kind of dishonest certainty of point estimates in predictions, to honest uncertainty in predictive intervals. This involves covering a lot of theoretical and methodological territory, and is fairly challenging. However we do this in order to make it easier for the end user of statistical models, decision makers, to get the kind of information they need and value most from models."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "",
    "text": "The aim of this this post is to show how the best parameter combinations tend to be estimated from a model’s log likelihood in practice, using an optimisation algorithm that iteratively tries out new parameter values, and keeps trying and trying until some kind of condition is met. This is what the last figure in the first post is trying to illustrate."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#optimisation-algorithms-getting-there-faster",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#optimisation-algorithms-getting-there-faster",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Optimisation algorithms: getting there faster",
    "text": "Optimisation algorithms: getting there faster\nIn the previous post, we ‘cheated’ a bit when using the log likelihood function, fixing the value for one of the parameters \\(\\sigma^2\\) to the value we used when we generated the data, so we could instead look at how the log likelihood surface varied as different combinations of \\(\\beta_0\\) and \\(\\beta_1\\) were plugged into the formula. \\(\\beta_0\\) and \\(\\beta_1\\) values ranging from -5 to 5, and at steps of 0.1, were considered: 101 values of \\(\\beta_0\\), 101 values of \\(\\beta_1\\), and so over 10,0001 unique \\(\\{\\beta_0, \\beta_1\\}\\) combinations were stepped through. This approach is known as grid search, and seldom used in practice (except for illustration purposes) because the number of calculations involved can very easily get out of hand. For example, if we were to use it to explore as many distinct values of \\(\\sigma^2\\) as we considered for \\(\\beta_0\\) and \\(\\beta_1\\), the total number of \\(\\{\\beta_0, \\beta_1, \\sigma^2 \\}\\) combinations we would crawl through would be over 100,000 2 rather than over 10,000.\nOne feature we noticed with the likelihood surface over \\(\\beta_0\\) and \\(\\beta_1\\) in the previous post is that it appears to look like a hill, with a clearly defined highest point (the region of maximum likelihood) and descent in all directions from this highest point. Where likelihood surfaces have this feature of being single-peaked in this way (known as ‘unimodal’), then a class of algorithms known as ‘hill climbing algorithms’ can be applied to find the top of such peaks in a way that tends to be both quicker (fewer steps) and more precise than the grid search approach used for illustration in the previous post."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#code-recap",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#code-recap",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Code recap",
    "text": "Code recap\nLet’s copy over the code we used in the previous post for:\n\n\nCalculating log likelihood\n\n\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\nAnd\n\n\nGenerating our tame toy dataset of 10 data points\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nTo recap, the toy dataset looks as follows:\n\n\nCode\nlibrary(tidyverse)\ntibble(x=x, y=y) |&gt;\n    ggplot(aes(x, y)) + \n    geom_point()"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#optim-our-robo-chauffeur",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#optim-our-robo-chauffeur",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "optim: our Robo-Chauffeur",
    "text": "optim: our Robo-Chauffeur\nNote how the llNormal function takes a single argument, pars, which packages up all the specific candidate parameter values we want to try out. In our previous post, we also had a ‘feeder function’, feed_to_ll, which takes the various \\(\\beta\\) candidate values from the grid and packages them into pars. In our previous post, we had to specify the candidate values to try to feed to llNormal packages inside pars.\nBut we don’t have to do this. We can instead use an algorithm to take candidate parameters, try them out, then make new candidate parameters and try them out, for us. Much as a taxi driver needs to know where to meet a passenger, but doesn’t want the passenger to tell them exactly which route to take, we just need to specify a starting set of values for the parameters to optimise. R’s standard way of doing this is with the optim function. Here’s it in action:\n\n\nCode\noptim_results &lt;-  optim(\n    # par contains our initial guesses for the three parameters to estimate\n    par = c(0, 0, 0), \n\n    # by default, most optim algorithms prefer to search for a minima (lowest point) rather than maxima \n    # (highest point). So, I'm making a function to call which simply inverts the log likelihood by multiplying \n    # what it returns by -1\n    fn = function(par, y, X) {-llNormal(par, y, X)}, \n\n    # in addition to the par vector, our function also needs the observed output (y)\n    # and the observed predictors (X). These have to be specified as additional arguments.\n    y = y, X = X\n    )\n\noptim_results\n\n\n$par\n[1]  2.460571  1.375421 -1.336209\n\n$value\n[1] -1.51397\n\n$counts\nfunction gradient \n     216       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nThe optim function returns a fairly complex output structure, with the following components:\n\npar: the values for the parameters (in our case \\(\\{\\beta_0, \\beta_1, \\eta \\}\\)) which the optimisation algorithm ended up with.\nvalue: the value returned by the function fn when the optim routine was stopped.\ncounts: the number of times the function fn was repeatedly called by optim before optim decided it had had enough\nconvergence: whether the algorithm used by optim completed successfully (i.e. reached what it considers a good set of parameter estimates in par), or not.\n\nIn this case, convergence is 0, which (perhaps counterintuitively) indicates a successful completion. counts indicates that optim called the log likelihood function 216 times before stopping, and par indicates values of \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\eta = -1.34\\}\\) were arrived at. As \\(\\sigma^2 = e^\\eta\\), this means \\(\\theta = \\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\sigma^2 = 0.26 \\}\\). As a reminder, the ‘true’ values are \\(\\{\\beta_0 = 2.50, \\beta_1 = 1.40, \\sigma^2 = 0.25\\}\\).\nSo, the optim algorithm has arrived at pretty much the correct answers for all three parameters, in 216 calls to the log likelihood function, whereas for the grid search approach in the last post we made over 10,000 calls to the log likelihood function for just two of the three parameters.\nLet’s see if we can get more information on exactly what kind of path optim took to get to this set of parameter estimates. We should be able to do this by specifying a value in the trace component in the control argument slot…"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#comparisons",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#comparisons",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Comparisons",
    "text": "Comparisons\nFor comparison let’s see what lm and glm produce.\nFirst lm:\n\n\nCode\ntoy_df &lt;- tibble(\n    x = x, \n    y = y\n)\n\n\nmod_lm &lt;- lm(y ~ x, data = toy_df)\nsummary(mod_lm)\n\n\n\nCall:\nlm(formula = y ~ x, data = toy_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6082 -0.3852 -0.1668  0.2385  1.1092 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5813 on 7 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9767 \nF-statistic:   336 on 1 and 7 DF,  p-value: 3.564e-07\n\n\n\\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\), i.e. the same to 2 decimal places.\nAnd now with glm:\n\n\nCode\nmod_glm &lt;- glm(y ~ x, data = toy_df, family = gaussian(link = \"identity\"))\n\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = toy_df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nOnce again, \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\)"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#discussion",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#discussion",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Discussion",
    "text": "Discussion\nIn the above, we’ve successfully used optim, our Robo-Chauffeur, to arrive very quickly at some good estimates for our parameters of interest, \\(\\beta_0\\) and \\(\\beta_1\\), which are in effect identical to those produced by the lm and glm functions.\nThis isn’t a coincidence. What we’ve done the hard way is what the glm function (in particular) largely does ‘under the hood’."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#coming-up",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#coming-up",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Coming up",
    "text": "Coming up\nIn the next part of this series, we’ll see how other outputs available from optim can be used to estimate uncertainty in the parameters of interest, how this information can be used to produce the kinds of estimates of standard errors around coefficients which are summarised in glm and lm summary() functions, and which many (ab)users of statistical models obsess about when star-gazing, and how information about uncertainty in parameter estimates allows for more honest model-based predictions."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#footnotes",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#footnotes",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(101^2 = 10201\\)↩︎\n\\(101^3 = 1030301\\)↩︎"
  },
  {
    "objectID": "pages/intro-to-glms/index.html#aim",
    "href": "pages/intro-to-glms/index.html#aim",
    "title": "Introduction to Generalised Linear Models",
    "section": "",
    "text": "The aims of this web page is to provide an overview of generalised linear models, and ways of thinking about modelling that go beyond ‘star-gazing’."
  },
  {
    "objectID": "pages/intro-to-glms/index.html#page-discussion",
    "href": "pages/intro-to-glms/index.html#page-discussion",
    "title": "Introduction to Generalised Linear Models",
    "section": "Page discussion",
    "text": "Page discussion\nThis section of the course has aimed to reintroduce statistics from the perspective of generalised linear models (GLMs), in order to make the following clearer:\n\nThat linear regression is just one member of a broader ‘family’ of regression models\nThat all regression models can be thought of as just ‘types’ of GLM, with more in common than divides them\nThat we can and should aim for substantive significance when using the outputs of GLMs, i.e. use them for prediction and simulation rather than focus on whether individual coefficients are ‘statistically significant’ or not.\n\nThe next section of the course delves further into the fundamentals of model fitting and statistical inference, including likelihood theory."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html",
    "href": "pages/likelihood-and-simulation-theory/index.html",
    "title": "Likelihood and Simulation Theory",
    "section": "",
    "text": "In the first part of the course, I stated that statistical model fitting, within the generalised model framework presented in King, Tomz, and Wittenberg (2000), involves adjusting candidate values for elements of \\(\\beta = \\{\\beta_0, \\beta_1, ..., \\beta_K \\}\\) such that the difference between what the model predicts given some predictor values, \\(Y_i | X_i\\), and what has been observed alongside the predictors, \\(y_i\\), is minimised on average1 in some way.\nThe aim of this post is to show how this process is typically implemented in GLMs, using likelihood theory."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#aim",
    "href": "pages/likelihood-and-simulation-theory/index.html#aim",
    "title": "Likelihood and Simulation Theory",
    "section": "Aim",
    "text": "Aim\nIn this post, we’ll now, finally, show how this knowledge can be applied to do something with statistical models that ought to be done far more often: report on what King, Tomz, and Wittenberg (2000) calls quantities of interest, including predicted values, expected values, and first differences. Quantities of interest are not the direction and statistical significance (P-values) that many users of statistical models convince themselves matter, leading to the kind of mindless stargazing summaries of model outputs described in post four. Instead, they’re the kind of questions that someone, not trained to think that stargazing is satisfactory, might reasonably want answers to. These might include:\n\nWhat is the expected income of someone who completes course X in the five years after graduation? (Expected values)\nWhat is the expected range of incomes of someone who completes course X in the five years after graduation? (Predicted values)\nWhat is the expected difference in incomes between someone who completes course X, compared to course Y, in the five years after graduation? (First Differences)\n\nIn post four, we showed how to answer some of the questions of this form, for both standard linear regression and logistic regression. We showed that for linear regression such answers tend to come directly from the summary of coefficients, but that for logistic regression such answers tend to be both more ambiguous and dependent on other factors (such as gender of graduate, degree, ethnicity, age and so on), and require more processing in order to produce estimates for.\nHowever, we previously produced only point estimates for these questions, and so in a sense misled the questioner with the apparent certainty of our estimates. We now know, from post eight, that we can use information about parameter uncertainty to produce parameter estimates \\(\\tilde{\\theta}\\) that do convey parameter uncertainty, and so we can do better than the point estimates alone to answer such questions in way that takes into account such uncertainty, with a range of values rather than a single value."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#bayes-rule-and-likelihood",
    "href": "pages/likelihood-and-simulation-theory/index.html#bayes-rule-and-likelihood",
    "title": "Likelihood and Simulation Theory",
    "section": "Bayes’ Rule and Likelihood",
    "text": "Bayes’ Rule and Likelihood\nStatisticians and more advanced users of statistical models often divide themselves into ‘frequentists’ and ‘Bayesians’. To some extent the distinction is really between ‘improper Bayesians’ and ‘proper Bayesians’, however, as Bayes’ Rule is at the root of both approaches. Bayes’ Rule is:\n\\[\nP(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n\\]\nNote in the above the left hand side of the equation is \\(P(A|B)\\) and the right hand side of the equation includes \\(P(B|A)\\). To write it out as awkward prose, therefore, Bayes’ Rule is a way of expressing that given this in terms of this given that.\nAs with much of algebra, \\(A\\) and \\(B\\) are just placeholders. We could instead use different symbols instead, such as:\n\\[\nP(\\tilde{\\theta} | y) = \\frac{P(y | \\tilde{\\theta})P(\\tilde{\\theta})}{P(y)}\n\\]\nLikelihood theory offers a way of thinking about how good a model is in terms of its relationship to the data. According to King (1998) (p. 59), it can be expressed as:\n\\[\nL(\\tilde{\\theta}| y) = k(y) P(y | \\tilde{\\theta})\n\\]\nOr\n\\[\nL(\\tilde{\\theta} | y) \\propto P(y | \\tilde{\\theta})\n\\]\nWhere \\(\\tilde{\\theta}\\) is a proposed parameter or parameter combination for the model, and \\(y\\) is the observed outcome.2\nThe important thing to note is that both Bayes’ Rule and Likelihood Theory are ways of expressing this given that as a function of that given this. Specifically, the model given the data, as a function of the data given the model. 3"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#likelihood-for-linear-regression",
    "href": "pages/likelihood-and-simulation-theory/index.html#likelihood-for-linear-regression",
    "title": "Likelihood and Simulation Theory",
    "section": "Likelihood for linear regression",
    "text": "Likelihood for linear regression\nWhen, many years ago, I completed the course from this modelling framework is most associated, a hazing ritual employed near the start of the course was to require participants to derive the likelihood of different model specifications. However, I don’t feel like hazing myself right now, so instead we can use the derivation shown on slide 8 of these slides:\n\\[\nL(\\beta, \\sigma^2 | y) = \\prod{L(y_i | \\mu_i, \\sigma^2)}\n\\]\nWhere \\(\\mu = X \\beta\\), \\(i\\) indicates an observation in the data (a row of \\(X\\) when \\(X\\) is in matrix form), and \\(\\prod\\) indicates the likelihoods from each observation should be multiplied with each other to derive the overall likelihood for all observed data.\nIn practice the log Likelihood, rather than the likelihood itself, is used, because this allows calculation of a sum of terms (\\(\\sum\\)) rather than product of terms (\\(\\prod\\)), and the latter tends to be computationally easier to calculate.\nAs we are interested only in how likelihood varies as a function of those model parameters we wish to estimate, \\(\\theta = \\{\\beta, \\sigma^2\\}\\), some of the terms in the log likelihood expression can be omitted, leaving us with:\n\\[\n\\log{L(\\beta, \\sigma^2 | y)} \\doteq \\sum{-\\frac{1}{2}[\\log{\\sigma^2} + \\frac{(y_i - X_i\\beta)^2}{\\sigma^2}]}\n\\]\nFor all the complexity of the above expression, at heart it takes three inputs:\n\n\\(\\theta = \\{\\beta, \\sigma^2\\}\\) : The candidate parameters for the model.\n\\(y\\) : the observed response value from the dataset \\(D\\)\n\\(X\\) : the observed predictor values from the dataset \\(D\\)\n\nAnd returns one value, the log likelihood \\(\\log{L(.)}\\).\nTo reiterate, we can’t change the data, but we can keep changing the candidate parameters \\(\\theta\\). Each time we do so, \\(\\log{L(.)}\\) will change too.\nThe aim of model calibration, in the Likelihood framework, is to maximise the Likelihood. The parameter set that maximises the likelihood is also the parameter set that maximises the log likelihood.\nTo continue the example from the slides, we can write out a function for calculating the log likelihood of standard linear regression as follows:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\nIn the above, pars is (almost but not quite) \\(\\theta\\), the parameters to estimate. For standard linear regression \\(\\theta = \\{\\beta, \\sigma^2\\}\\), where \\(\\beta = \\{\\beta_0, \\beta_1, ..., \\beta_k\\}\\), i.e. a vector of beta parameters, one for each column (variable) in \\(X\\), the predictor matrix of observations; this is why \\(beta\\) is selected from the first K values in pars where K is the number of columns in \\(X\\).\nThe last value in pars is used to derive the proposed \\(\\sigma^2\\). If we call this last value eta (\\(\\eta\\)), then we can say \\(\\sigma^2 = e^{\\eta}\\). So, whereas \\(\\theta\\) is a vector that ‘packs’ \\(\\beta\\) and \\(\\sigma^2\\) into a single ordered series of values, pars packs eta in place of \\(\\sigma^2\\). This substitution of eta for \\(\\sigma^2\\) is done to make it easier for standard parameter fitting algorithms to work, as they tend to operate over the full real number range, rather than just over positive values.\nIn order to illustrate how the log likelihood function llNormal works in practice, let’s construct a simple toy dataset \\(D\\), and decompose \\(D = \\{y, X\\}\\), the two types of data input that go into the llNormal function.\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nIn the code above we have created \\(y\\), a vector of nine observed responses; and \\(X\\), a matrix of predictors with two columns (the number of variables for which \\(beta\\) terms need to be estimated) and nine rows (the number of observations).\nGraphically, the relationship between x and y looks as follows:\n\n\nCode\nlibrary(tidyverse)\ntibble(x=x, y=y) |&gt;\n    ggplot(aes(x, y)) + \n    geom_point()\n\n\n\n\n\nIn this toy example, but almost never in reality, we know the correct parameters for the model. These are \\({\\beta_0 = 2.5, \\beta_1 = 1.4}\\) and \\(\\sigma^2 = 0.25\\). 4 Soon, we will see how effectively we can use optimisation algorithms to recover these true model parameters. But first, let’s see how the log likelihood varies as a function jointly of different candidate values of \\(\\beta_0\\) (the intercept) and \\(\\beta_1\\) (the slope parameter), if we already set \\(\\sigma^2\\) to 0.25.\n\n\nCode\ncandidate_param_values &lt;- expand_grid(\n    beta_0 = seq(-5, 5, by = 0.1),\n    beta_1 = seq(-5, 5, by = 0.1)\n)\n\nfeed_to_ll &lt;- function(b0, b1){\n    pars &lt;- c(b0, b1, log(0.25))\n    llNormal(pars, y, X)\n}\n\ncandidate_param_values &lt;- candidate_param_values |&gt;\n    mutate(\n        ll = map2_dbl(beta_0, beta_1, feed_to_ll)\n    )\n\n\n\n\nCode\ncandidate_param_values |&gt;\n    ggplot(aes(beta_0, beta_1, z = ll)) + \n    geom_contour_filled() + \n    geom_vline(xintercept = 0) +\n    geom_hline(yintercept = 0) +\n    labs(\n        title = \"Log likelihood as a function of possible values of beta_0 and beta_1\",\n        x = \"beta0 (the intercept)\",\n        y = \"beta1 (the slope)\"\n    )\n\n\n\n\n\nLooking at this joint surface of values, we can see a ‘hotspot’ where \\(\\beta_0\\) is around 2.5, and \\(\\beta_1\\) is around 1.4, just as we should expect. We can check this further by filtering candidate_param_values on the highest observed values of ll.\n\n\nCode\ncandidate_param_values |&gt; \n    filter(ll == max(ll))\n\n\n# A tibble: 1 × 3\n  beta_0 beta_1    ll\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    2.4    1.4  1.41"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#summary",
    "href": "pages/likelihood-and-simulation-theory/index.html#summary",
    "title": "Likelihood and Simulation Theory",
    "section": "Summary",
    "text": "Summary\nThis post has shown how optim(), which in its vanilla state only returns point estimates, can be configured to also calculater and report the Hessian, a record of instantaneous curvature around the point estimates. Even without a fine-grained and exhausive search throughout the likelihood surface, this measure of curvature can be used to produce similar measures of uncertainty to the more exhausive approach, in a fraction of the number of computations.\nMore importantly, it can be used to generate draws of plausible combinations of parameter values, something denoted as \\(\\tilde{\\theta}\\) earlier. This is something especially useful for producing honest quantities of interest, which both tell users of models something they want to know, while also representing how uncertain we are in this knowledge."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#coming-up",
    "href": "pages/likelihood-and-simulation-theory/index.html#coming-up",
    "title": "Likelihood and Simulation Theory",
    "section": "Coming up",
    "text": "Coming up\nThe next post covers the same kind of exercise we’ve performed for standard linear regression - specifying the likelihood function, and fitting it using optim() - but for logistic regression instead. This same kind of exercise could be repeated for all kinds of other model types. But hopefully this one additional example is sufficient."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#footnotes",
    "href": "pages/likelihood-and-simulation-theory/index.html#footnotes",
    "title": "Likelihood and Simulation Theory",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf \\(Y_i\\) is what the model predicts given observations \\(X_i\\), and \\(y_i\\) is the outcome observed to have occurred alongside \\(X_i\\), then we can call \\(\\delta_i = h(y_i, Y_i)\\) the difference, or error, between predicted and observed value. The function \\(h(.,.)\\) is typically the squared difference between predicted and observed values, \\((Y_i - y_i)^2\\), but could also in principle be the absolute difference \\(|Y_i - y_i|\\). Term-fitting algorithms usually compare not any individual \\(\\delta_i\\), but a sum of these error terms \\(\\delta\\). The aim of the algorithm is to find the set of \\(\\beta\\) terms that is least wrong for the whole dataset \\(D\\), rather than any specific row in the dataset \\(D_i\\).↩︎\nAs King (1998) (p. 59) describes it, “\\(k(y)\\) is an unknown fuction of the data. Whereas traditional probability is a measure of absolute uncertainty … the constant \\(k(y)\\) means that likelihood is only a relative measure of uncertainty”↩︎\nFrequentist approaches can thus be considered a kind of ‘improper Bayesian’ approach by considering \\(k(y)\\) in the Likelihood formula as a stand-in for \\(\\frac{P(\\tilde{\\theta})}{P(y)}\\) in Bayes’ Rule. Roughly speaking, it’s because of the improperness of treating the two terms as equivalent, and the relativeness of \\(k(y)\\), that mean frequentist probability statements can’t be interpreted as Bayesian probability statements. But thinking of the two terms as equivalent can be helpful for spotting the similarity between the two formulae.↩︎\ni.e. the square of the sd passed to rnorm() of 0.5↩︎\n\\(101^2 = 10201\\)↩︎\n\\(101^3 = 1030301\\)↩︎\nThough I had assumed Hessian matrices are called Hessian matrices because they sort-of resemble the criss-crossing grids of Hessian bags, they’re actually named after Otto Hesse, who proposed them.↩︎\nI’ve narrowed the space between values slightly, and increased the range of permutations of values to search through, for an even more precise recovery of the likelihood landscape.↩︎\nIn practice, the algorithm seeks to minimise the value returned by the function, not maximise it, hence the negative being applied through the argument fnscale = -1 in the control argument. But the principle is identical.↩︎\nThis means that, whereas the standard Normal returns a single output, the Multivariate Normal returns a vector of outputs, one for each parameter in \\(\\theta\\), which should also be the length of the diagonal (or alternatively either the number of rows or columns) of \\(\\Sigma\\).↩︎\nThe values will not be identical because the values for \\(\\eta\\), and so \\(\\sigma^2\\), have not been fixed at the true value in this example.↩︎\nWhere \\(\\sigma^2\\) is from \\(\\eta\\) and we defined \\(e^{\\eta} = \\sigma^2\\), a transformation which allowed optim() to search over an unbounded rather than bounded real number line↩︎\nIt can be easier to see this by using the more conventional way of expressing Normal linear regression: \\(Y_i = x_i \\beta + \\epsilon\\), where \\(\\epsilon \\sim Normal(0, \\sigma^2)\\). The expectation is therefore \\(E(Y_i) = E( x_i \\beta + \\epsilon ) = E(x_i \\beta) + E(\\epsilon)\\). For the first part of this equation, \\(E(x_i \\beta) = x_i \\beta\\), because the systematic component is always the same value, no matter how many times a draw is taken from the model. And for the second part, \\(E(\\epsilon) = 0\\), because Normal distributions are symmetrical around their central value over the long term: on average, every large positive value drawn from this distribution will become cancelled out by an equally large negative value, meaning the expected value returned by the distribution is zero. Hence, \\(E(Y) = x_i \\beta\\).↩︎\nBecause these estimates depend on random variation, these intervals may be slightly different to two decimal places than the values I’m quoting here.↩︎\nThanks to this post. My calculus is a bit rusty these days.↩︎\nAn important point to note is that, though bill_size is derived from other variables, it’s its own variable, and so has another distinct ‘slot’ in the vector of \\(\\beta\\) parameters. It’s just another dimension in the search space for optim to search through.↩︎\nThis is fancy-speak for when two terms aren’t independent, or both adding unique information. For example, length in mm, length in cm, and length in inches would all be perfectly collinear, so shouldn’t all be included in the model.↩︎"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#optimisation-algorithms-getting-there-faster",
    "href": "pages/likelihood-and-simulation-theory/index.html#optimisation-algorithms-getting-there-faster",
    "title": "Likelihood and Simulation Theory",
    "section": "Optimisation algorithms: getting there faster",
    "text": "Optimisation algorithms: getting there faster\nPreviously, we ‘cheated’ a bit when using the log likelihood function, fixing the value for one of the parameters \\(\\sigma^2\\) to the value we used when we generated the data, so we could instead look at how the log likelihood surface varied as different combinations of \\(\\beta_0\\) and \\(\\beta_1\\) were plugged into the formula. \\(\\beta_0\\) and \\(\\beta_1\\) values ranging from -5 to 5, and at steps of 0.1, were considered: 101 values of \\(\\beta_0\\), 101 values of \\(\\beta_1\\), and so over 10,0005 unique \\(\\{\\beta_0, \\beta_1\\}\\) combinations were stepped through. This approach is known as grid search, and seldom used in practice (except for illustration purposes) because the number of calculations involved can very easily get out of hand. For example, if we were to use it to explore as many distinct values of \\(\\sigma^2\\) as we considered for \\(\\beta_0\\) and \\(\\beta_1\\), the total number of \\(\\{\\beta_0, \\beta_1, \\sigma^2 \\}\\) combinations we would crawl through would be over 100,000 6 rather than over 10,000.\nOne feature we noticed with the likelihood surface over \\(\\beta_0\\) and \\(\\beta_1\\) in the previous post is that it appears to look like a hill, with a clearly defined highest point (the region of maximum likelihood) and descent in all directions from this highest point. Where likelihood surfaces have this feature of being single-peaked in this way (known as ‘unimodal’), then a class of algorithms known as ‘hill climbing algorithms’ can be applied to find the top of such peaks in a way that tends to be both quicker (fewer steps) and more precise than the grid search approach used for illustration in the previous post."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#code-recap",
    "href": "pages/likelihood-and-simulation-theory/index.html#code-recap",
    "title": "Likelihood and Simulation Theory",
    "section": "Code recap",
    "text": "Code recap\nLet’s copy over the code we used in the previous post for:\n\n\nCalculating log likelihood\n\n\n\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\nAnd\n\n\nGenerating our tame toy dataset of 10 data points\n\n\n\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\nTo recap, the toy dataset looks as follows:\n\nlibrary(tidyverse)\ntibble(x=x, y=y) |&gt;\n    ggplot(aes(x, y)) + \n    geom_point()"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#optim-our-robo-chauffeur",
    "href": "pages/likelihood-and-simulation-theory/index.html#optim-our-robo-chauffeur",
    "title": "Likelihood and Simulation Theory",
    "section": "optim: our Robo-Chauffeur",
    "text": "optim: our Robo-Chauffeur\nNote how the llNormal function takes a single argument, pars, which packages up all the specific candidate parameter values we want to try out. In our previous post, we also had a ‘feeder function’, feed_to_ll, which takes the various \\(\\beta\\) candidate values from the grid and packages them into pars. In our previous post, we had to specify the candidate values to try to feed to llNormal packages inside pars.\nBut we don’t have to do this. We can instead use an algorithm to take candidate parameters, try them out, then make new candidate parameters and try them out, for us. Much as a taxi driver needs to know where to meet a passenger, but doesn’t want the passenger to tell them exactly which route to take, we just need to specify a starting set of values for the parameters to optimise. R’s standard way of doing this is with the optim function. Here’s it in action:\n\noptim_results &lt;-  optim(\n    # par contains our initial guesses for the three parameters to estimate\n    par = c(0, 0, 0), \n\n    # by default, most optim algorithms prefer to search for a minima (lowest point) rather than maxima \n    # (highest point). So, I'm making a function to call which simply inverts the log likelihood by multiplying \n    # what it returns by -1\n    fn = function(par, y, X) {-llNormal(par, y, X)}, \n\n    # in addition to the par vector, our function also needs the observed output (y)\n    # and the observed predictors (X). These have to be specified as additional arguments.\n    y = y, X = X\n    )\n\noptim_results\n\n$par\n[1]  2.460571  1.375421 -1.336209\n\n$value\n[1] -1.51397\n\n$counts\nfunction gradient \n     216       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nThe optim function returns a fairly complex output structure, with the following components:\n\npar: the values for the parameters (in our case \\(\\{\\beta_0, \\beta_1, \\eta \\}\\)) which the optimisation algorithm ended up with.\nvalue: the value returned by the function fn when the optim routine was stopped.\ncounts: the number of times the function fn was repeatedly called by optim before optim decided it had had enough\nconvergence: whether the algorithm used by optim completed successfully (i.e. reached what it considers a good set of parameter estimates in par), or not.\n\nIn this case, convergence is 0, which (perhaps counterintuitively) indicates a successful completion. counts indicates that optim called the log likelihood function 216 times before stopping, and par indicates values of \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\eta = -1.34\\}\\) were arrived at. As \\(\\sigma^2 = e^\\eta\\), this means \\(\\theta = \\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\sigma^2 = 0.26 \\}\\). As a reminder, the ‘true’ values are \\(\\{\\beta_0 = 2.50, \\beta_1 = 1.40, \\sigma^2 = 0.25\\}\\).\nSo, the optim algorithm has arrived at pretty much the correct answers for all three parameters, in 216 calls to the log likelihood function, whereas for the grid search approach in the last post we made over 10,000 calls to the log likelihood function for just two of the three parameters.\nLet’s see if we can get more information on exactly what kind of path optim took to get to this set of parameter estimates. We should be able to do this by specifying a value in the trace component in the control argument slot…\n\nComparisons\nFor comparison let’s see what lm and glm produce.\nFirst lm:\n\ntoy_df &lt;- tibble(\n    x = x, \n    y = y\n)\n\n\nmod_lm &lt;- lm(y ~ x, data = toy_df)\nsummary(mod_lm)\n\n\nCall:\nlm(formula = y ~ x, data = toy_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6082 -0.3852 -0.1668  0.2385  1.1092 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5813 on 7 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9767 \nF-statistic:   336 on 1 and 7 DF,  p-value: 3.564e-07\n\n\n\\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\), i.e. the same to 2 decimal places.\nAnd now with glm:\n\nmod_glm &lt;- glm(y ~ x, data = toy_df, family = gaussian(link = \"identity\"))\n\nsummary(mod_glm)\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = toy_df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nOnce again, \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\)\n\n\nOptim discussion\nIn the above, we’ve successfully used optim, our Robo-Chauffeur, to arrive very quickly at some good estimates for our parameters of interest, \\(\\beta_0\\) and \\(\\beta_1\\), which are in effect identical to those produced by the lm and glm functions.\nThis isn’t a coincidence. What we’ve done the hard way is what the glm function (in particular) largely does ‘under the hood’."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#comparisons",
    "href": "pages/likelihood-and-simulation-theory/index.html#comparisons",
    "title": "Likelihood and Simulation Theory",
    "section": "Comparisons",
    "text": "Comparisons\nFor comparison let’s see what lm and glm produce.\nFirst lm:\n\ntoy_df &lt;- tibble(\n    x = x, \n    y = y\n)\n\n\nmod_lm &lt;- lm(y ~ x, data = toy_df)\nsummary(mod_lm)\n\n\nCall:\nlm(formula = y ~ x, data = toy_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6082 -0.3852 -0.1668  0.2385  1.1092 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5813 on 7 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9767 \nF-statistic:   336 on 1 and 7 DF,  p-value: 3.564e-07\n\n\n\\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\), i.e. the same to 2 decimal places.\nAnd now with glm:\n\nmod_glm &lt;- glm(y ~ x, data = toy_df, family = gaussian(link = \"identity\"))\n\nsummary(mod_glm)\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = toy_df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nOnce again, \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\)\n\nOptim discussion\nIn the above, we’ve successfully used optim, our Robo-Chauffeur, to arrive very quickly at some good estimates for our parameters of interest, \\(\\beta_0\\) and \\(\\beta_1\\), which are in effect identical to those produced by the lm and glm functions.\nThis isn’t a coincidence. What we’ve done the hard way is what the glm function (in particular) largely does ‘under the hood’."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#discussion",
    "href": "pages/likelihood-and-simulation-theory/index.html#discussion",
    "title": "Likelihood and Simulation Theory",
    "section": "Discussion",
    "text": "Discussion\nIn the above, we’ve successfully used optim, our Robo-Chauffeur, to arrive very quickly at some good estimates for our parameters of interest, \\(\\beta_0\\) and \\(\\beta_1\\), which are in effect identical to those produced by the lm and glm functions.\nThis isn’t a coincidence. What we’ve done the hard way is what the glm function (in particular) largely does ‘under the hood’."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#barefoot-and-blind-a-weird-analogy-for-a-complicated-idea",
    "href": "pages/likelihood-and-simulation-theory/index.html#barefoot-and-blind-a-weird-analogy-for-a-complicated-idea",
    "title": "Likelihood and Simulation Theory",
    "section": "Barefoot and Blind: A weird analogy for a complicated idea",
    "text": "Barefoot and Blind: A weird analogy for a complicated idea\nImagine optim, your hill-finding robo-chauffeur, has taken you to the top of a likelihood surface. Then it leaves you there…\n… and you’re blind, and have no shoes. (You also have an uncanny sense of your orientation, whether north-south, east-west, or some other angle.)\nSo, you know you’re at the top of the hill, but you can’t see what the landscape around you looks like. However, you still want to get a sense of this landscape, and how it varies around the spot you’re standing on.\nWhat do you do?\nIf you’re playing along with this weird thought experiment, one approach would be to use your feet as depth sensors. You make sure you never stray from where you started, and to always keep one foot planted on this initial spot (which you understand to be the highest point on the landscape). Then you use your other foot to work out how much further down the surface is from the highest point as you venture away from the highest point in different directions.\nSay you keep your left foot planted on the highest point, and make sure your right foot is always positioned (say) 10 cm horizontally from your left foot. Initially your two feet are arranged east-west; let’s call this 0 degrees. When you put your right foot down, you notice it needs to travel 2 cm further down to reach terra ferma relative to your left foot.\n2cm at 0 degrees. You’ll remember that.\nNow you rotate yourself 45 degrees, and repeat the same right foot drop. This time it needs to travel 3cm down relative to your left foot.\n3cm at 45 degrees. You remember that too.\nNow you rotate another 45 degrees, north-south orientation, place your right foot down; now it falls 5cm down relative to your left foot.\n2cm at 0 degrees; 3cm at 45 degrees; 5cm at 90 degrees.\nNow with this information, you try to construct the landscape you’re on top of with your mind’s eye, making the assumption that the way it has to have curved from the peak you’re on to lead to the drops you’ve observed is consistent all around you; i.e. that there’s only one hill, you’re on top of it, and it’s smoothly curved in all directions.\nIf you could further entertain the idea that your feet are infinitely small, and the gap between feet is also infinitely small (rather than the 10cm above), then you have the intuition behind this scary-looking but very important formula from (King98?) (p. 89):\n\\[\n\\widehat{V(\\hat{\\theta})} = - \\frac{1}{n}[\\frac{\\delta^2lnL(\\tilde{\\theta}|y)}{\\delta \\tilde{\\theta} \\delta \\tilde{\\theta}^{'}}]^{-1}_{\\tilde{\\theta} = \\hat{\\theta}}\n\\]\nWhat this is saying, in something closer to humanese, is something like:\n\nOur best estimate of the amount of uncertainty we have in our estimates is a function of how much the likelihood surface curves at the highest point on the surface. (It also gets less uncertain, the more observations we have)."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#information-and-uncertainty",
    "href": "pages/likelihood-and-simulation-theory/index.html#information-and-uncertainty",
    "title": "Likelihood and Simulation Theory",
    "section": "Information and uncertainty",
    "text": "Information and uncertainty\nAmongst the various bells, whistles and decals in the previous formula is the superscript \\((.)^{-1}\\). This means invert, which for a single value means \\(\\frac{1}{.}\\) but for a matrix means something conceptually the same but technically not.\nAnd what’s being inverted in the last formula? A horrible-looking expression, \\([\\frac{\\delta^2lnL(\\tilde{\\theta}|y)}{\\delta \\tilde{\\theta} \\delta \\tilde{\\theta}^{'}}]_{\\tilde{\\theta} = \\hat{\\theta}}\\), that’s basically an answer to the question of how curvy is the log likelihood surface at its peak position?\nWithin (King98?) (p.89, eq. 4.18), this expression (or rather the negative of the term) is defined as \\(I(\\hat{\\theta} | y)\\), where \\(I(.)\\) stands for information.\nSo, the algebra are saying\n\nUncertainty is inversely related to information\n\nOr perhaps even more intuitively\n\nThe more information we have, the less uncertain we are\n\nOf course this makes sense. If you ask someone “How long will this task take?”, and they say “Between one hour and one month”, they likely have less information about how long the task will actually than if they had said “Between two and a half and three hours”. More generally:\n\nShallow gradients mean wide uncertainty intervals mean low information\nSharp gradients mean narrow uncertaintly intervals mean high information\n\nThis is, fundamentally, what the blind and barefoot person in the previous analogy is trying to achieve: by feeling out the local curvature around the highest point, they are trying to work out how much information they have about different pieces of the model. The curvature along any one dimension of the surface (equivalent to the 0 and 90 degree explorations) indicates how much information there is about any single coefficient, and the curvature along the equivalent of a 45 degree plane gives a measure of how associated any two coefficients tend to be.\nWith these many analogies and equations spinning in our heads, let’s now see how these concepts can be applied in practice."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#optimal-uncertainty",
    "href": "pages/likelihood-and-simulation-theory/index.html#optimal-uncertainty",
    "title": "Likelihood and Simulation Theory",
    "section": "optimal uncertainty",
    "text": "optimal uncertainty\nWhen using optim() above, we managed to get it to return a set of parameter values for our model that it thought was ‘best’, i.e. minimised the loss function specified by the log likelihood. These are known as point estimates, and are effectively the coefficients presented by lm or glm or equivalent statistical functions and packages. However optim() just returned these point estimates, without any indication of how uncertain we should be about these point estimates. A standard statistical model summary will tend to also report measures of uncertainty around the point estimates, in the form of standard errors. When these are implicitly combined with a Null hypothesis, namely that the ‘true’ value of a parameter may be zero, the point estimate together with its standard error allows the calculation of z values and p values.\nHow can we use optim() to return measures of uncertainty, which will allow the standard errors to be estimated as well as the point values?\nWe’ll start with a weird analogy to get an intuition for how this can be done with optim().\n\nBarefoot and Blind: A weird analogy for a complicated idea\nImagine optim, your hill-finding robo-chauffeur, has taken you to the top of a likelihood surface. Then it leaves you there…\n… and you’re blind, and have no shoes. (You also have an uncanny sense of your orientation, whether north-south, east-west, or some other angle.)\nSo, you know you’re at the top of the hill, but you can’t see what the landscape around you looks like. However, you still want to get a sense of this landscape, and how it varies around the spot you’re standing on.\nWhat do you do?\nIf you’re playing along with this weird thought experiment, one approach would be to use your feet as depth sensors. You make sure you never stray from where you started, and to always keep one foot planted on this initial spot (which you understand to be the highest point on the landscape). Then you use your other foot to work out how much further down the surface is from the highest point as you venture away from the highest point in different directions.\nSay you keep your left foot planted on the highest point, and make sure your right foot is always positioned (say) 10 cm horizontally from your left foot. Initially your two feet are arranged east-west; let’s call this 0 degrees. When you put your right foot down, you notice it needs to travel 2 cm further down to reach terra ferma relative to your left foot.\n2cm at 0 degrees. You’ll remember that.\nNow you rotate yourself 45 degrees, and repeat the same right foot drop. This time it needs to travel 3cm down relative to your left foot.\n3cm at 45 degrees. You remember that too.\nNow you rotate another 45 degrees, north-south orientation, place your right foot down; now it falls 5cm down relative to your left foot.\n2cm at 0 degrees; 3cm at 45 degrees; 5cm at 90 degrees.\nNow with this information, you try to construct the landscape you’re on top of with your mind’s eye, making the assumption that the way it has to have curved from the peak you’re on to lead to the drops you’ve observed is consistent all around you; i.e. that there’s only one hill, you’re on top of it, and it’s smoothly curved in all directions.\n\n\nInformation and uncertainty\nIf you could further entertain the idea that your feet are infinitely small, and the gap between feet is also infinitely small (rather than the 10cm above), then you have the intuition behind this scary-looking but very important formula from King (1998) (p. 89):\n\\[\n\\widehat{V(\\hat{\\theta})} = - \\frac{1}{n}[\\frac{\\delta^2lnL(\\tilde{\\theta}|y)}{\\delta \\tilde{\\theta} \\delta \\tilde{\\theta}^{'}}]^{-1}_{\\tilde{\\theta} = \\hat{\\theta}}\n\\]\nWhat this is saying, in something closer to humanese, is something like:\n\nOur best estimate of the amount of uncertainty we have in our estimates is a function of how much the likelihood surface curves at the highest point on the surface. (It also gets less uncertain, the more observations we have).\n\nAmongst the various bells, whistles and decals in the previous formula is the superscript \\((.)^{-1}\\). This means invert, which for a single value means \\(\\frac{1}{.}\\) but for a matrix means something conceptually the same but technically not.\nAnd what’s being inverted in the last formula? A horrible-looking expression, \\([\\frac{\\delta^2lnL(\\tilde{\\theta}|y)}{\\delta \\tilde{\\theta} \\delta \\tilde{\\theta}^{'}}]_{\\tilde{\\theta} = \\hat{\\theta}}\\), that’s basically an answer to the question of how curvy is the log likelihood surface at its peak position?\nWithin King (1998) (p.89, eq. 4.18), this expression (or rather the negative of the term) is defined as \\(I(\\hat{\\theta} | y)\\), where \\(I(.)\\) stands for information.\nSo, the algebra are saying\n\nUncertainty is inversely related to information\n\nOr perhaps even more intuitively\n\nThe more information we have, the less uncertain we are\n\nOf course this makes sense. If you ask someone “How long will this task take?”, and they say “Between one hour and one month”, they likely have less information about how long the task will actually than if they had said “Between two and a half and three hours”. More generally:\n\nShallow gradients mean wide uncertainty intervals mean low information\nSharp gradients mean narrow uncertaintly intervals mean high information\n\nThis is, fundamentally, what the blind and barefoot person in the previous analogy is trying to achieve: by feeling out the local curvature around the highest point, they are trying to work out how much information they have about different pieces of the model. The curvature along any one dimension of the surface (equivalent to the 0 and 90 degree explorations) indicates how much information there is about any single coefficient, and the curvature along the equivalent of a 45 degree plane gives a measure of how associated any two coefficients tend to be.\nWith these many analogies and equations spinning in our heads, let’s now see how these concepts can be applied in practice.\n\n\nHow to get optim() to return this information\nHaving reminded myself of the particular options for optim that are typically used to report parameter uncertainty, let’s run the follows:\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nWe have used a slightly different algorithm (‘BFGS’), and a different way of specifying the function to search over (using fnscale = -1 to invert the likelihood), but we have the same par estimates as before: \\(\\beta = \\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\). So the changes we’ve made to the optim arguments haven’t changed what it estimates.\nOne new argument we’ve set in optim is hessian = TRUE. Hessian is a kind of coarse fabric made from vegetable waste, typically woven in a criss-crossing, grid-like pattern. Hessian matrices are matrices of second derivatives, as described in the wikipedia article. 7 If you can bear to recall the really complex expression above, for calculating the curvature around a point on a surface, you’ll recall it’s also about second derivatives.\nNone of this is a coincidence. The hessian component of the optim output above contains what we need.\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\nhess\n\n\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nYou might notice that the Hessian matrix is square, with as many columns as rows. And, that the number of columns (or rows) is equal to the number of parameters we have estimated, i.e. three in this case.\nYou might also notice that the values are symmetrical about the diagonal running from the top left to the bottom right.\nAgain, this is no accident.\nRemember that variation is inversely related to information, and that \\((.)^{-1}\\) is the inversion operator on \\(I(.)\\), the Information Matrix. Well, this Hessian is (pretty much) \\(I(.)\\). So let’s see what happens when we invert it (using the solve operator):\n\n\nCode\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\nAs with hess, inv_hess is symmetric around the top-left to bottom-right diagonal. For example, the value on row 2 and column 1 is the same as on row 1, column 2.\nWe’re mainly interested in the first two columns and rows, as these contain the values most comparable with the glm summary reports\n\n\nCode\ninv_hess_betas &lt;- inv_hess[1:2, 1:2]\n\ninv_hess_betas\n\n\n             [,1]         [,2]\n[1,]  0.033577455 -0.004379668\n[2,] -0.004379668  0.004379668\n\n\nWhat the elements of the above matrix provide are estimates of the variances of a single parameter \\(\\beta_j\\), and/or the covariances between any two parameters \\(\\{\\beta_0, \\beta_1\\}\\). In this example:\n\\[\n\\begin{bmatrix}\nvar(\\beta_0) & cov(\\beta_0, \\beta_1) \\\\\ncov(\\beta_1, \\beta_0) & var(\\beta_1)\n\\end{bmatrix}\n\\]\nIt’s because the on-diagonal terms are variances of uncertaintly for a single term, that it can be useful to take the square root of these terms to get estimates of the standard errors:\n\n\nCode\nsqrt(diag(inv_hess_betas))\n\n\n[1] 0.18324152 0.06617906\n\n\nCompare with the Std Err term in the following:\n\n\nCode\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = toy_df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nThe estimates from the Hessian in optim, of \\(\\{0.18, 0.07\\}\\), are not exactly the same as the \\(\\{0.21, 0.08\\}\\) reported for mod_glm; the methods employed are not identical. But they are hopefully similar enough to demonstrate they provide similar information about similar quantities of uncertainty.\nBack in part five, we used this same dataset to show how the log likelihood varies for various, equally spaced, candidate values for \\(\\beta_0\\) and \\(\\beta_1\\) (having fixed \\(\\eta = \\exp({\\sigma^2})\\) at its true value). This led to the followng map of the landscape8\n\n\nCode\nlibrary(tidyverse)\ncandidate_param_values &lt;- expand_grid(\n    beta_0 = seq(-15, 15, by = 0.05),\n    beta_1 = seq(-15, 15, by = 0.05)\n)\n\nfeed_to_ll &lt;- function(b0, b1){\n    pars &lt;- c(b0, b1, log(0.25))\n    llNormal(pars, y, X)\n}\n\ncandidate_param_values &lt;- candidate_param_values |&gt;\n    mutate(\n        ll = map2_dbl(beta_0, beta_1, feed_to_ll)\n    )\n\ncandidate_param_values |&gt;\n    ggplot(aes(beta_0, beta_1, z = ll)) + \n    geom_contour_filled() + \n    geom_vline(xintercept = 0) +\n    geom_hline(yintercept = 0) +\n    labs(\n        title = \"Log likelihood as a function of possible values of beta_0 and beta_1\",\n        x = \"beta0 (the intercept)\",\n        y = \"beta1 (the slope)\"\n    )\n\n\n\n\n\nWithin the above we can see that the log likelihood landscape for these two parameters looks like a bivariate normal distribution, we can also see a bit of a slant in this normal distribution. This implies a correlation between the two candidate values. The direction of the slant is downwards from left to right, implying the correlation is negative.\nFirstly let’s check that the correlation between \\(\\beta_0\\) and \\(\\beta_1\\) implied by the Hessian is negative. These are the off-diagonal elements, either first row, second column, or second row, first column:\n\n\nCode\ninv_hess_betas[1,2]\n\n\n[1] -0.004379668\n\n\nCode\ninv_hess_betas[2,1]\n\n\n[1] -0.004379668\n\n\nYes they are!\nAs mentioned previously, the likelihood surface produced by the gridsearch method involves a lot of computations, so a lot of steps, and likely a lot of trial and error, if it were to be used to try to find the maximum likelihood value for the parameters. By contrast, the optim() algorithm typically involves far fewer steps, ‘feeling’ its way up the hill until it reaches a point where there’s nowhere higher. 9 When it then reaches this highest point, it then ‘feels’ the curvature around this point in multiple directions, producing the Hessian. The algorithm doesn’t see the likelihood surface, because it hasn’t travelled along most of it. But the Hessian can be used to infer the likelihood surface, subject to subject (usually) reasonable assumptions.\nWhat are these (usually) reasonable assumptions? Well, that the likelihood surface can be approximated by a multivariate normal distribution, which is a generalisation of the standard Normal distribution over more than one dimensions.10\nWe can use the mvrnorm function from the MASS package, alongside the point estimates and Hessian from optim, in order to produce estimates of \\(\\theta = \\{ \\beta_0, \\beta_1, \\eta \\}\\) which represent reasonable uncertainty about the true values of each of these parameters. Algebraically, this can be expressed as something like the following:\n\\[\n\\tilde{\\theta} \\sim Multivariate Normal(\\mu = \\dot{\\theta}, \\sigma^2 = \\Sigma)\n\\]\nWhere \\(\\dot{\\theta}\\) are the point estimates from optim() and \\(\\Sigma\\) is the implied variance-covariance matrix recovered from the Hessian.\nLet’s create this MVN model and see what kinds of outputs it produces.\n\n\nCode\nlibrary(MASS)\n\npoint_estimates &lt;- fuller_optim_output$par\n\nvcov &lt;- -solve(fuller_optim_output$hessian)\nparam_draws &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = point_estimates, \n    Sigma = vcov\n)\n\ncolnames(param_draws) &lt;- c(\n    \"beta0\", \"beta1\", \"eta\"\n)\n\nhead(param_draws)\n\n\n        beta0    beta1         eta\n[1,] 2.564978 1.375636 -0.30407255\n[2,] 2.440111 1.367774 -1.16815288\n[3,] 2.775332 1.338583 -0.05574937\n[4,] 2.283011 1.481799 -0.26095101\n[5,] 2.695635 1.228565 -1.18369341\n[6,] 2.686818 1.483601 -0.44262363\n\n\nWe can see that mvrnorm(), with these inputs from optim() produces three columns: one for each parameter being estimated \\(\\{ \\beta_0, \\beta_1, \\eta \\}\\). The n argumment indicates the number of draws to take; in this case, 10000. This number of draws makes it easier to see how much variation there is in each of the estimates.\n\n\nCode\ndf_param_draws &lt;- \nparam_draws |&gt;\n    as_tibble(\n        rownames = 'draw'\n    ) |&gt;\n    mutate(\n        sig2 = exp(eta)\n    ) |&gt;\n    pivot_longer(\n        -draw, \n        names_to = \"param\",\n        values_to = \"value\"\n    ) \n    \ndf_param_draws |&gt;\n    ggplot(aes(x = value)) + \n    geom_density() + \n    facet_grid(param ~ .) + \n    geom_vline(xintercept=0)\n\n\n\n\n\nThere are a number of things to note here: firstly, that the average of the \\(\\beta_0\\) and \\(\\beta_1\\) values appear close to their known ‘true’ values of 2.5 and 1.4 respectively. Secondly, that whereas the \\(\\eta\\) values are normally distributed, the \\(\\sigma^2\\) values derived from them are not, and are never below zero; this is the effect of the exponential link between quantities. Thirdly, that the implied values of \\(\\sigma^2\\) do appear to be centred around 0.25, as they should be as \\(\\sigma\\) was set to 0.50 in the model.\nAnd forthly, that the density around \\(\\beta_1\\) is more peaked than around \\(\\beta_0\\). This concords with what we saw previously in the filled contour map: both the horizontal beta0 axis and vertical beta1 axis are on the same scale, but the oval is broader along the horizontal axis than the vertical axis. This in effect implies that we have more information about the true value of \\(\\beta_1\\), the slope, than about the true value of \\(\\beta_0\\), the intercept.\nWe can also use these draws to reproduce something similar to, but not identical to, 11 the previous filled contour map:\n\n\nCode\n# param_draws |&gt;\n#     as_tibble(\n#         rownames = 'draw'\n#     ) |&gt;\n#     ggplot(aes(x = beta0, y = beta1)) + \n#     geom_point(alpha = 0.1) + \n#     coord_cartesian(xlim = c(-10, 10), ylim = c(-10, 10))\n\nparam_draws |&gt;\n    as_tibble(\n        rownames = 'draw'\n    ) |&gt;\n    ggplot(aes(x = beta0, y = beta1)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nOnce again, we see the same qualities as the contour map produced by interrogating the likelihood surface exhaustively: the distribution appears bivariate normal; there is a greater range in the distribution along the beta0 than the beta1 axis; and there is evidence of some negative correlation between the two parameters.\n\n\nSummary\nThis post has shown how optim(), which in its vanilla state only returns point estimates, can be configured to also calculater and report the Hessian, a record of instantaneous curvature around the point estimates. Even without a fine-grained and exhausive search throughout the likelihood surface, this measure of curvature can be used to produce similar measures of uncertainty to the more exhausive approach, in a fraction of the number of computations.\nMore importantly, it can be used to generate draws of plausible combinations of parameter values, something denoted as \\(\\tilde{\\theta}\\) earlier. This is something especially useful for producing honest quantities of interest, which both tell users of models something they want to know, while also representing how uncertain we are in this knowledge."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#optim-for-parameter-point-estimation-our-robo-chauffeur",
    "href": "pages/likelihood-and-simulation-theory/index.html#optim-for-parameter-point-estimation-our-robo-chauffeur",
    "title": "Likelihood and Simulation Theory",
    "section": "optim for parameter point estimation: our Robo-Chauffeur",
    "text": "optim for parameter point estimation: our Robo-Chauffeur\nNote how the llNormal function takes a single argument, pars, which packages up all the specific candidate parameter values we want to try out. In our previous post, we also had a ‘feeder function’, feed_to_ll, which takes the various \\(\\beta\\) candidate values from the grid and packages them into pars. In our previous post, we had to specify the candidate values to try to feed to llNormal packages inside pars.\nBut we don’t have to do this. We can instead use an algorithm to take candidate parameters, try them out, then make new candidate parameters and try them out, for us. Much as a taxi driver needs to know where to meet a passenger, but doesn’t want the passenger to tell them exactly which route to take, we just need to specify a starting set of values for the parameters to optimise. R’s standard way of doing this is with the optim function. Here’s it in action:\n\n\nCode\noptim_results &lt;-  optim(\n    # par contains our initial guesses for the three parameters to estimate\n    par = c(0, 0, 0), \n\n    # by default, most optim algorithms prefer to search for a minima (lowest point) rather than maxima \n    # (highest point). So, I'm making a function to call which simply inverts the log likelihood by multiplying \n    # what it returns by -1\n    fn = function(par, y, X) {-llNormal(par, y, X)}, \n\n    # in addition to the par vector, our function also needs the observed output (y)\n    # and the observed predictors (X). These have to be specified as additional arguments.\n    y = y, X = X\n    )\n\noptim_results\n\n\n$par\n[1]  2.460571  1.375421 -1.336209\n\n$value\n[1] -1.51397\n\n$counts\nfunction gradient \n     216       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nThe optim function returns a fairly complex output structure, with the following components:\n\npar: the values for the parameters (in our case \\(\\{\\beta_0, \\beta_1, \\eta \\}\\)) which the optimisation algorithm ended up with.\nvalue: the value returned by the function fn when the optim routine was stopped.\ncounts: the number of times the function fn was repeatedly called by optim before optim decided it had had enough\nconvergence: whether the algorithm used by optim completed successfully (i.e. reached what it considers a good set of parameter estimates in par), or not.\n\nIn this case, convergence is 0, which (perhaps counterintuitively) indicates a successful completion. counts indicates that optim called the log likelihood function 216 times before stopping, and par indicates values of \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\eta = -1.34\\}\\) were arrived at. As \\(\\sigma^2 = e^\\eta\\), this means \\(\\theta = \\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\sigma^2 = 0.26 \\}\\). As a reminder, the ‘true’ values are \\(\\{\\beta_0 = 2.50, \\beta_1 = 1.40, \\sigma^2 = 0.25\\}\\).\nSo, the optim algorithm has arrived at pretty much the correct answers for all three parameters, in 216 calls to the log likelihood function, whereas for the grid search approach in the last post we made over 10,000 calls to the log likelihood function for just two of the three parameters.\nLet’s see if we can get more information on exactly what kind of path optim took to get to this set of parameter estimates. We should be able to do this by specifying a value in the trace component in the control argument slot…\n\nComparisons with ‘canned’ functions\nFor comparison let’s see what lm and glm produce.\nFirst lm:\n\n\nCode\ntoy_df &lt;- tibble(\n    x = x, \n    y = y\n)\n\n\nmod_lm &lt;- lm(y ~ x, data = toy_df)\nsummary(mod_lm)\n\n\n\nCall:\nlm(formula = y ~ x, data = toy_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6082 -0.3852 -0.1668  0.2385  1.1092 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5813 on 7 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9767 \nF-statistic:   336 on 1 and 7 DF,  p-value: 3.564e-07\n\n\n\\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\), i.e. the same to 2 decimal places.\nAnd now with glm:\n\n\nCode\nmod_glm &lt;- glm(y ~ x, data = toy_df, family = gaussian(link = \"identity\"))\n\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = toy_df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nOnce again, \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\)\n\n\nDiscussion\nIn the above, we’ve successfully used optim, our Robo-Chauffeur, to arrive very quickly at some good estimates for our parameters of interest, \\(\\beta_0\\) and \\(\\beta_1\\), which are in effect identical to those produced by the lm and glm functions.\nThis isn’t a coincidence. What we’ve done the hard way is what the glm function (in particular) largely does ‘under the hood’."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#method",
    "href": "pages/likelihood-and-simulation-theory/index.html#method",
    "title": "Likelihood and Simulation Theory",
    "section": "Method",
    "text": "Method\nLet’s make use of our toy dataset one last time, and go through the motions to produce the \\(\\tilde{\\theta}\\) draws we ended with on the last post:\n\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\n\npoint_estimates &lt;- fuller_optim_output$par\n\nvcov &lt;- -solve(fuller_optim_output$hessian)\nparam_draws &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = point_estimates, \n    Sigma = vcov\n)\n\ncolnames(param_draws) &lt;- c(\n    \"beta0\", \"beta1\", \"eta\"\n)\n\nLet’s now look at our toy data again, and decide on some specific questions to answer:\n\nlibrary(tidyverse)\ntoy_df &lt;- tibble(x = x, y = y)\n\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() \n\n\n\n\nWithin the data itself, we have only supplied x and y values for whole numbers of x between -3 and 5. But we can use the model to produce estimates for non-integer values of x. Let’s try 2.5. For this single value of x, we can produce both predicted values and expected values, by passing the same value of x to each of the plausible estimates of \\(\\theta\\) returned by the multivariate normal function above.\n\ncandidate_x &lt;- 2.5"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#expected-values",
    "href": "pages/likelihood-and-simulation-theory/index.html#expected-values",
    "title": "Likelihood and Simulation Theory",
    "section": "Expected values",
    "text": "Expected values\nHere’s an example of estimating the expected value of y for x = 2.5 using loops and standard algebra:\n\n# Using standard algebra and loops\nN &lt;- nrow(param_draws)\nexpected_y_simpler &lt;- vector(\"numeric\", N)\nfor (i in 1:N){\n    expected_y_simpler[i] &lt;- param_draws[i, \"beta0\"] + candidate_x * param_draws[i, \"beta1\"]\n}\n\nhead(expected_y_simpler)\n\n[1] 6.004068 5.859547 6.121791 5.987509 5.767047 6.395820\n\n\nWe can see just from the first few values that each estimate is slightly different. Let’s order the values from lowest to highest, and find the range where 95% of values sit:\n\nev_range &lt;- quantile(expected_y_simpler,  probs = c(0.025, 0.500, 0.975)) \n\nev_range\n\n    2.5%      50%    97.5% \n5.505104 5.898148 6.291150 \n\n\nThe 95% interval is therefore between 5.51 and 6.29, with the median (similar but not quite the point estimate) being 5.90. Let’s plot this against the data:\n\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() + \n    annotate(\"point\", x = candidate_x, y =  median(expected_y_simpler), size = 1.2, shape = 2, colour = \"blue\") + \n    annotate(\"segment\", x = candidate_x, xend=candidate_x, y = ev_range[1], yend = ev_range[3], colour = \"blue\")\n\n\n\n\nThe vertical blue line therefore shows the range of estimates for \\(Y|x=2.5\\) that contain 95% of the expected values given the draws of \\(\\beta = \\{\\beta_0, \\beta_1\\}\\) which we produced from the Multivariate Normal given the point estimates and Hessian from optim(). This is our estimated range for the expected value, not predicted value. What’s the difference?"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#predicted-values",
    "href": "pages/likelihood-and-simulation-theory/index.html#predicted-values",
    "title": "Likelihood and Simulation Theory",
    "section": "Predicted values",
    "text": "Predicted values\nOne clue about the difference between expected value lies in the parameters from optim() we did and did not use: Whereas we have both point estimates and uncertainty estimates for the parameters \\(\\{\\beta_0, \\beta_1, \\sigma^2\\}\\),12 we only made use of the the two \\(\\beta\\) parameters when producing this estimate.\nNow let’s recall the general model formula, from the start of King, Tomz, and Wittenberg (2000), which we repeated for the first few posts in the series:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nThe manual for Zelig, the (now defunct) R package that used to support analysis using this approach, states that for Normal Linear Regression these two components are resolved as follows:\nStochastic Component\n\\[\nY_i \\sim Normal(\\mu_i, \\sigma^2)\n\\]\nSystematic Component\n\\[\n\\mu_i = x_i \\beta\n\\]\nThe page then goes onto state that the expected value, \\(E(Y)\\), is :\n\\[\nE(Y) = \\mu_i = x_i \\beta\n\\]\nSo, in this case, the expected value is the systematic component only, and does not involve the dispersion parameter in the stochastic component, which for normal linear regression is the \\(\\sigma^2\\) term. That’s why we didn’t use estimates of \\(\\sigma^2\\) when simulating the expected values.\nBut why is this? Well, it comes from the expectation operator, \\(E(.)\\). This operator means something like, return to me the value that would be expected if this experiment were performed an infinite number of times.\nThere are two types of uncertainty which give rise to variation in the predicted estimate: sampling uncertainty, and stochastic variation. In the expected value condition, this second source of variation falls to zero,13 leaving only the influence of sampling uncertainty, as in uncertainty about the true value of the \\(\\beta\\) parameters, remaining on uncertainty on the predicted outputs.\nFor predicted values, we therefore need to reintroduce stochastic variation as a source of variation in the range of estimates produced. Each \\(\\eta\\) value we have implies a different \\(\\sigma^2\\) value in the stochastic part of the equation, which we can then add onto the variation caused by parameter uncertainty alone:\n\nN &lt;- nrow(param_draws)\npredicted_y_simpler &lt;- vector(\"numeric\", N)\nfor (i in 1:N){\n    predicted_y_simpler[i] &lt;- param_draws[i, \"beta0\"] + candidate_x * param_draws[i, \"beta1\"] + \n        rnorm(\n            1, mean = 0, \n            sd = sqrt(exp(param_draws[i, \"eta\"]))\n        )\n}\n\nhead(predicted_y_simpler)\n\n[1] 4.802092 6.706397 7.073450 6.118750 6.757717 7.461254\n\n\nLet’s now get the 95% prediction interval for the predicted values, and compare them with the expected values predicted interval earlier\n\npv_range &lt;- \n    quantile(\n        predicted_y_simpler, \n        probs = c(0.025, 0.500, 0.975)\n    )\n\npv_range\n\n    2.5%      50%    97.5% \n4.766300 5.895763 7.055408 \n\n\nSo, whereas the median is similar to before, 5.90, the 95% interval is now from 4.77 to 7.0614. This compares with the 5.51 to 6.29 range for the expected values. Let’s now plot this predicted value range just as we did with the expected values:\n\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() + \n    annotate(\"point\", x = candidate_x, y =  pv_range[2], size = 1.2, shape = 2, colour = \"blue\") + \n    annotate(\"segment\", x = candidate_x, xend=candidate_x, y = pv_range[1], yend = pv_range[3], colour = \"red\")\n\n\n\n\nClearly considerably wider."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#summary-1",
    "href": "pages/likelihood-and-simulation-theory/index.html#summary-1",
    "title": "Likelihood and Simulation Theory",
    "section": "Summary",
    "text": "Summary\nIn the exercise above we did for logistic regression what the previous few posts in section two did for standard regression: i.e. we derived the log likelihood, applied it using optim, and compared with results from the glm() package. We saw in this case that fitting models isn’t always straightforward. We were - well, I was - overly ambitious in building and applying an overly parameterised model specification. But we eventually got to similar parameter values using both approaches.\nThough this wasn’t as straightforward as I was hoping for, I’m presenting it warts-and-all. In principle, the log-likelihood maximisation approach generalises to a great many model specifications, even if in practice some model structures aren’t as straightforward to fit as others."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#recap",
    "href": "pages/likelihood-and-simulation-theory/index.html#recap",
    "title": "Likelihood and Simulation Theory",
    "section": "Recap",
    "text": "Recap\nWithin this series, parts 1-4 formed what we might call ‘section one’, and part 5-9 ‘section two’.\nSection one (re)introduced statistical models as siblings, children of a mother model which combines a systematic component (an equation with a \\(=\\) symbol in it) and a stochastic component (an equation with a \\(\\sim\\) in it, which can largely be read as ‘drawn from’). Part one provided a graphical representation of the challenge of model fitting from an algorithmic perspective, in which the parameters that go into the two component are tweaked and tweaked until some condition is met: usually that the discrepency between model predictions and observed outcomes are minimised some way. The two component mother model is largely equivalent to the concept of the generalised linear model: parts two and three explored this association a bit more. Part four demonstrated how, for statistical models other than standard linear regression, the kinds of answer one usually wants from a model are not readily apparent from the model coefficients themselves, and so careful use of model predictions, and calibration of the questions, are required to use models to answer substantivelly meaningful questions.\nSection two aimed to show how likelihood theory is used in practice in order to justify a loss function that algorithms can be used to try to ‘solve’.15 These loss functions and optimisation algorithms are usually called implicitly by statistical model functions, but we did things the hard way by building the loss function from scratch, and evoking the algorithms more directly, using R’s optim() function. As well as the pedagogical value (and bragging rights) of being able to create and fit statistical models directly, an additional benefit of using optim() (with some of its algorithms) is that it returns something called the Hessian. The Hessian is what allows us to be honest when making model predictions and projections, showing how our uncertainty about the true value of the model parameters (the multiple inputs that optim() algorithms try to tweak until they’re good enough) leads to uncertainty in what we’re predicting and projecting.\nUnfortunately, we’re still in section two. The material below aims to repeat the same kind of exercise performed for standard linear regression, but using logistic regression instead."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#log-likelihood-for-logistic-regression",
    "href": "pages/likelihood-and-simulation-theory/index.html#log-likelihood-for-logistic-regression",
    "title": "Likelihood and Simulation Theory",
    "section": "Log likelihood for logistic regression",
    "text": "Log likelihood for logistic regression\nPreviously we derived the log likelihood for Normal (Gaussian) regression and did some cool things with it. Let’s now do the same with logistic regression. We need to start with definition, then calculate log likelihood, then write it as a function in R that optim() can work its magic with.\nAccording to the relevant section of the Zelig website:\nStochastic component \\[\nY_i \\sim Bernoulli(y_i | \\pi_i )\n\\]\n\\[\nY_i = \\pi_i^{y_i}(1 - \\pi_i)^{1-y_i}\n\\]\nwhere \\(\\pi_i = P(Y_i = 1)\\)\nAnd\nSystematic Component\n\\[\n\\pi_i = \\frac{1}{1 + \\exp{(-x_i \\beta)}}\n\\]\nThe likelihood is the product of the above for all observations in the dataset \\(i \\in N\\)\n\\[\nL(.) = \\prod{\\pi_i^{y_i}(1 - \\pi_i)^{1-y_i}}\n\\]\nThe effect of logging the above15:\n\\[\n\\log{L(.)} = \\sum{[y_i \\log{\\pi_i} + (1-y_i)\\log{(1-y_i)}]}\n\\]\nThis can now be implemented as a function:\n\n\nCode\nllogit &lt;- function(par, y, X){\n    xform &lt;- function(z) {1 / (1 + exp(-z))}\n    p &lt;- xform(X%*%par)\n    sum(y * log(p) + (1-y) * log(1 - p))\n}\n\n\nLet’s pick an appropriate dataset. How about… picking a Palmer Penguin!?\n\n\nCode\nlibrary(tidyverse)\npalmerpenguins::penguins\n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nLet’s say we want to predict whether a penguin is of the Chinstrap species\n\n\nCode\npalmerpenguins::penguins %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(is_chinstrap = species == \"Chinstrap\") |&gt;\n    ggplot(aes(x = bill_length_mm, y = bill_depth_mm, colour = is_chinstrap, shape = sex)) + \n    geom_point()\n\n\n\n\n\nNeither bill length nor bill depth alone appears to distinguish between chinstrap and other species. But perhaps the interaction (product) of the two terms would do:\n\n\nCode\npalmerpenguins::penguins %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(is_chinstrap = species == \"Chinstrap\") |&gt;\n    mutate(bill_size = bill_length_mm * bill_depth_mm) |&gt;\n    ggplot(aes(x = bill_size, fill = is_chinstrap)) + \n    facet_wrap(~sex) + \n    geom_histogram()\n\n\n\n\n\nThe interaction term isn’t great at separating the two classes, but seems to be better than either length or size alone. So I’ll include it in the model.\n\n\nCode\ndf &lt;- palmerpenguins::penguins %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(is_chinstrap = species == \"Chinstrap\") |&gt;\n    mutate(bill_size = bill_length_mm * bill_depth_mm) |&gt;\n    mutate(is_male = as.numeric(sex == \"male\"))\n\ny &lt;- df$is_chinstrap\n\nX &lt;- cbind(1, df[,c(\"bill_length_mm\", \"bill_depth_mm\", \"bill_size\", \"is_male\")]) |&gt;\nas.matrix()\n\n\nSo, including the intercept term, our predictor matrix \\(X\\) contains 5 columns, including the interaction term bill_size. 16\nLet’s try now to use the above in optim()\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = rep(0, 5), \n    fn = llogit,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1] 82.9075239 -2.4368673 -6.4311531  0.1787047 -6.4900678\n\n$value\n[1] -33.31473\n\n$counts\nfunction gradient \n     137       45 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n             [,1]         [,2]          [,3]         [,4]         [,5]\n[1,]   -12.103063    -550.0621    -209.30944    -9674.925    -3.700623\n[2,]  -550.062097  -25256.3082   -9500.55848  -443670.225  -184.360139\n[3,]  -209.309443   -9500.5585   -3650.65107  -168517.417   -68.158844\n[4,] -9674.924703 -443670.2251 -168517.41718 -7846293.352 -3464.964868\n[5,]    -3.700623    -184.3601     -68.15884    -3464.965    -3.700623\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n            [,1]         [,2]         [,3]          [,4]         [,5]\n[1,] 41.95816335 -0.156192235 -0.309892876 -4.036895e-02  9.329019450\n[2,] -0.15619224 -0.005017392 -0.024806420  1.070652e-03 -0.139430425\n[3,] -0.30989288 -0.024806420 -0.042869947  2.854565e-03 -0.337480429\n[4,] -0.04036895  0.001070652  0.002854565 -7.331214e-05  0.003098092\n[5,]  9.32901945 -0.139430425 -0.337480429  3.098092e-03  1.202424836\n\n\nNow let’s compare with glm()\n\n\nCode\nmod_glm &lt;- glm(is_chinstrap ~ bill_length_mm * bill_depth_mm +is_male, data = df, \nfamily = binomial())\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = is_chinstrap ~ bill_length_mm * bill_depth_mm + \n    is_male, family = binomial(), data = df)\n\nCoefficients:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                  365.2924    88.3341   4.135 3.54e-05 ***\nbill_length_mm                -8.9312     2.0713  -4.312 1.62e-05 ***\nbill_depth_mm                -23.6184     5.5003  -4.294 1.75e-05 ***\nis_male                      -11.8725     2.6121  -4.545 5.49e-06 ***\nbill_length_mm:bill_depth_mm   0.5752     0.1292   4.452 8.53e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 337.113  on 332  degrees of freedom\nResidual deviance:  49.746  on 328  degrees of freedom\nAIC: 59.746\n\nNumber of Fisher Scoring iterations: 9\n\n\nUh oh! On this occasion it appears one or both approaches have become confused. A five dimensional search space might be too much for the algorithms to cope with, especially with collinearity 17 between some of the terms. Let’s simplify the task a bit, and just use intercept, bill size, and is_male as covariates. First with the standard package:\n\n\nCode\nmod_glm_simpler &lt;- glm(is_chinstrap ~ bill_size +is_male,   data = df, \nfamily = binomial())\nsummary(mod_glm_simpler)\n\n\n\nCall:\nglm(formula = is_chinstrap ~ bill_size + is_male, family = binomial(), \n    data = df)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -32.815339   4.325143  -7.587 3.27e-14 ***\nbill_size     0.043433   0.005869   7.400 1.36e-13 ***\nis_male      -7.038215   1.207740  -5.828 5.62e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 337.11  on 332  degrees of freedom\nResidual deviance:  90.60  on 330  degrees of freedom\nAIC: 96.6\n\nNumber of Fisher Scoring iterations: 7\n\n\nAnd now with the bespoke function and optim\n\n\nCode\nX &lt;- cbind(1, df[,c(\"bill_size\", \"is_male\")]) |&gt;\nas.matrix()\n\nfuller_optim_output &lt;- optim(\n    par = rep(0, 3), \n    fn = llogit,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1] -32.60343219   0.04314546  -6.98585077\n\n$value\n[1] -45.30114\n\n$counts\nfunction gradient \n      73       18 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]         [,2]         [,3]\n[1,]    -13.008605   -10662.078    -5.201308\n[2,] -10662.078251 -8846787.584 -4846.390833\n[3,]     -5.201308    -4846.391    -5.201308\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n             [,1]          [,2]         [,3]\n[1,] -536.7022079  0.7206703142 -134.7923170\n[2,]    0.7206703 -0.0009674672    0.1807806\n[3,] -134.7923170  0.1807806218  -33.4602664\n\n\nThe estimates from the two approaches are now much closer, even if they aren’t as close to each other as in the earlier examples. Using optim(), we have parameter estimates \\(\\beta = \\{\\beta_0 = -32.60, \\beta_1 = 0.04, \\beta_2 = -6.99\\}\\), and using glm(), we have estimates \\(\\beta = \\{\\beta_0 = -32.82, \\beta_1 = 0.04, \\beta_2 = -7.04 \\}\\)\nIf we cheat a bit, and give the five dimensional version starting values closer to the estimates from glm(), we can probably get similar estimates too.\n\n\nCode\nX &lt;- cbind(1, df[,c(\"bill_length_mm\", \"bill_depth_mm\", \"bill_size\", \"is_male\")]) |&gt;\nas.matrix()\n\nfuller_optim_output &lt;- optim(\n    par = c(300, -10, -29, 0.5, -10), \n    fn = llogit,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1] 299.5512512  -7.3684567 -19.3951742   0.4747209  -9.7521255\n\n$value\n[1] -25.33208\n\n$counts\nfunction gradient \n     153       22 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n             [,1]          [,2]          [,3]         [,4]         [,5]\n[1,]    -8.378918    -370.41592    -140.86865    -6342.301    -1.800406\n[2,]  -370.415921  -16580.87909   -6238.75358  -284403.350   -91.239716\n[3,]  -140.868648   -6238.75358   -2387.19776  -107598.410   -33.018551\n[4,] -6342.300809 -284403.34960 -107598.40987 -4906697.476 -1685.235507\n[5,]    -1.800406     -91.23972     -33.01855    -1685.236    -1.800406\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n            [,1]         [,2]        [,3]          [,4]         [,5]\n[1,] -59.5448267  2.316365876  5.14842594 -0.1737609491 10.383684649\n[2,]   2.3163659 -0.064512887 -0.16844980  0.0044962968 -0.166413655\n[3,]   5.1484259 -0.168449797 -0.33888931  0.0106735535 -0.387558164\n[4,]  -0.1737609  0.004496297  0.01067355 -0.0002712683  0.004068597\n[5,]  10.3836846 -0.166413655 -0.38755816  0.0040685965  1.904433768\n\n\nWell, they are closer, but they aren’t very close. As mentioned, the glm() model produced warnings, and some of the variables are likely to be collinear, so this initial specification may have been especially difficult to fit. Both approaches found an answer, but neither seem happy about it!\n\nSummary\nIn the exercise above we did for logistic regression what the previous few posts in section two did for standard regression: i.e. we derived the log likelihood, applied it using optim, and compared with results from the glm() package. We saw in this case that fitting models isn’t always straightforward. We were - well, I was - overly ambitious in building and applying an overly parameterised model specification. But we eventually got to similar parameter values using both approaches.\nThough this wasn’t as straightforward as I was hoping for, I’m presenting it warts-and-all. In principle, the log-likelihood maximisation approach generalises to a great many model specifications, even if in practice some model structures aren’t as straightforward to fit as others."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#summary-2",
    "href": "pages/likelihood-and-simulation-theory/index.html#summary-2",
    "title": "Likelihood and Simulation Theory",
    "section": "Summary",
    "text": "Summary\nIn the exercise above we did for logistic regression what the previous few posts in section two did for standard regression: i.e. we derived the log likelihood, applied it using optim, and compared with results from the glm() package. We saw in this case that fitting models isn’t always straightforward. We were - well, I was - overly ambitious in building and applying an overly parameterised model specification. But we eventually got to similar parameter values using both approaches.\nThough this wasn’t as straightforward as I was hoping for, I’m presenting it warts-and-all. In principle, the log-likelihood maximisation approach generalises to a great many model specifications, even if in practice some model structures aren’t as straightforward to fit as others."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#coming-up-1",
    "href": "pages/likelihood-and-simulation-theory/index.html#coming-up-1",
    "title": "Likelihood and Simulation Theory",
    "section": "Coming up",
    "text": "Coming up\nIn the next post, I’ll finally be moving off ‘section two’, with its algebra and algorithms, and showing some tools that can be used to make honest prediction and projections with models, but without all the efforts undertaken here and in the last few posts."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/index.html#quantities-of-interest",
    "href": "pages/likelihood-and-simulation-theory/index.html#quantities-of-interest",
    "title": "Likelihood and Simulation Theory",
    "section": "Quantities of interest",
    "text": "Quantities of interest\nWe’ll now, finally, show how this knowledge can be applied to do something with statistical models that ought to be done far more often: report on what King, Tomz, and Wittenberg (2000) calls quantities of interest, including predicted values, expected values, and first differences. Quantities of interest are not the direction and statistical significance (P-values) that many users of statistical models convince themselves matter, leading to the kind of mindless stargazing summaries of model outputs described in section one. Instead, they’re the kind of questions that someone, not trained to think that stargazing is satisfactory, might reasonably want answers to. These might include:\n\nWhat is the expected income of someone who completes course X in the five years after graduation? (Expected values)\nWhat is the expected range of incomes of someone who completes course X in the five years after graduation? (Predicted values)\nWhat is the expected difference in incomes between someone who completes course X, compared to course Y, in the five years after graduation? (First Differences)\n\nIn section one, we showed how to answer some of the questions of this form, for both standard linear regression and logistic regression. We showed that for linear regression such answers tend to come directly from the summary of coefficients, but that for logistic regression such answers tend to be both more ambiguous and dependent on other factors (such as gender of graduate, degree, ethnicity, age and so on), and require more processing in order to produce estimates for.\nHowever, we previously produced only point estimates for these questions, and so in a sense misled the questioner with the apparent certainty of our estimates. We now know, from earlier in this section, that we can use information about parameter uncertainty to produce parameter estimates \\(\\tilde{\\theta}\\) that do convey parameter uncertainty, and so we can do better than the point estimates alone to answer such questions in way that takes into account such uncertainty, with a range of values rather than a single value.\n\nMethod\nLet’s make use of our toy dataset one last time, and go through the motions to produce the \\(\\tilde{\\theta}\\) draws we ended with on the last post:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\n\n\nCode\npoint_estimates &lt;- fuller_optim_output$par\n\nvcov &lt;- -solve(fuller_optim_output$hessian)\nparam_draws &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = point_estimates, \n    Sigma = vcov\n)\n\ncolnames(param_draws) &lt;- c(\n    \"beta0\", \"beta1\", \"eta\"\n)\n\n\nLet’s now look at our toy data again, and decide on some specific questions to answer:\n\n\nCode\nlibrary(tidyverse)\ntoy_df &lt;- tibble(x = x, y = y)\n\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() \n\n\n\n\n\nWithin the data itself, we have only supplied x and y values for whole numbers of x between -3 and 5. But we can use the model to produce estimates for non-integer values of x. Let’s try 2.5. For this single value of x, we can produce both predicted values and expected values, by passing the same value of x to each of the plausible estimates of \\(\\theta\\) returned by the multivariate normal function above.\n\n\nCode\ncandidate_x &lt;- 2.5\n\n\n\n\nExpected values\nHere’s an example of estimating the expected value of y for x = 2.5 using loops and standard algebra:\n\n\nCode\n# Using standard algebra and loops\nN &lt;- nrow(param_draws)\nexpected_y_simpler &lt;- vector(\"numeric\", N)\nfor (i in 1:N){\n    expected_y_simpler[i] &lt;- param_draws[i, \"beta0\"] + candidate_x * param_draws[i, \"beta1\"]\n}\n\nhead(expected_y_simpler)\n\n\n[1] 6.004068 5.859547 6.121791 5.987509 5.767047 6.395820\n\n\nWe can see just from the first few values that each estimate is slightly different. Let’s order the values from lowest to highest, and find the range where 95% of values sit:\n\n\nCode\nev_range &lt;- quantile(expected_y_simpler,  probs = c(0.025, 0.500, 0.975)) \n\nev_range\n\n\n    2.5%      50%    97.5% \n5.505104 5.898148 6.291150 \n\n\nThe 95% interval is therefore between 5.51 and 6.29, with the median (similar but not quite the point estimate) being 5.90. Let’s plot this against the data:\n\n\nCode\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() + \n    annotate(\"point\", x = candidate_x, y =  median(expected_y_simpler), size = 1.2, shape = 2, colour = \"blue\") + \n    annotate(\"segment\", x = candidate_x, xend=candidate_x, y = ev_range[1], yend = ev_range[3], colour = \"blue\")\n\n\n\n\n\nThe vertical blue line therefore shows the range of estimates for \\(Y|x=2.5\\) that contain 95% of the expected values given the draws of \\(\\beta = \\{\\beta_0, \\beta_1\\}\\) which we produced from the Multivariate Normal given the point estimates and Hessian from optim(). This is our estimated range for the expected value, not predicted value. What’s the difference?\n\n\nPredicted values\nOne clue about the difference between expected value lies in the parameters from optim() we did and did not use: Whereas we have both point estimates and uncertainty estimates for the parameters \\(\\{\\beta_0, \\beta_1, \\sigma^2\\}\\),12 we only made use of the the two \\(\\beta\\) parameters when producing this estimate.\nNow let’s recall the general model formula, from the start of King, Tomz, and Wittenberg (2000), which we repeated for the first few posts in the series:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nThe manual for Zelig, the (now defunct) R package that used to support analysis using this approach, states that for Normal Linear Regression these two components are resolved as follows:\nStochastic Component\n\\[\nY_i \\sim Normal(\\mu_i, \\sigma^2)\n\\]\nSystematic Component\n\\[\n\\mu_i = x_i \\beta\n\\]\nThe page then goes onto state that the expected value, \\(E(Y)\\), is :\n\\[\nE(Y) = \\mu_i = x_i \\beta\n\\]\nSo, in this case, the expected value is the systematic component only, and does not involve the dispersion parameter in the stochastic component, which for normal linear regression is the \\(\\sigma^2\\) term. That’s why we didn’t use estimates of \\(\\sigma^2\\) when simulating the expected values.\nBut why is this? Well, it comes from the expectation operator, \\(E(.)\\). This operator means something like, return to me the value that would be expected if this experiment were performed an infinite number of times.\nThere are two types of uncertainty which give rise to variation in the predicted estimate: sampling uncertainty, and stochastic variation. In the expected value condition, this second source of variation falls to zero,13 leaving only the influence of sampling uncertainty, as in uncertainty about the true value of the \\(\\beta\\) parameters, remaining on uncertainty on the predicted outputs.\nFor predicted values, we therefore need to reintroduce stochastic variation as a source of variation in the range of estimates produced. Each \\(\\eta\\) value we have implies a different \\(\\sigma^2\\) value in the stochastic part of the equation, which we can then add onto the variation caused by parameter uncertainty alone:\n\n\nCode\nN &lt;- nrow(param_draws)\npredicted_y_simpler &lt;- vector(\"numeric\", N)\nfor (i in 1:N){\n    predicted_y_simpler[i] &lt;- param_draws[i, \"beta0\"] + candidate_x * param_draws[i, \"beta1\"] + \n        rnorm(\n            1, mean = 0, \n            sd = sqrt(exp(param_draws[i, \"eta\"]))\n        )\n}\n\nhead(predicted_y_simpler)\n\n\n[1] 4.802092 6.706397 7.073450 6.118750 6.757717 7.461254\n\n\nLet’s now get the 95% prediction interval for the predicted values, and compare them with the expected values predicted interval earlier\n\n\nCode\npv_range &lt;- \n    quantile(\n        predicted_y_simpler, \n        probs = c(0.025, 0.500, 0.975)\n    )\n\npv_range\n\n\n    2.5%      50%    97.5% \n4.766300 5.895763 7.055408 \n\n\nSo, whereas the median is similar to before, 5.90, the 95% interval is now from 4.77 to 7.0614. This compares with the 5.51 to 6.29 range for the expected values. Let’s now plot this predicted value range just as we did with the expected values:\n\n\nCode\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() + \n    annotate(\"point\", x = candidate_x, y =  pv_range[2], size = 1.2, shape = 2, colour = \"blue\") + \n    annotate(\"segment\", x = candidate_x, xend=candidate_x, y = pv_range[1], yend = pv_range[3], colour = \"red\")\n\n\n\n\n\nClearly considerably wider."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html",
    "href": "pages/complete-simulation-example/index.html",
    "title": "Statistical Simulation: A Complete Example",
    "section": "",
    "text": "Section One of this course introduced generalised linear models (GLMs) and statistical simuation. Section Two then delved more into the underlying theory and technicalities involved in implementing GLMs and using them for simulation.\nThis section gives a complete example of the methodology developed in these two sections, from start to finish. It also shows how the methodology is similar enough to Bayesian methods of statistical inference that applying a fully Bayesian modelling framework is just a small hop and jump from where we already are."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#recap",
    "href": "pages/complete-simulation-example/index.html#recap",
    "title": "Statistical Simulation: A Complete Example",
    "section": "",
    "text": "Section One of this course introduced generalised linear models (GLMs) and statistical simuation. Section Two then delved more into the underlying theory and technicalities involved in implementing GLMs and using them for simulation.\nThis section gives a complete example of the methodology developed in these two sections, from start to finish. It also shows how the methodology is similar enough to Bayesian methods of statistical inference that applying a fully Bayesian modelling framework is just a small hop and jump from where we already are."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#aim",
    "href": "pages/complete-simulation-example/index.html#aim",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Aim",
    "text": "Aim\nIn the last post we reached the end of a winding journey. This post will show how Bayesian approaches to model fitting, rather than the frequentist approaches more commonly used, can reach the intended destination of this journey more quickly, despite being a bit more conceptually challenging to start with."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#linear-regression-example",
    "href": "pages/complete-simulation-example/index.html#linear-regression-example",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Linear regression example",
    "text": "Linear regression example\nLet’s start with one of the built-in datasets, ToothGrowth, which is described as follows:\n\nThe response is the length of odontoblasts (cells responsible for tooth growth) in 60 guinea pigs. Each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods, orange juice or ascorbic acid (a form of vitamin C and coded as VC).\n\nLet’s load the dataset and visualise\n\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nWhat does it look like?\n\ndf |&gt;\n    ggplot(aes(y = len, x = dose, shape = supp, colour = supp)) + \n    geom_point() + \n    expand_limits(x = 0, y = 0)\n\n\n\n\nSo, although this has just three variables, there is some complexity involved in thinking about how the two predictor variables, supp and dose, relate to the response variable len. These include:\n\nWhether the relationship between len and dose is linear in a straightforward sense, or associated in a more complicated wway\nWhether supp has the same effect on len regardless of dose, or whether there is an interaction between dose and supp.\n\n\nStage One: model fitting\nWe can address each of these questions in turn, but should probably start with a model which includes both predictors:\n\nmod_01 &lt;- lm(len ~ dose + supp, data = df)\n\nsummary(mod_01)\n\n\nCall:\nlm(formula = len ~ dose + supp, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.600 -3.700  0.373  2.116  8.800 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.2725     1.2824   7.231 1.31e-09 ***\ndose          9.7636     0.8768  11.135 6.31e-16 ***\nsuppVC       -3.7000     1.0936  -3.383   0.0013 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.236 on 57 degrees of freedom\nMultiple R-squared:  0.7038,    Adjusted R-squared:  0.6934 \nF-statistic: 67.72 on 2 and 57 DF,  p-value: 8.716e-16\n\n\nEach term is statistically significant at the conventional thresholds (P &lt; 0.05), with higher doses associated with greater lengths. Compared to OJ, the reference category, a vitamin C (VC) supplement is associated with lower lengths.\nTurning to the first question, about the type of relationship between len and dose, one possibility is that greater doses lead to greater lengths, but there are diminishing marginal returns: the first mg has the biggest marginal effect, then the second mg has a lower marginal effect. An easy way to model this would be to include the log of dose in the regression model, rather than the dose itself.1 We can get a sense of whether this log dose specification might be preferred by plotting the data with a log scale on the x axis, and seeing if the points look like they ‘line up’ better:\n\ndf |&gt;\n    ggplot(aes(y = len, x = dose, shape = supp, colour = supp)) + \n    geom_point() + \n    scale_x_log10() + \n    expand_limits(x = 0.250, y = 0)\n\n\n\n\nYes, with this scaling, the points associated with the three dosage regimes look like they line up better. Let’s now build this model specification:\n\nmod_02 &lt;- lm(len ~ log(dose) + supp, data = df)\n\nsummary(mod_02)\n\n\nCall:\nlm(formula = len ~ log(dose) + supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2108 -2.9896 -0.5633  2.2842  9.1892 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.6633     0.7033   29.38  &lt; 2e-16 ***\nlog(dose)    11.1773     0.8788   12.72  &lt; 2e-16 ***\nsuppVC       -3.7000     0.9947   -3.72 0.000457 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.852 on 57 degrees of freedom\nMultiple R-squared:  0.755, Adjusted R-squared:  0.7464 \nF-statistic: 87.81 on 2 and 57 DF,  p-value: &lt; 2.2e-16\n\n\nAgain, the same kind of relationship between variables is observed: higher log dose: greater length; and VC rather than OJ is associated with lower growth. But is this model actually any better? The model summary for the linear dose model gives an adjusted \\(R^2\\) of 0.69, whereas for the log dose model the adjusted \\(R^2\\) is 0.75. So, as the data are fundamentally the same,2 this suggests it is. However, as we know that linear regression models are really just another kind of generalised linear models, and that model fitting tends to involve trying to maximise the log likelihood, we can also compare the log likelihoods of the two models, using the logLik() function, and so which is higher:\n\nlogLik(mod_01)\n\n'log Lik.' -170.2078 (df=4)\n\nlogLik(mod_02)\n\n'log Lik.' -164.5183 (df=4)\n\n\nBoth report the same number of degrees of freedom (‘df’), which shouldn’t be suprising as they involve the same number of parameters. But the log likelihood for mod_02 is higher, which like the Adjusted R-squared metric suggests a better fit.\nAnother approach, which generalises better to other types of model, is to compare the AICs, which are metrics that try to show the trade off between model complexity (based on number of parameters), and model fit (based on the log likelihood). By this criterion, the lower the score, the better the model:\n\nAIC(mod_01, mod_02)\n\n       df      AIC\nmod_01  4 348.4155\nmod_02  4 337.0367\n\n\nAs both models have exactly the same number of parameters, it should be of no surprise that mod_02 is still preferred.\nLet’s now address the second question: is there an interaction between dose and supp. This interaction term can be specified in one of two ways:\n\n# add interaction term explicitly, using the : symbol\nmod_03a &lt;- lm(len ~ log(dose) + supp + log(dose) : supp, data = df)\n\n# add interaction term implicitly, using the * symbol \nmod_03b &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(mod_03a)\n\n\nCall:\nlm(formula = len ~ log(dose) + supp + log(dose):supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\nsummary(mod_03b)\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nWe can see from the summaries that both ways of specifying the models lead to exactly the same model, with exactly the same estimates, standared errors, adjusted \\(R^2\\)s, and so on. The adjusted \\(R^2\\) is now 0.76, a slight improvement on the 0.75 value for the model without the interaction term. As before, we can also compare the trade-off between additional complexity and improved fit using AIC\n\nAIC(mod_02, mod_03a)\n\n        df      AIC\nmod_02   4 337.0367\nmod_03a  5 333.7750\n\n\nSo, the AIC of the more complex model is lower, suggesting a better model, but the additional improvement in fit is small.\nWe can also compare the fit, and answer the question of whether the two models can be compared, in a couple of other ways. Firstly, we can use BIC, AIC’s (usually) stricter cousin, which tends to penalise model complexity more harshly:\n\nBIC(mod_02, mod_03a)\n\n        df      BIC\nmod_02   4 345.4140\nmod_03a  5 344.2467\n\n\nEven using BIC, the more complex model is still preferred, though the difference in values is now much smaller.\nThe other way we can compare the models is using an F-test using the anova (analysis of variance) function:\n\nanova(mod_02, mod_03a)\n\nAnalysis of Variance Table\n\nModel 1: len ~ log(dose) + supp\nModel 2: len ~ log(dose) + supp + log(dose):supp\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     57 845.91                              \n2     56 774.89  1    71.022 5.1327 0.02737 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere anova compares the two models, notes that the first model can be understood as a restricted variant of the second model,3 and compares the change in model fit between the two models against the change in number of parameters used to fit the model. The key parts of the summary to look at are the F test value, 5.13, and the associated P value, which is between 0.01 and 0.05. This, again, suggests the interaction term is worth keeping.\nSo, after all that, we finally have a fitted model. Let’s look now at making some predictions from it.\n\n\nStage Two: Model predictions\nThe simplest approach to getting model predictions is to use the predict function, passing it a dataframe of values for which we want predictions:\n\npredictor_df &lt;- expand_grid(\n    supp = c('VC', 'OJ'), \n    dose = seq(0.25, 2.25, by = 0.01)\n)\npreds_predictors_df &lt;- predictor_df |&gt;\n    mutate(pred_len = predict(mod_03a, predictor_df))\n\npreds_predictors_df\n\n# A tibble: 402 × 3\n   supp   dose pred_len\n   &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 VC     0.25   -1.20 \n 2 VC     0.26   -0.683\n 3 VC     0.27   -0.189\n 4 VC     0.28    0.288\n 5 VC     0.29    0.748\n 6 VC     0.3     1.19 \n 7 VC     0.31    1.62 \n 8 VC     0.32    2.04 \n 9 VC     0.33    2.44 \n10 VC     0.34    2.83 \n# ℹ 392 more rows\n\n\nWe can visualise these predictions as follows, with the predicted values as lines, and the observed values as points:\n\npreds_predictors_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, y = pred_len, colour = supp, linetype = interextrap)) + \n    geom_line() + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\"\n    )\n\n\n\n\nIn the above, I’ve shown the lines as solid when they represent interpolations of the data, i.e. are in the range of measured doses, and as dashed when they represent extrapolations from the data, meaning they are are predictions made outside the range of observed values. We can see an obvious issue when we extrapolate too far to the left: for low doses, and for the VC supplement, the model predicts negative tooth lengths. Extrapolation is dangerous! And gets more dangerous the further we extrapolate from available observations.\nWe can also use the predict function to produce uncertainty intervals, either of expected values, or predicted values. By default these are 95% intervals, meaning they are expected to contain 95% of the range of expected or predicted values from the model.\nLet’s first look at expected values, which include uncertainty about parameter estimates, but not observed variation in outcomes:\n\ndf_pred_intvl &lt;- predict(mod_03a, newdata = predictor_df, interval = \"confidence\")\n\npreds_predictors_intervals_df &lt;- \n    bind_cols(predictor_df, df_pred_intvl)\n\npreds_predictors_intervals_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, linetype = interextrap)) + \n    geom_line(aes(y = fit, colour = supp)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = supp), alpha = 0.2) + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\",\n        subtitle = \"Range of expected values\"\n    )\n\n\n\n\nAnd the following shows the equivalent prediction intervals, which also incorporate known variance, as well as parameter uncertainty:\n\ndf_pred_intvl &lt;- predict(mod_03a, newdata = predictor_df, interval = \"prediction\")\n\npreds_predictors_intervals_df &lt;- \n    bind_cols(predictor_df, df_pred_intvl)\n\npreds_predictors_intervals_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, linetype = interextrap)) + \n    geom_line(aes(y = fit, colour = supp)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = supp), alpha = 0.2) + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\",\n        subtitle = \"Range of predicted values\"\n    )\n\n\n\n\nAs should be clear from the above, and discussion of the difference between expected and predicted values in previous posts, predicted values and expected values are very different, and it is important to be aware of the difference between these two quantities of interest. Regardless, we can see once again how dangerous it is to use this particular model specification to extrapolate beyond the range of observations, expecially for lower doses."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#stage-two-model-predictions",
    "href": "pages/complete-simulation-example/index.html#stage-two-model-predictions",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Stage Two: Model predictions",
    "text": "Stage Two: Model predictions\nThe simplest approach to getting model predictions is to use the predict function, passing it a dataframe of values for which we want predictions:\n\n\nCode\npredictor_df &lt;- expand_grid(\n    supp = c('VC', 'OJ'), \n    dose = seq(0.25, 2.25, by = 0.01)\n)\npreds_predictors_df &lt;- predictor_df |&gt;\n    mutate(pred_len = predict(mod_03a, predictor_df))\n\npreds_predictors_df\n\n\n# A tibble: 402 × 3\n   supp   dose pred_len\n   &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 VC     0.25   -1.20 \n 2 VC     0.26   -0.683\n 3 VC     0.27   -0.189\n 4 VC     0.28    0.288\n 5 VC     0.29    0.748\n 6 VC     0.3     1.19 \n 7 VC     0.31    1.62 \n 8 VC     0.32    2.04 \n 9 VC     0.33    2.44 \n10 VC     0.34    2.83 \n# ℹ 392 more rows\n\n\nWe can visualise these predictions as follows, with the predicted values as lines, and the observed values as points:\n\n\nCode\npreds_predictors_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, y = pred_len, colour = supp, linetype = interextrap)) + \n    geom_line() + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\"\n    )\n\n\n\n\n\nIn the above, I’ve shown the lines as solid when they represent interpolations of the data, i.e. are in the range of measured doses, and as dashed when they represent extrapolations from the data, meaning they are are predictions made outside the range of observed values. We can see an obvious issue when we extrapolate too far to the left: for low doses, and for the VC supplement, the model predicts negative tooth lengths. Extrapolation is dangerous! And gets more dangerous the further we extrapolate from available observations.\nWe can also use the predict function to produce uncertainty intervals, either of expected values, or predicted values. By default these are 95% intervals, meaning they are expected to contain 95% of the range of expected or predicted values from the model.\nLet’s first look at expected values, which include uncertainty about parameter estimates, but not observed variation in outcomes:\n\n\nCode\ndf_pred_intvl &lt;- predict(mod_03a, newdata = predictor_df, interval = \"confidence\")\n\npreds_predictors_intervals_df &lt;- \n    bind_cols(predictor_df, df_pred_intvl)\n\npreds_predictors_intervals_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, linetype = interextrap)) + \n    geom_line(aes(y = fit, colour = supp)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = supp), alpha = 0.2) + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\",\n        subtitle = \"Range of expected values\"\n    )\n\n\n\n\n\nAnd the following shows the equivalent prediction intervals, which also incorporate known variance, as well as parameter uncertainty:\n\n\nCode\ndf_pred_intvl &lt;- predict(mod_03a, newdata = predictor_df, interval = \"prediction\")\n\npreds_predictors_intervals_df &lt;- \n    bind_cols(predictor_df, df_pred_intvl)\n\npreds_predictors_intervals_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, linetype = interextrap)) + \n    geom_line(aes(y = fit, colour = supp)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = supp), alpha = 0.2) + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\",\n        subtitle = \"Range of predicted values\"\n    )\n\n\n\n\n\nAs should be clear from the above, and discussion of the difference between expected and predicted values in previous posts, predicted values and expected values are very different, and it is important to be aware of the difference between these two quantities of interest. Regardless, we can see once again how dangerous it is to use this particular model specification to extrapolate beyond the range of observations, expecially for lower doses."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#summary-and-coming-up",
    "href": "pages/complete-simulation-example/index.html#summary-and-coming-up",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Summary and coming up",
    "text": "Summary and coming up\nIn this post, we’ve started and finished building the model, and started but not quite finished using the model to generate expected and predicted values. We’ve discussed some approaches to deciding on a model specification, by incrementally comparing a series of different specifications which test different ideas we have about how the predictor variables might be related to each other, and to the response variable.\nAs we’ve done quite a lot of work on building the model, we’ve not covered everything that I was planning to in terms of model prediction, and what we can do with a linear regression model (and generalised linear regression model), beyond (yawn) stargazing, for using models to get expected values, predicted values, and especially first differences. So, I guess that’s coming up in the next post!"
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#aim-1",
    "href": "pages/complete-simulation-example/index.html#aim-1",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Aim",
    "text": "Aim\nThe last post ended by showing how the predict function can be used to show point estimates and uncertainty intervals for expected values and predicted values for a model based on a toothsome dataset. In this post we will start with that model and look at other information that can be recovered from it, information that will allow the effects of joint parameter uncertainty to be propagated through to prediction."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#recap-of-core-concepts",
    "href": "pages/complete-simulation-example/index.html#recap-of-core-concepts",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Recap of core concepts",
    "text": "Recap of core concepts\nBack in part 8 we stated that estimates of the cloud of uncertainty in model parameters, that results from having limited numbers of observations in the data, can be represented as:\n\\[\n\\tilde{\\theta} \\sim MVN(\\mu = \\dot{\\theta}, \\sigma^2 = \\Sigma)\n\\]\nWhere MVN means multivariate normal, and needs the two quantities \\(\\dot{\\theta}\\) and \\(\\Sigma\\) as parameters.\nPreviously we showed how to extract (estimates of) these two quantities from optim(), where the first quantity, \\(\\dot{\\theta}\\), was taken from the converged parameter point estimate slot par, and the second quantity, \\(\\Sigma\\), was derived from the hessian slot.\nBut we don’t need to use optim() directly in order to recover these quantities. Instead we can get them from the standard model objects produced by either lm() or glm(). Let’s check this out…"
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#building-our-model",
    "href": "pages/complete-simulation-example/index.html#building-our-model",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Building our model",
    "text": "Building our model\nLet’s load the data and model we arrived at previously\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nCode\nbest_model &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(best_model)\n\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s now look at some convenience functions, other than just summary, that work with lm() and glm() objects, and recover the quantities required from MVN to represent the uncertainty cloud."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#extracting-quantities-for-modelling-uncertainty",
    "href": "pages/complete-simulation-example/index.html#extracting-quantities-for-modelling-uncertainty",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Extracting quantities for modelling uncertainty",
    "text": "Extracting quantities for modelling uncertainty\nFirstly, for the point estimates \\(\\dot{\\theta}\\), we can use the coefficients() function\n\n\nCode\ncoef &lt;- coefficients(best_model)\n\ncoef\n\n\n     (Intercept)        log(dose)           suppVC log(dose):suppVC \n       20.663333         9.254889        -3.700000         3.844782 \n\n\nAnd for the variance-covariance matrix, for representing joint uncertainty about the above estimates, we can use the vcov function\n\n\nCode\nSig &lt;- vcov(best_model)\n\nSig\n\n\n                   (Intercept)     log(dose)        suppVC log(dose):suppVC\n(Intercept)       4.612422e-01 -8.768056e-17 -4.612422e-01    -7.224251e-17\nlog(dose)        -8.768056e-17  1.440023e+00  1.753611e-16    -1.440023e+00\nsuppVC           -4.612422e-01  1.753611e-16  9.224843e-01     1.748938e-16\nlog(dose):suppVC -7.224251e-17 -1.440023e+00  1.748938e-16     2.880045e+00\n\n\nFinally, we can extract the point estimate for stochastic variation in the model, i.e. variation assumed by the model even if parameter uncertainty were minimised, using the sigma function:\n\n\nCode\nsig &lt;- sigma(best_model)\n\nsig\n\n\n[1] 3.719847\n\n\nWe now have three quantities, coef, Sig and sig (note the upper and lower case s in the above). These provide something almost but not exactly equivalent to the contents of par and that derived from hessian when using optim() previously. The section below explains this distinction in more detail.\n\nBack to the weeds (potentially skippable)\nRecall the ‘grandmother formulae’, from King, Tomz, and Wittenberg (2000), which the first few posts in this series started with:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nFor standard linear regression this becomes:\nStochastic Component\n\\[\nY_i \\sim Norm(\\theta_i, \\sigma^2)\n\\]\nSystematic Component\n\\[\n\\theta_i =X_i \\beta\n\\]\nOur main parameters are \\(\\theta\\), which combined our predictors \\(X_i\\) and our model parameter estimates \\(\\beta\\). Of these two components we know the data - they are what they are - but are merely estimating our model parameters \\(\\beta\\). So, any estimation uncertainty in this part of the equation results from \\(\\beta\\) alone.\nOur ancillary parameter is \\(\\sigma^2\\). This is our estimate of how much fundamental variation there is in how the data (the response variables \\(Y\\)) is drawn from the stochastic data generating process.\nWhen we used optim() directly, we estimated \\(\\sigma^2\\) along with the other \\(\\beta\\) parameters, via the \\(\\eta\\) parameter eta, defined as \\(\\sigma^2 = e^{\\eta}\\) to allow optim() to search over an unbounded real number range. If there are k \\(\\beta\\) parameters, therefore, optim()’s par vector contained k + 1 values, with this last value being the point estimate for the eta parameter. Similarly, the number of rows, columns, and length of diagonal elements in the variance-covariance matrix recoverable through optim’s hessian slot was also k + 1 rather than k, with the last row, last column, and last diagonal element being measures of covariance between \\(\\eta\\) and the \\(\\beta\\) elements, and variance in \\(\\eta\\) itself.\nBy contrast, the length of coefficients returned by coefficients(best_model) is k, the number of \\(\\beta\\) parameters being estimated, and the dimensions of vcov(best_model) returned are also k by k.\nThis means there is one fewer piece/type of information about model parameters returned by coefficients(model), vcov(model) and sigma(model) than was potentially recoverable by optim()’s par and hessian parameter slots: namely, uncertainty about the true value of the ancillary parameter \\(\\sigma^2\\). The following table summarises this difference:\n\n\n\n\n\n\n\n\nInformation type\nvia optim\nvia lm and glm\n\n\n\n\nMain parameters: point\nfirst k elements of par\ncoefficients() function\n\n\nMain parameters: uncertainty\nfirst k rows and columns of hessian\nvcov() function\n\n\nAncillary parameters: point\nk+1th through to last element of par\nsigma() function or equivalent for glm()\n\n\nAncillary parameters: uncertainty\nlast columns and rows of hessian (after rows and columns k)\n—\n\n\n\nSo long as capturing uncertainty about the fundamental variability in the stochastic part of the model isn’t critical to our predictions then omission of a measure of uncertainty in the ancillary parameters \\(\\alpha\\) is likely a price worth paying for the additional convenience of being able to use the model objects directly. However we should be aware that, whereas with optim we potentially have both \\(\\tilde{\\beta}\\) and \\(\\tilde{\\alpha}\\) to represent model uncertainty, when using the three convenience functions coefficients(), vcov() and sigma() we technically ‘only’ have \\(\\tilde{\\beta}\\) and \\(\\dot{\\alpha}\\) (i.e. point estimates alone for the ancillary parameters).\nWith the above caveat in mind, let’s now look at using the results of coefficients(), vcov() and sigma() to generate (mostly) honest representations of expected values, predicted values, and first differences"
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#model-predictions",
    "href": "pages/complete-simulation-example/index.html#model-predictions",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Model predictions",
    "text": "Model predictions\nAs covered in section two, we can use the mvrnorm function from the MASS package to create \\(\\tilde{\\beta}\\), our parameter estimates with uncertainty:\n\nParameter simulation\n\n\nCode\nbeta_tilde &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = coef, \n    Sigma = Sig\n)\n\nhead(beta_tilde)\n\n\n     (Intercept) log(dose)    suppVC log(dose):suppVC\n[1,]    20.24094 10.215063 -3.746876         2.387160\n[2,]    20.82397  8.443172 -3.589557         2.883642\n[3,]    21.39830 11.584301 -3.812187         1.835707\n[4,]    19.46575 11.311284 -3.376451         2.455034\n[5,]    21.11652  7.487017 -3.352625         6.903544\n[6,]    21.85636  8.954243 -3.947732         5.630424\n\n\nLet’s first look at each of these parameters individually:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    pivot_longer(everything(), names_to = \"coefficient\", values_to = \"value\") |&gt; \n    ggplot(aes(x = value)) + \n    facet_grid(coefficient ~ .) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nNow let’s look at a couple of coefficients jointly, to see how they’re correlated. Firstly the association between the intercept and the log dosage:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `(Intercept)`, y = `log(dose)`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nHere the covariance between the two parameters appears very low. Now let’s look at how log dosage and Vitamin C supplement factor are associated:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `log(dose)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nAgain, the covariance appears low. Finally, the association between log dose and the interaction term\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `log(dose)`, y = `log(dose):suppVC`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nHere we have a much stronger negative covariance between the two coefficients. Let’s look at the variance-covariance extracted from the model previously to confirm this:\n\n\nCode\nknitr::kable(Sig)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\nlog(dose)\nsuppVC\nlog(dose):suppVC\n\n\n\n\n(Intercept)\n0.4612422\n0.000000\n-0.4612422\n0.000000\n\n\nlog(dose)\n0.0000000\n1.440023\n0.0000000\n-1.440023\n\n\nsuppVC\n-0.4612422\n0.000000\n0.9224843\n0.000000\n\n\nlog(dose):suppVC\n0.0000000\n-1.440023\n0.0000000\n2.880045\n\n\n\n\n\nHere we can see that the covariance between intercept and log dose is effectively zero, as is the covariance between the intercept and the interaction term, and the covariance between the log(dose) and suppVC factor. However, there is a negative covariance between log dose and the interaction term, i.e. what we have plotted above, and also between the intercept and the VC factor. For completeness, let’s look at this last assocation, which we expect to show negative association:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `(Intercept)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nYes it is! The parameter estimates follow the covariance provided by Sigma, as we would expect.\n\n\nExpected values\nLet’s stay we are initially interested in the expected values for a dosage of 1.25mg, with the OJ (rather than VC) supplement:\n\n\nCode\n# first element is 1 due to intercept\npredictor &lt;- c(1, log(1.25), 0, 0) \n\npredictions_ev &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor %*% this_beta\n)\n\nhead(predictions_ev)\n\n\n[1] 22.52037 22.70801 23.98326 21.98979 22.78720 23.85444\n\n\nLet’s now get a 95% credible interval:\n\n\nCode\nquantile(predictions_ev, probs = c(0.025, 0.500, 0.975))\n\n\n    2.5%      50%    97.5% \n21.25316 22.71753 24.14249 \n\n\nSo, the 95% interval for the expected value is between 21.31 and 24.14, with a middle (median) estimate of 22.73.4 Let’s check this against estimates from the predict() function:\n\n\nCode\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'confidence')\n\n\n      fit      lwr      upr\n1 22.7285 21.26607 24.19093\n\n\nThe expected values using the predict function give a 95% confidence interval of 21.27 to 24.19, with a point estimate of 22.73. These are not identical, as the methods employed are not identical,5 but they are hopefully similar enough to demonstrate they are attempts at getting at the same quantities of interest.\n\n\nPredicted values\nPredicted values also include inherent stochastic variation from the ancillary parameters \\(\\alpha\\), which for linear regression is \\(\\sigma^2\\). We can simply add these only the expected values above to produce predicted values:\n\n\nCode\nn &lt;- length(predictions_ev)\n\nshoogliness &lt;- rnorm(n=n, mean = 0, sd = sig)\n\npredictions_pv &lt;- predictions_ev + shoogliness\n\n\nhead(predictions_pv)\n\n\n[1] 17.32760 24.95960 23.66540 20.11720 26.22552 27.89927\n\n\nLet’s get the 95% interval from the above using quantile\n\n\nCode\nquantile(predictions_pv, probs = c(0.025, 0.5000, 0.975))\n\n\n    2.5%      50%    97.5% \n15.12090 22.82004 30.00322 \n\n\nAs expected, the interval is now much wider, with a 95% interval from 15.34 to 30.11. The central estimate should in theory, with an infinite number of runs, be the same, however because of random variation it will never be exactly the same to an arbitrary number of decimal places. In this case, the middle estimate is 22.75, not identical to the central estimate from the expected values distribution of 22.72. The number of simulations can always be increased to produce greater precision if needed.\nLet’s now compare this with the prediction interval produce by the predict function:\n\n\nCode\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'prediction')\n\n\n      fit      lwr     upr\n1 22.7285 15.13461 30.3224\n\n\nAgain, the interval estimates are not exactly the same, but they are very similar.\n\n\nFirst differences\nIt’s in the production of estimates of first differences - this, compared to that, holding all else constant - that the simulation approach shines for producing estimates with credible uncertainty. In our case, let’s say we are interested in asking:\n\nWhat is the expected effect of using the VC supplement, rather than the OJ supplement, where the dose is 1.25mg?\n\nSo, the first difference is from switching from OJ to VC, holding the other factor constant.\nWe can answer this question by using the same selection of \\(\\tilde{\\beta}\\) draws, but passing two different scenarios:\n\n\nCode\n#scenario 0: supplement is OJ\npredictor_x0 &lt;- c(1, log(1.25), 0, 0) \n\n#scenario 1: supplement is VC\npredictor_x1 &lt;- c(1, log(1.25), 1, 1 * log(1.25)) \n\n\npredictions_ev_x0 &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x0 %*% this_beta\n)\n\npredictions_ev_x1 &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x1 %*% this_beta\n)\n\npredictions_df &lt;- \n    tibble(\n        x0 = predictions_ev_x0,\n        x1 = predictions_ev_x1\n    ) |&gt;\n    mutate(\n        fd = x1 - x0\n    )\n\npredictions_df\n\n\n# A tibble: 10,000 × 3\n      x0    x1    fd\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  22.5  19.3 -3.21\n 2  22.7  19.8 -2.95\n 3  24.0  20.6 -3.40\n 4  22.0  19.2 -2.83\n 5  22.8  21.0 -1.81\n 6  23.9  21.2 -2.69\n 7  23.4  20.4 -2.98\n 8  23.4  20.2 -3.26\n 9  22.6  20.7 -1.94\n10  22.5  18.7 -3.83\n# ℹ 9,990 more rows\n\n\nLet’s look at the distribution of both scenarios individually:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario != \"fd\") |&gt;\n    ggplot(aes(x = estimate)) + \n    facet_wrap(~scenario, ncol = 1) + \n    geom_histogram(bins = 100)\n\n\n\n\n\nAnd the distribution of the pairwise differences between them:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario == \"fd\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nIt’s this last distribution which shows our first differences, i.e. our answer, hedged with an appropriate dose of uncertainty, to the specific question shown above. We can get a 95% interval of the first difference as follows:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario == \"fd\") |&gt; \n    pull('estimate') |&gt;\n    quantile(probs = c(0.025, 0.500, 0.975))\n\n\n      2.5%        50%      97.5% \n-4.8366800 -2.8406462 -0.8197071 \n\n\nSo, 95% of estimates of the first difference are between -4.85 and -0.81, with the middle of this distribution (on this occasion) being -2.83.\nUnlike with the expected values and predicted values, the predict() function does not return first differences with honest uncertainty in this way. What we have above is something new."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#summary",
    "href": "pages/complete-simulation-example/index.html#summary",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Summary",
    "text": "Summary\nIn this post we’ve finally combined all the learning we’ve developed over the 11 previous posts to answer three specific ‘what if?’ questions: one on expected values, one on predicted values, and one on first differences. These are what King, Tomz, and Wittenberg (2000) refer to as quantities of interest, and I hope you agree these are more organic and reasonable types of question to ask of data and statistical models than simply looking at coefficients and p-values and reporting which ones are ‘statistically significant’.\nIf you’ve been able to follow everything in these posts, and can generalise the approach shown above to other types of statistical model, then congratulations! You’ve learned the framework for answering meaningful questions using statistical models which is at the heart of one of the toughest methods courses for social scientists offered by one of the most prestigious universities in the world."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#coming-up",
    "href": "pages/complete-simulation-example/index.html#coming-up",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Coming up",
    "text": "Coming up\nThe next post uses the same dataset and model we’ve developed and applied, but shows how it can be implemented using a Bayesian rather than Frequentist modelling approach. In some ways it’s very familar, but in others it introduces a completely new paradigm to how models are fit and run."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#aim-2",
    "href": "pages/complete-simulation-example/index.html#aim-2",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Aim",
    "text": "Aim\nIn the last post we reached the end of a winding journey. This post will show how Bayesian approaches to model fitting, rather than the frequentist approaches more commonly used, can reach the intended destination of this journey more quickly, despite being a bit more conceptually challenging to start with."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#recap-1",
    "href": "pages/complete-simulation-example/index.html#recap-1",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Recap",
    "text": "Recap\nThe start of this blog series aimed to do two things:\n\nReintroduce statistical models via a generalised model formulae, comprising a systematic component and a stochastic component.\nReintroduce the fitting of statistical models from the perspective of algorithmic optimisation, in which the gap between what the model predicts and what’s observed is minimised in some way.\n\nThe rest of the first section of the series - posts two, three and four - added more context to the first post, and introduced the concept of using models for prediction - and the types of quantities of interest they can predict. The first section ended with post four, which illustrated some of the complexities of getting meaningful effect estimates - the overall effect of one specific predictor variable on the outcome being predicted - for model structures under than standard linear regression.\nThe second section - covering posts five to ten - delved into a lot more detail about how statistical models are fit. It introduced the concept of likelihood as a means of deciding what the target of a statistical optimisation algorithm should be. And it also showed - in sometimes excruciating detail - how to perform numeric optimisation based on likelihood in order to extract not just the best set of model parameters, but estimates of joint uncertainty in the best estimated set of model parameters. It’s this joint uncertainty in parameter estimates, estimated via the Hessian from the optim() function, which allowed uncertainty in model parameter estimates to be propagated and percolated through specific ‘what-if?’ questions - i.e. specific configurations of predictor variables passed through to the model - in order to produce honest answers to these ‘what-if?’ questions, which provide a range of answers, rather than a single answer, in order to show how model parameter estimation uncertainty leads to uncertainty in the answers the model provides.\nThe third section - posts 10-12 - completed the journey, showing how many of the concepts and ideas learned through considerable effort in sections one and (especially) two allow more intelligent and effective use of standard statistical model outputs - produced using R’s lm() and glm() functions - for honest prediction.\nThis post will extend the third section to show why the kind of honest prediction which we managed to produce using the kind of frequentist modelling framework used by lm() and glm() are, in fact, easier to produce using Bayesian models."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#on-marbles-and-jumping-beans",
    "href": "pages/complete-simulation-example/index.html#on-marbles-and-jumping-beans",
    "title": "Statistical Simulation: A Complete Example",
    "section": "On marbles and jumping beans",
    "text": "On marbles and jumping beans\nSection Two introduced Bayes’ Rule and the Likelihood axiom. It pointed out that, at heart, Bayes’ Rule is a way of expressing that given this in terms of this given that; and that Likelihood is also a claim about how that given this relates to this given that. More specifically, the claim of Likelihood is:\n\nThe likelihood of the model given the data is proportional to the probability of the data given the model.\n\nThere are two aspects to the model: firstly its structure; secondly its parameters. The structure includes the type of statistical model - whether it is a standard linear regression, negative binomial regression, logistic regression, Poisson regression model and so on - and also the specific types of columns from the dataset selected as either predictor variables (\\(X\\)) or response variables (\\(Y\\)). It is only after both the higher level structure of the model family, and the lower level structure of the data inputs (what’s being regressed on what?) have been decided that the Likelihood theory is used.\nAnd how is Likelihood theory used? Well, it defines a landscape over which an algorithm searches. This landscape has as many dimensions as there are parameters to fit. Where there are just two parameters, \\(\\beta_0\\) and \\(\\beta_1\\) to fit, we can visualise this landscape using something like a contour plot, with \\(\\beta_0\\) as latitude, \\(\\beta_1\\) as longitude, and the likelihood at this position its elevation or depth. Each possible joint value \\(\\beta = \\{\\beta_0, \\beta_1\\}\\) which the algorithm might wish to propose leads to a different long-lat coordinate over the surface, and each coordinate has a different elevation or depth. Although we can’t see beyond three dimensions (latitude, longitude, and elevation/depth), mathematics has no problem extending the concept of multidimensional space into far more dimensions than we can see or meaningfully comprehenend. If a model has ten parameters to fit, for example, the likelihood search space really is ten dimensional, and so on.\nNoticed I used elevation and depth interchangably in the description above. Well, this is because it really doesn’t matter whether an optimisation algorithm is trying to find the greatest elevation over a surface, or the greatest depth over the surface. The aim of maximum likelihood estimation is to find the configuration of parameters that maximises the likelihood, i.e. finds the top of the surface. However we saw that when passing the likelihood function to optim() we often inverted the function by multiplying it by -1. This is because the optimisation algorithms themselves seek to minimise the objective function they’re passed, not maximise it. By multiplying the likelihood function by -1 we made what we were trying to seek compatible with what the optimisation algorithms seek to do: find the greatest depth over a surface, rather than the highest elevation over the surface.\nTo make this all a bit less abstract let’s develop the intuition of an algorithm that seeks to minimise a function by way of a(nother) weird little story:\n\nImagine there is a landscape made out of transparent perspex. It’s not just transparent, it’s invisible to the naked eye. And you want to know where the lowest point of this surface is. All you have to do this is a magical leaking marble. The marble is just like any other marble, except every few moments, at regular intervals (say every tenth of a second), it dribbles out a white dye that you can see. And this dye sticks on and stains the otherwise invisible landscape whose lowest point you wish to find.\n\n\nNow, you drop the marble somewhere on the surface. You see the first point it hits on the surface - a white blob appears. The second blob appears some distance away from the first blob; and the third blob slightly less far away from the second blob as the second was to the second. After a few seconds, a trail of white spots is visible, the first few of which form something like a straight line, each consecutive point slightly less closer to the previous one. A second or two later, and the rumbling sounds of the marble rolling over the surface cease; the marble has clearly run out of momentum. And as you look at the trail of dots it’s generated, and is still generating, and you see it keeps highlighting the same point on the otherwise invisible surface, again and again.\n\nPreviously I used the analogy of a magical robo-chauffer, taking you to the top of a landscape. But the falling marble is probably a closer analogy to how many of optim()’s algorithms actually work. Using gravity and its shape alone, it finds the lowest point on the surface, and with its magical leaking dye, it tells you where this lowest point is.\nNow let’s extend the story to convert the analogy of the barefoot-and-blind person from part seven as well:\n\nThe marble has now ‘told’ you where the lowest point on the invisible surface is. However you also want to know more about the shape of the depression it’s in. You want to know if it’s a steep depression, or a shallow depression. And you want to know if it’s as steep or shallow in every direction, or if it’s steeper in some ways than the other.\n\n\nSo you now have to do a bit more work. You move your hand to just above the marble, and with your forefinger ‘flick’ it in a particular direction (say east-west): you see it move in the direction you flick it briefly, before rolling back towards (and beyond, and then towards) the depression point. As it does so, it leaks dye onto the surface, revealing a bit more about the landscape’s steepness or shallowness in this dimension. Then you do the same, but along a different dimension (say, north-south). After you’ve done this enough times, you are left with a collection of dyed points on the part of the surface closest to its deepest depression. The spacing and shape of these points tells you something about the nature of the depression and the part of the landscape it’s surrounding.\n\nNotice in this analogy you had to do extra work to get the marble to reveal more information about the surface. By default, the marble tells you the specific location of the depression, but not what the surface is like around this point. Instead, you need to intervene twice: firstly by dropping the marble onto the surface; secondly by flicking it around once it’s reached the lowest point on the surface.\nNow, let’s imagine swapping out our magical leaking marble for something even weirder: a magical leaking jumping bean.\n\nThe magical jumping bean does two things: it leaks and it jumps. (Okay, it does three things: when it leaks it also sticks to the surface it’s dying). When the bean is first dropped onto the surface, it marks the location it lands on. Then, it jumps up and across in a random direction. After jumping, it drops onto another part of the surface, marks it, and the process starts again. Jumping, sticking, marking; jumping, sticking, marking; jumping, sticking, marking… potentially forever.\n\n\nBecause of the effect of gravity, though the jumping bean jumps in a random direction, after a few jump-stick-mark steps it’s still, like the marble, very likely to move towards the depression. However, unlike the marble, even when it gets towards the lowest point in the depression, it’s not going to just rest there. The magical jumping bean is never at rest. It’s forever jump-stick-marking, jump-stick-marking.\n\n\nHowever, once the magical bean has moved towards the depression, though it keeps moving, it’s likely never to move too far from the depression. Instead, it’s likely to bounce around the depression. And as it does so, it drops ever more marks on the surface, which keep showing what the surface looks like around the depression in ever more detail.\n\nSo, because of the behaviour of the jumping bean, you only have to act on it once, by choosing where to drop it, rather than twice as with the marble: first choosing where to drop it, then flicking it around once it’s reached the lowest point on the surface."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#so-what",
    "href": "pages/complete-simulation-example/index.html#so-what",
    "title": "Statistical Simulation: A Complete Example",
    "section": "So what?",
    "text": "So what?\nIn the analogies above, the marble is to frequentist statistics as the jumping bean is to Bayesian statistics. A technical distinction between the marble and the jumping bean is that the marble converges towards a point (meaning it reaches a point of rest on the surface) whereas the jumping bean converges towards a distribution (meaning it never rests).\nIt’s Bayesian statistics’ 7 property of converging to a distribution rather than a point that makes the converged posterior distribution of parameter estimates Bayesian models produce ideal for the kind of honest prediction so much of this blog series has been focused on.\nLet’s now do some Bayesian modelling to compare…"
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#bayesian-modelling-now-significantly-less-terrifying-than-it-used-to-be",
    "href": "pages/complete-simulation-example/index.html#bayesian-modelling-now-significantly-less-terrifying-than-it-used-to-be",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Bayesian modelling: now significantly less terrifying than it used to be",
    "text": "Bayesian modelling: now significantly less terrifying than it used to be\nThere are a lot of packages and approaches for building Bayesian models. In fact there are whole statistical programming languages - like JAGS, BUGS 8 and Stan - dedicated to precisely describing every assumption the statistician wants to make about how a Bayesian model should be built. For more complicated and bespoke models these are ideal.\nHowever there are also an increasingly large number of Bayesian modelling packages that abstract away some of the assumptions and complexity apparent in the above specialised Bayesian modelling languages, and allow Bayesian versions of the kinds of model we’re already familiar with to be specified using formulae interfaces almost identical to what we’ve already worked with. Let’s look at one of them, rstanarm, which allows us to use stan, a full Bayesian statistical programming language, without quite as much thinking and set-up being required on our part.\nLet’s try to use this to build a Bayesian equivalent of the hamster tooth model we worked on in the last couple of posts.\n\nData Preparation and Frequentist modelling\nLet’s start by getting the dataset and building the frequentist version of the model we’re already familiar with:\n\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\nbest_model_frequentist &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(best_model_frequentist)\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nBuilding the Bayesian equivalent\nNow how would we build a Bayesian equivalent of this? Firstly let’s load (and if necessary install9) rstanarm.\n\nlibrary(rstanarm)\n\nWhereas for the frequentist model we used the function lm(), rstanarm has what looks like a broadly equivalent function stan_lm(). However, as I’ve just discovered, it’s actually more straightforward with stan_glm instead:\n\nbest_model_bayesian &lt;- stan_glm(len ~ log(dose) * supp, data = df)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000816 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 8.16 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.03 seconds (Warm-up)\nChain 1:                0.031 seconds (Sampling)\nChain 1:                0.061 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 8e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.034 seconds (Warm-up)\nChain 2:                0.032 seconds (Sampling)\nChain 2:                0.066 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.1e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.034 seconds (Warm-up)\nChain 3:                0.032 seconds (Sampling)\nChain 3:                0.066 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 5e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.031 seconds (Warm-up)\nChain 4:                0.034 seconds (Sampling)\nChain 4:                0.065 seconds (Total)\nChain 4: \n\nsummary(best_model_bayesian)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      len ~ log(dose) * supp\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 60\n predictors:   4\n\nEstimates:\n                   mean   sd   10%   50%   90%\n(Intercept)      20.7    0.7 19.7  20.6  21.6 \nlog(dose)         9.2    1.2  7.6   9.2  10.8 \nsuppVC           -3.7    1.0 -4.9  -3.7  -2.4 \nlog(dose):suppVC  3.9    1.8  1.6   3.9   6.2 \nsigma             3.8    0.4  3.3   3.8   4.3 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 18.8    0.7 17.9  18.8  19.7 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                 mcse Rhat n_eff\n(Intercept)      0.0  1.0  3507 \nlog(dose)        0.0  1.0  2194 \nsuppVC           0.0  1.0  3221 \nlog(dose):suppVC 0.0  1.0  2274 \nsigma            0.0  1.0  3084 \nmean_PPD         0.0  1.0  3721 \nlog-posterior    0.0  1.0  1655 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nSome parts of the summary for the Bayesian model look fairly familiar compared with the frequentist model summary; other bits a lot more exotic. We’ll skip over a detailed discussion of these outputs for now, though it is worth comparing the estimates section of the summary directly above, from the Bayesian approach, with the frequentist model produced earlier.\nThe frequentist model had point estimates of \\(\\{20.7, 9.3, -3.7, 3.8\\}\\). The analogous section of the Bayesian model summary is the mean column of the estimates section. These are reported to fewer decimal places by default - Bayesians are often more mindful of spurious precision - but are also \\(\\{20.7, 9.3, -3.7, 3.8\\}\\), so the same to this number of decimal places.\nNote also the Bayesian model reports an estimate for an additional parameter, sigma. This should be expected if we followed along with some of the examples using optim() for linear regression: the likelihood function required the ancillary parameters (referred to as \\(\\alpha\\) in the ‘mother model’ which this series started with, and part of the stochastic component \\(f(.)\\)) be estimated as well as the primary model parameters (referred to as \\(\\beta\\) in the ‘mother model’, and part of the systematic component \\(g(.)\\)). The Bayesian model’s coefficients (Intercept), log(dose), suppVC and the interaction term log(dose):suppVC are all part of \\(\\beta\\), whereas the sigma parameter is part of \\(\\alpha\\). The Bayesian model has just been more explicit about exactly which parameters it’s estimated from the data.\nFor the \\(\\beta\\) parameters, the Std. Error column in the Frequentist model summary is broadly comparable with the sd column in the Bayesian model summary. For the \\(\\beta\\) parameters these values are \\(\\{0.7, 1.2, 1.0, 1.7\\}\\) in the Frequentist model, and \\(\\{0.7, 1.2, 1.0, 1.7\\}\\) in the Bayesian model the summary. i.e. they’re the same to the degree of precision offered in the Bayesian model summary.\nBut let’s get to the crux of the argument: with Bayesian models honest predictions are easier.\nAnd they are, with the posterior_predict() function, passing what we want to predict on through the newdata argument, much as we did with the predict() function with frequentist models.\n\n\nScenario modelling\nLet’s recall the scenarios we looked at previously:\n\npredicted and expected values: length when dosage is 1.25mg and supplement is OJ\nfirst difference difference between OJ and VC supplement when dosage is 1.25mg\n\nLet’s start with the first question:\n\npredictors &lt;- data.frame(supp = \"OJ\", dose = 1.25)\n\npredictions &lt;- rstanarm::posterior_predict(\n    best_model_bayesian,\n    newdata = predictors\n)\n\nhead(predictions)\n\n            1\n[1,] 19.53831\n[2,] 21.74326\n[3,] 27.11863\n[4,] 25.79660\n[5,] 20.25985\n[6,] 21.96020\n\ndim(predictions)\n\n[1] 4000    1\n\n\nBy default posterior_predict() returns a matrix, which in this case has 4000 rows and just a single column. Let’s do a little work on this and visualise the distribution of estimates it produces:\n\npreds_df &lt;- tibble(estimate = predictions[,1])\n\n# lower, median, upper\nlmu &lt;- quantile(preds_df$estimate, c(0.025, 0.500, 0.975))\n\nlwr &lt;- lmu[1]\nmed &lt;- lmu[2]\nupr &lt;- lmu[3]\n\npreds_df |&gt;\n    mutate(\n        in_range = between(estimate, lwr, upr)\n    ) |&gt;\n    ggplot(aes(x = estimate, fill = in_range)) + \n    geom_histogram(bins = 100) + \n    scale_fill_manual(\n        values = c(`FALSE` = 'lightgray', `TRUE` = 'darkgray')\n    ) +\n    theme(legend.position = \"none\") + \n    geom_vline(xintercept = med, linewidth = 1.2, colour = \"steelblue\")\n\n\n\n\nThe darker-shaded parts of the histogram show the 95% uncertainty interval, and the blue vertical line the median estimate. This 95% interval range is 15.15 to 30.34.\nRemember we previously estimated both the expected values and the predicted values for this condition. Our 95% range for the expected values were 20.27 to 24.19 (or thereabouts), whereas our 95% range for the predicted values were (by design) wider, at 15.34 to 30.11. The 95% uncertainty interval above is therefore of predicted values, which include fundamental variation due to the ancillary parameters \\(\\sigma\\), rather than expected values, which result from parameter uncertainty alone.\nThere are a couple of other functions in rstanarm we can look at: predictive_error() and predictive_interval()\nFirst here’s predictive_interval. It is a convenience function that the posterior distribution generated previously, predictions, and returns an uncertainty interval:\n\npredictive_interval(\n    predictions\n)\n\n       5%      95%\n1 16.2481 29.01502\n\n\nWe can see by default the intervals returned are from 5% to 95%, i.e. are the 90% intervals rather than the 95% intervals considered previously. We can change the intervals requested with the prob argument:\n\npredictive_interval(\n    predictions, \n    prob = 0.95\n)\n\n      2.5%    97.5%\n1 15.14936 30.33882\n\n\nAs expected, this requested interval returns an interval closer to (but not identical to) the interval estimated using the quantile function.\nLet’s see if we can also use the model directly, specifying newdata directly to predictive_interval:\n\npredictive_interval(\n    best_model_bayesian,\n    newdata = predictors, \n    prob = 0.95\n)\n\n      2.5%    97.5%\n1 15.22315 30.33166\n\n\nYes. This approach works too. The values aren’t identical as, no doubt, a more sophisticated approach is used by predictive_interval to estimate the interval than simply arranging the posterior estimates in order using quantile.\nFor producing expected values we can use the function posterior_epred:\n\nepreds &lt;- posterior_epred(\n    best_model_bayesian,\n    newdata = predictors\n)\n\nexp_values &lt;- epreds[,1]\n\nquantile(exp_values, probs = c(0.025, 0.500, 0.975))\n\n    2.5%      50%    97.5% \n21.22628 22.70983 24.18352 \n\n\nFor comparison, the expected value 95% interval we obtained from the Frequentist model was 21.3 to 24.2 when drawing from the quasi-posterior distribution, and 22.7 to 24.2 when using the predict() function with the interval argument set to \"confidence\".\nNow, finally, let’s see if we can produce first differences: the estimated effect of using VC rather than OJ as a supplement when the dose is 1.25mg\n\npredictors_x0 &lt;- data.frame(supp = \"OJ\", dose = 1.25)\npredictors_x1 &lt;- data.frame(supp = \"VC\", dose = 1.25)\n\npredictors_fd &lt;- rbind(predictors_x0, predictors_x1)\n\npredictions_fd &lt;- rstanarm::posterior_predict(\n    best_model_bayesian,\n    newdata = predictors_fd\n)\n\nhead(predictions_fd)\n\n            1        2\n[1,] 19.58169 17.93272\n[2,] 24.06914 13.56129\n[3,] 21.66818 24.05286\n[4,] 28.27754 21.81103\n[5,] 27.50420 21.87387\n[6,] 26.59873 14.80452\n\n\nThe newdata argument to posterior_predict now has two rows, one for the OJ supplement and the other for the VC supplement scenario. And the predictions matrix returned by posterior_predict now has two columns: one for each scenario (row) in predictors_fd. We can look at the distribution of both of these columns, as well as the rowwise comparisions between columns, which will give our distribution of first differences for the predicted values:\n\npreds_fd_df &lt;- \n    predictions_fd |&gt;\n        as_tibble(rownames = \"draw\") |&gt;\n        rename(x0 = `1`, x1 = `2`) |&gt;\n        mutate(fd = x1 - x0)\n\npreds_fd_df |&gt; \n    select(-fd) |&gt;\n    pivot_longer(cols = c(\"x0\", \"x1\"), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    facet_wrap(~ scenario, nrow = 2)\n\n\n\n\nTo reiterate, these are predicted values for the two scenarios, not the expected values shown in the first differences section of post 12. This explains why there is greater overlap between the two distributions. Let’s visualise and calculate the first differences in predicted values:\n\npreds_fd_df |&gt;\n    select(fd) |&gt;\n    ggplot(aes(x = fd)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\nWe can see that the average of the distribution is below 0, but as we are looking at predicted values the range of distributions is much higher. Let’s get 95% intervals:\n\nquantile(preds_fd_df$fd, probs = c(0.025, 0.500, 0.975))\n\n      2.5%        50%      97.5% \n-13.425527  -2.987180   7.656461 \n\n\nThe 95% intervals for first differences in predicted values is from -13.6 to +7.9, with the median estimate at -3.0. As expected, the median is similar to the equivalent value from using expected values (-2.9) but the range is wider.\nNow let’s use posterior_epred to produce estimates of first differences in expected values, which will be more directly comparable to our first differences estimates in part 12:\n\npredictions_fd_ev &lt;- posterior_epred(\n    best_model_bayesian,\n    newdata = predictors_fd\n)\n\nhead(predictions_fd_ev)\n\n          \niterations        1        2\n      [1,] 22.87146 19.45426\n      [2,] 22.13428 19.98284\n      [3,] 22.03512 20.71388\n      [4,] 24.11545 20.26789\n      [5,] 23.27148 19.55058\n      [6,] 22.82728 20.14719\n\n\n\npreds_fd_df_ev &lt;- \n    predictions_fd_ev |&gt;\n        as_tibble(rownames = \"draw\") |&gt;\n        rename(x0 = `1`, x1 = `2`) |&gt;\n        mutate(fd = x1 - x0)\n\npreds_fd_df_ev |&gt; \n    select(-fd) |&gt;\n    pivot_longer(cols = c(\"x0\", \"x1\"), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    facet_wrap(~ scenario, nrow = 2)\n\n\n\n\nThis time, as the stochastic variation related to the \\(\\sigma\\) term has been removed, the distributions of the expected values are more distinct, with less overlap. Let’s visualise and compare the first differences of the expected values:\n\npreds_fd_df_ev |&gt;\n    select(fd) |&gt;\n    ggplot(aes(x = fd)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nquantile(preds_fd_df_ev$fd, probs = c(0.025, 0.500, 0.975))\n\n      2.5%        50%      97.5% \n-4.8901131 -2.8312784 -0.6689083 \n\n\nWe now have a 95% interval for the first difference in expected values of -4.9 to -0.7. By contrast, the equivalent range estimated using the Frequentist model in part 12 was -4.8 to -0.8. So, although they’re not identical, they do seem to be very similar."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#summing-up",
    "href": "pages/complete-simulation-example/index.html#summing-up",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Summing up",
    "text": "Summing up\nUp until now we’ve been using Frequentist approaches to modelling. However the simulation approach required to produce honest uncertainty depends on ‘tricking’ Frequentist models into producing something like the converged posterior distributions which, in Bayesian modelling approaches, come ‘for free’ from the way in which Bayesian frameworks estimate model parameters.\nAlthough Bayesian models are generally more technically and computationally demanding than Frequentist models, we have shown the folllowing:\n\nThat packages like rstanarm abstract away some of the challenges of building Bayesian models from scratch;\nThat the posterior distributions produced by Bayesian models produce estimates of expected values, predicted values, and first differences - our substantive quantities of interest - that are similar to those produced previously from Frequentist models\nThat for the estimation of these quantities of interest, the posterior distributions Bayesian models generate make it more straightforward, not less, to produce using Bayesian methods than using Frequentist methods.\n\nThanks for reading, and congratulations on getting this far through the series."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#footnotes",
    "href": "pages/complete-simulation-example/index.html#footnotes",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr perhaps more accurately Bayesian statistical model estimation rather than Bayesian statistics more generally? Bayes’ Rule can be usefully applied to interpret results derived from frequentist models. But the term Bayesian Modelling generally implies that Bayes’ Rule is used as part of the model parameter estimation process, in which a prior distribution is updated according to some algorithm, and then crucially the posterior distribution produced then forms the prior distribution at the next step in the estimation. The specific algorithm that works as the ‘jumping bean’ is usually something like Hamiltonian Monte Carlo, HMC, and the general simulation framework in which a posterior distribution generated from applying Bayes’ Rule is repeatedly fed back into the Bayes’ Rule equation as the prior distribution is known as Markov Chain Monte Carlo, MCMC.↩︎\nBecause the simulation approach relies on random numbers, the draws will never be the same unless the same random number seed is using using set.seed(). However with more simulations, using the n parameter from mvrnorm, the distributions of estimates should become ever closer to each other.↩︎\nIn this example, our more complex model has coefficients fit from the data for the intercept, log(dose), supp and the interaction term log(dose):supp, whereas the less complex model has coefficients fit from the data for the intercept, log(dose), and supp. This means the less complex model can be specified as a restricted version of the more complex model, where the value of the coefficient on the interaction term log(dose):supp is set to be equal to zero, rather than determined from the data. An equivalent way of phrasing and thinking about this is that the two model specifications are nested, with the restricted model nested inside the unrestricted model, which includes the interaction term. It’s this requirement for models to be nested in this way which meant that mod_01 and mod_02 could not be compared using an F-test, as neither model could be described strictly as restricted variants of the other model: they’re siblings, not mothers and daughters. However, both mod_01 and mod_02 could be compared against a common ancestor model which only includes the intercept term.↩︎\nOr perhaps more accurately Bayesian statistical model estimation rather than Bayesian statistics more generally? Bayes’ Rule can be usefully applied to interpret results derived from frequentist models. But the term Bayesian Modelling generally implies that Bayes’ Rule is used as part of the model parameter estimation process, in which a prior distribution is updated according to some algorithm, and then crucially the posterior distribution produced then forms the prior distribution at the next step in the estimation. The specific algorithm that works as the ‘jumping bean’ is usually something like Hamiltonian Monte Carlo, HMC, and the general simulation framework in which a posterior distribution generated from applying Bayes’ Rule is repeatedly fed back into the Bayes’ Rule equation as the prior distribution is known as Markov Chain Monte Carlo, MCMC.↩︎\nBecause the simulation approach relies on random numbers, the draws will never be the same unless the same random number seed is using using set.seed(). However with more simulations, using the n parameter from mvrnorm, the distributions of estimates should become ever closer to each other.↩︎\nOr perhaps more accurately Bayesian statistical model estimation rather than Bayesian statistics more generally? Bayes’ Rule can be usefully applied to interpret results derived from frequentist models. But the term Bayesian Modelling generally implies that Bayes’ Rule is used as part of the model parameter estimation process, in which a prior distribution is updated according to some algorithm, and then crucially the posterior distribution produced then forms the prior distribution at the next step in the estimation. The specific algorithm that works as the ‘jumping bean’ is usually something like Hamiltonian Monte Carlo, HMC, and the general simulation framework in which a posterior distribution generated from applying Bayes’ Rule is repeatedly fed back into the Bayes’ Rule equation as the prior distribution is known as Markov Chain Monte Carlo, MCMC.↩︎\nOminously named.↩︎\nrstanarm has a lot of dependencies. It’s the friendly, cuddly face of a beast!↩︎"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html",
    "title": "Part Seven: Feeling Uncertain",
    "section": "",
    "text": "In the previous post we managed to use numerical optimisation, with the optim() function, to good \\(\\beta\\) estimates for linear regression model fit to some toy data. In this post, we will explore how the optim() function can be used to produce estimates of uncertainty about these \\(\\beta\\) coefficients, and how these relates to measures of uncertainty presented in the standard lm and glm summary functions."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#aim",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#aim",
    "title": "Part Seven: Feeling Uncertain",
    "section": "",
    "text": "In the previous post we managed to use numerical optimisation, with the optim() function, to good \\(\\beta\\) estimates for linear regression model fit to some toy data. In this post, we will explore how the optim() function can be used to produce estimates of uncertainty about these \\(\\beta\\) coefficients, and how these relates to measures of uncertainty presented in the standard lm and glm summary functions."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#prereqs",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#prereqs",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Prereqs",
    "text": "Prereqs\nAs before, we’ll be using the same toy dataset, and same log likelihood function, as in the last two posts in this series. Let’s create these again:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nLet’s also run and save our parameter estimates produced both ‘the hard way’ (using optim), and ‘the easier way’ (using ‘glm’)\n\n\nCode\noptim_results &lt;-  optim(\n    # par contains our initial guesses for the three parameters to estimate\n    par = c(0, 0, 0), \n\n    # by default, most optim algorithms prefer to search for a minima (lowest point) rather than maxima \n    # (highest point). So, I'm making a function to call which simply inverts the log likelihood by multiplying \n    # what it returns by -1\n    fn = function(par, y, X) {-llNormal(par, y, X)}, \n\n    # in addition to the par vector, our function also needs the observed output (y)\n    # and the observed predictors (X). These have to be specified as additional arguments.\n    y = y, X = X\n    )\n\noptim_results\n\n\n$par\n[1]  2.460571  1.375421 -1.336209\n\n$value\n[1] -1.51397\n\n$counts\nfunction gradient \n     216       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nCode\npars_optim &lt;- optim_results$par\n\nnames(pars_optim) &lt;- c(\"beta0\", \"beta1\", \"eta\")\n\npars_optim\n\n\n    beta0     beta1       eta \n 2.460571  1.375421 -1.336209 \n\n\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- tibble(x = x, y = y)\nmod_glm &lt;- glm(y ~ x, data = df, family = gaussian(link=\"identity\"))\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nSo, both optim and the summary to mod_glm report \\(\\{\\beta_0 = 2.36, \\beta_1 = 1.38\\}\\), so both approaches appear to arrive at the same point on the log likelihood surface.\nHowever, note that the glm summary reports not just the estimates themselves (in the Estimate column of coefficients), but also standard errors (the Std. Error column) and derived quantities (t value, Pr(&gt;|t|), and the damnable stars at the very right of the table). How can these measures of uncertainty about the true value of the \\(\\beta\\) coefficients be derived from optim?"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#summary",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#summary",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Summary",
    "text": "Summary\nThis is probably the most difficult single section so far. Don’t worry: it’s likely to get easier from here on in."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#coming-up",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#coming-up",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Coming up",
    "text": "Coming up\nThe next part of the series goes into more detail about how numerical optimisation works."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#propagating-uncertainty",
    "href": "pages/complete-simulation-example/index.html#propagating-uncertainty",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Propagating uncertainty",
    "text": "Propagating uncertainty\nIn this section we will start with the model developed above and look at other information that can be recovered from it, information that will allow the effects of joint parameter uncertainty to be propagated through to prediction.\n\nRecap of core concepts\nBack in Section Two we stated that estimates of the cloud of uncertainty in model parameters, that results from having limited numbers of observations in the data, can be represented as:\n\\[\n\\tilde{\\theta} \\sim MVN(\\mu = \\dot{\\theta}, \\sigma^2 = \\Sigma)\n\\]\nWhere MVN means multivariate normal, and needs the two quantities \\(\\dot{\\theta}\\) and \\(\\Sigma\\) as parameters.\nPreviously we showed how to extract (estimates of) these two quantities from optim(), where the first quantity, \\(\\dot{\\theta}\\), was taken from the converged parameter point estimate slot par, and the second quantity, \\(\\Sigma\\), was derived from the hessian slot.\nBut we don’t need to use optim() directly in order to recover these quantities. Instead we can get them from the standard model objects produced by either lm() or glm(). Let’s check this out…\nWith the model developed previously, let’s now look at some convenience functions, other than just summary, that work with lm() and glm() objects, and recover the quantities required from MVN to represent the uncertainty cloud.\n\n\nExtracting quantities for modelling uncertainty\nFirstly, for the point estimates \\(\\dot{\\theta}\\), we can use the coefficients() function\n\n## Building our model \n\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\nbest_model &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(best_model)\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\ncoef &lt;- coefficients(best_model)\n\ncoef\n\n     (Intercept)        log(dose)           suppVC log(dose):suppVC \n       20.663333         9.254889        -3.700000         3.844782 \n\n\nAnd for the variance-covariance matrix, for representing joint uncertainty about the above estimates, we can use the vcov function\n\nSig &lt;- vcov(best_model)\n\nSig\n\n                   (Intercept)     log(dose)        suppVC log(dose):suppVC\n(Intercept)       4.612422e-01 -8.768056e-17 -4.612422e-01    -7.224251e-17\nlog(dose)        -8.768056e-17  1.440023e+00  1.753611e-16    -1.440023e+00\nsuppVC           -4.612422e-01  1.753611e-16  9.224843e-01     1.748938e-16\nlog(dose):suppVC -7.224251e-17 -1.440023e+00  1.748938e-16     2.880045e+00\n\n\nFinally, we can extract the point estimate for stochastic variation in the model, i.e. variation assumed by the model even if parameter uncertainty were minimised, using the sigma function:\n\nsig &lt;- sigma(best_model)\n\nsig\n\n[1] 3.719847\n\n\nWe now have three quantities, coef, Sig and sig (note the upper and lower case s in the above). These provide something almost but not exactly equivalent to the contents of par and that derived from hessian when using optim() previously. The section below explains this distinction in more detail.\n\nBack to the weeds (potentially skippable)\nRecall the ‘grandmother formulae’, from King, Tomz, and Wittenberg (2000), which the first few posts in this series started with:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nFor standard linear regression this becomes:\nStochastic Component\n\\[\nY_i \\sim Norm(\\theta_i, \\sigma^2)\n\\]\nSystematic Component\n\\[\n\\theta_i =X_i \\beta\n\\]\nOur main parameters are \\(\\theta\\), which combined our predictors \\(X_i\\) and our model parameter estimates \\(\\beta\\). Of these two components we know the data - they are what they are - but are merely estimating our model parameters \\(\\beta\\). So, any estimation uncertainty in this part of the equation results from \\(\\beta\\) alone.\nOur ancillary parameter is \\(\\sigma^2\\). This is our estimate of how much fundamental variation there is in how the data (the response variables \\(Y\\)) is drawn from the stochastic data generating process.\nWhen we used optim() directly, we estimated \\(\\sigma^2\\) along with the other \\(\\beta\\) parameters, via the \\(\\eta\\) parameter eta, defined as \\(\\sigma^2 = e^{\\eta}\\) to allow optim() to search over an unbounded real number range. If there are k \\(\\beta\\) parameters, therefore, optim()’s par vector contained k + 1 values, with this last value being the point estimate for the eta parameter. Similarly, the number of rows, columns, and length of diagonal elements in the variance-covariance matrix recoverable through optim’s hessian slot was also k + 1 rather than k, with the last row, last column, and last diagonal element being measures of covariance between \\(\\eta\\) and the \\(\\beta\\) elements, and variance in \\(\\eta\\) itself.\nBy contrast, the length of coefficients returned by coefficients(best_model) is k, the number of \\(\\beta\\) parameters being estimated, and the dimensions of vcov(best_model) returned are also k by k.\nThis means there is one fewer piece/type of information about model parameters returned by coefficients(model), vcov(model) and sigma(model) than was potentially recoverable by optim()’s par and hessian parameter slots: namely, uncertainty about the true value of the ancillary parameter \\(\\sigma^2\\). The following table summarises this difference:\n\n\n\n\n\n\n\n\nInformation type\nvia optim\nvia lm and glm\n\n\n\n\nMain parameters: point\nfirst k elements of par\ncoefficients() function\n\n\nMain parameters: uncertainty\nfirst k rows and columns of hessian\nvcov() function\n\n\nAncillary parameters: point\nk+1th through to last element of par\nsigma() function or equivalent for glm()\n\n\nAncillary parameters: uncertainty\nlast columns and rows of hessian (after rows and columns k)\n—\n\n\n\nSo long as capturing uncertainty about the fundamental variability in the stochastic part of the model isn’t critical to our predictions then omission of a measure of uncertainty in the ancillary parameters \\(\\alpha\\) is likely a price worth paying for the additional convenience of being able to use the model objects directly. However we should be aware that, whereas with optim we potentially have both \\(\\tilde{\\beta}\\) and \\(\\tilde{\\alpha}\\) to represent model uncertainty, when using the three convenience functions coefficients(), vcov() and sigma() we technically ‘only’ have \\(\\tilde{\\beta}\\) and \\(\\dot{\\alpha}\\) (i.e. point estimates alone for the ancillary parameters).\nWith the above caveat in mind, let’s now look at using the results of coefficients(), vcov() and sigma() to generate (mostly) honest representations of expected values, predicted values, and first differences\n\n\n\nModel predictions\nAs covered in section two, we can use the mvrnorm function from the MASS package to create \\(\\tilde{\\beta}\\), our parameter estimates with uncertainty:\n\nParameter simulation\n\nbeta_tilde &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = coef, \n    Sigma = Sig\n)\n\nhead(beta_tilde)\n\n     (Intercept) log(dose)    suppVC log(dose):suppVC\n[1,]    19.28142 11.350126 -2.476069         2.156141\n[2,]    20.63281 11.331101 -3.554217         2.296231\n[3,]    20.29771  9.046539 -2.878383         5.427045\n[4,]    19.65064  7.575844 -3.125506         6.400589\n[5,]    20.74269  9.280805 -2.857679         5.190796\n[6,]    21.20619  8.907086 -5.193592         4.511823\n\n\nLet’s first look at each of these parameters individually:\n\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    pivot_longer(everything(), names_to = \"coefficient\", values_to = \"value\") |&gt; \n    ggplot(aes(x = value)) + \n    facet_grid(coefficient ~ .) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\nNow let’s look at a couple of coefficients jointly, to see how they’re correlated. Firstly the association between the intercept and the log dosage:\n\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `(Intercept)`, y = `log(dose)`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\nHere the covariance between the two parameters appears very low. Now let’s look at how log dosage and Vitamin C supplement factor are associated:\n\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `log(dose)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\nAgain, the covariance appears low. Finally, the association between log dose and the interaction term\n\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `log(dose)`, y = `log(dose):suppVC`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\nHere we have a much stronger negative covariance between the two coefficients. Let’s look at the variance-covariance extracted from the model previously to confirm this:\n\nknitr::kable(Sig)\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\nlog(dose)\nsuppVC\nlog(dose):suppVC\n\n\n\n\n(Intercept)\n0.4612422\n0.000000\n-0.4612422\n0.000000\n\n\nlog(dose)\n0.0000000\n1.440023\n0.0000000\n-1.440023\n\n\nsuppVC\n-0.4612422\n0.000000\n0.9224843\n0.000000\n\n\nlog(dose):suppVC\n0.0000000\n-1.440023\n0.0000000\n2.880045\n\n\n\n\n\nHere we can see that the covariance between intercept and log dose is effectively zero, as is the covariance between the intercept and the interaction term, and the covariance between the log(dose) and suppVC factor. However, there is a negative covariance between log dose and the interaction term, i.e. what we have plotted above, and also between the intercept and the VC factor. For completeness, let’s look at this last assocation, which we expect to show negative association:\n\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `(Intercept)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\nYes it is! The parameter estimates follow the covariance provided by Sigma, as we would expect.\n\n\n\nExpected values\nLet’s stay we are initially interested in the expected values for a dosage of 1.25mg, with the OJ (rather than VC) supplement:\n\n# first element is 1 due to intercept\npredictor &lt;- c(1, log(1.25), 0, 0) \n\npredictions_ev &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor %*% this_beta\n)\n\nhead(predictions_ev)\n\n[1] 21.81413 23.16128 22.31638 21.34114 22.81364 23.19374\n\n\nLet’s now get a 95% credible interval:\n\nquantile(predictions_ev, probs = c(0.025, 0.500, 0.975))\n\n    2.5%      50%    97.5% \n21.30076 22.72980 24.13689 \n\n\nSo, the 95% interval for the expected value is between 21.31 and 24.14, with a middle (median) estimate of 22.73.4 Let’s check this against estimates from the predict() function:\n\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'confidence')\n\n      fit      lwr      upr\n1 22.7285 21.26607 24.19093\n\n\nThe expected values using the predict function give a 95% confidence interval of 21.27 to 24.19, with a point estimate of 22.73. These are not identical, as the methods employed are not identical,5 but they are hopefully similar enough to demonstrate they are attempts at getting at the same quantities of interest.\n\n\nPredicted values\nPredicted values also include inherent stochastic variation from the ancillary parameters \\(\\alpha\\), which for linear regression is \\(\\sigma^2\\). We can simply add these only the expected values above to produce predicted values:\n\nn &lt;- length(predictions_ev)\n\nshoogliness &lt;- rnorm(n=n, mean = 0, sd = sig)\n\npredictions_pv &lt;- predictions_ev + shoogliness\n\n\nhead(predictions_pv)\n\n[1] 23.90780 21.48269 18.18728 20.95734 27.29535 20.88399\n\n\nLet’s get the 95% interval from the above using quantile\n\nquantile(predictions_pv, probs = c(0.025, 0.5000, 0.975))\n\n    2.5%      50%    97.5% \n15.49003 22.76191 30.00879 \n\n\nAs expected, the interval is now much wider, with a 95% interval from 15.34 to 30.11. The central estimate should in theory, with an infinite number of runs, be the same, however because of random variation it will never be exactly the same to an arbitrary number of decimal places. In this case, the middle estimate is 22.75, not identical to the central estimate from the expected values distribution of 22.72. The number of simulations can always be increased to produce greater precision if needed.\nLet’s now compare this with the prediction interval produce by the predict function:\n\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'prediction')\n\n      fit      lwr     upr\n1 22.7285 15.13461 30.3224\n\n\nAgain, the interval estimates are not exactly the same, but they are very similar.\n\n\nFirst differences\nIt’s in the production of estimates of first differences - this, compared to that, holding all else constant - that the simulation approach shines for producing estimates with credible uncertainty. In our case, let’s say we are interested in asking:\n\nWhat is the expected effect of using the VC supplement, rather than the OJ supplement, where the dose is 1.25mg?\n\nSo, the first difference is from switching from OJ to VC, holding the other factor constant.\nWe can answer this question by using the same selection of \\(\\tilde{\\beta}\\) draws, but passing two different scenarios:\n\n#scenario 0: supplement is OJ\npredictor_x0 &lt;- c(1, log(1.25), 0, 0) \n\n#scenario 1: supplement is VC\npredictor_x1 &lt;- c(1, log(1.25), 1, 1 * log(1.25)) \n\n\npredictions_ev_x0 &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x0 %*% this_beta\n)\n\npredictions_ev_x1 &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x1 %*% this_beta\n)\n\npredictions_df &lt;- \n    tibble(\n        x0 = predictions_ev_x0,\n        x1 = predictions_ev_x1\n    ) |&gt;\n    mutate(\n        fd = x1 - x0\n    )\n\npredictions_df\n\n# A tibble: 10,000 × 3\n      x0    x1    fd\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  21.8  19.8 -1.99\n 2  23.2  20.1 -3.04\n 3  22.3  20.6 -1.67\n 4  21.3  19.6 -1.70\n 5  22.8  21.1 -1.70\n 6  23.2  19.0 -4.19\n 7  23.4  18.7 -4.66\n 8  23.8  19.6 -4.22\n 9  23.5  19.0 -4.45\n10  22.3  21.0 -1.25\n# ℹ 9,990 more rows\n\n\nLet’s look at the distribution of both scenarios individually:\n\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario != \"fd\") |&gt;\n    ggplot(aes(x = estimate)) + \n    facet_wrap(~scenario, ncol = 1) + \n    geom_histogram(bins = 100)\n\n\n\n\nAnd the distribution of the pairwise differences between them:\n\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario == \"fd\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\nIt’s this last distribution which shows our first differences, i.e. our answer, hedged with an appropriate dose of uncertainty, to the specific question shown above. We can get a 95% interval of the first difference as follows:\n\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario == \"fd\") |&gt; \n    pull('estimate') |&gt;\n    quantile(probs = c(0.025, 0.500, 0.975))\n\n      2.5%        50%      97.5% \n-4.8877866 -2.8385155 -0.7893288 \n\n\nSo, 95% of estimates of the first difference are between -4.85 and -0.81, with the middle of this distribution (on this occasion) being -2.83.\nUnlike with the expected values and predicted values, the predict() function does not return first differences with honest uncertainty in this way. What we have above is something new."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "",
    "text": "The previous post, perhaps the toughest of the series, showed how some special settings within R’s numerical optimisation optim() function can be used to estimate how much uncertainty there is in our estimates of the the model parameters \\(\\beta\\). We covered the concept that information and uncertainty are inversely related: the more information we have, the less uncertain we are, and vice versa. We estimated parameter uncertainty around the point that maximised (log) likelihood by asking the algorithm to take small steps from this highest point in different directions (dimensions, in effect variables), and report how steep the fall is in different directions. Steeper falls along a dimension imply less uncertainty and so more more information and narrower confidence intervals; as usual, the converse is also true. The component returned by optim() which reports the results of this ‘stepping out’ is a square matrix called the Hessian, which can be inverted to produce estimates of the variances and covarainces of each of the parameters being estimated in our model."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#recap",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#recap",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "",
    "text": "The previous post, perhaps the toughest of the series, showed how some special settings within R’s numerical optimisation optim() function can be used to estimate how much uncertainty there is in our estimates of the the model parameters \\(\\beta\\). We covered the concept that information and uncertainty are inversely related: the more information we have, the less uncertain we are, and vice versa. We estimated parameter uncertainty around the point that maximised (log) likelihood by asking the algorithm to take small steps from this highest point in different directions (dimensions, in effect variables), and report how steep the fall is in different directions. Steeper falls along a dimension imply less uncertainty and so more more information and narrower confidence intervals; as usual, the converse is also true. The component returned by optim() which reports the results of this ‘stepping out’ is a square matrix called the Hessian, which can be inverted to produce estimates of the variances and covarainces of each of the parameters being estimated in our model."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#aim",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#aim",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Aim",
    "text": "Aim\nThe aims of this post are to show how estimates of uncertainty around the point estimates produced from the Hessian, based around the curvature measured around the point of maximum likelihood, are similar to those produced using a much more extensive (and computationally intensive) interrogation of the likelihood surface using a grid-search approach. It will also show how representations of joint uncertainty for parameter values can be generated using the multivariate normal distribution."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#comparing-inferred-and-observed-likelihood-surfaces",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#comparing-inferred-and-observed-likelihood-surfaces",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Comparing inferred and observed likelihood surfaces",
    "text": "Comparing inferred and observed likelihood surfaces\nLet’s return once again to the toy dataset used in the last two posts, whose true parameters we know because we made them up; and also the log likelihood function:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nTo extract estimates of uncertainty about the uncertainty of each of these parameters, we used optim() with the options shown below, and then inverted the matrix to go from information to uncertainty.\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\nWe were especially interested in the first two rows and columns of this matrix, as they correspond to uncertainty in \\(\\beta = \\{ \\beta_0, \\beta_1 \\}\\).\n\n\nCode\ninv_hess_betas &lt;- inv_hess[1:2, 1:2]\n\ninv_hess_betas\n\n\n             [,1]         [,2]\n[1,]  0.033577455 -0.004379668\n[2,] -0.004379668  0.004379668\n\n\nBack in part five, we used this same dataset to show how the log likelihood varies for various, equally spaced, candidate values for \\(\\beta_0\\) and \\(\\beta_1\\) (having fixed \\(\\eta = \\exp({\\sigma^2})\\) at its true value). This led to the followng map of the landscape1\n\n\nCode\nlibrary(tidyverse)\ncandidate_param_values &lt;- expand_grid(\n    beta_0 = seq(-15, 15, by = 0.05),\n    beta_1 = seq(-15, 15, by = 0.05)\n)\n\nfeed_to_ll &lt;- function(b0, b1){\n    pars &lt;- c(b0, b1, log(0.25))\n    llNormal(pars, y, X)\n}\n\ncandidate_param_values &lt;- candidate_param_values |&gt;\n    mutate(\n        ll = map2_dbl(beta_0, beta_1, feed_to_ll)\n    )\n\ncandidate_param_values |&gt;\n    ggplot(aes(beta_0, beta_1, z = ll)) + \n    geom_contour_filled() + \n    geom_vline(xintercept = 0) +\n    geom_hline(yintercept = 0) +\n    labs(\n        title = \"Log likelihood as a function of possible values of beta_0 and beta_1\",\n        x = \"beta0 (the intercept)\",\n        y = \"beta1 (the slope)\"\n    )\n\n\n\n\n\nWithin the above we can see that the log likelihood landscape for these two parameters looks like a bivariate normal distribution, we can also see a bit of a slant in this normal distribution. This implies a correlation between the two candidate values. The direction of the slant is downwards from left to right, implying the correlation is negative.\nFirstly let’s check that the correlation between \\(\\beta_0\\) and \\(\\beta_1\\) implied by the Hessian is negative. These are the off-diagonal elements, either first row, second column, or second row, first column:\n\n\nCode\ninv_hess_betas[1,2]\n\n\n[1] -0.004379668\n\n\nCode\ninv_hess_betas[2,1]\n\n\n[1] -0.004379668\n\n\nYes they are!\nAs mentioned previously, the likelihood surface produced by the gridsearch method involves a lot of computations, so a lot of steps, and likely a lot of trial and error, if it were to be used to try to find the maximum likelihood value for the parameters. By contrast, the optim() algorithm typically involves far fewer steps, ‘feeling’ its way up the hill until it reaches a point where there’s nowhere higher. 2 When it then reaches this highest point, it then ‘feels’ the curvature around this point in multiple directions, producing the Hessian. The algorithm doesn’t see the likelihood surface, because it hasn’t travelled along most of it. But the Hessian can be used to infer the likelihood surface, subject to subject (usually) reasonable assumptions.\nWhat are these (usually) reasonable assumptions? Well, that the likelihood surface can be approximated by a multivariate normal distribution, which is a generalisation of the standard Normal distribution over more than one dimensions.3\nWe can use the mvrnorm function from the MASS package, alongside the point estimates and Hessian from optim, in order to produce estimates of \\(\\theta = \\{ \\beta_0, \\beta_1, \\eta \\}\\) which represent reasonable uncertainty about the true values of each of these parameters. Algebraically, this can be expressed as something like the following:\n\\[\n\\tilde{\\theta} \\sim Multivariate Normal(\\mu = \\dot{\\theta}, \\sigma^2 = \\Sigma)\n\\]\nWhere \\(\\dot{\\theta}\\) are the point estimates from optim() and \\(\\Sigma\\) is the implied variance-covariance matrix recovered from the Hessian.\nLet’s create this MVN model and see what kinds of outputs it produces.\n\n\nCode\nlibrary(MASS)\n\npoint_estimates &lt;- fuller_optim_output$par\n\nvcov &lt;- -solve(fuller_optim_output$hessian)\nparam_draws &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = point_estimates, \n    Sigma = vcov\n)\n\ncolnames(param_draws) &lt;- c(\n    \"beta0\", \"beta1\", \"eta\"\n)\n\nhead(param_draws)\n\n\n        beta0    beta1         eta\n[1,] 2.564978 1.375636 -0.30407255\n[2,] 2.440111 1.367774 -1.16815288\n[3,] 2.775332 1.338583 -0.05574937\n[4,] 2.283011 1.481799 -0.26095101\n[5,] 2.695635 1.228565 -1.18369341\n[6,] 2.686818 1.483601 -0.44262363\n\n\nWe can see that mvrnorm(), with these inputs from optim() produces three columns: one for each parameter being estimated \\(\\{ \\beta_0, \\beta_1, \\eta \\}\\). The n argumment indicates the number of draws to take; in this case, 10000. This number of draws makes it easier to see how much variation there is in each of the estimates.\n\n\nCode\ndf_param_draws &lt;- \nparam_draws |&gt;\n    as_tibble(\n        rownames = 'draw'\n    ) |&gt;\n    mutate(\n        sig2 = exp(eta)\n    ) |&gt;\n    pivot_longer(\n        -draw, \n        names_to = \"param\",\n        values_to = \"value\"\n    ) \n    \ndf_param_draws |&gt;\n    ggplot(aes(x = value)) + \n    geom_density() + \n    facet_grid(param ~ .) + \n    geom_vline(xintercept=0)\n\n\n\n\n\nThere are a number of things to note here: firstly, that the average of the \\(\\beta_0\\) and \\(\\beta_1\\) values appear close to their known ‘true’ values of 2.5 and 1.4 respectively. Secondly, that whereas the \\(\\eta\\) values are normally distributed, the \\(\\sigma^2\\) values derived from them are not, and are never below zero; this is the effect of the exponential link between quantities. Thirdly, that the implied values of \\(\\sigma^2\\) do appear to be centred around 0.25, as they should be as \\(\\sigma\\) was set to 0.50 in the model.\nAnd forthly, that the density around \\(\\beta_1\\) is more peaked than around \\(\\beta_0\\). This concords with what we saw previously in the filled contour map: both the horizontal beta0 axis and vertical beta1 axis are on the same scale, but the oval is broader along the horizontal axis than the vertical axis. This in effect implies that we have more information about the true value of \\(\\beta_1\\), the slope, than about the true value of \\(\\beta_0\\), the intercept.\nWe can also use these draws to reproduce something similar to, but not identical to, 4 the previous filled contour map:\n\n\nCode\n# param_draws |&gt;\n#     as_tibble(\n#         rownames = 'draw'\n#     ) |&gt;\n#     ggplot(aes(x = beta0, y = beta1)) + \n#     geom_point(alpha = 0.1) + \n#     coord_cartesian(xlim = c(-10, 10), ylim = c(-10, 10))\n\nparam_draws |&gt;\n    as_tibble(\n        rownames = 'draw'\n    ) |&gt;\n    ggplot(aes(x = beta0, y = beta1)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nOnce again, we see the same qualities as the contour map produced by interrogating the likelihood surface exhaustively: the distribution appears bivariate normal; there is a greater range in the distribution along the beta0 than the beta1 axis; and there is evidence of some negative correlation between the two parameters."
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#summary",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#summary",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Summary",
    "text": "Summary\nThis post has shown how optim(), which in its vanilla state only returns point estimates, can be configured to also calculater and report the Hessian, a record of instantaneous curvature around the point estimates. Even without a fine-grained and exhausive search throughout the likelihood surface, this measure of curvature can be used to produce similar measures of uncertainty to the more exhausive approach, in a fraction of the number of computations.\nMore importantly, it can be used to generate draws of plausible combinations of parameter values, something denoted as \\(\\tilde{\\theta}\\) earlier. This is something especially useful for producing honest quantities of interest, which both tell users of models something they want to know, while also representing how uncertain we are in this knowledge.\nWe’ll cover that in the next post… 5"
  },
  {
    "objectID": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#footnotes",
    "href": "pages/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#footnotes",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’ve narrowed the space between values slightly, and increased the range of permutations of values to search through, for an even more precise recovery of the likelihood landscape.↩︎\nIn practice, the algorithm seeks to minimise the value returned by the function, not maximise it, hence the negative being applied through the argument fnscale = -1 in the control argument. But the principle is identical.↩︎\nThis means that, whereas the standard Normal returns a single output, the Multivariate Normal returns a vector of outputs, one for each parameter in \\(\\theta\\), which should also be the length of the diagonal (or alternatively either the number of rows or columns) of \\(\\Sigma\\).↩︎\nThe values will not be identical because the values for \\(\\eta\\), and so \\(\\sigma^2\\), have not been fixed at the true value in this example.↩︎\nI was expecting to cover it in the current post, but this is probably enough content for now!↩︎"
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#modelling-hamster-tooth-growth",
    "href": "pages/complete-simulation-example/index.html#modelling-hamster-tooth-growth",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Modelling Hamster Tooth Growth",
    "text": "Modelling Hamster Tooth Growth\nLet’s start with one of the built-in datasets, ToothGrowth, which is described as follows:\n\nThe response is the length of odontoblasts (cells responsible for tooth growth) in 60 guinea pigs. Each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods, orange juice or ascorbic acid (a form of vitamin C and coded as VC).\n\nLet’s load the dataset and visualise\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nWhat does it look like?\n\n\nCode\ndf |&gt;\n    ggplot(aes(y = len, x = dose, shape = supp, colour = supp)) + \n    geom_point() + \n    expand_limits(x = 0, y = 0)\n\n\n\n\n\nSo, although this has just three variables, there is some complexity involved in thinking about how the two predictor variables, supp and dose, relate to the response variable len. These include:\n\nWhether the relationship between len and dose is linear in a straightforward sense, or associated in a more complicated wway\nWhether supp has the same effect on len regardless of dose, or whether there is an interaction between dose and supp.\n\n\nStage One: model fitting\nWe can address each of these questions in turn, but should probably start with a model which includes both predictors:\n\n\nCode\nmod_01 &lt;- lm(len ~ dose + supp, data = df)\n\nsummary(mod_01)\n\n\n\nCall:\nlm(formula = len ~ dose + supp, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.600 -3.700  0.373  2.116  8.800 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.2725     1.2824   7.231 1.31e-09 ***\ndose          9.7636     0.8768  11.135 6.31e-16 ***\nsuppVC       -3.7000     1.0936  -3.383   0.0013 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.236 on 57 degrees of freedom\nMultiple R-squared:  0.7038,    Adjusted R-squared:  0.6934 \nF-statistic: 67.72 on 2 and 57 DF,  p-value: 8.716e-16\n\n\nEach term is statistically significant at the conventional thresholds (P &lt; 0.05), with higher doses associated with greater lengths. Compared to OJ, the reference category, a vitamin C (VC) supplement is associated with lower lengths.\nTurning to the first question, about the type of relationship between len and dose, one possibility is that greater doses lead to greater lengths, but there are diminishing marginal returns: the first mg has the biggest marginal effect, then the second mg has a lower marginal effect. An easy way to model this would be to include the log of dose in the regression model, rather than the dose itself.1 We can get a sense of whether this log dose specification might be preferred by plotting the data with a log scale on the x axis, and seeing if the points look like they ‘line up’ better:\n\n\nCode\ndf |&gt;\n    ggplot(aes(y = len, x = dose, shape = supp, colour = supp)) + \n    geom_point() + \n    scale_x_log10() + \n    expand_limits(x = 0.250, y = 0)\n\n\n\n\n\nYes, with this scaling, the points associated with the three dosage regimes look like they line up better. Let’s now build this model specification:\n\n\nCode\nmod_02 &lt;- lm(len ~ log(dose) + supp, data = df)\n\nsummary(mod_02)\n\n\n\nCall:\nlm(formula = len ~ log(dose) + supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2108 -2.9896 -0.5633  2.2842  9.1892 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.6633     0.7033   29.38  &lt; 2e-16 ***\nlog(dose)    11.1773     0.8788   12.72  &lt; 2e-16 ***\nsuppVC       -3.7000     0.9947   -3.72 0.000457 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.852 on 57 degrees of freedom\nMultiple R-squared:  0.755, Adjusted R-squared:  0.7464 \nF-statistic: 87.81 on 2 and 57 DF,  p-value: &lt; 2.2e-16\n\n\nAgain, the same kind of relationship between variables is observed: higher log dose: greater length; and VC rather than OJ is associated with lower growth. But is this model actually any better? The model summary for the linear dose model gives an adjusted \\(R^2\\) of 0.69, whereas for the log dose model the adjusted \\(R^2\\) is 0.75. So, as the data are fundamentally the same,2 this suggests it is. However, as we know that linear regression models are really just another kind of generalised linear models, and that model fitting tends to involve trying to maximise the log likelihood, we can also compare the log likelihoods of the two models, using the logLik() function, and so which is higher:\n\n\nCode\nlogLik(mod_01)\n\n\n'log Lik.' -170.2078 (df=4)\n\n\nCode\nlogLik(mod_02)\n\n\n'log Lik.' -164.5183 (df=4)\n\n\nBoth report the same number of degrees of freedom (‘df’), which shouldn’t be suprising as they involve the same number of parameters. But the log likelihood for mod_02 is higher, which like the Adjusted R-squared metric suggests a better fit.\nAnother approach, which generalises better to other types of model, is to compare the AICs, which are metrics that try to show the trade off between model complexity (based on number of parameters), and model fit (based on the log likelihood). By this criterion, the lower the score, the better the model:\n\n\nCode\nAIC(mod_01, mod_02)\n\n\n       df      AIC\nmod_01  4 348.4155\nmod_02  4 337.0367\n\n\nAs both models have exactly the same number of parameters, it should be of no surprise that mod_02 is still preferred.\nLet’s now address the second question: is there an interaction between dose and supp. This interaction term can be specified in one of two ways:\n\n\nCode\n# add interaction term explicitly, using the : symbol\nmod_03a &lt;- lm(len ~ log(dose) + supp + log(dose) : supp, data = df)\n\n# add interaction term implicitly, using the * symbol \nmod_03b &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(mod_03a)\n\n\n\nCall:\nlm(formula = len ~ log(dose) + supp + log(dose):supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(mod_03b)\n\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nWe can see from the summaries that both ways of specifying the models lead to exactly the same model, with exactly the same estimates, standared errors, adjusted \\(R^2\\)s, and so on. The adjusted \\(R^2\\) is now 0.76, a slight improvement on the 0.75 value for the model without the interaction term. As before, we can also compare the trade-off between additional complexity and improved fit using AIC\n\n\nCode\nAIC(mod_02, mod_03a)\n\n\n        df      AIC\nmod_02   4 337.0367\nmod_03a  5 333.7750\n\n\nSo, the AIC of the more complex model is lower, suggesting a better model, but the additional improvement in fit is small.\nWe can also compare the fit, and answer the question of whether the two models can be compared, in a couple of other ways. Firstly, we can use BIC, AIC’s (usually) stricter cousin, which tends to penalise model complexity more harshly:\n\n\nCode\nBIC(mod_02, mod_03a)\n\n\n        df      BIC\nmod_02   4 345.4140\nmod_03a  5 344.2467\n\n\nEven using BIC, the more complex model is still preferred, though the difference in values is now much smaller.\nThe other way we can compare the models is using an F-test using the anova (analysis of variance) function:\n\n\nCode\nanova(mod_02, mod_03a)\n\n\nAnalysis of Variance Table\n\nModel 1: len ~ log(dose) + supp\nModel 2: len ~ log(dose) + supp + log(dose):supp\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     57 845.91                              \n2     56 774.89  1    71.022 5.1327 0.02737 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere anova compares the two models, notes that the first model can be understood as a restricted variant of the second model,3 and compares the change in model fit between the two models against the change in number of parameters used to fit the model. The key parts of the summary to look at are the F test value, 5.13, and the associated P value, which is between 0.01 and 0.05. This, again, suggests the interaction term is worth keeping.\nSo, after all that, we finally have a fitted model. Let’s look now at making some predictions from it.\n\n\nStage Two: Model predictions\nThe simplest approach to getting model predictions is to use the predict function, passing it a dataframe of values for which we want predictions:\n\n\nCode\npredictor_df &lt;- expand_grid(\n    supp = c('VC', 'OJ'), \n    dose = seq(0.25, 2.25, by = 0.01)\n)\npreds_predictors_df &lt;- predictor_df |&gt;\n    mutate(pred_len = predict(mod_03a, predictor_df))\n\npreds_predictors_df\n\n\n# A tibble: 402 × 3\n   supp   dose pred_len\n   &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 VC     0.25   -1.20 \n 2 VC     0.26   -0.683\n 3 VC     0.27   -0.189\n 4 VC     0.28    0.288\n 5 VC     0.29    0.748\n 6 VC     0.3     1.19 \n 7 VC     0.31    1.62 \n 8 VC     0.32    2.04 \n 9 VC     0.33    2.44 \n10 VC     0.34    2.83 \n# ℹ 392 more rows\n\n\nWe can visualise these predictions as follows, with the predicted values as lines, and the observed values as points:\n\n\nCode\npreds_predictors_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, y = pred_len, colour = supp, linetype = interextrap)) + \n    geom_line() + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\"\n    )\n\n\n\n\n\nIn the above, I’ve shown the lines as solid when they represent interpolations of the data, i.e. are in the range of measured doses, and as dashed when they represent extrapolations from the data, meaning they are are predictions made outside the range of observed values. We can see an obvious issue when we extrapolate too far to the left: for low doses, and for the VC supplement, the model predicts negative tooth lengths. Extrapolation is dangerous! And gets more dangerous the further we extrapolate from available observations.\nWe can also use the predict function to produce uncertainty intervals, either of expected values, or predicted values. By default these are 95% intervals, meaning they are expected to contain 95% of the range of expected or predicted values from the model.\nLet’s first look at expected values, which include uncertainty about parameter estimates, but not observed variation in outcomes:\n\n\nCode\ndf_pred_intvl &lt;- predict(mod_03a, newdata = predictor_df, interval = \"confidence\")\n\npreds_predictors_intervals_df &lt;- \n    bind_cols(predictor_df, df_pred_intvl)\n\npreds_predictors_intervals_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, linetype = interextrap)) + \n    geom_line(aes(y = fit, colour = supp)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = supp), alpha = 0.2) + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\",\n        subtitle = \"Range of expected values\"\n    )\n\n\n\n\n\nAnd the following shows the equivalent prediction intervals, which also incorporate known variance, as well as parameter uncertainty:\n\n\nCode\ndf_pred_intvl &lt;- predict(mod_03a, newdata = predictor_df, interval = \"prediction\")\n\npreds_predictors_intervals_df &lt;- \n    bind_cols(predictor_df, df_pred_intvl)\n\npreds_predictors_intervals_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, linetype = interextrap)) + \n    geom_line(aes(y = fit, colour = supp)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = supp), alpha = 0.2) + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\",\n        subtitle = \"Range of predicted values\"\n    )\n\n\n\n\n\nAs should be clear from the above, and discussion of the difference between expected and predicted values in previous posts, predicted values and expected values are very different, and it is important to be aware of the difference between these two quantities of interest. Regardless, we can see once again how dangerous it is to use this particular model specification to extrapolate beyond the range of observations, expecially for lower doses."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#a-lower-level-example",
    "href": "pages/complete-simulation-example/index.html#a-lower-level-example",
    "title": "Statistical Simulation: A Complete Example",
    "section": "A lower level example",
    "text": "A lower level example\nWe’re now going to use the same hamster tooth growth model we used previously, but do some slightly more complicated things with it, relying a bit less on convenience functions and a bit more on our understanding of some of the fundamentals from Section Two.\n\nRecap of core concepts\nBack in Section Two we stated that estimates of the cloud of uncertainty in model parameters, that results from having limited numbers of observations in the data, can be represented as:\n\\[\n\\tilde{\\theta} \\sim MVN(\\mu = \\dot{\\theta}, \\sigma^2 = \\Sigma)\n\\]\nWhere MVN means multivariate normal, and needs the two quantities \\(\\dot{\\theta}\\) and \\(\\Sigma\\) as parameters.\nPreviously we showed how to extract (estimates of) these two quantities from optim(), where the first quantity, \\(\\dot{\\theta}\\), was taken from the converged parameter point estimate slot par, and the second quantity, \\(\\Sigma\\), was derived from the hessian slot.\nBut we don’t need to use optim() directly in order to recover these quantities. Instead we can get them from the standard model objects produced by either lm() or glm(). Let’s check this out…\nWith the model developed previously, let’s now look at some convenience functions, other than just summary, that work with lm() and glm() objects, and recover the quantities required from MVN to represent the uncertainty cloud.\n\n\nExtracting quantities for modelling uncertainty\nFirstly, for the point estimates \\(\\dot{\\theta}\\), we can use the coefficients() function\n\n\nCode\n## Building our model \n\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nCode\nbest_model &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(best_model)\n\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nCode\ncoef &lt;- coefficients(best_model)\n\ncoef\n\n\n     (Intercept)        log(dose)           suppVC log(dose):suppVC \n       20.663333         9.254889        -3.700000         3.844782 \n\n\nAnd for the variance-covariance matrix, for representing joint uncertainty about the above estimates, we can use the vcov function\n\n\nCode\nSig &lt;- vcov(best_model)\n\nSig\n\n\n                   (Intercept)     log(dose)        suppVC log(dose):suppVC\n(Intercept)       4.612422e-01 -8.768056e-17 -4.612422e-01    -7.224251e-17\nlog(dose)        -8.768056e-17  1.440023e+00  1.753611e-16    -1.440023e+00\nsuppVC           -4.612422e-01  1.753611e-16  9.224843e-01     1.748938e-16\nlog(dose):suppVC -7.224251e-17 -1.440023e+00  1.748938e-16     2.880045e+00\n\n\nFinally, we can extract the point estimate for stochastic variation in the model, i.e. variation assumed by the model even if parameter uncertainty were minimised, using the sigma function:\n\n\nCode\nsig &lt;- sigma(best_model)\n\nsig\n\n\n[1] 3.719847\n\n\nWe now have three quantities, coef, Sig and sig (note the upper and lower case s in the above). These provide something almost but not exactly equivalent to the contents of par and that derived from hessian when using optim() previously. The section below explains this distinction in more detail.\n\nBack to the weeds (potentially skippable)\nRecall the ‘grandmother formulae’, from King, Tomz, and Wittenberg (2000), which the first few posts in this series started with:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nFor standard linear regression this becomes:\nStochastic Component\n\\[\nY_i \\sim Norm(\\theta_i, \\sigma^2)\n\\]\nSystematic Component\n\\[\n\\theta_i =X_i \\beta\n\\]\nOur main parameters are \\(\\theta\\), which combined our predictors \\(X_i\\) and our model parameter estimates \\(\\beta\\). Of these two components we know the data - they are what they are - but are merely estimating our model parameters \\(\\beta\\). So, any estimation uncertainty in this part of the equation results from \\(\\beta\\) alone.\nOur ancillary parameter is \\(\\sigma^2\\). This is our estimate of how much fundamental variation there is in how the data (the response variables \\(Y\\)) is drawn from the stochastic data generating process.\nWhen we used optim() directly, we estimated \\(\\sigma^2\\) along with the other \\(\\beta\\) parameters, via the \\(\\eta\\) parameter eta, defined as \\(\\sigma^2 = e^{\\eta}\\) to allow optim() to search over an unbounded real number range. If there are k \\(\\beta\\) parameters, therefore, optim()’s par vector contained k + 1 values, with this last value being the point estimate for the eta parameter. Similarly, the number of rows, columns, and length of diagonal elements in the variance-covariance matrix recoverable through optim’s hessian slot was also k + 1 rather than k, with the last row, last column, and last diagonal element being measures of covariance between \\(\\eta\\) and the \\(\\beta\\) elements, and variance in \\(\\eta\\) itself.\nBy contrast, the length of coefficients returned by coefficients(best_model) is k, the number of \\(\\beta\\) parameters being estimated, and the dimensions of vcov(best_model) returned are also k by k.\nThis means there is one fewer piece/type of information about model parameters returned by coefficients(model), vcov(model) and sigma(model) than was potentially recoverable by optim()’s par and hessian parameter slots: namely, uncertainty about the true value of the ancillary parameter \\(\\sigma^2\\). The following table summarises this difference:\n\n\n\n\n\n\n\n\nInformation type\nvia optim\nvia lm and glm\n\n\n\n\nMain parameters: point\nfirst k elements of par\ncoefficients() function\n\n\nMain parameters: uncertainty\nfirst k rows and columns of hessian\nvcov() function\n\n\nAncillary parameters: point\nk+1th through to last element of par\nsigma() function or equivalent for glm()\n\n\nAncillary parameters: uncertainty\nlast columns and rows of hessian (after rows and columns k)\n—\n\n\n\nSo long as capturing uncertainty about the fundamental variability in the stochastic part of the model isn’t critical to our predictions then omission of a measure of uncertainty in the ancillary parameters \\(\\alpha\\) is likely a price worth paying for the additional convenience of being able to use the model objects directly. However we should be aware that, whereas with optim we potentially have both \\(\\tilde{\\beta}\\) and \\(\\tilde{\\alpha}\\) to represent model uncertainty, when using the three convenience functions coefficients(), vcov() and sigma() we technically ‘only’ have \\(\\tilde{\\beta}\\) and \\(\\dot{\\alpha}\\) (i.e. point estimates alone for the ancillary parameters).\nWith the above caveat in mind, let’s now look at using the results of coefficients(), vcov() and sigma() to generate (mostly) honest representations of expected values, predicted values, and first differences\n\n\n\nModel predictions\nAs covered in section two, we can use the mvrnorm function from the MASS package to create \\(\\tilde{\\beta}\\), our parameter estimates with uncertainty:\n\nParameter simulation\n\n\nCode\nbeta_tilde &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = coef, \n    Sigma = Sig\n)\n\nhead(beta_tilde)\n\n\n     (Intercept) log(dose)    suppVC log(dose):suppVC\n[1,]    21.61503  9.809988 -3.433280         4.228822\n[2,]    20.42343 10.143190 -2.631949         2.273298\n[3,]    20.95245  9.141178 -2.161788         5.508275\n[4,]    21.63064  9.986450 -4.718762         3.082064\n[5,]    20.64959 10.056879 -3.779049         1.106520\n[6,]    21.60778  9.693264 -4.229600         2.862196\n\n\nLet’s first look at each of these parameters individually:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    pivot_longer(everything(), names_to = \"coefficient\", values_to = \"value\") |&gt; \n    ggplot(aes(x = value)) + \n    facet_grid(coefficient ~ .) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nNow let’s look at a couple of coefficients jointly, to see how they’re correlated. Firstly the association between the intercept and the log dosage:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `(Intercept)`, y = `log(dose)`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nHere the covariance between the two parameters appears very low. Now let’s look at how log dosage and Vitamin C supplement factor are associated:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `log(dose)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nAgain, the covariance appears low. Finally, the association between log dose and the interaction term\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `log(dose)`, y = `log(dose):suppVC`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nHere we have a much stronger negative covariance between the two coefficients. Let’s look at the variance-covariance extracted from the model previously to confirm this:\n\n\nCode\nknitr::kable(Sig)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\nlog(dose)\nsuppVC\nlog(dose):suppVC\n\n\n\n\n(Intercept)\n0.4612422\n0.000000\n-0.4612422\n0.000000\n\n\nlog(dose)\n0.0000000\n1.440023\n0.0000000\n-1.440023\n\n\nsuppVC\n-0.4612422\n0.000000\n0.9224843\n0.000000\n\n\nlog(dose):suppVC\n0.0000000\n-1.440023\n0.0000000\n2.880045\n\n\n\n\n\nHere we can see that the covariance between intercept and log dose is effectively zero, as is the covariance between the intercept and the interaction term, and the covariance between the log(dose) and suppVC factor. However, there is a negative covariance between log dose and the interaction term, i.e. what we have plotted above, and also between the intercept and the VC factor. For completeness, let’s look at this last assocation, which we expect to show negative association:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `(Intercept)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nYes it is! The parameter estimates follow the covariance provided by Sigma, as we would expect.\n\n\n\nExpected values\nLet’s stay we are initially interested in the expected values for a dosage of 1.25mg, with the OJ (rather than VC) supplement:\n\n\nCode\n# first element is 1 due to intercept\npredictor &lt;- c(1, log(1.25), 0, 0) \n\npredictions_ev &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor %*% this_beta\n)\n\nhead(predictions_ev)\n\n\n[1] 23.80406 22.68681 22.99225 23.85905 22.89371 23.77077\n\n\nLet’s now get a 95% credible interval:\n\n\nCode\nquantile(predictions_ev, probs = c(0.025, 0.500, 0.975))\n\n\n    2.5%      50%    97.5% \n21.27584 22.71071 24.17225 \n\n\nSo, the 95% interval for the expected value is between 21.31 and 24.14, with a middle (median) estimate of 22.73.4 Let’s check this against estimates from the predict() function:\n\n\nCode\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'confidence')\n\n\n      fit      lwr      upr\n1 22.7285 21.26607 24.19093\n\n\nThe expected values using the predict function give a 95% confidence interval of 21.27 to 24.19, with a point estimate of 22.73. These are not identical, as the methods employed are not identical,5 but they are hopefully similar enough to demonstrate they are attempts at getting at the same quantities of interest.\n\n\nPredicted values\nPredicted values also include inherent stochastic variation from the ancillary parameters \\(\\alpha\\), which for linear regression is \\(\\sigma^2\\). We can simply add these only the expected values above to produce predicted values:\n\n\nCode\nn &lt;- length(predictions_ev)\n\nshoogliness &lt;- rnorm(n=n, mean = 0, sd = sig)\n\npredictions_pv &lt;- predictions_ev + shoogliness\n\n\nhead(predictions_pv)\n\n\n[1] 21.83280 24.55774 19.38543 23.78526 26.18489 21.07653\n\n\nLet’s get the 95% interval from the above using quantile\n\n\nCode\nquantile(predictions_pv, probs = c(0.025, 0.5000, 0.975))\n\n\n    2.5%      50%    97.5% \n15.23557 22.69762 30.06805 \n\n\nAs expected, the interval is now much wider, with a 95% interval from 15.34 to 30.11. The central estimate should in theory, with an infinite number of runs, be the same, however because of random variation it will never be exactly the same to an arbitrary number of decimal places. In this case, the middle estimate is 22.75, not identical to the central estimate from the expected values distribution of 22.72. The number of simulations can always be increased to produce greater precision if needed.\nLet’s now compare this with the prediction interval produce by the predict function:\n\n\nCode\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'prediction')\n\n\n      fit      lwr     upr\n1 22.7285 15.13461 30.3224\n\n\nAgain, the interval estimates are not exactly the same, but they are very similar.\n\n\nFirst differences\nIt’s in the production of estimates of first differences - this, compared to that, holding all else constant - that the simulation approach shines for producing estimates with credible uncertainty. In our case, let’s say we are interested in asking:\n\nWhat is the expected effect of using the VC supplement, rather than the OJ supplement, where the dose is 1.25mg?\n\nSo, the first difference is from switching from OJ to VC, holding the other factor constant.\nWe can answer this question by using the same selection of \\(\\tilde{\\beta}\\) draws, but passing two different scenarios:\n\n\nCode\n#scenario 0: supplement is OJ\npredictor_x0 &lt;- c(1, log(1.25), 0, 0) \n\n#scenario 1: supplement is VC\npredictor_x1 &lt;- c(1, log(1.25), 1, 1 * log(1.25)) \n\n\npredictions_ev_x0 &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x0 %*% this_beta\n)\n\npredictions_ev_x1 &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x1 %*% this_beta\n)\n\npredictions_df &lt;- \n    tibble(\n        x0 = predictions_ev_x0,\n        x1 = predictions_ev_x1\n    ) |&gt;\n    mutate(\n        fd = x1 - x0\n    )\n\npredictions_df\n\n\n# A tibble: 10,000 × 3\n      x0    x1     fd\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  23.8  21.3 -2.49 \n 2  22.7  20.6 -2.12 \n 3  23.0  22.1 -0.933\n 4  23.9  19.8 -4.03 \n 5  22.9  19.4 -3.53 \n 6  23.8  20.2 -3.59 \n 7  23.5  20.1 -3.42 \n 8  22.7  20.7 -2.01 \n 9  23.5  20.4 -3.09 \n10  23.5  20.0 -3.47 \n# ℹ 9,990 more rows\n\n\nLet’s look at the distribution of both scenarios individually:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario != \"fd\") |&gt;\n    ggplot(aes(x = estimate)) + \n    facet_wrap(~scenario, ncol = 1) + \n    geom_histogram(bins = 100)\n\n\n\n\n\nAnd the distribution of the pairwise differences between them:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario == \"fd\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nIt’s this last distribution which shows our first differences, i.e. our answer, hedged with an appropriate dose of uncertainty, to the specific question shown above. We can get a 95% interval of the first difference as follows:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario == \"fd\") |&gt; \n    pull('estimate') |&gt;\n    quantile(probs = c(0.025, 0.500, 0.975))\n\n\n      2.5%        50%      97.5% \n-4.8337957 -2.8331646 -0.8278675 \n\n\nSo, 95% of estimates of the first difference are between -4.85 and -0.81, with the middle of this distribution (on this occasion) being -2.83.\nUnlike with the expected values and predicted values, the predict() function does not return first differences with honest uncertainty in this way. What we have above is something new.\n\n\nSummary\nIn this subsection we’ve finally combined all the learning we’ve developed over the two previous sectionsto answer three specific ‘what if?’ questions: one on expected values, one on predicted values, and one on first differences. These are what King, Tomz, and Wittenberg (2000) refer to as quantities of interest, and I hope you agree these are more organic and reasonable types of question to ask of data and statistical models than simply looking at coefficients and p-values and reporting which ones are ‘statistically significant’.\nIf you’ve been able to follow everything in these posts, and can generalise the approach shown above to other types of statistical model, then congratulations! You’ve learned the framework for answering meaningful questions using statistical models which is at the heart of one of the toughest methods courses for social scientists offered by one of the most prestigious universities in the world."
  },
  {
    "objectID": "pages/complete-simulation-example/index.html#introducing-bayesian-statistics-on-marbles-and-jumping-beans",
    "href": "pages/complete-simulation-example/index.html#introducing-bayesian-statistics-on-marbles-and-jumping-beans",
    "title": "Statistical Simulation: A Complete Example",
    "section": "Introducing Bayesian Statistics: On marbles and jumping beans",
    "text": "Introducing Bayesian Statistics: On marbles and jumping beans\nSection Two introduced Bayes’ Rule and the Likelihood axiom. It pointed out that, at heart, Bayes’ Rule is a way of expressing that given this in terms of this given that; and that Likelihood is also a claim about how that given this relates to this given that. More specifically, the claim of Likelihood is:\n\nThe likelihood of the model given the data is proportional to the probability of the data given the model.\n\nThere are two aspects to the model: firstly its structure; secondly its parameters. The structure includes the type of statistical model - whether it is a standard linear regression, negative binomial regression, logistic regression, Poisson regression model and so on - and also the specific types of columns from the dataset selected as either predictor variables (\\(X\\)) or response variables (\\(Y\\)). It is only after both the higher level structure of the model family, and the lower level structure of the data inputs (what’s being regressed on what?) have been decided that the Likelihood theory is used.\nAnd how is Likelihood theory used? Well, it defines a landscape over which an algorithm searches. This landscape has as many dimensions as there are parameters to fit. Where there are just two parameters, \\(\\beta_0\\) and \\(\\beta_1\\) to fit, we can visualise this landscape using something like a contour plot, with \\(\\beta_0\\) as latitude, \\(\\beta_1\\) as longitude, and the likelihood at this position its elevation or depth. Each possible joint value \\(\\beta = \\{\\beta_0, \\beta_1\\}\\) which the algorithm might wish to propose leads to a different long-lat coordinate over the surface, and each coordinate has a different elevation or depth. Although we can’t see beyond three dimensions (latitude, longitude, and elevation/depth), mathematics has no problem extending the concept of multidimensional space into far more dimensions than we can see or meaningfully comprehenend. If a model has ten parameters to fit, for example, the likelihood search space really is ten dimensional, and so on.\nNoticed I used elevation and depth interchangably in the description above. Well, this is because it really doesn’t matter whether an optimisation algorithm is trying to find the greatest elevation over a surface, or the greatest depth over the surface. The aim of maximum likelihood estimation is to find the configuration of parameters that maximises the likelihood, i.e. finds the top of the surface. However we saw that when passing the likelihood function to optim() we often inverted the function by multiplying it by -1. This is because the optimisation algorithms themselves seek to minimise the objective function they’re passed, not maximise it. By multiplying the likelihood function by -1 we made what we were trying to seek compatible with what the optimisation algorithms seek to do: find the greatest depth over a surface, rather than the highest elevation over the surface.\nTo make this all a bit less abstract let’s develop the intuition of an algorithm that seeks to minimise a function by way of a(nother) weird little story:\n\nImagine there is a landscape made out of transparent perspex. It’s not just transparent, it’s invisible to the naked eye. And you want to know where the lowest point of this surface is. All you have to do this is a magical leaking marble. The marble is just like any other marble, except every few moments, at regular intervals (say every tenth of a second), it dribbles out a white dye that you can see. And this dye sticks on and stains the otherwise invisible landscape whose lowest point you wish to find.\n\n\nNow, you drop the marble somewhere on the surface. You see the first point it hits on the surface - a white blob appears. The second blob appears some distance away from the first blob; and the third blob slightly less far away from the second blob as the second was to the second. After a few seconds, a trail of white spots is visible, the first few of which form something like a straight line, each consecutive point slightly less closer to the previous one. A second or two later, and the rumbling sounds of the marble rolling over the surface cease; the marble has clearly run out of momentum. And as you look at the trail of dots it’s generated, and is still generating, and you see it keeps highlighting the same point on the otherwise invisible surface, again and again.\n\nPreviously I used the analogy of a magical robo-chauffer, taking you to the top of a landscape. But the falling marble is probably a closer analogy to how many of optim()’s algorithms actually work. Using gravity and its shape alone, it finds the lowest point on the surface, and with its magical leaking dye, it tells you where this lowest point is.\nNow let’s extend the story to convert the analogy of the barefoot-and-blind person from section two as well:\n\nThe marble has now ‘told’ you where the lowest point on the invisible surface is. However you also want to know more about the shape of the depression it’s in. You want to know if it’s a steep depression, or a shallow depression. And you want to know if it’s as steep or shallow in every direction, or if it’s steeper in some ways than the other.\n\n\nSo you now have to do a bit more work. You move your hand to just above the marble, and with your forefinger ‘flick’ it in a particular direction (say east-west): you see it move in the direction you flick it briefly, before rolling back towards (and beyond, and then towards) the depression point. As it does so, it leaks dye onto the surface, revealing a bit more about the landscape’s steepness or shallowness in this dimension. Then you do the same, but along a different dimension (say, north-south). After you’ve done this enough times, you are left with a collection of dyed points on the part of the surface closest to its deepest depression. The spacing and shape of these points tells you something about the nature of the depression and the part of the landscape it’s surrounding.\n\nNotice in this analogy you had to do extra work to get the marble to reveal more information about the surface. By default, the marble tells you the specific location of the depression, but not what the surface is like around this point. Instead, you need to intervene twice: firstly by dropping the marble onto the surface; secondly by flicking it around once it’s reached the lowest point on the surface.\nNow, let’s imagine swapping out our magical leaking marble for something even weirder: a magical leaking jumping bean.\n\nThe magical jumping bean does two things: it leaks and it jumps. (Okay, it does three things: when it leaks it also sticks to the surface it’s dying). When the bean is first dropped onto the surface, it marks the location it lands on. Then, it jumps up and across in a random direction. After jumping, it drops onto another part of the surface, marks it, and the process starts again. Jumping, sticking, marking; jumping, sticking, marking; jumping, sticking, marking… potentially forever.\n\n\nBecause of the effect of gravity, though the jumping bean jumps in a random direction, after a few jump-stick-mark steps it’s still, like the marble, very likely to move towards the depression. However, unlike the marble, even when it gets towards the lowest point in the depression, it’s not going to just rest there. The magical jumping bean is never at rest. It’s forever jump-stick-marking, jump-stick-marking.\n\n\nHowever, once the magical bean has moved towards the depression, though it keeps moving, it’s likely never to move too far from the depression. Instead, it’s likely to bounce around the depression. And as it does so, it drops ever more marks on the surface, which keep showing what the surface looks like around the depression in ever more detail.\n\nSo, because of the behaviour of the jumping bean, you only have to act on it once, by choosing where to drop it, rather than twice as with the marble: first choosing where to drop it, then flicking it around once it’s reached the lowest point on the surface.\n\nSo what?\nIn the analogies above, the marble is to frequentist statistics as the jumping bean is to Bayesian statistics. A technical distinction between the marble and the jumping bean is that the marble converges towards a point (meaning it reaches a point of rest on the surface) whereas the jumping bean converges towards a distribution (meaning it never rests).\nIt’s Bayesian statistics’ 6 property of converging to a distribution rather than a point that makes the converged posterior distribution of parameter estimates Bayesian models produce ideal for the kind of honest prediction so much of this blog series has been focused on.\nLet’s now do some Bayesian modelling to compare…\n\n\nBayesian modelling: now significantly less terrifying than it used to be\nThere are a lot of packages and approaches for building Bayesian models. In fact there are whole statistical programming languages - like JAGS, BUGS 7 and Stan - dedicated to precisely describing every assumption the statistician wants to make about how a Bayesian model should be built. For more complicated and bespoke models these are ideal.\nHowever there are also an increasingly large number of Bayesian modelling packages that abstract away some of the assumptions and complexity apparent in the above specialised Bayesian modelling languages, and allow Bayesian versions of the kinds of model we’re already familiar with to be specified using formulae interfaces almost identical to what we’ve already worked with. Let’s look at one of them, rstanarm, which allows us to use stan, a full Bayesian statistical programming language, without quite as much thinking and set-up being required on our part.\nLet’s try to use this to build a Bayesian equivalent of the hamster tooth model we worked on in the last couple of posts.\n\n\nData Preparation and Frequentist modelling\nLet’s start by getting the dataset and building the frequentist version of the model we’re already familiar with:\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nCode\nbest_model_frequentist &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(best_model_frequentist)\n\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nBuilding the Bayesian equivalent\nNow how would we build a Bayesian equivalent of this? Firstly let’s load (and if necessary install8) rstanarm.\n\n\nCode\nlibrary(rstanarm)\n\n\nWhereas for the frequentist model we used the function lm(), rstanarm has what looks like a broadly equivalent function stan_lm(). However, as I’ve just discovered, it’s actually more straightforward with stan_glm instead:\n\n\nCode\nbest_model_bayesian &lt;- stan_glm(len ~ log(dose) * supp, data = df)\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.35 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.032 seconds (Warm-up)\nChain 1:                0.033 seconds (Sampling)\nChain 1:                0.065 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.031 seconds (Warm-up)\nChain 2:                0.026 seconds (Sampling)\nChain 2:                0.057 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 5e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.032 seconds (Warm-up)\nChain 3:                0.031 seconds (Sampling)\nChain 3:                0.063 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 4e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.029 seconds (Warm-up)\nChain 4:                0.029 seconds (Sampling)\nChain 4:                0.058 seconds (Total)\nChain 4: \n\n\nCode\nsummary(best_model_bayesian)\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      len ~ log(dose) * supp\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 60\n predictors:   4\n\nEstimates:\n                   mean   sd   10%   50%   90%\n(Intercept)      20.6    0.7 19.8  20.7  21.5 \nlog(dose)         9.3    1.2  7.7   9.3  10.8 \nsuppVC           -3.7    1.0 -4.9  -3.7  -2.4 \nlog(dose):suppVC  3.8    1.7  1.7   3.8   6.0 \nsigma             3.8    0.4  3.3   3.7   4.2 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 18.8    0.7 18.0  18.8  19.6 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                 mcse Rhat n_eff\n(Intercept)      0.0  1.0  3815 \nlog(dose)        0.0  1.0  2299 \nsuppVC           0.0  1.0  3644 \nlog(dose):suppVC 0.0  1.0  2636 \nsigma            0.0  1.0  3249 \nmean_PPD         0.0  1.0  3821 \nlog-posterior    0.0  1.0  1908 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nSome parts of the summary for the Bayesian model look fairly familiar compared with the frequentist model summary; other bits a lot more exotic. We’ll skip over a detailed discussion of these outputs for now, though it is worth comparing the estimates section of the summary directly above, from the Bayesian approach, with the frequentist model produced earlier.\nThe frequentist model had point estimates of \\(\\{20.7, 9.3, -3.7, 3.8\\}\\). The analogous section of the Bayesian model summary is the mean column of the estimates section. These are reported to fewer decimal places by default - Bayesians are often more mindful of spurious precision - but are also \\(\\{20.7, 9.3, -3.7, 3.8\\}\\), so the same to this number of decimal places.\nNote also the Bayesian model reports an estimate for an additional parameter, sigma. This should be expected if we followed along with some of the examples using optim() for linear regression: the likelihood function required the ancillary parameters (referred to as \\(\\alpha\\) in the ‘mother model’ which this series started with, and part of the stochastic component \\(f(.)\\)) be estimated as well as the primary model parameters (referred to as \\(\\beta\\) in the ‘mother model’, and part of the systematic component \\(g(.)\\)). The Bayesian model’s coefficients (Intercept), log(dose), suppVC and the interaction term log(dose):suppVC are all part of \\(\\beta\\), whereas the sigma parameter is part of \\(\\alpha\\). The Bayesian model has just been more explicit about exactly which parameters it’s estimated from the data.\nFor the \\(\\beta\\) parameters, the Std. Error column in the Frequentist model summary is broadly comparable with the sd column in the Bayesian model summary. For the \\(\\beta\\) parameters these values are \\(\\{0.7, 1.2, 1.0, 1.7\\}\\) in the Frequentist model, and \\(\\{0.7, 1.2, 1.0, 1.7\\}\\) in the Bayesian model the summary. i.e. they’re the same to the degree of precision offered in the Bayesian model summary.\nBut let’s get to the crux of the argument: with Bayesian models honest predictions are easier.\nAnd they are, with the posterior_predict() function, passing what we want to predict on through the newdata argument, much as we did with the predict() function with frequentist models.\n\n\nScenario modelling\nLet’s recall the scenarios we looked at previously:\n\npredicted and expected values: length when dosage is 1.25mg and supplement is OJ\nfirst difference difference between OJ and VC supplement when dosage is 1.25mg\n\nLet’s start with the first question:\n\n\nCode\npredictors &lt;- data.frame(supp = \"OJ\", dose = 1.25)\n\npredictions &lt;- rstanarm::posterior_predict(\n    best_model_bayesian,\n    newdata = predictors\n)\n\nhead(predictions)\n\n\n            1\n[1,] 19.61122\n[2,] 29.06774\n[3,] 25.37852\n[4,] 24.62945\n[5,] 28.64383\n[6,] 24.33114\n\n\nCode\ndim(predictions)\n\n\n[1] 4000    1\n\n\nBy default posterior_predict() returns a matrix, which in this case has 4000 rows and just a single column. Let’s do a little work on this and visualise the distribution of estimates it produces:\n\n\nCode\npreds_df &lt;- tibble(estimate = predictions[,1])\n\n# lower, median, upper\nlmu &lt;- quantile(preds_df$estimate, c(0.025, 0.500, 0.975))\n\nlwr &lt;- lmu[1]\nmed &lt;- lmu[2]\nupr &lt;- lmu[3]\n\npreds_df |&gt;\n    mutate(\n        in_range = between(estimate, lwr, upr)\n    ) |&gt;\n    ggplot(aes(x = estimate, fill = in_range)) + \n    geom_histogram(bins = 100) + \n    scale_fill_manual(\n        values = c(`FALSE` = 'lightgray', `TRUE` = 'darkgray')\n    ) +\n    theme(legend.position = \"none\") + \n    geom_vline(xintercept = med, linewidth = 1.2, colour = \"steelblue\")\n\n\n\n\n\nThe darker-shaded parts of the histogram show the 95% uncertainty interval, and the blue vertical line the median estimate. This 95% interval range is 14.93 to 30.14.\nRemember we previously estimated both the expected values and the predicted values for this condition. Our 95% range for the expected values were 20.27 to 24.19 (or thereabouts), whereas our 95% range for the predicted values were (by design) wider, at 15.34 to 30.11. The 95% uncertainty interval above is therefore of predicted values, which include fundamental variation due to the ancillary parameters \\(\\sigma\\), rather than expected values, which result from parameter uncertainty alone.\nThere are a couple of other functions in rstanarm we can look at: predictive_error() and predictive_interval()\nFirst here’s predictive_interval. It is a convenience function that the posterior distribution generated previously, predictions, and returns an uncertainty interval:\n\n\nCode\npredictive_interval(\n    predictions\n)\n\n\n        5%      95%\n1 16.18055 28.97743\n\n\nWe can see by default the intervals returned are from 5% to 95%, i.e. are the 90% intervals rather than the 95% intervals considered previously. We can change the intervals requested with the prob argument:\n\n\nCode\npredictive_interval(\n    predictions, \n    prob = 0.95\n)\n\n\n     2.5%   97.5%\n1 14.9348 30.1403\n\n\nAs expected, this requested interval returns an interval closer to (but not identical to) the interval estimated using the quantile function.\nLet’s see if we can also use the model directly, specifying newdata directly to predictive_interval:\n\n\nCode\npredictive_interval(\n    best_model_bayesian,\n    newdata = predictors, \n    prob = 0.95\n)\n\n\n      2.5%    97.5%\n1 14.92205 30.28054\n\n\nYes. This approach works too. The values aren’t identical as, no doubt, a more sophisticated approach is used by predictive_interval to estimate the interval than simply arranging the posterior estimates in order using quantile.\nFor producing expected values we can use the function posterior_epred:\n\n\nCode\nepreds &lt;- posterior_epred(\n    best_model_bayesian,\n    newdata = predictors\n)\n\nexp_values &lt;- epreds[,1]\n\nquantile(exp_values, probs = c(0.025, 0.500, 0.975))\n\n\n    2.5%      50%    97.5% \n21.19042 22.71725 24.12859 \n\n\nFor comparison, the expected value 95% interval we obtained from the Frequentist model was 21.3 to 24.2 when drawing from the quasi-posterior distribution, and 22.7 to 24.2 when using the predict() function with the interval argument set to \"confidence\".\nNow, finally, let’s see if we can produce first differences: the estimated effect of using VC rather than OJ as a supplement when the dose is 1.25mg\n\n\nCode\npredictors_x0 &lt;- data.frame(supp = \"OJ\", dose = 1.25)\npredictors_x1 &lt;- data.frame(supp = \"VC\", dose = 1.25)\n\npredictors_fd &lt;- rbind(predictors_x0, predictors_x1)\n\npredictions_fd &lt;- rstanarm::posterior_predict(\n    best_model_bayesian,\n    newdata = predictors_fd\n)\n\nhead(predictions_fd)\n\n\n            1        2\n[1,] 23.46942 24.60202\n[2,] 22.06149 20.35834\n[3,] 24.07686 19.06467\n[4,] 24.63437 27.01567\n[5,] 15.64022 17.77962\n[6,] 11.45183 34.21425\n\n\nThe newdata argument to posterior_predict now has two rows, one for the OJ supplement and the other for the VC supplement scenario. And the predictions matrix returned by posterior_predict now has two columns: one for each scenario (row) in predictors_fd. We can look at the distribution of both of these columns, as well as the rowwise comparisions between columns, which will give our distribution of first differences for the predicted values:\n\n\nCode\npreds_fd_df &lt;- \n    predictions_fd |&gt;\n        as_tibble(rownames = \"draw\") |&gt;\n        rename(x0 = `1`, x1 = `2`) |&gt;\n        mutate(fd = x1 - x0)\n\npreds_fd_df |&gt; \n    select(-fd) |&gt;\n    pivot_longer(cols = c(\"x0\", \"x1\"), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    facet_wrap(~ scenario, nrow = 2)\n\n\n\n\n\nTo reiterate, these are predicted values for the two scenarios, not the expected values shown in the first differences section of post 12. This explains why there is greater overlap between the two distributions. Let’s visualise and calculate the first differences in predicted values:\n\n\nCode\npreds_fd_df |&gt;\n    select(fd) |&gt;\n    ggplot(aes(x = fd)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nWe can see that the average of the distribution is below 0, but as we are looking at predicted values the range of distributions is much higher. Let’s get 95% intervals:\n\n\nCode\nquantile(preds_fd_df$fd, probs = c(0.025, 0.500, 0.975))\n\n\n      2.5%        50%      97.5% \n-13.838512  -2.860489   7.733071 \n\n\nThe 95% intervals for first differences in predicted values is from -13.6 to +7.9, with the median estimate at -3.0. As expected, the median is similar to the equivalent value from using expected values (-2.9) but the range is wider.\nNow let’s use posterior_epred to produce estimates of first differences in expected values, which will be more directly comparable to our first differences estimates in section two:\n\n\nCode\npredictions_fd_ev &lt;- posterior_epred(\n    best_model_bayesian,\n    newdata = predictors_fd\n)\n\nhead(predictions_fd_ev)\n\n\n          \niterations        1        2\n      [1,] 22.11832 21.00645\n      [2,] 23.24191 19.50229\n      [3,] 22.54839 19.81125\n      [4,] 22.66001 20.25431\n      [5,] 23.12972 19.85558\n      [6,] 21.16934 20.18880\n\n\n\n\nCode\npreds_fd_df_ev &lt;- \n    predictions_fd_ev |&gt;\n        as_tibble(rownames = \"draw\") |&gt;\n        rename(x0 = `1`, x1 = `2`) |&gt;\n        mutate(fd = x1 - x0)\n\npreds_fd_df_ev |&gt; \n    select(-fd) |&gt;\n    pivot_longer(cols = c(\"x0\", \"x1\"), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    facet_wrap(~ scenario, nrow = 2)\n\n\n\n\n\nThis time, as the stochastic variation related to the \\(\\sigma\\) term has been removed, the distributions of the expected values are more distinct, with less overlap. Let’s visualise and compare the first differences of the expected values:\n\n\nCode\npreds_fd_df_ev |&gt;\n    select(fd) |&gt;\n    ggplot(aes(x = fd)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\n\n\nCode\nquantile(preds_fd_df_ev$fd, probs = c(0.025, 0.500, 0.975))\n\n\n      2.5%        50%      97.5% \n-4.9299560 -2.8195143 -0.6882904 \n\n\nWe now have a 95% interval for the first difference in expected values of -4.9 to -0.7. By contrast, the equivalent range estimated using the Frequentist model in part 12 was -4.8 to -0.8. So, although they’re not identical, they do seem to be very similar.\n\n\nBayesian Statistics Summary\nUp until now we’ve been using Frequentist approaches to modelling. However the simulation approach required to produce honest uncertainty depends on ‘tricking’ Frequentist models into producing something like the converged posterior distributions which, in Bayesian modelling approaches, come ‘for free’ from the way in which Bayesian frameworks estimate model parameters.\nAlthough Bayesian models are generally more technically and computationally demanding than Frequentist models, we have shown the folllowing:\n\nThat packages like rstanarm abstract away some of the challenges of building Bayesian models from scratch;\nThat the posterior distributions produced by Bayesian models produce estimates of expected values, predicted values, and first differences - our substantive quantities of interest - that are similar to those produced previously from Frequentist models\nThat for the estimation of these quantities of interest, the posterior distributions Bayesian models generate make it more straightforward, not less, to produce using Bayesian methods than using Frequentist methods.\n\nThanks for reading, and congratulations on getting this far through the series."
  }
]