{
  "hash": "bd3780f70d84ad3d2bedfbb17fcb6013",
  "result": {
    "markdown": "---\ntitle: \"Part Seven: Feeling Uncertain\"\nauthor: \"Jon Minton\"\ndate: \"2024-01-04\"\ncode-fold: true\nwarning: false\nmessage: false\ncategories: [statistics, R]\nbibliography: references.bib\n---\n\n\n## Aim \n\nIn [the previous post](../lms-are-glms-part-06/index.qmd) we managed to use numerical optimisation, with the `optim()` function, to good $\\beta$ estimates for linear regression model fit to some toy data. In this post, we will explore how the `optim()` function can be used to produce estimates of uncertainty about these $\\beta$ coefficients, and how these relates to measures of uncertainty presented in the standard `lm` and `glm` `summary` functions. \n\n## Prereqs \n\nAs before, we'll be using the same toy dataset, and same log likelihood function, as in the last two posts in this series. Let's create these again:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nllNormal <- function(pars, y, X){\n    beta <- pars[1:ncol(X)]\n    sigma2 <- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx <- (-3):5\n# Record the number of observations in x\nN <- length(x)\n# Create a response variable with variability\ny <- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX <- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) <- NULL\n```\n:::\n\n\nLet's also run and save our parameter estimates produced both 'the hard way' (using `optim`), and 'the easier way' (using 'glm')\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptim_results <-  optim(\n    # par contains our initial guesses for the three parameters to estimate\n    par = c(0, 0, 0), \n\n    # by default, most optim algorithms prefer to search for a minima (lowest point) rather than maxima \n    # (highest point). So, I'm making a function to call which simply inverts the log likelihood by multiplying \n    # what it returns by -1\n    fn = function(par, y, X) {-llNormal(par, y, X)}, \n\n    # in addition to the par vector, our function also needs the observed output (y)\n    # and the observed predictors (X). These have to be specified as additional arguments.\n    y = y, X = X\n    )\n\noptim_results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$par\n[1]  2.460571  1.375421 -1.336209\n\n$value\n[1] -1.51397\n\n$counts\nfunction gradient \n     216       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n```\n:::\n\n```{.r .cell-code}\npars_optim <- optim_results$par\n\nnames(pars_optim) <- c(\"beta0\", \"beta1\", \"eta\")\n\npars_optim\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    beta0     beta1       eta \n 2.460571  1.375421 -1.336209 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndf <- tibble(x = x, y = y)\nmod_glm <- glm(y ~ x, data = df, family = gaussian(link=\"identity\"))\nsummary(mod_glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n```\n:::\n:::\n\n\nSo, both optim and the summary to `mod_glm` report $\\{\\beta_0 = 2.36, \\beta_1 = 1.38\\}$, so both approaches appear to arrive at the same point on the log likelihood surface. \n\nHowever, note that the glm summary reports not just the estimates themselves (in the `Estimate` column of coefficients), but also standard errors (the `Std. Error` column) and derived quantities (`t value`, `Pr(>|t|)`, and the damnable stars at the very right of the table). How can these measures of uncertainty about the true value of the $\\beta$ coefficients be derived from `optim`?\n\n\n\n\n## Summary\n\nThis is probably the most difficult single section so far. Don't worry: it's likely to get easier from here on in. \n\n## Coming up\n\nThe [next part of the series](../lms-are-glms-part-08/index.qmd) goes into more detail about how numerical optimisation works. ",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}