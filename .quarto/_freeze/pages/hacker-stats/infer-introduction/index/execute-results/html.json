{
  "hash": "ee505e79d23a66004291db78e0e7e383",
  "result": {
    "markdown": "---\ntitle: \"Getting started with the infer package\"\nauthor: \"Jon Minton\"\ndate: \"2024-07-16\"\nresampling-order: 4\ncategories: [statistics, r, hypothesis tests, resampling, bootstrapping, hacker stats]\n---\n\n\n## Introduction \n\nThis post continues a short series on resampling methods, sometimes also known as 'Hacker Stats', for hypothesis testing. To recap: resampling *with replacement* is known as **bootstrapping**. Resampling *without replacement* can be used for **permutation tests**: testing whether *apparent* patterns in the data, including *apparent* associations between variables in the data, could likely have emerged from the Null distribution. \n\nIn [a previous post introducing bootstrapping](../bootstrapping/index.qmd), I showed how the approach can be used to perform something like hypothesis tests for quantities of interest that aren't as easily amenable as means to being assessed parametrically, such as differences in medians. In [the next post, on resampling and permutation tests](../permutation-with-base-r/index.qmd), I described the intuition and methodology behind resampling with replacement to produce Null distributions, and how to implement the procedure using base R. \n\nIn this post, I show how [the infer package](https://infer.netlify.app/), can be used to perform both bootstrapping and permutation testing in a way that's slightly easier, and more declarative in the context of a general hypothesis testing framework. \n\n## Setting up\n\nLet's install the `infer` packge and try a couple of examples from the documentation. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"infer\") # First time around\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nlibrary(infer)\n```\n:::\n\n\n\n## The infer package \n\nFrom [the vignette page](https://infer.netlify.app/articles/infer) we can see that `infer`'s workflow is framed around four verbs:\n\n- **`specify()`** allows you to specify the variable, or relationship between variables, that you’re interested in.\n- **`hypothesize()`** allows you to declare the null hypothesis.\n- **`generate()`** allows you to generate data reflecting the null hypothesis.\n- **`calculate()`** allows you to calculate a distribution of statistics from the generated data to form the null distribution.\n\nThe package describes the problem of hypothesis testing as being somewhat generic, regardless of the specific test, hypothesis, or dataset being used: \n\n> Regardless of which hypothesis test we’re using, we’re still asking the same kind of question: is the effect/difference in our observed data real, or due to chance? To answer this question, we start by assuming that the observed data came from some world where “nothing is going on” (i.e. the observed effect was simply due to random chance), and call this assumption our *null hypothesis*. (In reality, we might not believe in the null hypothesis at all—the null hypothesis is in opposition to the *alternate hypothesis*, which supposes that the effect present in the observed data is actually due to the fact that “something is going on.”) We then calculate a *test statistic* from our data that describes the observed effect. We can use this test statistic to calculate a *p-value*, giving the probability that our observed data could come about if the null hypothesis was true. If this probability is below some pre-defined *significance level $\\alpha$*, then we can reject our null hypothesis.\n\n\n## The gss dataset\n\nLet's look through - and in some places adapt - the examples used. These mainly make use of the `gss` dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(gss)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(gss)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 500\nColumns: 11\n$ year    <dbl> 2014, 1994, 1998, 1996, 1994, 1996, 1990, 2016, 2000, 1998, 20…\n$ age     <dbl> 36, 34, 24, 42, 31, 32, 48, 36, 30, 33, 21, 30, 38, 49, 25, 56…\n$ sex     <fct> male, female, male, male, male, female, female, female, female…\n$ college <fct> degree, no degree, degree, no degree, degree, no degree, no de…\n$ partyid <fct> ind, rep, ind, ind, rep, rep, dem, ind, rep, dem, dem, ind, de…\n$ hompop  <dbl> 3, 4, 1, 4, 2, 4, 2, 1, 5, 2, 4, 3, 4, 4, 2, 2, 3, 2, 1, 2, 5,…\n$ hours   <dbl> 50, 31, 40, 40, 40, 53, 32, 20, 40, 40, 23, 52, 38, 72, 48, 40…\n$ income  <ord> $25000 or more, $20000 - 24999, $25000 or more, $25000 or more…\n$ class   <fct> middle class, working class, working class, working class, mid…\n$ finrela <fct> below average, below average, below average, above average, ab…\n$ weight  <dbl> 0.8960034, 1.0825000, 0.5501000, 1.0864000, 1.0825000, 1.08640…\n```\n:::\n:::\n\n\n## Example 1: Categorical Predictor; Continuous Response\n\nLet's go slightly off piste and say we are interested in seeing if there is a relationship between age, a cardinal variable, and sex, a categorical variable. We can start by stating our null and alternative hypotheses explicitly:\n\n- **Null hypothesis**: There is no difference between age and sex\n- **Alt hypothesis**: There is a difference between age and sex\n\nLet's see if we can start by just looking at the data to see if, informally, it looks like it might better fit the Null or Alt hypothesis. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ngss |> \n    ggplot(aes(x=age, group = sex, colour = sex)) + \n    geom_density()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\nIt looks like the densities of age distributions are similar for both sexes. However, they're not identical. Are the differences more likely to be due to chance, or are they more structural? \n\nWe can start by calculating, say, the differences in average ages between males and females:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngss |>\n    group_by(sex) |>\n    summarise(n = n(), mean_age = mean(age))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  sex        n mean_age\n  <fct>  <int>    <dbl>\n1 male     263     40.6\n2 female   237     39.9\n```\n:::\n:::\n\n\n### Our first testable hypothesis (using permutation testing/sampling without replacement)\n\nThe mean age is `40.6` for males and `39.9` for females, a difference of about `0.7` years of age. Could this have occurred by chance?\n\nThere are `263` male observations, and `237` female observations, in the dataset. Imagine that the ages are values, and the sexes are labels that are added to these values. \n\nOne approach to operationalising the concept of the Null Hypothesis is to ask: *If we shifted around the labels assigned to the values, so there were still as many male and female labels, but they were randomly reassigned, what would the difference in mean age between these two groups be? What would happen if we did this many times?*\n\nThis is the essence of building a Null distribution using a permutation test, which is similar to a bootstrap except it involves resampling with replacement rather than without replacement. \n\nWe can perform this permutation test using the infer package as follows: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- gss |>\n    specify(age ~ sex) |>\n    hypothesize(null = 'independence') |>\n    generate(reps = 10000, type = 'permute')\n\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResponse: age (numeric)\nExplanatory: sex (factor)\nNull Hypothesis: independence\n# A tibble: 5,000,000 × 3\n# Groups:   replicate [10,000]\n     age sex    replicate\n   <dbl> <fct>      <int>\n 1    28 male           1\n 2    22 female         1\n 3    19 male           1\n 4    55 male           1\n 5    50 male           1\n 6    57 female         1\n 7    23 female         1\n 8    20 female         1\n 9    48 female         1\n10    42 female         1\n# ℹ 4,999,990 more rows\n```\n:::\n:::\n\n\nThe infer package has now arbitrarily shifted around the labels assigned to the age values 10000 times. Each time is labelled with a different replicate number. Let's take the first nine replicates and show what the densities by sex look like: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel |>\n    filter(replicate <= 9) |>\n    ggplot(aes(x=age, group = sex, colour = sex)) + \n    geom_density() + \n    facet_wrap(~replicate)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nWhat if we now look at the differences in means apparent in each of these permutations\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel |>\n    calculate(stat = \"diff in means\", order = c(\"male\", \"female\")) |>\n    visualize()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nHere we can see the distribution of differences in means follows broadly a normal distribution, which appears to be centred on 0. \n\nLet's now calculate and save the observed difference in means.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp <- gss |>\n    group_by(sex) |>\n    summarise(mean_age = mean(age))\n\ntmp \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  sex    mean_age\n  <fct>     <dbl>\n1 male       40.6\n2 female     39.9\n```\n:::\n\n```{.r .cell-code}\ndiff_means <- tmp$mean_age[tmp$sex == \"male\"] - tmp$mean_age[tmp$sex == \"female\"]\n\ndiff_means\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7463541\n```\n:::\n:::\n\n\n### A two-sided hypothesis\n\nLet's now show where the observed difference in means falls along the distribution of differences in means generated by this permutation-based Null distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel |>\n    calculate(stat = \"diff in means\", order = c(\"male\", \"female\")) |>\n    visualize() +\n    shade_p_value(obs_stat = diff_means, direction = \"two-sided\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThe observed difference in means appears to be quite close to the centre of mass for the distribution of differences in means generated by the Null distribution. So it appears very likely that this observed difference could be generated from a data generating process in which there's no real difference in mean ages between the two groups. We can formalise this slightly by calcuating a p-value:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel |>\n    calculate(stat = \"diff in means\", order = c(\"male\", \"female\")) |>\n    get_p_value(obs_stat = diff_means, direction = \"two-sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.523\n```\n:::\n:::\n\n\nThe p value is much, much greater than 0.05, suggesting there's little evidence to reject the Null hypothesis, that in this dataset age is not influenced by sex. \n\n\n## Example 2: Categorical Predictor; Categorical Response\n\nNow let's look at the two variables `college` and `partyid`:\n\n- **college**: Can be `degree` or `no degree`\n- **partyid**: Can be `ind` `rep`, `dem`, `other`\n\nThe simplest type of hypothesis to state is probably something like:\n\n- **Null Hypothesis**: There is *no* relationship between `partyid` and `college`\n- **Alt Hypothesis**: There *is* a relationship between `partyid` and `college`\n\nWe can then consider more specific and targetted hypotheses at a later date. \n\nLet's see how we could use `infer` to help decide between these hypotheses, using a permutation test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- gss |>\n    specify(partyid ~ college) |>\n    hypothesize(null = 'independence') |>\n    generate(reps = 10000, type = 'permute')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nDropping unused factor levels DK from the supplied response variable 'partyid'.\n```\n:::\n\n```{.r .cell-code}\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResponse: partyid (factor)\nExplanatory: college (factor)\nNull Hypothesis: independence\n# A tibble: 5,000,000 × 3\n# Groups:   replicate [10,000]\n   partyid college   replicate\n   <fct>   <fct>         <int>\n 1 ind     degree            1\n 2 dem     no degree         1\n 3 dem     degree            1\n 4 dem     no degree         1\n 5 ind     degree            1\n 6 ind     no degree         1\n 7 ind     no degree         1\n 8 ind     degree            1\n 9 ind     degree            1\n10 dem     no degree         1\n# ℹ 4,999,990 more rows\n```\n:::\n:::\n\n\nLet's visualise the relationship between `partyid` and `college` in the first nine replicates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel |>\n    filter(replicate <= 9) |>\n    ggplot(aes(x = college, fill = partyid)) + \n    geom_bar(position = \"fill\") + \n    facet_wrap(~replicate) +\n    labs(title = \"Permuted (fake) datasets\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nAnd how does this compare with the observed dataset? \n\n\n::: {.cell}\n\n```{.r .cell-code}\ngss |>\n    ggplot(aes(x = college, fill = partyid)) + \n    geom_bar(position = \"fill\") + \n    labs(title = \"Relationship in real dataset\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nBut what summary statistic can we use for comparing the observed level of extremeness of any apparent association between the two variables, with summary statistics under the Null hypothesis (i.e. using permutation testing)? The standard answer is to calculate the [Chi-squared statistic](https://en.wikipedia.org/wiki/Chi-squared_distribution), as detailed [here](https://cran.r-project.org/web/packages/infer/vignettes/observed_stat_examples.html#two-categorical-2-level-chi-squared-test-of-independence). \n\nFirst, what's the Chi-squared value we get from the observed data?\n\n::: {.cell}\n\n```{.r .cell-code}\nChisq_obs <- gss |>\n    specify(partyid ~ college) |>\n    hypothesize(null = \"independence\") |>\n    calculate(stat = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nDropping unused factor levels DK from the supplied response variable 'partyid'.\n```\n:::\n\n```{.r .cell-code}\nChisq_obs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResponse: partyid (factor)\nExplanatory: college (factor)\nNull Hypothesis: independence\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1  4.15\n```\n:::\n:::\n\n\nSo, the value is `4.15`. Is this a big or a small value? \n\nTo answer that let's calculate the same statistic from the Null distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchi_dist_null <- model |>\n    calculate(stat = \"Chisq\")\n\nchi_dist_null\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResponse: partyid (factor)\nExplanatory: college (factor)\nNull Hypothesis: independence\n# A tibble: 10,000 × 2\n   replicate  stat\n       <int> <dbl>\n 1         1 6.89 \n 2         2 0.807\n 3         3 3.52 \n 4         4 3.82 \n 5         5 2.83 \n 6         6 3.16 \n 7         7 1.86 \n 8         8 4.82 \n 9         9 0.399\n10        10 3.98 \n# ℹ 9,990 more rows\n```\n:::\n:::\n\n\nSo, is the observed value something that could have been plausibly generated from the Null distribution? We can answer this by seeing how extreme the observed Chi-squared value is compared with the distribution of values under the Null:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisualise(chi_dist_null) +\n    shade_p_value(obs_stat = Chisq_obs, direction = \"greater\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nSo, it looks like it's *fairly likely* that the value we observed could have been observed under the Null, a scenario in which there's no true relationship between the variables. But how likely? \n\n\n::: {.cell}\n\n```{.r .cell-code}\nchi_dist_null |>\n    get_p_value(obs_stat = Chisq_obs, direction = \"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.249\n```\n:::\n:::\n\n\nAround a quarter of Chi-squared values under the Null are as greater or greater than that observed in the real dataset. So there's not great evidence of there being a relationship between having a degree and distribution of party affiliations. \n\nInfer makes it fairly straightforward to calculate the extremeness of our observed test statistic using the analytic/theoretical approach too, using the `assume()` verb: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_dist_theory <- gss %>%\n    specify(partyid ~ college) |>\n    assume(distribution = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nDropping unused factor levels DK from the supplied response variable 'partyid'.\n```\n:::\n\n```{.r .cell-code}\nvisualize(null_dist_theory) +\n  shade_p_value(obs_stat = Chisq_obs, direction = \"greater\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_dist_theory |>\n    get_p_value(obs_stat = Chisq_obs, direction = \"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.386\n```\n:::\n:::\n\n\nHere the theoretical distribution suggests the observed value is even more likely to have been observed by chance under the Null, than using the permutation-based approach. \n\nAnd we can show both approaches together: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nchi_dist_null |>\n    visualise(method = \"both\") +\n    shade_p_value(obs_stat = Chisq_obs, direction = \"greater\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Check to make sure the conditions have been met for the theoretical method.\ninfer currently does not check these for you.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nHere we can see the resampling-based distribution (the histogram) has more values lower than the observed value, and fewer values higher than the observed value, than the theoretical distribution (the density line), which helps to explain the difference in p-values calculated. \n\n## Summing up \n\nSo, that's a brief introduction to the `infer` package. It provides a clear and opinionated way of thinking about and constructing hypothesis tests using a small series of verbs, and as part of this handles a lot of the code for performing permutation tests, visualising data, and comparing resampling-based estimates of the Null distribution with theoretical estimates of the same quantities. And, though both of the examples I've shown above are about permutation testing, it also allows for bootstrapped calculations to be performed too. \n\nIn some ways, `infer` seems largely intended as a pedagogic/teaching tool, for understanding the intuition behind the concept of the Null hypothesis and distribution, and so what a p-value actually means. However you can see that it does abstract away some of the computational complexity involved in producing Null distributions using both resampling and 'traditional' approaches. In previous posts we showed that it's not necessarily too difficult to produce resampled distributions without this, but there's still potentially some quality-of-life benefits to using it. \n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}