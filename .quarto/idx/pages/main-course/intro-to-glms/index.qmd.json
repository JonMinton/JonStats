{"title":"Introduction to Generalised Linear Models","markdown":{"yaml":{"title":"Introduction to Generalised Linear Models","code-fold":"show","warning":false,"message":false,"bibliography":"references.bib"},"headingText":"Aim","containsRefs":false,"markdown":"\n\n\nThe aims of this web page is to provide an overview of generalised linear models, and ways of thinking about modelling that go beyond 'star-gazing'. \n\n\n## What are statistical models and how are they fit?\n\nIt's common for different statistical methods to be taught as if they're completely different species or families. In particular, for standard linear regression to be taught first, then additional, more exotic models, like logistic or Poisson regression, to be introduced at a later stage, in an advanced course.\n\nThe disadvantage with this standard approach to teaching statistics is that it obscures the way that almost all statistical models are, fundamentally, trying to do something very similar, and work in very similar ways.\n\nSomething I've found immensely helpful over the years is the following pair of equations:\n\n**Stochastic Component**\n\n$$\nY_i \\sim f(\\theta_i, \\alpha)\n$$\n\n**Systematic Component**\n\n$$\n\\theta_i = g(X_i, \\beta)\n$$\n\nIn words, the above is saying something like:\n\n-   The predicted response $Y_i$ for a set of predictors $X_i$ is assumed to be drawn from (the $\\sim$ symbol) a stochastic distribution ($f(.,.)$)\n-   The stochastic distribution contains both parameters we're interested in, and which are determined by the data $\\theta_i$, and parameters we're not interested in and might just have to assume, $\\alpha$.\n-   The parameters we're interested in determining from the data $\\theta_i$ are themselves determined by a systematic component $g(.,.)$ which take and transform two inputs: The observed predictor data $X_i$, and a set of coefficients $\\beta$\n\nAnd graphically this looks something like:\n\n```{mermaid}\nflowchart LR\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  \n  X --> g\n  beta --> g\n  g --> theta\n  theta --> f\n  alpha --> f\n  \n  f --> Y\n\n\n```\n\nTo understand how this fits into the 'whole game' of modelling, it's worth introducing another term, $D$, for the data we're using, and to say that $D$ is partitioned into observed predictors $X_i$, and observed responses, $y_i$.\n\nFor each observation, $i$, we therefore have a predicted response, $Y_i$, and an observed response, $y_i$. We can compare $Y_i$ with $y_i$ to get the difference between the two, $\\delta_i$.\n\nNow, obviously can't change the data to make it fit our model better. But what we can do is calibrate the model a little better. How do we do this? Through adjusting the $\\beta$ parameters that feed into the systematic component $g$. Graphically, this process of comparison, adjustment, and calibration looks as follows:\n\n```{mermaid}\nflowchart LR\n  D\n  y\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  diff\n  \n  D -->|partition| X\n  D -->|partition| y\n  X --> g\n  beta -->|rerun| g\n  g -->|transform| theta\n  theta --> f\n  alpha --> f\n  \n  f -->|predict| Y\n  \n  Y -->|compare| diff\n  y -->|compare| diff\n  \n  diff -->|adjust| beta\n  \n  \n  \n  linkStyle default stroke:blue, stroke-width:1px\n\n```\n\nPretty much all statistical model fitting involves iterating along this $g \\to \\beta$ and $\\beta \\to g$ feedback loop until some kind of condition is met involving minimising $\\delta$.\n\n\n## Systematic components and link functions\n\nThe two part equation shown above is too general and abstract to be implemented directly. Instead, specific choices about the $f(.)$ and $g(.)$ need to be made. @KinTomWit00 gives the following examples:\n\n**Logistic Regression**\n\n$$\nY_i \\sim Bernoulli(\\pi_i) \n$$\n\n$$\n\\pi_i = \\frac{1}{1 + e^{-X_i\\beta}}\n$$\n\n**Linear Regression**\n\n$$\nY_i \\sim N(\\mu_i, \\sigma^2) \n$$ $$\n \\mu_i = X_i\\beta\n$$\n\nSo, what's so special about linear regression, in this framework?\n\nIn one sense, not so much. It's got a systematic component, and it's got a stochastic component. But so do other models. But in another sense, quite a lot. It's a rare case where the systematic component, $g(.)$, *doesn't* transform its inputs in some weird and wonderful way. We can say that $g(.)$ is the identity transform, $I(.)$, which in words means *take what you're given, do nothing to it, and pass it on*.\n\nBy contrast, the systematic component for logistic regression is known as the logistic function. $logistic(x) := \\frac{1}{1 + e^{-x}}$ It transforms inputs that could be anywhere on the real number line to values that lay somewhere between 0 and 1. Why 0 to 1? Because what logistic regression models produce aren't predicted values, but predicted *probabilities*, and nothing can be more probable than certain (1) or less probable than impossible (0).\n\nWe can compare the transformations used in linear and logistic regression as follows:[^1]\n\n[^1]: Using some base R graphics functions as I'm feeling masochistic\n\n```{r}\n#| layout-ncol: 2\n#| fig-cap: \n#|   - \"Identity Transformation\"\n#|   - \"Logistic Transformation\"\n\n\n# Define transformations\nident <- function(x) {x}\nlgt <- function(x) {1 / (1 + exp(-x))}\n\n\n# Draw the associations\ncurve(ident, -6, 6,\n      xlab = \"x (before transform)\",\n      ylab = \"z (after transform)\",\n      main = \"The Identity 'Transformation'\"\n      )\n\ncurve(lgt, -6, 6, \n      xlab = \"x (before transform)\", \n      ylab = \"z (after transform)\",\n      main = \"The Logistic Transformation\"\n      )\n\n\n```\n\nThe usual input to the transformation function $g(.)$ is a sum of products. For three variables, for example, this could be $\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$. In matrix algebra this generalises to $\\boldsymbol{X\\beta}$ , where $\\boldsymbol{X}$ is the predictor data whose rows are observations, columns are variables, and whose first column is a vector of 1s (for the intercept term). The $\\boldsymbol{\\beta}$ term is a row-wise vector comprising each specific $\\beta$ term, such as $\\boldsymbol{\\beta} = \\{ \\beta_0, \\beta_1, \\beta_2 \\}$ in the three variable example above.\n\nWhat's special about the identity transformation, and so linear regression, is that there is a fairly clear correspondence between a $\\beta_j$ term and the estimated influence of changing a predictor variable $x_j$ on the predicted outcome $Y$, i.e. the 'effect of $x_j$ on $Y$'. For other transformations this tends to not be the case.\n\n## How to express a linear model as a generalised linear model\n\nIn R, there's the `lm` function for linear models, and the `glm` function for generalised linear models.\n\nI've argued previously that the standard linear regression is just a specific type of generalised linear model, one that makes use of an identity transformation `I(.)` for its systematic component `g(.)`. Let's now demonstrate that by producing the same model specification using both `lm` and `glm`.\n\nWe can start by being painfully unimaginative and picking using one of R's standard datasets\n\n```{r}\nlibrary(tidyverse)\n\niris |> \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n\n```\n\nIt looks like, where the petal length is over 2.5, the relationship with sepal length is fairly linear\n\n```{r}\niris |> \n  filter(Petal.Length > 2.5) |> \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n\n```\n\nSo, let's make a linear regression just of this subset\n\n```{r}\niris_ss <- \n  iris |> \n  filter(Petal.Length > 2.5) \n```\n\nWe can produce the regression using `lm` as follows:\n\n```{r}\nmod_lm <- lm(Sepal.Length ~ Petal.Length, data = iris_ss)\n```\n\nAnd we can use the `summary` function (which checks the type of `mod_lm` and evokes `summary.lm` implicitly) to get the following:\n\n```{r}\nsummary(mod_lm)\n```\n\nWoohoo! Three stars next to the `Petal.Length` coefficient! Definitely publishable!\n\nTo do the same using `glm`.\n\n```{r}\nmod_glm <- glm(Sepal.Length ~ Petal.Length, data = iris_ss)\n```\n\nAnd we can use the `summary` function for this data too. In this case, `summary` evokes `summary.glm` because it knows the class of `mod_glm` contains `glm`.\n\n```{r}\nsummary(mod_glm)\n```\n\nSo, the coefficients are exactly the same. But there's also some additional information in the summary, including on the type of 'family' used. Why is this?\n\nIf we look at the help for `glm` we can see that, by default, the `family` argument is set to `gaussian`.\n\nAnd if we delve a bit further into the help file, in the details about the family argument, it links to the `family` help page. The usage statement of the `family` help file is as follows:\n\n```         \nfamily(object, ...)\n\nbinomial(link = \"logit\")\ngaussian(link = \"identity\")\nGamma(link = \"inverse\")\ninverse.gaussian(link = \"1/mu^2\")\npoisson(link = \"log\")\nquasi(link = \"identity\", variance = \"constant\")\nquasibinomial(link = \"logit\")\nquasipoisson(link = \"log\")\n```\n\nEach family has a default `link` argument, and for this `gaussian` family, this link is the identity function.\n\nWe can also see that, for both the `binomial` and `quasibinomial` family, the default link is `logit`, which transforms all predictors onto a 0-1 scale, as shown in the last post.\n\nSo, by using the default family, the Gaussian family is selected, and by using the default Gaussian family member, the identity link is selected.\n\nWe can confirm this by setting the family and link explicitly, showing that we get the same results\n\n```{r}\nmod_glm2 <- glm(Sepal.Length ~ Petal.Length, family = gaussian(link = \"identity\"), data = iris_ss)\nsummary(mod_glm2)\n```\n\nIt's the same!\n\nHow do these terms used in the `glm` function, `family` and `link`, relate to the general framework in @KinTomWit00?\n\n-   `family` is the stochastic component, `f(.)`\n-   `link` is the systematic component, `g(.)`\n\nThey're different terms, but it's the same broad framework.\n\nLinear models are just one type of general linear model!\n\n## Why only betas look at betas\n\n### Why overuse of linear regression leads people to look at models in the wrong way\n\nThough it's not always phrased this way, a motivating question behind the construction of most statistical models is, \"What influence does a single input to the model, $x_j$, have on the output, $Y$?\"[^1] For a single variable $x_j$ which is either present (`1`) or absent (`0`), this is in effect asking what is $E(Y | x_j = 1) - E(Y | x_j = 0)$ ?[^2]\n\n[^1]: Note here I'm using $x_j$, not $x_i$, and that $X\\beta$ is shorthand for $\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3$ and so on. In using the $j$ suffix, I'm referring to just one of the specific $x$ values, $x_1$, $x_2$, $x_3$, which is equivalent to selecting one of the *columns in* $X$. By contrast $i$ should be considered shorthand for selection of one of the *rows of* $X$, i.e. one of the series of observations that goes into the dataset $D$.\n\n[^2]: $E(.)$ is the expectation operator, and $|$ indicates a condition. So, the two terms mean, respectively, *what is the expected value of the outcome if the variable of interest is 'switched on'?*, and *what is the expected value of the outcome if the variable of interest is 'switched off'?*\n\nLet's look at a linear regression case, then a logistic regression case.\n\n### Linear Regression example\n\nUsing the iris dataset, let's try to predict Sepal Width (a continuous variable) on Sepal Length (a continuous variable) and whether the species is setosa or not (a discrete variable). As a reminder, the data relating these three variables look as follows:\n\n```{r}\nlibrary(ggplot2)\n\niris |>\n    ggplot(aes(Sepal.Length, Sepal.Width, group = Species, colour = Species, shape = Species)) + \n    geom_point()\n\n\n```\n\nLet's now build the model:\n\n```{r}\nlibrary(tidyverse)\ndf <- iris |> mutate(is_setosa = Species == 'setosa')\n\nmod_lm <- lm(Sepal.Width ~ Sepal.Length + is_setosa, data = df)\n\nmod_lm\n```\n\nThe coefficients $\\boldsymbol{\\beta} = \\{\\beta_0, \\beta_1, \\beta_2\\}$ are $\\{0.73, 0.34, 0.99\\}$, and refer to the intercept, Sepal Length and `is_setosa` respectively.\n\nIf we assume a Sepel Length of 6, for example, then the expected Sepal Width (the thing we are predicting) is `0.73 + 6 * 0.34 + 0.99` or about `3.77` in the case where `is_setosa` is true, and `0.73 + 6 * 0.34` or about `2.78` where `is_setosa` is false.\n\nThe difference between these two values, `3.77` and `2.78`, i.e. the 'influence of setosa' on the outcome, is `0.99`, i.e. the $\\beta_2$ coefficient shown before. In fact, for any conceivable (and non-conceivable, i.e. negative) value of Sepal Length, the difference is still `0.99`.\n\nThis is the $\\beta_2$ coefficient, and the reason why, **for linear regression, and almost exclusively linear regression, looking at the coefficients themselves provides substantively meaningful information** (something @KinTomWit00 calls a 'quantity of interest') about the size of influence that a predictor has on a response.\n\n### Logistic Regression example\n\nNow let's look at an example using logistic regression. We will use another tiresomely familiar dataset, `mtcars`. We are interested in estimating the effect that having a straight engine (`vs=1`) has on the probability of the car having a manual transmission (`am=1`). Our model also tries to control for the miles-per-gallon (`mpg`). The model specification is shown, the model is run, and the coefficeints are all shown below:\n\n```{r}\nmod_logistic <- glm(\n    am ~ mpg + vs,\n    data = mtcars, \n    family = binomial()\n    )\n\nmod_logistic\n```\n\nHere the coefficients $\\boldsymbol{\\beta} = \\{\\beta_0, \\beta_1, \\beta_2\\}$ are $\\{-9.92, 0.54, -2.80\\}$, and refer to the intercept, mpg, and vs respectively.\n\nBut what does this actually mean, substantively?\n\n### (Don't) Stargaze\n\nA very common approach to trying to answer this question is to look at the statistical significance of the coefficients, which we can do with the `summary()` function\n\n```{r}\nsummary(mod_logistic)\n\n```\n\nA common practice in many social and health sciences is to offer something like a narrative summary of the above, something like:\n\n> Our logistic regression model indicates that manualness is positively and significantly associated with our measure of fuel efficiency (p \\< 0.01). There is also an indication of a negative association with straight engine, but this effect does not quite meet conventional thresholds for statistical significance (p \\< 0.10).\n\nThis above practice is known as 'star-gazing', because summary tables like those above tend to have one or more `*` symbols in the final row, if the value of the `Pr(>|z|)` is below 0.05, and narrative summaries like those just above tend to involve looking at the number of stars in each row, alongside whether the `Estimate` values have a minus sign in front of them.\n\nStar gazing is a very common practice. It's also a terrible practice, which - ironically - turns the final presented output of a *quantitative* model into the crudest of *qualitative* summaries (positive, negative; significant, not significant). Star gazing is what researchers tend to default to when presented with model outputs from the above because, unlike in the linear regression example, the extent to which the $\\beta$ coefficients answer substantive 'how-much'-ness questions, like \"How much does having a straight engine change the probability of manual transmission?, is not easily apparent from the coefficients themselves.\n\n### Standardisation \n\nSo, how can we do better?\n\nOne approach is to standardise the data that goes into the model before passing them to the model. Standardisation means attempting to make the distribution and range of different variables more similar, and is especially useful when comparing between different continuous variables.\n\nTo give an example of this, let's look at a specification with weight (`wt`) and horsepower (`hp`) in place of `mpg`, but keeping engine-type indicator (`vs`):\n\n```{r}\nmod_logistic_2 <- glm(\n    am ~ vs + wt + hp,\n    data = mtcars, \n    family = binomial()\n    )\n\nsummary(mod_logistic_2)\n```\n\nHere both `wt` and `hp` are continuous variables.\n\nA star gazing zombie might say something like\n\n> manualness is negatively and significantly associated with weight (p \\< 0.05); there is a positive association with horsepower but this does not meet standard thresholds of statistical significance (0.05 \\< p \\< 0.10).\n\nA *slightly* better approach would be to standardise the variables `wt` and `hp` before passing to the model. Standardising means trying to set the variables to a common scale, and giving the variables more similar statistical characteristics.\n\n```{r}\n\nstandardise <- function(x){\n  (x - mean(x)) / sd(x)\n}\n\nmtcars_z <- mtcars\nmtcars_z$wt_z = standardise(mtcars$wt)\nmtcars_z$hp_z = standardise(mtcars$hp)\n\nmod_logistic_2_z <- glm(\n    am ~ vs + wt_z + hp_z,\n    data = mtcars_z, \n    family = binomial()\n    )\n\nsummary(mod_logistic_2_z)\n\n```\n\n`wt_z` is the standardised version of `wt`, and `hp_z` is the standardised version of `hp`. By convention, whereas unstandardised coefficients are usually referred to as $\\beta$ ('beta') coefficients, standardised coefficients are instead referred to as $b$ coefficients. But really, it's the same model.\n\nNote the p value of `wt_z` is the same as for `wt`, and the p value of `hp_z` is the same as that for `hp`. Note also the directions of effect are the same: the coefficients on `wt` and `wt_z` are both negative, and the coefficients of `hp` and `hp_z` are both positive.\n\nThis isn't a coincidence. Of course standardising can't really add any new information, can't really change the relationship between a predictor and a response. It's not *really* a new variable, it's the same old variable, so the relationship between predictor and response that there used to be is still there now.\n\nSo why bother standardising?\n\nOne reason is it gives, subject to some assumptions and caveats, a way of gauging the relative importance of the two different continuous variables, by allowing a slightly more meaningful comparison between the two coefficients.\n\nIn this case, we have a standardised $b$ coefficient of `-9.44` for `wt_z`, and of `2.22` for `hp_z`. As with the unstandardised coefficients we can still assert that manualness is negatively associated with weight, and positively associated with horsepower. But now we can also compare the two numbers `-9.44` and `2.22`. The ratio of these two numbers is around `4.3`. So, we might hazard to suggest something like:\n\n> a given increase in weight is around four times as important in *negatively predicting* manual transmission (i.e. in predicting an automatic transmission) as an equivalent increase in horsepower is in *positively predicting* manual transmission.\n\nThis isn't a statement that's easy to parse, but does at least allow slightly more information to be gleamed from the model. For example, it implies that, if a proposed change to a vehicle leads to similar relative (standardised) increases in both weight and horsepower then, as the weight effect is greater than the horsepower effect, the model will predict a *decreased* probability of manualness as a result.\n\nBut what about the motivating question, \"What's *the* effect of a straight engine (`vs=1`) on the probability of manual transmission (`am=1`)?\"\n\nThe problem, unlike with the linear regression, is this is now a badly formulated question, based on an incorrect premise. The problem is with the word 'the', which implies there should be a single answer to this question, i.e. that the effect of `vs` on the probability of `am=1` should always be the same. But, at least when it comes to absolute changes in the probability of `am=1`, this is no longer the case, as it depends on the values of the other variables in the model.\n\nInstead of assuming `vs=1` has a single effect on `P(am=1)`, we instead need to think about predictions of the marginal effects of `vs` on `am` in the context of other plausible values of the other predictors in the model, `wt` and `hp`. This involves asking the model a series of well formulated and specific questions.\n\n### Maximum marginal effects: Divide-by-four\n\nBefore we do that, however, there's a useful heuristic that can be employed when looking at discrete variables and using a logistic regression specification. The heuristic, which is based on the properties of the logistic function,[^3] is called *divide-by-four*. What this means is that, if we take the coefficient on `vs` of `-3.13`, and divide this value by four, we get a value of `-0.78`. Notice that the absolute value of `-0.78` is between 0 and 1.[^4] What this value gives is the *maximum possible* effect that the discrete variable (the presence rather than absence of a straight engine) has on the probability of being a manual transmission. We can say, \"a straight engine *reduces* the probability of a manual transmission by *up to* 78%\"\n\n[^3]: The logistic function maps any real number `z` onto the value range 0 to 1. `z` is $X\\beta$, which in non-matrix notation is equivalent to a sum of products $\\sum_{k=0}^{K}x_k\\beta_k$ (where, usually, $x_0$ is 1, i.e. the intercept term). Another way of expressing this would be something like $\\sum_{k \\in S}x_k\\beta_k$ where by default $S = \\{0, 1, 2, ..., K\\}$. We can instead imagine partitioning out $S = \\{S^{-J}, S^{J}\\}$ where the superscript $J$ indicates the Jth variable, and $-J$ indicates everything in $S$ *apart from* the Jth variable. Where J is a discrete variable, the effect of J on $P(Y=1)$ is $logistic({\\sum_{k \\in S^{-J}}x_k\\beta_k + \\beta_J}) - logistic({\\sum_{k \\in S^{-J}}x_k\\beta_k})$, where $logistic(z) = \\frac{1}{1 + e^{-z}}$. The marginal effect of the $\\beta_J$ coefficient thus depends on the other term $\\sum_{k \\in S^{-J}}x_k\\beta_k$. Where this other term is set to 0 the marginal effect of $\\beta_J$ becomes $logistic(\\beta_J) - logistic(0)$. According to p.82 of [this chapter by Gelman](https://vulstats.ucsd.edu/pdf/Gelman.ch-05.logistic-regression.pdf) we can equivalently ask the question 'what is the first derivative of the logistic regression with respect to $\\beta$?'. Asking more about this to Wolfram Alpha we get [this page of information](https://www.wolframalpha.com/input?i=what+is+the+the+maximum+absolute+value+of+the+derivative+of+1+%2F+%281+%2B+exp%28beta%29%29+with+respect+to+beta%3F), and scrolling down to the section on the global minimum we indeed get an absolute value of $\\frac{1}{4}$, so the maximum change in $P(Y=1)$ given a unit change in $\\beta$ is indeed one quarter of the value of $\\beta$, hence why the 'divide-by-four' heuristic 'works'. This isn't quite a full derivation, but more explanation than I was planning for a footnote! In general, it's better just to remember 'divide-by-four' than go down the rabbit warren of derivation each time! (As I've just learned, to my cost, writing this footnote!)\n\n[^4]: We should *always* expect the absolute value of a coefficient for a discrete variable to be less than four, for this reason.\n\nBut, as mentioned, this doesn't quite answer the motivating question, it gives an upper bound to the answer, not the answer itself.[^5] We can instead start to get a sense of 'the' effect of the variable `vs` on `P(am=1)` by asking the model a series of questions.\n\n[^5]: The lower bound for the marginal effect of a discrete variable, or any variable, is zero. This is when the absolute value of the sum of the product of the other variables is infinite.\n\n### Predictions on a matrix\n\nWe can start by getting the range of observed values for the two continuous variables, `hp` and `mpg`:\n\n```{r}\nmin(mtcars$hp)\nmax(mtcars$hp)\n\nmin(mtcars$wt)\nmax(mtcars$wt)\n```\n\nWe can then ask the model to make predictions of $P(am=1)$ for a large number of values of `hp` and `wt` within the observed range, both in the condition in which `vs=0` and in the condition in which `vs=1`. The `expand_grid` function[^6] can help us do this:\n\n[^6]: Or the base R `expand.grid` function\n\n```{r}\npredictors <- expand_grid(\n  hp = seq(min(mtcars$hp), max(mtcars$hp), length.out = 100),\n  wt = seq(min(mtcars$wt), max(mtcars$wt), length.out = 100)\n)\n\npredictors_straight <- predictors |> \n  mutate(vs = 1)\n\npredictors_vshaped <- predictors |> \n  mutate(vs = 0)\n\n```\n\n\nFor each of these permutations of inputs, we can use the model to get a conditional prediction. For convenience, we can also attach this as an additional column to the predictor data frame:\n\n```{r}\npredictions_predictors_straight <- predictors_straight |> \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictors_straight)\n  )\n\npredictions_predictors_vshaped <- predictors_vshaped |> \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictors_vshaped)\n  )\n\n```\n\nWe can see how the predictions vary over `hp` and `wt` using a heat map or contour map:\n\n```{r}\npredictions_predictors_straight |> \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |> \n  ggplot(aes(x = hp, y = wt, z = p_manual)) + \n  geom_contour_filled() + \n  facet_wrap(~vs) +\n  labs(\n    title = \"Predicted probability of manual transmission by wt, hp, and vs\"\n  )\n\n```\n\nWe can also produce a contour map of the differences between these two contour maps, i.e. the effect of a straight (`vs=1`) compared with v-shaped (`vs=0`) engine, which gets us a bit closer to the answer: \n\n```{r}\npredictions_predictors_straight |> \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |> \n  group_by(hp, wt) |> \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |> \n  ungroup() |> \n  ggplot(\n    aes(x = hp, y = wt, z = diff_p_manual)\n  ) + \n  geom_contour_filled() + \n  labs(\n    title = \"Marginal effect of vs=1 given wt and hp on P(am=1)\"\n  )\n\n```\n\nWe can see here that, for large ranges of wt and hp, the marginal effect of `vs=1` is small. However, for particular combinations of hp and wt, such as where hp is around 200 and wt is slightly below 3, then the marginal effect of `vs=1` becomes large, up to around a -70% reduction in the probability of manual transmission. (i.e. similar to the theoretical maximum marginal effect of around -78%). \n\nSo, what's *the* effect of `vs=1` on `P(am=1)`? i.e. how should we boil down all these 10,000 predicted effect sizes into a single effect size? \n\nI guess, if we have to try to answer this silly question, then we could take the average effect size...\n\n```{r}\npredictions_predictors_straight |> \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |> \n  group_by(hp, wt) |> \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |> \n  ungroup() |> \n  summarise(\n    mean_diff_p_manual = mean(diff_p_manual)\n  )\n\n```\n\nSo, we get an average difference of around `-0.08`, i.e. about an 8% reduction in probability of manual transmission. \n\n### Marginal effects on observed data\n\nIs this a reasonable answer? Probably not, because although the permutations of `wt` and `hp` we looked at come from the observed range, most of these combinations are likely very 'theoretical'. We can get a sense of this by plotting the observed values of `wt` and `hp` onto the above contour map:\n\n```{r}\npredictions_predictors_straight |> \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |> \n  group_by(hp, wt) |> \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |> \n  ungroup() |> \n  ggplot(\n    aes(x = hp, y = wt, z = diff_p_manual)\n  ) + \n  geom_contour_filled(alpha = 0.2, show.legend = FALSE) + \n  labs(\n    title = \"Observations from mtcars on the predicted probability surface\"\n  ) +\n  geom_point(\n    aes(x = hp, y = wt), inherit.aes = FALSE,\n    data = mtcars\n  )\n\n```\n\n\nPerhaps a better option, then, would be to calculate an average marginal effect using the observed values, but switching the observations for `vs` to 1 in one scenario, and 0 in another scenario:\n\n```{r}\npredictions_predictors_observed_straight <- mtcars |> \n  select(hp, wt) |> \n  mutate(vs = 1)\n\npredictions_predictors_observed_straight <- predictions_predictors_observed_straight |> \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictions_predictors_observed_straight)\n  )\n\npredictions_predictors_observed_vshaped <- mtcars |> \n  select(hp, wt) |> \n  mutate(vs = 0) \n\npredictions_predictors_observed_vshaped <- predictions_predictors_observed_vshaped |> \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictions_predictors_observed_vshaped)\n  )\n  \n\npredictions_predictors_observed <- \n  bind_rows(\n    predictions_predictors_observed_straight,\n    predictions_predictors_observed_vshaped\n  )\n\npredictions_marginal <- \n  predictions_predictors_observed |> \n    group_by(hp, wt) |> \n    summarise(\n      diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n    )\n\npredictions_marginal |> \n  ggplot(aes(x = diff_p_manual)) + \n  geom_histogram() +\n  geom_vline(aes(xintercept = mean(diff_p_manual)), colour = \"red\") + \n  geom_vline(aes(xintercept = median(diff_p_manual)), colour = \"green\")\n\n```\n\nIn the above the red line indicates the mean value of these marginal differences, which is `-0.12`, and the green line the median value of these differences, which is around `-0.02`. So, even with just these two measures of central tendency, there's around a six-fold difference in the estimate of 'the effect'. We can also see there's a lot of variation, from around nothing (right hand side), to around a 65% reduction (left hand side). \n\nIf forced to give a simple answer (to this overly simplistic question), we might plump for the mean for theoretical reasons, and say something like \"The effect of a straight engine is to reduce the probability of a manual transmission by around an eighth\". But I'm sure, having seen how much variation there is in these marginal effects, we can agree this 'around an eighth' answer, or any single number answer, is likely to be overly reductive. \n\nHopefully, however, it is more informative than 'statistically significant and negative', (the stargazing approach) or 'up to around 78%' (the divide-by-four approach). \n\n\n### Conclusion\n\nLinear regression tends to give a false impression about how straightforward it is to use a model to answer questions of the form \"What is the effect of x on y?\". This is because, for linear regression, but few other model specifications, the answer to this question is in the $\\beta$ coefficients themselves. For other model specifications, like the logistic regression example above, the correct-but-uninformative answer tends to be \"it depends\", and potentially more informative answers tend to require a bit more work to derive and interpret.  \n\n\n## Page discussion\n\nThis section of the course has aimed to reintroduce statistics from the perspective of generalised linear models (GLMs), in order to make the following clearer: \n\n- That linear regression is just one member of a broader 'family' of regression models\n- That all regression models can be thought of as just 'types' of GLM, with more in common than divides them\n- That we can and should aim for *substantive significance* when using the outputs of GLMs, i.e. use them for prediction and simulation rather than focus on whether individual coefficients are 'statistically significant' or not. \n\nThe [next section](../likelihood-and-simulation-theory/index.qmd) of the course delves further into the fundamentals of model fitting and statistical inference, including likelihood theory. \n\n","srcMarkdownNoYaml":"\n\n## Aim\n\nThe aims of this web page is to provide an overview of generalised linear models, and ways of thinking about modelling that go beyond 'star-gazing'. \n\n\n## What are statistical models and how are they fit?\n\nIt's common for different statistical methods to be taught as if they're completely different species or families. In particular, for standard linear regression to be taught first, then additional, more exotic models, like logistic or Poisson regression, to be introduced at a later stage, in an advanced course.\n\nThe disadvantage with this standard approach to teaching statistics is that it obscures the way that almost all statistical models are, fundamentally, trying to do something very similar, and work in very similar ways.\n\nSomething I've found immensely helpful over the years is the following pair of equations:\n\n**Stochastic Component**\n\n$$\nY_i \\sim f(\\theta_i, \\alpha)\n$$\n\n**Systematic Component**\n\n$$\n\\theta_i = g(X_i, \\beta)\n$$\n\nIn words, the above is saying something like:\n\n-   The predicted response $Y_i$ for a set of predictors $X_i$ is assumed to be drawn from (the $\\sim$ symbol) a stochastic distribution ($f(.,.)$)\n-   The stochastic distribution contains both parameters we're interested in, and which are determined by the data $\\theta_i$, and parameters we're not interested in and might just have to assume, $\\alpha$.\n-   The parameters we're interested in determining from the data $\\theta_i$ are themselves determined by a systematic component $g(.,.)$ which take and transform two inputs: The observed predictor data $X_i$, and a set of coefficients $\\beta$\n\nAnd graphically this looks something like:\n\n```{mermaid}\nflowchart LR\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  \n  X --> g\n  beta --> g\n  g --> theta\n  theta --> f\n  alpha --> f\n  \n  f --> Y\n\n\n```\n\nTo understand how this fits into the 'whole game' of modelling, it's worth introducing another term, $D$, for the data we're using, and to say that $D$ is partitioned into observed predictors $X_i$, and observed responses, $y_i$.\n\nFor each observation, $i$, we therefore have a predicted response, $Y_i$, and an observed response, $y_i$. We can compare $Y_i$ with $y_i$ to get the difference between the two, $\\delta_i$.\n\nNow, obviously can't change the data to make it fit our model better. But what we can do is calibrate the model a little better. How do we do this? Through adjusting the $\\beta$ parameters that feed into the systematic component $g$. Graphically, this process of comparison, adjustment, and calibration looks as follows:\n\n```{mermaid}\nflowchart LR\n  D\n  y\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  diff\n  \n  D -->|partition| X\n  D -->|partition| y\n  X --> g\n  beta -->|rerun| g\n  g -->|transform| theta\n  theta --> f\n  alpha --> f\n  \n  f -->|predict| Y\n  \n  Y -->|compare| diff\n  y -->|compare| diff\n  \n  diff -->|adjust| beta\n  \n  \n  \n  linkStyle default stroke:blue, stroke-width:1px\n\n```\n\nPretty much all statistical model fitting involves iterating along this $g \\to \\beta$ and $\\beta \\to g$ feedback loop until some kind of condition is met involving minimising $\\delta$.\n\n\n## Systematic components and link functions\n\nThe two part equation shown above is too general and abstract to be implemented directly. Instead, specific choices about the $f(.)$ and $g(.)$ need to be made. @KinTomWit00 gives the following examples:\n\n**Logistic Regression**\n\n$$\nY_i \\sim Bernoulli(\\pi_i) \n$$\n\n$$\n\\pi_i = \\frac{1}{1 + e^{-X_i\\beta}}\n$$\n\n**Linear Regression**\n\n$$\nY_i \\sim N(\\mu_i, \\sigma^2) \n$$ $$\n \\mu_i = X_i\\beta\n$$\n\nSo, what's so special about linear regression, in this framework?\n\nIn one sense, not so much. It's got a systematic component, and it's got a stochastic component. But so do other models. But in another sense, quite a lot. It's a rare case where the systematic component, $g(.)$, *doesn't* transform its inputs in some weird and wonderful way. We can say that $g(.)$ is the identity transform, $I(.)$, which in words means *take what you're given, do nothing to it, and pass it on*.\n\nBy contrast, the systematic component for logistic regression is known as the logistic function. $logistic(x) := \\frac{1}{1 + e^{-x}}$ It transforms inputs that could be anywhere on the real number line to values that lay somewhere between 0 and 1. Why 0 to 1? Because what logistic regression models produce aren't predicted values, but predicted *probabilities*, and nothing can be more probable than certain (1) or less probable than impossible (0).\n\nWe can compare the transformations used in linear and logistic regression as follows:[^1]\n\n[^1]: Using some base R graphics functions as I'm feeling masochistic\n\n```{r}\n#| layout-ncol: 2\n#| fig-cap: \n#|   - \"Identity Transformation\"\n#|   - \"Logistic Transformation\"\n\n\n# Define transformations\nident <- function(x) {x}\nlgt <- function(x) {1 / (1 + exp(-x))}\n\n\n# Draw the associations\ncurve(ident, -6, 6,\n      xlab = \"x (before transform)\",\n      ylab = \"z (after transform)\",\n      main = \"The Identity 'Transformation'\"\n      )\n\ncurve(lgt, -6, 6, \n      xlab = \"x (before transform)\", \n      ylab = \"z (after transform)\",\n      main = \"The Logistic Transformation\"\n      )\n\n\n```\n\nThe usual input to the transformation function $g(.)$ is a sum of products. For three variables, for example, this could be $\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$. In matrix algebra this generalises to $\\boldsymbol{X\\beta}$ , where $\\boldsymbol{X}$ is the predictor data whose rows are observations, columns are variables, and whose first column is a vector of 1s (for the intercept term). The $\\boldsymbol{\\beta}$ term is a row-wise vector comprising each specific $\\beta$ term, such as $\\boldsymbol{\\beta} = \\{ \\beta_0, \\beta_1, \\beta_2 \\}$ in the three variable example above.\n\nWhat's special about the identity transformation, and so linear regression, is that there is a fairly clear correspondence between a $\\beta_j$ term and the estimated influence of changing a predictor variable $x_j$ on the predicted outcome $Y$, i.e. the 'effect of $x_j$ on $Y$'. For other transformations this tends to not be the case.\n\n## How to express a linear model as a generalised linear model\n\nIn R, there's the `lm` function for linear models, and the `glm` function for generalised linear models.\n\nI've argued previously that the standard linear regression is just a specific type of generalised linear model, one that makes use of an identity transformation `I(.)` for its systematic component `g(.)`. Let's now demonstrate that by producing the same model specification using both `lm` and `glm`.\n\nWe can start by being painfully unimaginative and picking using one of R's standard datasets\n\n```{r}\nlibrary(tidyverse)\n\niris |> \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n\n```\n\nIt looks like, where the petal length is over 2.5, the relationship with sepal length is fairly linear\n\n```{r}\niris |> \n  filter(Petal.Length > 2.5) |> \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n\n```\n\nSo, let's make a linear regression just of this subset\n\n```{r}\niris_ss <- \n  iris |> \n  filter(Petal.Length > 2.5) \n```\n\nWe can produce the regression using `lm` as follows:\n\n```{r}\nmod_lm <- lm(Sepal.Length ~ Petal.Length, data = iris_ss)\n```\n\nAnd we can use the `summary` function (which checks the type of `mod_lm` and evokes `summary.lm` implicitly) to get the following:\n\n```{r}\nsummary(mod_lm)\n```\n\nWoohoo! Three stars next to the `Petal.Length` coefficient! Definitely publishable!\n\nTo do the same using `glm`.\n\n```{r}\nmod_glm <- glm(Sepal.Length ~ Petal.Length, data = iris_ss)\n```\n\nAnd we can use the `summary` function for this data too. In this case, `summary` evokes `summary.glm` because it knows the class of `mod_glm` contains `glm`.\n\n```{r}\nsummary(mod_glm)\n```\n\nSo, the coefficients are exactly the same. But there's also some additional information in the summary, including on the type of 'family' used. Why is this?\n\nIf we look at the help for `glm` we can see that, by default, the `family` argument is set to `gaussian`.\n\nAnd if we delve a bit further into the help file, in the details about the family argument, it links to the `family` help page. The usage statement of the `family` help file is as follows:\n\n```         \nfamily(object, ...)\n\nbinomial(link = \"logit\")\ngaussian(link = \"identity\")\nGamma(link = \"inverse\")\ninverse.gaussian(link = \"1/mu^2\")\npoisson(link = \"log\")\nquasi(link = \"identity\", variance = \"constant\")\nquasibinomial(link = \"logit\")\nquasipoisson(link = \"log\")\n```\n\nEach family has a default `link` argument, and for this `gaussian` family, this link is the identity function.\n\nWe can also see that, for both the `binomial` and `quasibinomial` family, the default link is `logit`, which transforms all predictors onto a 0-1 scale, as shown in the last post.\n\nSo, by using the default family, the Gaussian family is selected, and by using the default Gaussian family member, the identity link is selected.\n\nWe can confirm this by setting the family and link explicitly, showing that we get the same results\n\n```{r}\nmod_glm2 <- glm(Sepal.Length ~ Petal.Length, family = gaussian(link = \"identity\"), data = iris_ss)\nsummary(mod_glm2)\n```\n\nIt's the same!\n\nHow do these terms used in the `glm` function, `family` and `link`, relate to the general framework in @KinTomWit00?\n\n-   `family` is the stochastic component, `f(.)`\n-   `link` is the systematic component, `g(.)`\n\nThey're different terms, but it's the same broad framework.\n\nLinear models are just one type of general linear model!\n\n## Why only betas look at betas\n\n### Why overuse of linear regression leads people to look at models in the wrong way\n\nThough it's not always phrased this way, a motivating question behind the construction of most statistical models is, \"What influence does a single input to the model, $x_j$, have on the output, $Y$?\"[^1] For a single variable $x_j$ which is either present (`1`) or absent (`0`), this is in effect asking what is $E(Y | x_j = 1) - E(Y | x_j = 0)$ ?[^2]\n\n[^1]: Note here I'm using $x_j$, not $x_i$, and that $X\\beta$ is shorthand for $\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3$ and so on. In using the $j$ suffix, I'm referring to just one of the specific $x$ values, $x_1$, $x_2$, $x_3$, which is equivalent to selecting one of the *columns in* $X$. By contrast $i$ should be considered shorthand for selection of one of the *rows of* $X$, i.e. one of the series of observations that goes into the dataset $D$.\n\n[^2]: $E(.)$ is the expectation operator, and $|$ indicates a condition. So, the two terms mean, respectively, *what is the expected value of the outcome if the variable of interest is 'switched on'?*, and *what is the expected value of the outcome if the variable of interest is 'switched off'?*\n\nLet's look at a linear regression case, then a logistic regression case.\n\n### Linear Regression example\n\nUsing the iris dataset, let's try to predict Sepal Width (a continuous variable) on Sepal Length (a continuous variable) and whether the species is setosa or not (a discrete variable). As a reminder, the data relating these three variables look as follows:\n\n```{r}\nlibrary(ggplot2)\n\niris |>\n    ggplot(aes(Sepal.Length, Sepal.Width, group = Species, colour = Species, shape = Species)) + \n    geom_point()\n\n\n```\n\nLet's now build the model:\n\n```{r}\nlibrary(tidyverse)\ndf <- iris |> mutate(is_setosa = Species == 'setosa')\n\nmod_lm <- lm(Sepal.Width ~ Sepal.Length + is_setosa, data = df)\n\nmod_lm\n```\n\nThe coefficients $\\boldsymbol{\\beta} = \\{\\beta_0, \\beta_1, \\beta_2\\}$ are $\\{0.73, 0.34, 0.99\\}$, and refer to the intercept, Sepal Length and `is_setosa` respectively.\n\nIf we assume a Sepel Length of 6, for example, then the expected Sepal Width (the thing we are predicting) is `0.73 + 6 * 0.34 + 0.99` or about `3.77` in the case where `is_setosa` is true, and `0.73 + 6 * 0.34` or about `2.78` where `is_setosa` is false.\n\nThe difference between these two values, `3.77` and `2.78`, i.e. the 'influence of setosa' on the outcome, is `0.99`, i.e. the $\\beta_2$ coefficient shown before. In fact, for any conceivable (and non-conceivable, i.e. negative) value of Sepal Length, the difference is still `0.99`.\n\nThis is the $\\beta_2$ coefficient, and the reason why, **for linear regression, and almost exclusively linear regression, looking at the coefficients themselves provides substantively meaningful information** (something @KinTomWit00 calls a 'quantity of interest') about the size of influence that a predictor has on a response.\n\n### Logistic Regression example\n\nNow let's look at an example using logistic regression. We will use another tiresomely familiar dataset, `mtcars`. We are interested in estimating the effect that having a straight engine (`vs=1`) has on the probability of the car having a manual transmission (`am=1`). Our model also tries to control for the miles-per-gallon (`mpg`). The model specification is shown, the model is run, and the coefficeints are all shown below:\n\n```{r}\nmod_logistic <- glm(\n    am ~ mpg + vs,\n    data = mtcars, \n    family = binomial()\n    )\n\nmod_logistic\n```\n\nHere the coefficients $\\boldsymbol{\\beta} = \\{\\beta_0, \\beta_1, \\beta_2\\}$ are $\\{-9.92, 0.54, -2.80\\}$, and refer to the intercept, mpg, and vs respectively.\n\nBut what does this actually mean, substantively?\n\n### (Don't) Stargaze\n\nA very common approach to trying to answer this question is to look at the statistical significance of the coefficients, which we can do with the `summary()` function\n\n```{r}\nsummary(mod_logistic)\n\n```\n\nA common practice in many social and health sciences is to offer something like a narrative summary of the above, something like:\n\n> Our logistic regression model indicates that manualness is positively and significantly associated with our measure of fuel efficiency (p \\< 0.01). There is also an indication of a negative association with straight engine, but this effect does not quite meet conventional thresholds for statistical significance (p \\< 0.10).\n\nThis above practice is known as 'star-gazing', because summary tables like those above tend to have one or more `*` symbols in the final row, if the value of the `Pr(>|z|)` is below 0.05, and narrative summaries like those just above tend to involve looking at the number of stars in each row, alongside whether the `Estimate` values have a minus sign in front of them.\n\nStar gazing is a very common practice. It's also a terrible practice, which - ironically - turns the final presented output of a *quantitative* model into the crudest of *qualitative* summaries (positive, negative; significant, not significant). Star gazing is what researchers tend to default to when presented with model outputs from the above because, unlike in the linear regression example, the extent to which the $\\beta$ coefficients answer substantive 'how-much'-ness questions, like \"How much does having a straight engine change the probability of manual transmission?, is not easily apparent from the coefficients themselves.\n\n### Standardisation \n\nSo, how can we do better?\n\nOne approach is to standardise the data that goes into the model before passing them to the model. Standardisation means attempting to make the distribution and range of different variables more similar, and is especially useful when comparing between different continuous variables.\n\nTo give an example of this, let's look at a specification with weight (`wt`) and horsepower (`hp`) in place of `mpg`, but keeping engine-type indicator (`vs`):\n\n```{r}\nmod_logistic_2 <- glm(\n    am ~ vs + wt + hp,\n    data = mtcars, \n    family = binomial()\n    )\n\nsummary(mod_logistic_2)\n```\n\nHere both `wt` and `hp` are continuous variables.\n\nA star gazing zombie might say something like\n\n> manualness is negatively and significantly associated with weight (p \\< 0.05); there is a positive association with horsepower but this does not meet standard thresholds of statistical significance (0.05 \\< p \\< 0.10).\n\nA *slightly* better approach would be to standardise the variables `wt` and `hp` before passing to the model. Standardising means trying to set the variables to a common scale, and giving the variables more similar statistical characteristics.\n\n```{r}\n\nstandardise <- function(x){\n  (x - mean(x)) / sd(x)\n}\n\nmtcars_z <- mtcars\nmtcars_z$wt_z = standardise(mtcars$wt)\nmtcars_z$hp_z = standardise(mtcars$hp)\n\nmod_logistic_2_z <- glm(\n    am ~ vs + wt_z + hp_z,\n    data = mtcars_z, \n    family = binomial()\n    )\n\nsummary(mod_logistic_2_z)\n\n```\n\n`wt_z` is the standardised version of `wt`, and `hp_z` is the standardised version of `hp`. By convention, whereas unstandardised coefficients are usually referred to as $\\beta$ ('beta') coefficients, standardised coefficients are instead referred to as $b$ coefficients. But really, it's the same model.\n\nNote the p value of `wt_z` is the same as for `wt`, and the p value of `hp_z` is the same as that for `hp`. Note also the directions of effect are the same: the coefficients on `wt` and `wt_z` are both negative, and the coefficients of `hp` and `hp_z` are both positive.\n\nThis isn't a coincidence. Of course standardising can't really add any new information, can't really change the relationship between a predictor and a response. It's not *really* a new variable, it's the same old variable, so the relationship between predictor and response that there used to be is still there now.\n\nSo why bother standardising?\n\nOne reason is it gives, subject to some assumptions and caveats, a way of gauging the relative importance of the two different continuous variables, by allowing a slightly more meaningful comparison between the two coefficients.\n\nIn this case, we have a standardised $b$ coefficient of `-9.44` for `wt_z`, and of `2.22` for `hp_z`. As with the unstandardised coefficients we can still assert that manualness is negatively associated with weight, and positively associated with horsepower. But now we can also compare the two numbers `-9.44` and `2.22`. The ratio of these two numbers is around `4.3`. So, we might hazard to suggest something like:\n\n> a given increase in weight is around four times as important in *negatively predicting* manual transmission (i.e. in predicting an automatic transmission) as an equivalent increase in horsepower is in *positively predicting* manual transmission.\n\nThis isn't a statement that's easy to parse, but does at least allow slightly more information to be gleamed from the model. For example, it implies that, if a proposed change to a vehicle leads to similar relative (standardised) increases in both weight and horsepower then, as the weight effect is greater than the horsepower effect, the model will predict a *decreased* probability of manualness as a result.\n\nBut what about the motivating question, \"What's *the* effect of a straight engine (`vs=1`) on the probability of manual transmission (`am=1`)?\"\n\nThe problem, unlike with the linear regression, is this is now a badly formulated question, based on an incorrect premise. The problem is with the word 'the', which implies there should be a single answer to this question, i.e. that the effect of `vs` on the probability of `am=1` should always be the same. But, at least when it comes to absolute changes in the probability of `am=1`, this is no longer the case, as it depends on the values of the other variables in the model.\n\nInstead of assuming `vs=1` has a single effect on `P(am=1)`, we instead need to think about predictions of the marginal effects of `vs` on `am` in the context of other plausible values of the other predictors in the model, `wt` and `hp`. This involves asking the model a series of well formulated and specific questions.\n\n### Maximum marginal effects: Divide-by-four\n\nBefore we do that, however, there's a useful heuristic that can be employed when looking at discrete variables and using a logistic regression specification. The heuristic, which is based on the properties of the logistic function,[^3] is called *divide-by-four*. What this means is that, if we take the coefficient on `vs` of `-3.13`, and divide this value by four, we get a value of `-0.78`. Notice that the absolute value of `-0.78` is between 0 and 1.[^4] What this value gives is the *maximum possible* effect that the discrete variable (the presence rather than absence of a straight engine) has on the probability of being a manual transmission. We can say, \"a straight engine *reduces* the probability of a manual transmission by *up to* 78%\"\n\n[^3]: The logistic function maps any real number `z` onto the value range 0 to 1. `z` is $X\\beta$, which in non-matrix notation is equivalent to a sum of products $\\sum_{k=0}^{K}x_k\\beta_k$ (where, usually, $x_0$ is 1, i.e. the intercept term). Another way of expressing this would be something like $\\sum_{k \\in S}x_k\\beta_k$ where by default $S = \\{0, 1, 2, ..., K\\}$. We can instead imagine partitioning out $S = \\{S^{-J}, S^{J}\\}$ where the superscript $J$ indicates the Jth variable, and $-J$ indicates everything in $S$ *apart from* the Jth variable. Where J is a discrete variable, the effect of J on $P(Y=1)$ is $logistic({\\sum_{k \\in S^{-J}}x_k\\beta_k + \\beta_J}) - logistic({\\sum_{k \\in S^{-J}}x_k\\beta_k})$, where $logistic(z) = \\frac{1}{1 + e^{-z}}$. The marginal effect of the $\\beta_J$ coefficient thus depends on the other term $\\sum_{k \\in S^{-J}}x_k\\beta_k$. Where this other term is set to 0 the marginal effect of $\\beta_J$ becomes $logistic(\\beta_J) - logistic(0)$. According to p.82 of [this chapter by Gelman](https://vulstats.ucsd.edu/pdf/Gelman.ch-05.logistic-regression.pdf) we can equivalently ask the question 'what is the first derivative of the logistic regression with respect to $\\beta$?'. Asking more about this to Wolfram Alpha we get [this page of information](https://www.wolframalpha.com/input?i=what+is+the+the+maximum+absolute+value+of+the+derivative+of+1+%2F+%281+%2B+exp%28beta%29%29+with+respect+to+beta%3F), and scrolling down to the section on the global minimum we indeed get an absolute value of $\\frac{1}{4}$, so the maximum change in $P(Y=1)$ given a unit change in $\\beta$ is indeed one quarter of the value of $\\beta$, hence why the 'divide-by-four' heuristic 'works'. This isn't quite a full derivation, but more explanation than I was planning for a footnote! In general, it's better just to remember 'divide-by-four' than go down the rabbit warren of derivation each time! (As I've just learned, to my cost, writing this footnote!)\n\n[^4]: We should *always* expect the absolute value of a coefficient for a discrete variable to be less than four, for this reason.\n\nBut, as mentioned, this doesn't quite answer the motivating question, it gives an upper bound to the answer, not the answer itself.[^5] We can instead start to get a sense of 'the' effect of the variable `vs` on `P(am=1)` by asking the model a series of questions.\n\n[^5]: The lower bound for the marginal effect of a discrete variable, or any variable, is zero. This is when the absolute value of the sum of the product of the other variables is infinite.\n\n### Predictions on a matrix\n\nWe can start by getting the range of observed values for the two continuous variables, `hp` and `mpg`:\n\n```{r}\nmin(mtcars$hp)\nmax(mtcars$hp)\n\nmin(mtcars$wt)\nmax(mtcars$wt)\n```\n\nWe can then ask the model to make predictions of $P(am=1)$ for a large number of values of `hp` and `wt` within the observed range, both in the condition in which `vs=0` and in the condition in which `vs=1`. The `expand_grid` function[^6] can help us do this:\n\n[^6]: Or the base R `expand.grid` function\n\n```{r}\npredictors <- expand_grid(\n  hp = seq(min(mtcars$hp), max(mtcars$hp), length.out = 100),\n  wt = seq(min(mtcars$wt), max(mtcars$wt), length.out = 100)\n)\n\npredictors_straight <- predictors |> \n  mutate(vs = 1)\n\npredictors_vshaped <- predictors |> \n  mutate(vs = 0)\n\n```\n\n\nFor each of these permutations of inputs, we can use the model to get a conditional prediction. For convenience, we can also attach this as an additional column to the predictor data frame:\n\n```{r}\npredictions_predictors_straight <- predictors_straight |> \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictors_straight)\n  )\n\npredictions_predictors_vshaped <- predictors_vshaped |> \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictors_vshaped)\n  )\n\n```\n\nWe can see how the predictions vary over `hp` and `wt` using a heat map or contour map:\n\n```{r}\npredictions_predictors_straight |> \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |> \n  ggplot(aes(x = hp, y = wt, z = p_manual)) + \n  geom_contour_filled() + \n  facet_wrap(~vs) +\n  labs(\n    title = \"Predicted probability of manual transmission by wt, hp, and vs\"\n  )\n\n```\n\nWe can also produce a contour map of the differences between these two contour maps, i.e. the effect of a straight (`vs=1`) compared with v-shaped (`vs=0`) engine, which gets us a bit closer to the answer: \n\n```{r}\npredictions_predictors_straight |> \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |> \n  group_by(hp, wt) |> \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |> \n  ungroup() |> \n  ggplot(\n    aes(x = hp, y = wt, z = diff_p_manual)\n  ) + \n  geom_contour_filled() + \n  labs(\n    title = \"Marginal effect of vs=1 given wt and hp on P(am=1)\"\n  )\n\n```\n\nWe can see here that, for large ranges of wt and hp, the marginal effect of `vs=1` is small. However, for particular combinations of hp and wt, such as where hp is around 200 and wt is slightly below 3, then the marginal effect of `vs=1` becomes large, up to around a -70% reduction in the probability of manual transmission. (i.e. similar to the theoretical maximum marginal effect of around -78%). \n\nSo, what's *the* effect of `vs=1` on `P(am=1)`? i.e. how should we boil down all these 10,000 predicted effect sizes into a single effect size? \n\nI guess, if we have to try to answer this silly question, then we could take the average effect size...\n\n```{r}\npredictions_predictors_straight |> \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |> \n  group_by(hp, wt) |> \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |> \n  ungroup() |> \n  summarise(\n    mean_diff_p_manual = mean(diff_p_manual)\n  )\n\n```\n\nSo, we get an average difference of around `-0.08`, i.e. about an 8% reduction in probability of manual transmission. \n\n### Marginal effects on observed data\n\nIs this a reasonable answer? Probably not, because although the permutations of `wt` and `hp` we looked at come from the observed range, most of these combinations are likely very 'theoretical'. We can get a sense of this by plotting the observed values of `wt` and `hp` onto the above contour map:\n\n```{r}\npredictions_predictors_straight |> \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |> \n  group_by(hp, wt) |> \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |> \n  ungroup() |> \n  ggplot(\n    aes(x = hp, y = wt, z = diff_p_manual)\n  ) + \n  geom_contour_filled(alpha = 0.2, show.legend = FALSE) + \n  labs(\n    title = \"Observations from mtcars on the predicted probability surface\"\n  ) +\n  geom_point(\n    aes(x = hp, y = wt), inherit.aes = FALSE,\n    data = mtcars\n  )\n\n```\n\n\nPerhaps a better option, then, would be to calculate an average marginal effect using the observed values, but switching the observations for `vs` to 1 in one scenario, and 0 in another scenario:\n\n```{r}\npredictions_predictors_observed_straight <- mtcars |> \n  select(hp, wt) |> \n  mutate(vs = 1)\n\npredictions_predictors_observed_straight <- predictions_predictors_observed_straight |> \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictions_predictors_observed_straight)\n  )\n\npredictions_predictors_observed_vshaped <- mtcars |> \n  select(hp, wt) |> \n  mutate(vs = 0) \n\npredictions_predictors_observed_vshaped <- predictions_predictors_observed_vshaped |> \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictions_predictors_observed_vshaped)\n  )\n  \n\npredictions_predictors_observed <- \n  bind_rows(\n    predictions_predictors_observed_straight,\n    predictions_predictors_observed_vshaped\n  )\n\npredictions_marginal <- \n  predictions_predictors_observed |> \n    group_by(hp, wt) |> \n    summarise(\n      diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n    )\n\npredictions_marginal |> \n  ggplot(aes(x = diff_p_manual)) + \n  geom_histogram() +\n  geom_vline(aes(xintercept = mean(diff_p_manual)), colour = \"red\") + \n  geom_vline(aes(xintercept = median(diff_p_manual)), colour = \"green\")\n\n```\n\nIn the above the red line indicates the mean value of these marginal differences, which is `-0.12`, and the green line the median value of these differences, which is around `-0.02`. So, even with just these two measures of central tendency, there's around a six-fold difference in the estimate of 'the effect'. We can also see there's a lot of variation, from around nothing (right hand side), to around a 65% reduction (left hand side). \n\nIf forced to give a simple answer (to this overly simplistic question), we might plump for the mean for theoretical reasons, and say something like \"The effect of a straight engine is to reduce the probability of a manual transmission by around an eighth\". But I'm sure, having seen how much variation there is in these marginal effects, we can agree this 'around an eighth' answer, or any single number answer, is likely to be overly reductive. \n\nHopefully, however, it is more informative than 'statistically significant and negative', (the stargazing approach) or 'up to around 78%' (the divide-by-four approach). \n\n\n### Conclusion\n\nLinear regression tends to give a false impression about how straightforward it is to use a model to answer questions of the form \"What is the effect of x on y?\". This is because, for linear regression, but few other model specifications, the answer to this question is in the $\\beta$ coefficients themselves. For other model specifications, like the logistic regression example above, the correct-but-uninformative answer tends to be \"it depends\", and potentially more informative answers tend to require a bit more work to derive and interpret.  \n\n\n## Page discussion\n\nThis section of the course has aimed to reintroduce statistics from the perspective of generalised linear models (GLMs), in order to make the following clearer: \n\n- That linear regression is just one member of a broader 'family' of regression models\n- That all regression models can be thought of as just 'types' of GLM, with more in common than divides them\n- That we can and should aim for *substantive significance* when using the outputs of GLMs, i.e. use them for prediction and simulation rather than focus on whether individual coefficients are 'statistically significant' or not. \n\nThe [next section](../likelihood-and-simulation-theory/index.qmd) of the course delves further into the fundamentals of model fitting and statistical inference, including likelihood theory. \n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"cosmo","title":"Introduction to Generalised Linear Models","message":false,"bibliography":["references.bib"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}