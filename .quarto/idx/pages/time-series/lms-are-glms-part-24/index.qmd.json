{"title":"Part Twenty Four: Time series - Vector Autoregression and multivariate models","markdown":{"yaml":{"title":"Part Twenty Four: Time series - Vector Autoregression and multivariate models","author":"Jon Minton","date":"2024-05-26","message":false,"warning":false,"code-fold":true,"categories":["life expectancy","time series","vector autoregression","multivariate regression","statistics"]},"headingText":"Time Series recap","containsRefs":false,"markdown":"\n\n\nSo far in this short series on time series, we've looked at time series modelling from some first principles, learning how the types of data and challenge in time series analysis both are similar and different from those of statistical modelling more generally. We started by [looking at the concept of auto-regression](../lms-are-glms-part-19/index.qmd), then [differentiation and integration](../lms-are-glms-part-20/index.qmd), and then [the moving average model specification](../lms-are-glms-part-21/index.qmd), before combining these three components - `AR`, `I`, and `MA` - to produce [the ARIMA model specification](../lms-are-glms-part-22/index.qmd) common in time series analysis. Afterwards, we then extended the ARIMA specification slightly to deal with seasonally varying data, the ARIMA specification begetting [the Seasonal ARIMA modelling framework, or SARIMA](../lms-are-glms-part-23/index.qmd). As part of the post on Seasonality, we also looked at time series decomposition, using the STL decomposition framework. \n\n## Aim of this post \n\nIn this post, we'll take time series in a different direction, to show an application of **multivariate regression** common in time series, called **vector autoregression** (VAR). VAR is both simpler in some ways, and more complex in other ways, than SARIMA modelling. It's simpler in that, as the name suggests, moving average (`MA`) terms tend to not be part of VAR models; we'll also not be considering seasonality either. But it's more complicated in the sense that we are jointly modelling two outcomes at the same time.\n\n## Model family tree\n\nThe following figure aims to show the family resemblances between model specifications and challenges: \n\n```{mermaid}\nflowchart TB \n    uvm(univariate models)\n    mvm(multivariate models)\n\n    ar(\"AR(p)\")\n    i(\"I(d)\")\n    ma(\"MA(q)\")\n    arima(\"ARIMA(p, d, q)\")\n    sarima(\"ARIMA(p, d, q)[P, D, Q]_s\")\n    var(VAR)\n\n    ar --> var\n    mvm --> var\n\n    i -.-> var\n\n    uvm -- autoregression --> ar\n    uvm -- differencing --> i\n    uvm -- moving average --> ma\n    ar & i & ma --> arima\n\n    arima -- seasonality -->  sarima\n\n```\n\nSo, the VAR model is an extension of the autoregressive component of a standard, univariate `AR(p)` specification models to multivariate models. It can also include both predictor and response variables that are differenced, hence the the dashed line from `I(d)` to VAR. \n\n## So what is a multivariate model? \n\nYou might have seen the term *multivariate model* before, and think you're familiar with what it means. \n\nIn particular, you might have been taught that whereas a univariate regression model looks something like this:\n\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\epsilon\n$$\n\nA multivariate regression model looks more like this:\n\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n$$\n\ni.e. You might have been taught that, if the predictors include one term for the *intercept* (the $\\beta_0$ term) and one term for the *slope* (the $\\beta_1$ term), then this is a *univariate model*. But if there are two or more terms that can claim to be 'the slope' then this is a *multivariate model*. \n\nHowever, this isn't the real distinction between a univariate model and a multivariate model. To see this distinction we have to return, for the umpeenth time, to the 'grandmother model' specification first introduced at the start of [the very first post](../intro-to-glms/lms-are-glms-part-01/index.qmd):\n\n**Stochastic Component**\n\n$$\nY \\sim f(\\theta, \\alpha)\n$$\n\n**Systematic Component**\n\n$$\n\\theta = g(X, \\beta)\n$$\n\nNow, both the response data, $Y$, and the predictor data, $X$, are both taken from the same rectangular dataset, $D$. Let's say this dataset, $D$, has six rows and five columns. As a matrix it would look something like this:\n\n\n$$\nD = \n\\begin{pmatrix}\nd_{1,1} & d_{1,2} & d_{1,3} & d_{1, 4} & d_{1,5} \\\\\nd_{2,1} & d_{2,2} & d_{2,3} & d_{2, 4} & d_{2,5} \\\\\nd_{3,1} & d_{3,2} & d_{3,3} & d_{3, 4} & d_{3,5} \\\\\nd_{4,1} & d_{4,2} & d_{4,3} & d_{4, 4} & d_{4,5} \\\\\nd_{5,1} & d_{5,2} & d_{5,3} & d_{5, 4} & d_{5,5} \\\\\nd_{6,1} & d_{6,2} & d_{6,3} & d_{6, 4} & d_{6,5} \n\\end{pmatrix}\n$$\n\n\nHere the dataset $D$ is made up of a whole series of elements $d_{i,j}$, where the first subset value indicates the row number $i$ and the second subset value indicates the column number $j$. So, for example, $d_{5, 2}$ indicates the value of the 5th row and 2nd column, whereas $d_{2, 5}$ indicates the value of the 2nd row and 5th column. \n\nFundamentally, the first challenge in building a model is deciding which *columns* from $D$ we put in the predictor matrix $X$, and which parts we put into the response matrix $Y$. For example, if we wanted to predict the third column $j=3$ given the fifth column $j=5$ our predictor and response matrices would look as follows:\n\n\n:::: {.columns}\n\n::: {.column width=\"25%\"}\n$$\nY = \\begin{pmatrix}\nd_{1,3} \\\\\nd_{2,3} \\\\\nd_{3,3} \\\\\nd_{4,3} \\\\\nd_{5,3} \\\\\nd_{6,3}  \n\\end{pmatrix}\n$$\n:::\n\n::: {.column width=\"25%\"}\n$$\nX = \\begin{pmatrix}\n1 & d_{1,5} \\\\\n1 & d_{2,5} \\\\\n1 & d_{3,5} \\\\\n1 & d_{4,5} \\\\\n1 & d_{5,5} \\\\\n1 & d_{6,5}  \n\\end{pmatrix}\n$$\n:::\n::::\nWhere does the column of 1s come from? This is how we specify, in matrix notation, that we want an intercept term to be calculated. Models don't *have* to have intercept terms, but in almost all cases we're likely to be familiar with, they tend to. \n\nLet's say we now want to include two columns, 2 and 5, from $D$ in the predictor matrix, leading to what's commonly (and wrongly) called a 'multivariate regression'. This means that $Y$ stays the same, but X is now as follows:\n\n$$\nX = \\begin{pmatrix}\n1 & d_{1,2}  & d_{1,5}\\\\\n1 & d_{2,2}  & d_{2,5}\\\\\n1 & d_{3,2}  & d_{3,5}\\\\\n1 & d_{4,2}  & d_{4,5}\\\\\n1 & d_{5,2}  & d_{5,5}\\\\\n1 & d_{6,2}  & d_{6,5} \n\\end{pmatrix}\n$$\n\nNo matter now many columns we include in the predictor matrix, X, however, we still don't have a real **multivariate regression** model specification. Even if X had a hundred columns, or a thousand, it would still not be a **multivariate regression** in the more technical sense of the term. \n\nInstead, *here's* an example of a **multivariate regression** model:\n\n:::: {.columns}\n::: {.column width=\"25%\"}\n$$\nY = \\begin{pmatrix}\nd_{1,1} & d_{1,3} \\\\\nd_{2,1} & d_{2,3} \\\\\nd_{3,1} & d_{3,3} \\\\\nd_{4,1} & d_{4,3} \\\\\nd_{5,1} & d_{5,3} \\\\\nd_{6,1} & d_{6,3}  \n\\end{pmatrix}\n$$\n:::\n::: {.column width=\"25%\"}\n$$\nX = \\begin{pmatrix}\n1 & d_{1,5} \\\\\n1 & d_{2,5} \\\\\n1 & d_{3,5} \\\\\n1 & d_{4,5} \\\\\n1 & d_{5,5} \\\\\n1 & d_{6,5}  \n\\end{pmatrix}\n$$\n:::\n::::\nThis is an example of a **multivariate regression model**. We encountered it before when we used the multivariate normal distribution [in post 12](../complete-simulation-example/lms-are-glms-part-12/index.qmd), and when we draw from the posterior distribution of Bayesian models [in post 13](../complete-simulation-example/lms-are-glms-part-14/index.qmd), but this is the first time we've considered multivariate modelling in the context of trying to represent something we suspect to be true about the world, rather than our uncertainty about the world. And it's the first example of multivariate regression we've encountered in this series. For every previous model, no matter how apparently disparate, complicated or exotic they may appear, they've been *univariate* regression models in the sense that the response component $Y$ has always only contained one column only. \n\nSo, with this definition of multivariate regression, let's now look at VAR as a particular application of multivariate regression used in time series.\n\n## Vector Autoregression\n\nLet's start with a semi-technical definition: \n\n> In vector autoregression (VAR) the values of two or more outcomes, $\\{Y_1(T), Y_2(T)\\}$, are predicted based on previous values of those same outcomes $\\{Y_1(T-k), Y_2(T-k)\\}$, for various lag periods $k$. \n\nWhere $Y$ has two columns, and an `AR(1)` specification (i.e. `k` is just 1), how is this different from simply running two separate `AR(1)` regression models, one for $Y_1$, and the other for $Y_2$? \n\nWell, graphically, two separate `AR(1)` models proposes the following paths of influence: \n\n```{mermaid}\nflowchart LR\nY1_T[\"Y1(T)\"]\nY2_T[\"Y2(T)\"]\n\nY1_T1[\"Y1(T-1)\"]\nY2_T1[\"Y2(T-1)\"]\n\nY1_T1 --> Y1_T\nY2_T1 --> Y2_T\n```\n\nBy contrast, the paths implied and allowed in the corresponding `VAR(1)` model look more like the following:\n\n\n```{mermaid}\nflowchart LR\nY1_T[\"Y1(T)\"]\nY2_T[\"Y2(T)\"]\n\nY1_T1[\"Y1(T-1)\"]\nY2_T1[\"Y2(T-1)\"]\n\nY1_T1 & Y2_T1 --> Y1_T & Y2_T\n```\n\nSo, each of the two outcomes at time T *is influenced both by its own previous value, but also by the previous value of the other outcome*. This *other outcome* influence is what is represented in the figure above by the diagonal lines: from `Y2(T-1)` to `Y1(T)`, and from `Y1(T-1)` to `Y2(T)`. \n\nExpressed verbally, if we imagine two entities - **self** and **other** - tracked through time, **self** is influenced both by *self's history*, but also by *other's history* too. \n\n\n## Example and application in R \n\nIn [a more substantivelly focused post](../../still-the-economy/index.qmd), I discussed how I suspect economic growth and longevity growth trends are correlated. What I proposed doesn't exactly lend itself to the simplest kind of `VAR(1)` model specification, because I suggested a longer lag between the influence of economic growth on longevity growth, and a change in the fundamentals of growth in both cases. However, as an example of VAR I will ignore these complexities, and use the data I prepared for that post:\n\n\n```{r}\nlibrary(tidyverse)\n\ngdp_growth_pct_series <- read_csv(\"still-the-economy-both-series.csv\") \n\ngdp_growth_pct_series\n```\n\nWe need to do a certain amount of reformatting to bring this into a useful format:\n\n```{r}\nwide_ts_series <- \ngdp_growth_pct_series |>\n    select(-c(`...1`, period)) |>\n    mutate(\n        short_series = case_when(\n            series == \"1. Per Capita GDP\" ~ 'gdp',\n            series == \"2. Life Expectancy at Birth\" ~ 'e0',\n            TRUE ~ NA_character_\n        )\n    ) |>\n    select(-series) |>\n    pivot_wider(names_from = short_series, values_from = pct_change) |>\n    arrange(year) |>\n    mutate(\n        lag_gdp = lag(gdp),\n        lag_e0 = lag(e0)\n    ) %>%\n    filter(complete.cases(.))\n\n\nwide_ts_series\n```\n\nSo, we can map columns to parts of the VAR specification as follows:\n\n- `Y1`: gdp\n- `Y2`: e0 (life expectancy at birth)\n- `period T`: `gdp` and `e0`\n- `period T-1`: `lag_gdp` and `lag_e0`\n\nTo include two or more variables as the response part, $Y$,  of a linear model we can use the `cbind()` function to combine more than one variable to the left hand side of the linear regression formula for `lm` or `glm`:\n\n```{r}\nvar_model <- lm(\n    cbind(gdp, e0) ~ lag_gdp + lag_e0,\n    data = wide_ts_series\n)\n\nvar_model\n```\n\nWe can see here that the model reports a small matrix of coefficients: three rows (one for each coefficient term) and two columns: one for each of the response variables. This is as we should expect. \n\nBack in [part 12 of the series](../complete-simulation-example/lms-are-glms-part-12/index.qmd), we saw we could extract the coefficients, variance-covariance matrix, and error terms of a linear regression model using the functions `coefficients`, `vcov`, and `sigma` respectively.[^1] Let's use those functions here too:\n\n[^1]:A primary aim extracting these components from a linear regression in this way is to allow something approximating a Bayesian posterior distribution of coefficients to be generated, using a multivariate normal distribution (the first place we actually encountered a multivariate regression), without using a Bayesian modelling approach. This allows for the estimating and propagation of 'honest uncertainty' in predicted and expected outcomes. However, as we saw in [part 13](../complete-simulation-example/lms-are-glms-part-13/index.qmd), it can sometimes be as or more straightforward to just use a Bayesian modelling approach.\n\nFirst the coefficients\n\n```{r}\ncoefficients(var_model)\n\n```\n\nAnd now the variance-covariance matrix:\n\n```{r}\nvcov(var_model)\n\n```\n\nAnd finally the error terms\n\n```{r}\nsigma(var_model)\n```\n\nThe coefficients returns the same kind of 3x2 matrix we saw previously: two models run simultaneously. The error terms is now a vector of length 2: one for each of these models. The variance-covariance matrix is a square matrix of dimension 6: i.e. 6 rows and 6 columns. This is the number of predictor coefficients in each model (the number of columns of $X$, i.e. 3) *times* the number of models simultaneously run, i.e. 2. \n\n$6^2$ is `36`, which is the number of elements in the variance-covariance matrix of this VAR model. By contrast, if we had run two independent models - one for gdp and the other for e0 - we would have two 3x3 variance-variance matrices, producing a total of 18 [^2] terms. This should provide some reassurance that, when we run a multivariate regression model of two outcomes, we're not *just* doing the equivalent of running separate regression models for each outcome, but in slightly fewer lines. \n\n[^2]: i.e. two times three squared. \n\nNow, let's look at the model summary:\n\n```{r}\nsummary(var_model)\n```\n\nThe summary is now reported for each of the two outcomes: first `gdp`, then `e0`. \n\nRemember that the outcome is percentage annual change in the outcome of interest from the previous year. i.e. both series have already been 'differenced' to produce approximately stationary series. It also means that the intercept terms are especially important, as they indicate the long-term trends observed in each series.\n\nIn this case the intercepts for both series are positive and statistically significant: over the long term, GDP has grown on average around 2% each year, and life expectancy by around 0.28%. As [the post this relates to](../../still-the-economy/index.qmd) makes clear, however, these long-term trends may no longer apply.\n\nOf the four lag (`AR(1)`) terms in the model(s), three are not statistically significant; not even close. The exception is the `lag_e0` term for the `e0` response model, which is statistically significant and negative. Its coefficient is also of similar magnitude to the intercept too. \n\nWhat does this mean in practice? In effect, that annual mortality improvement trends have a tendency to *oscillate*: a better-than-average year tends to be followed by a worse-than-average year, and a worse-than-average year to be followed by a better-than-average year, in both cases at higher-than-chance rates. \n\nWhat could be the cause of this oscillatory phenomenon? When it comes to longevity, the phenomenon is somewhat well understood (though perhaps not widely enough), and referred to as either 'forward mortality displacement' or, more chillingly, 'harvesting'. This outcome likely comes about because, if there were an exceptionally bad year in terms of (say) influenza mortality, the most frail and vulnerable are likely to be those who die disproportionately from this additional mortality event. This means that the 'stock' of people remaining the following year have been selected, on average, to be slightly less frail and vulnerable than those who started the previous year. Similarly, an exceptionally 'good' year can mean that the average 'stock' of the population in the following year is slightly more frail than in an average year, so more susceptible to mortality. And so, by this means, comparatively-bad-years tend to be followed by comparatively-good-years, and comparatively-good-years by comparatively-bad-years. \n\nThough this general process is not pleasant to think about or reason through, statistical signals such as the negative `AR(1)` coefficient identified here tend to keep appearing, whether we are looking for them or not. \n\n## Conclusion\n\nIn this post we've both concluded the time series subseries, and returned to and expanded on a few posts earlier in the series. This includes the very first post, where we were first introduced to the `grandmother formulae`, the posts on statistical modelling using both frequentist and Bayesian methods, and a substantive post linking life expectancy with economic growth. \n\nAlthough we've now first encountered multivariate regression models in the context of time series, they are a much more general phenomenon. Pretty much any type of model we can think of and apply in a univariate fashion - where $Y$ has just a single column - can conceivably be expanded to two mor more columns, leading to their more complicated multiple regression variants.\n","srcMarkdownNoYaml":"\n\n## Time Series recap\n\nSo far in this short series on time series, we've looked at time series modelling from some first principles, learning how the types of data and challenge in time series analysis both are similar and different from those of statistical modelling more generally. We started by [looking at the concept of auto-regression](../lms-are-glms-part-19/index.qmd), then [differentiation and integration](../lms-are-glms-part-20/index.qmd), and then [the moving average model specification](../lms-are-glms-part-21/index.qmd), before combining these three components - `AR`, `I`, and `MA` - to produce [the ARIMA model specification](../lms-are-glms-part-22/index.qmd) common in time series analysis. Afterwards, we then extended the ARIMA specification slightly to deal with seasonally varying data, the ARIMA specification begetting [the Seasonal ARIMA modelling framework, or SARIMA](../lms-are-glms-part-23/index.qmd). As part of the post on Seasonality, we also looked at time series decomposition, using the STL decomposition framework. \n\n## Aim of this post \n\nIn this post, we'll take time series in a different direction, to show an application of **multivariate regression** common in time series, called **vector autoregression** (VAR). VAR is both simpler in some ways, and more complex in other ways, than SARIMA modelling. It's simpler in that, as the name suggests, moving average (`MA`) terms tend to not be part of VAR models; we'll also not be considering seasonality either. But it's more complicated in the sense that we are jointly modelling two outcomes at the same time.\n\n## Model family tree\n\nThe following figure aims to show the family resemblances between model specifications and challenges: \n\n```{mermaid}\nflowchart TB \n    uvm(univariate models)\n    mvm(multivariate models)\n\n    ar(\"AR(p)\")\n    i(\"I(d)\")\n    ma(\"MA(q)\")\n    arima(\"ARIMA(p, d, q)\")\n    sarima(\"ARIMA(p, d, q)[P, D, Q]_s\")\n    var(VAR)\n\n    ar --> var\n    mvm --> var\n\n    i -.-> var\n\n    uvm -- autoregression --> ar\n    uvm -- differencing --> i\n    uvm -- moving average --> ma\n    ar & i & ma --> arima\n\n    arima -- seasonality -->  sarima\n\n```\n\nSo, the VAR model is an extension of the autoregressive component of a standard, univariate `AR(p)` specification models to multivariate models. It can also include both predictor and response variables that are differenced, hence the the dashed line from `I(d)` to VAR. \n\n## So what is a multivariate model? \n\nYou might have seen the term *multivariate model* before, and think you're familiar with what it means. \n\nIn particular, you might have been taught that whereas a univariate regression model looks something like this:\n\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\epsilon\n$$\n\nA multivariate regression model looks more like this:\n\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n$$\n\ni.e. You might have been taught that, if the predictors include one term for the *intercept* (the $\\beta_0$ term) and one term for the *slope* (the $\\beta_1$ term), then this is a *univariate model*. But if there are two or more terms that can claim to be 'the slope' then this is a *multivariate model*. \n\nHowever, this isn't the real distinction between a univariate model and a multivariate model. To see this distinction we have to return, for the umpeenth time, to the 'grandmother model' specification first introduced at the start of [the very first post](../intro-to-glms/lms-are-glms-part-01/index.qmd):\n\n**Stochastic Component**\n\n$$\nY \\sim f(\\theta, \\alpha)\n$$\n\n**Systematic Component**\n\n$$\n\\theta = g(X, \\beta)\n$$\n\nNow, both the response data, $Y$, and the predictor data, $X$, are both taken from the same rectangular dataset, $D$. Let's say this dataset, $D$, has six rows and five columns. As a matrix it would look something like this:\n\n\n$$\nD = \n\\begin{pmatrix}\nd_{1,1} & d_{1,2} & d_{1,3} & d_{1, 4} & d_{1,5} \\\\\nd_{2,1} & d_{2,2} & d_{2,3} & d_{2, 4} & d_{2,5} \\\\\nd_{3,1} & d_{3,2} & d_{3,3} & d_{3, 4} & d_{3,5} \\\\\nd_{4,1} & d_{4,2} & d_{4,3} & d_{4, 4} & d_{4,5} \\\\\nd_{5,1} & d_{5,2} & d_{5,3} & d_{5, 4} & d_{5,5} \\\\\nd_{6,1} & d_{6,2} & d_{6,3} & d_{6, 4} & d_{6,5} \n\\end{pmatrix}\n$$\n\n\nHere the dataset $D$ is made up of a whole series of elements $d_{i,j}$, where the first subset value indicates the row number $i$ and the second subset value indicates the column number $j$. So, for example, $d_{5, 2}$ indicates the value of the 5th row and 2nd column, whereas $d_{2, 5}$ indicates the value of the 2nd row and 5th column. \n\nFundamentally, the first challenge in building a model is deciding which *columns* from $D$ we put in the predictor matrix $X$, and which parts we put into the response matrix $Y$. For example, if we wanted to predict the third column $j=3$ given the fifth column $j=5$ our predictor and response matrices would look as follows:\n\n\n:::: {.columns}\n\n::: {.column width=\"25%\"}\n$$\nY = \\begin{pmatrix}\nd_{1,3} \\\\\nd_{2,3} \\\\\nd_{3,3} \\\\\nd_{4,3} \\\\\nd_{5,3} \\\\\nd_{6,3}  \n\\end{pmatrix}\n$$\n:::\n\n::: {.column width=\"25%\"}\n$$\nX = \\begin{pmatrix}\n1 & d_{1,5} \\\\\n1 & d_{2,5} \\\\\n1 & d_{3,5} \\\\\n1 & d_{4,5} \\\\\n1 & d_{5,5} \\\\\n1 & d_{6,5}  \n\\end{pmatrix}\n$$\n:::\n::::\nWhere does the column of 1s come from? This is how we specify, in matrix notation, that we want an intercept term to be calculated. Models don't *have* to have intercept terms, but in almost all cases we're likely to be familiar with, they tend to. \n\nLet's say we now want to include two columns, 2 and 5, from $D$ in the predictor matrix, leading to what's commonly (and wrongly) called a 'multivariate regression'. This means that $Y$ stays the same, but X is now as follows:\n\n$$\nX = \\begin{pmatrix}\n1 & d_{1,2}  & d_{1,5}\\\\\n1 & d_{2,2}  & d_{2,5}\\\\\n1 & d_{3,2}  & d_{3,5}\\\\\n1 & d_{4,2}  & d_{4,5}\\\\\n1 & d_{5,2}  & d_{5,5}\\\\\n1 & d_{6,2}  & d_{6,5} \n\\end{pmatrix}\n$$\n\nNo matter now many columns we include in the predictor matrix, X, however, we still don't have a real **multivariate regression** model specification. Even if X had a hundred columns, or a thousand, it would still not be a **multivariate regression** in the more technical sense of the term. \n\nInstead, *here's* an example of a **multivariate regression** model:\n\n:::: {.columns}\n::: {.column width=\"25%\"}\n$$\nY = \\begin{pmatrix}\nd_{1,1} & d_{1,3} \\\\\nd_{2,1} & d_{2,3} \\\\\nd_{3,1} & d_{3,3} \\\\\nd_{4,1} & d_{4,3} \\\\\nd_{5,1} & d_{5,3} \\\\\nd_{6,1} & d_{6,3}  \n\\end{pmatrix}\n$$\n:::\n::: {.column width=\"25%\"}\n$$\nX = \\begin{pmatrix}\n1 & d_{1,5} \\\\\n1 & d_{2,5} \\\\\n1 & d_{3,5} \\\\\n1 & d_{4,5} \\\\\n1 & d_{5,5} \\\\\n1 & d_{6,5}  \n\\end{pmatrix}\n$$\n:::\n::::\nThis is an example of a **multivariate regression model**. We encountered it before when we used the multivariate normal distribution [in post 12](../complete-simulation-example/lms-are-glms-part-12/index.qmd), and when we draw from the posterior distribution of Bayesian models [in post 13](../complete-simulation-example/lms-are-glms-part-14/index.qmd), but this is the first time we've considered multivariate modelling in the context of trying to represent something we suspect to be true about the world, rather than our uncertainty about the world. And it's the first example of multivariate regression we've encountered in this series. For every previous model, no matter how apparently disparate, complicated or exotic they may appear, they've been *univariate* regression models in the sense that the response component $Y$ has always only contained one column only. \n\nSo, with this definition of multivariate regression, let's now look at VAR as a particular application of multivariate regression used in time series.\n\n## Vector Autoregression\n\nLet's start with a semi-technical definition: \n\n> In vector autoregression (VAR) the values of two or more outcomes, $\\{Y_1(T), Y_2(T)\\}$, are predicted based on previous values of those same outcomes $\\{Y_1(T-k), Y_2(T-k)\\}$, for various lag periods $k$. \n\nWhere $Y$ has two columns, and an `AR(1)` specification (i.e. `k` is just 1), how is this different from simply running two separate `AR(1)` regression models, one for $Y_1$, and the other for $Y_2$? \n\nWell, graphically, two separate `AR(1)` models proposes the following paths of influence: \n\n```{mermaid}\nflowchart LR\nY1_T[\"Y1(T)\"]\nY2_T[\"Y2(T)\"]\n\nY1_T1[\"Y1(T-1)\"]\nY2_T1[\"Y2(T-1)\"]\n\nY1_T1 --> Y1_T\nY2_T1 --> Y2_T\n```\n\nBy contrast, the paths implied and allowed in the corresponding `VAR(1)` model look more like the following:\n\n\n```{mermaid}\nflowchart LR\nY1_T[\"Y1(T)\"]\nY2_T[\"Y2(T)\"]\n\nY1_T1[\"Y1(T-1)\"]\nY2_T1[\"Y2(T-1)\"]\n\nY1_T1 & Y2_T1 --> Y1_T & Y2_T\n```\n\nSo, each of the two outcomes at time T *is influenced both by its own previous value, but also by the previous value of the other outcome*. This *other outcome* influence is what is represented in the figure above by the diagonal lines: from `Y2(T-1)` to `Y1(T)`, and from `Y1(T-1)` to `Y2(T)`. \n\nExpressed verbally, if we imagine two entities - **self** and **other** - tracked through time, **self** is influenced both by *self's history*, but also by *other's history* too. \n\n\n## Example and application in R \n\nIn [a more substantivelly focused post](../../still-the-economy/index.qmd), I discussed how I suspect economic growth and longevity growth trends are correlated. What I proposed doesn't exactly lend itself to the simplest kind of `VAR(1)` model specification, because I suggested a longer lag between the influence of economic growth on longevity growth, and a change in the fundamentals of growth in both cases. However, as an example of VAR I will ignore these complexities, and use the data I prepared for that post:\n\n\n```{r}\nlibrary(tidyverse)\n\ngdp_growth_pct_series <- read_csv(\"still-the-economy-both-series.csv\") \n\ngdp_growth_pct_series\n```\n\nWe need to do a certain amount of reformatting to bring this into a useful format:\n\n```{r}\nwide_ts_series <- \ngdp_growth_pct_series |>\n    select(-c(`...1`, period)) |>\n    mutate(\n        short_series = case_when(\n            series == \"1. Per Capita GDP\" ~ 'gdp',\n            series == \"2. Life Expectancy at Birth\" ~ 'e0',\n            TRUE ~ NA_character_\n        )\n    ) |>\n    select(-series) |>\n    pivot_wider(names_from = short_series, values_from = pct_change) |>\n    arrange(year) |>\n    mutate(\n        lag_gdp = lag(gdp),\n        lag_e0 = lag(e0)\n    ) %>%\n    filter(complete.cases(.))\n\n\nwide_ts_series\n```\n\nSo, we can map columns to parts of the VAR specification as follows:\n\n- `Y1`: gdp\n- `Y2`: e0 (life expectancy at birth)\n- `period T`: `gdp` and `e0`\n- `period T-1`: `lag_gdp` and `lag_e0`\n\nTo include two or more variables as the response part, $Y$,  of a linear model we can use the `cbind()` function to combine more than one variable to the left hand side of the linear regression formula for `lm` or `glm`:\n\n```{r}\nvar_model <- lm(\n    cbind(gdp, e0) ~ lag_gdp + lag_e0,\n    data = wide_ts_series\n)\n\nvar_model\n```\n\nWe can see here that the model reports a small matrix of coefficients: three rows (one for each coefficient term) and two columns: one for each of the response variables. This is as we should expect. \n\nBack in [part 12 of the series](../complete-simulation-example/lms-are-glms-part-12/index.qmd), we saw we could extract the coefficients, variance-covariance matrix, and error terms of a linear regression model using the functions `coefficients`, `vcov`, and `sigma` respectively.[^1] Let's use those functions here too:\n\n[^1]:A primary aim extracting these components from a linear regression in this way is to allow something approximating a Bayesian posterior distribution of coefficients to be generated, using a multivariate normal distribution (the first place we actually encountered a multivariate regression), without using a Bayesian modelling approach. This allows for the estimating and propagation of 'honest uncertainty' in predicted and expected outcomes. However, as we saw in [part 13](../complete-simulation-example/lms-are-glms-part-13/index.qmd), it can sometimes be as or more straightforward to just use a Bayesian modelling approach.\n\nFirst the coefficients\n\n```{r}\ncoefficients(var_model)\n\n```\n\nAnd now the variance-covariance matrix:\n\n```{r}\nvcov(var_model)\n\n```\n\nAnd finally the error terms\n\n```{r}\nsigma(var_model)\n```\n\nThe coefficients returns the same kind of 3x2 matrix we saw previously: two models run simultaneously. The error terms is now a vector of length 2: one for each of these models. The variance-covariance matrix is a square matrix of dimension 6: i.e. 6 rows and 6 columns. This is the number of predictor coefficients in each model (the number of columns of $X$, i.e. 3) *times* the number of models simultaneously run, i.e. 2. \n\n$6^2$ is `36`, which is the number of elements in the variance-covariance matrix of this VAR model. By contrast, if we had run two independent models - one for gdp and the other for e0 - we would have two 3x3 variance-variance matrices, producing a total of 18 [^2] terms. This should provide some reassurance that, when we run a multivariate regression model of two outcomes, we're not *just* doing the equivalent of running separate regression models for each outcome, but in slightly fewer lines. \n\n[^2]: i.e. two times three squared. \n\nNow, let's look at the model summary:\n\n```{r}\nsummary(var_model)\n```\n\nThe summary is now reported for each of the two outcomes: first `gdp`, then `e0`. \n\nRemember that the outcome is percentage annual change in the outcome of interest from the previous year. i.e. both series have already been 'differenced' to produce approximately stationary series. It also means that the intercept terms are especially important, as they indicate the long-term trends observed in each series.\n\nIn this case the intercepts for both series are positive and statistically significant: over the long term, GDP has grown on average around 2% each year, and life expectancy by around 0.28%. As [the post this relates to](../../still-the-economy/index.qmd) makes clear, however, these long-term trends may no longer apply.\n\nOf the four lag (`AR(1)`) terms in the model(s), three are not statistically significant; not even close. The exception is the `lag_e0` term for the `e0` response model, which is statistically significant and negative. Its coefficient is also of similar magnitude to the intercept too. \n\nWhat does this mean in practice? In effect, that annual mortality improvement trends have a tendency to *oscillate*: a better-than-average year tends to be followed by a worse-than-average year, and a worse-than-average year to be followed by a better-than-average year, in both cases at higher-than-chance rates. \n\nWhat could be the cause of this oscillatory phenomenon? When it comes to longevity, the phenomenon is somewhat well understood (though perhaps not widely enough), and referred to as either 'forward mortality displacement' or, more chillingly, 'harvesting'. This outcome likely comes about because, if there were an exceptionally bad year in terms of (say) influenza mortality, the most frail and vulnerable are likely to be those who die disproportionately from this additional mortality event. This means that the 'stock' of people remaining the following year have been selected, on average, to be slightly less frail and vulnerable than those who started the previous year. Similarly, an exceptionally 'good' year can mean that the average 'stock' of the population in the following year is slightly more frail than in an average year, so more susceptible to mortality. And so, by this means, comparatively-bad-years tend to be followed by comparatively-good-years, and comparatively-good-years by comparatively-bad-years. \n\nThough this general process is not pleasant to think about or reason through, statistical signals such as the negative `AR(1)` coefficient identified here tend to keep appearing, whether we are looking for them or not. \n\n## Conclusion\n\nIn this post we've both concluded the time series subseries, and returned to and expanded on a few posts earlier in the series. This includes the very first post, where we were first introduced to the `grandmother formulae`, the posts on statistical modelling using both frequentist and Bayesian methods, and a substantive post linking life expectancy with economic growth. \n\nAlthough we've now first encountered multivariate regression models in the context of time series, they are a much more general phenomenon. Pretty much any type of model we can think of and apply in a univariate fashion - where $Y$ has just a single column - can conceivably be expanded to two mor more columns, leading to their more complicated multiple regression variants.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"cosmo","title":"Part Twenty Four: Time series - Vector Autoregression and multivariate models","author":"Jon Minton","date":"2024-05-26","message":false,"categories":["life expectancy","time series","vector autoregression","multivariate regression","statistics"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}