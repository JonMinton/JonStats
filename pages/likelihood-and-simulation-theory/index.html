<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>JonStats - Likelihood and Simulation Theory</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">JonStats</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-main-course-statistical-inference-and-simulation" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Main Course: Statistical Inference and Simulation</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-main-course-statistical-inference-and-simulation">    
        <li>
    <a class="dropdown-item" href="../../pages/intro-to-glms/index.html" rel="" target="">
 <span class="dropdown-text">(1) Intro to GLMs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/likelihood-and-simulation-theory/index.html" rel="" target="">
 <span class="dropdown-text">(2) Likelihood and Simulation Theory</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/complete-simulation-example/index.html" rel="" target="">
 <span class="dropdown-text">(3) Complete Simulation Example</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/likelihood-and-simulation-theory/index.html">(2) Likelihood and Simulation Theory</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/intro-to-glms/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">(1) Intro to GLMs</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/likelihood-and-simulation-theory/index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">(2) Likelihood and Simulation Theory</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/complete-simulation-example/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">(3) Complete Simulation Example</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#bayes-rule-and-likelihood" id="toc-bayes-rule-and-likelihood" class="nav-link active" data-scroll-target="#bayes-rule-and-likelihood">Bayes’ Rule and Likelihood</a></li>
  <li><a href="#likelihood-for-linear-regression" id="toc-likelihood-for-linear-regression" class="nav-link" data-scroll-target="#likelihood-for-linear-regression">Likelihood for linear regression</a></li>
  <li><a href="#optimisation-algorithms-getting-there-faster" id="toc-optimisation-algorithms-getting-there-faster" class="nav-link" data-scroll-target="#optimisation-algorithms-getting-there-faster">Optimisation algorithms: getting there faster</a></li>
  <li><a href="#optim-for-parameter-point-estimation-our-robo-chauffeur" id="toc-optim-for-parameter-point-estimation-our-robo-chauffeur" class="nav-link" data-scroll-target="#optim-for-parameter-point-estimation-our-robo-chauffeur"><code>optim</code> for parameter point estimation: our Robo-Chauffeur</a>
  <ul class="collapse">
  <li><a href="#comparisons-with-canned-functions" id="toc-comparisons-with-canned-functions" class="nav-link" data-scroll-target="#comparisons-with-canned-functions">Comparisons with ‘canned’ functions</a></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  </ul></li>
  <li><a href="#optimal-uncertainty" id="toc-optimal-uncertainty" class="nav-link" data-scroll-target="#optimal-uncertainty"><code>optim</code>al uncertainty</a>
  <ul class="collapse">
  <li><a href="#barefoot-and-blind-a-weird-analogy-for-a-complicated-idea" id="toc-barefoot-and-blind-a-weird-analogy-for-a-complicated-idea" class="nav-link" data-scroll-target="#barefoot-and-blind-a-weird-analogy-for-a-complicated-idea">Barefoot and Blind: A weird analogy for a complicated idea</a></li>
  <li><a href="#information-and-uncertainty" id="toc-information-and-uncertainty" class="nav-link" data-scroll-target="#information-and-uncertainty">Information and uncertainty</a></li>
  <li><a href="#how-to-get-optim-to-return-this-information" id="toc-how-to-get-optim-to-return-this-information" class="nav-link" data-scroll-target="#how-to-get-optim-to-return-this-information">How to get <code>optim()</code> to return this information</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul></li>
  <li><a href="#quantities-of-interest" id="toc-quantities-of-interest" class="nav-link" data-scroll-target="#quantities-of-interest">Quantities of interest</a>
  <ul class="collapse">
  <li><a href="#method" id="toc-method" class="nav-link" data-scroll-target="#method">Method</a></li>
  <li><a href="#expected-values" id="toc-expected-values" class="nav-link" data-scroll-target="#expected-values">Expected values</a></li>
  <li><a href="#predicted-values" id="toc-predicted-values" class="nav-link" data-scroll-target="#predicted-values">Predicted values</a></li>
  </ul></li>
  <li><a href="#log-likelihood-for-logistic-regression" id="toc-log-likelihood-for-logistic-regression" class="nav-link" data-scroll-target="#log-likelihood-for-logistic-regression">Log likelihood for logistic regression</a>
  <ul class="collapse">
  <li><a href="#summary-1" id="toc-summary-1" class="nav-link" data-scroll-target="#summary-1">Summary</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Likelihood and Simulation Theory</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In <a href="../../pages/intro-to-glms/index.html">the first part of the course</a>, I stated that statistical model fitting, within the generalised model framework presented in <span class="citation" data-cites="KinTomWit00">King, Tomz, and Wittenberg (<a href="#ref-KinTomWit00" role="doc-biblioref">2000</a>)</span>, involves adjusting candidate values for elements of <span class="math inline">\(\beta = \{\beta_0, \beta_1, ..., \beta_K \}\)</span> such that the difference between what the model predicts given some predictor values, <span class="math inline">\(Y_i | X_i\)</span>, and what has been observed alongside the predictors, <span class="math inline">\(y_i\)</span>, is minimised on average<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> in some way.</p>
<p>The aim of this post is to show how this process is typically implemented in GLMs, using likelihood theory.</p>
<section id="bayes-rule-and-likelihood" class="level2">
<h2 class="anchored" data-anchor-id="bayes-rule-and-likelihood">Bayes’ Rule and Likelihood</h2>
<p>Statisticians and more advanced users of statistical models often divide themselves into ‘frequentists’ and ‘Bayesians’. To some extent the distinction is really between ‘improper Bayesians’ and ‘proper Bayesians’, however, as Bayes’ Rule is at the root of both approaches. <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ Rule</a> is:</p>
<p><span class="math display">\[
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\]</span></p>
<p>Note in the above the left hand side of the equation is <span class="math inline">\(P(A|B)\)</span> and the right hand side of the equation <em>includes</em> <span class="math inline">\(P(B|A)\)</span>. To write it out as awkward prose, therefore, Bayes’ Rule is a way of expressing <strong>that given this</strong> in terms of <strong>this given that</strong>.</p>
<p>As with much of algebra, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are just placeholders. We could instead use different symbols instead, such as:</p>
<p><span class="math display">\[
P(\tilde{\theta} | y) = \frac{P(y | \tilde{\theta})P(\tilde{\theta})}{P(y)}
\]</span></p>
<p>Likelihood theory offers a way of thinking about how good a model is in terms of its relationship to the data. According to <span class="citation" data-cites="King98">King (<a href="#ref-King98" role="doc-biblioref">1998</a>)</span> (p.&nbsp;59), it can be expressed as:</p>
<p><span class="math display">\[
L(\tilde{\theta}| y) = k(y) P(y | \tilde{\theta})
\]</span></p>
<p>Or</p>
<p><span class="math display">\[
L(\tilde{\theta} | y) \propto P(y | \tilde{\theta})
\]</span></p>
<p>Where <span class="math inline">\(\tilde{\theta}\)</span> is a proposed parameter or parameter combination for the model, and <span class="math inline">\(y\)</span> is the observed outcome.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>The important thing to note is that both Bayes’ Rule and Likelihood Theory are ways of expressing <strong>this given that</strong> as a function of <strong>that given this</strong>. Specifically, the model given the data, as a function of the data given the model. <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
</section>
<section id="likelihood-for-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-for-linear-regression">Likelihood for linear regression</h2>
<p>When, many years ago, I completed <a href="https://scholar.harvard.edu/msen/classes/government-2001-advanced-quantitative-research-methodology-professor-gary-king">the course</a> from this modelling framework is most associated, a hazing ritual employed near the start of the course was to require participants to derive the likelihood of different model specifications. However, I don’t feel like hazing myself right now, so instead we can use the derivation shown on <a href="https://scholar.harvard.edu/files/msen/files/sen_gov2001_section5.pdf">slide 8 of these slides</a>:</p>
<p><span class="math display">\[
L(\beta, \sigma^2 | y) = \prod{L(y_i | \mu_i, \sigma^2)}
\]</span></p>
<p>Where <span class="math inline">\(\mu = X \beta\)</span>, <span class="math inline">\(i\)</span> indicates an observation in the data (a row of <span class="math inline">\(X\)</span> when <span class="math inline">\(X\)</span> is in matrix form), and <span class="math inline">\(\prod\)</span> indicates the likelihoods from each observation should be multiplied with each other to derive the overall likelihood for all observed data.</p>
<p>In practice the log Likelihood, rather than the likelihood itself, is used, because this allows calculation of a sum of terms (<span class="math inline">\(\sum\)</span>) rather than product of terms (<span class="math inline">\(\prod\)</span>), and the latter tends to be computationally easier to calculate.</p>
<p>As we are interested only in how likelihood varies as a function of those model parameters we wish to estimate, <span class="math inline">\(\theta = \{\beta, \sigma^2\}\)</span>, some of the terms in the log likelihood expression can be omitted, leaving us with:</p>
<p><span class="math display">\[
\log{L(\beta, \sigma^2 | y)} \doteq \sum{-\frac{1}{2}[\log{\sigma^2} + \frac{(y_i - X_i\beta)^2}{\sigma^2}]}
\]</span></p>
<p>For all the complexity of the above expression, at heart it takes three inputs:</p>
<ul>
<li><span class="math inline">\(\theta = \{\beta, \sigma^2\}\)</span> : The candidate <em>parameters</em> for the model.</li>
<li><span class="math inline">\(y\)</span> : the observed <strong>response</strong> value from the dataset <span class="math inline">\(D\)</span></li>
<li><span class="math inline">\(X\)</span> : the observed <strong>predictor</strong> values from the dataset <span class="math inline">\(D\)</span></li>
</ul>
<p>And returns one value, the log likelihood <span class="math inline">\(\log{L(.)}\)</span>.</p>
<p>To reiterate, we can’t change the data, but we can keep changing the candidate parameters <span class="math inline">\(\theta\)</span>. Each time we do so, <span class="math inline">\(\log{L(.)}\)</span> will change too.</p>
<p><em>The aim of model calibration, in the Likelihood framework, is to maximise the Likelihood.</em> The parameter set that maximises the likelihood is also the parameter set that maximises the log likelihood.</p>
<p>To continue the example from the slides, we can write out a function for calculating the log likelihood of standard linear regression as follows:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>llNormal <span class="ot">&lt;-</span> <span class="cf">function</span>(pars, y, X){</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    beta <span class="ot">&lt;-</span> pars[<span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(X)]</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    sigma2 <span class="ot">&lt;-</span> <span class="fu">exp</span>(pars[<span class="fu">ncol</span>(X)<span class="sc">+</span><span class="dv">1</span>])</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="sc">-</span><span class="dv">1</span><span class="sc">/</span><span class="dv">2</span> <span class="sc">*</span> (<span class="fu">sum</span>(<span class="fu">log</span>(sigma2) <span class="sc">+</span> (y <span class="sc">-</span> (X<span class="sc">%*%</span>beta))<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> sigma2))</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In the above, <code>pars</code> is (almost but not quite) <span class="math inline">\(\theta\)</span>, the parameters to estimate. For standard linear regression <span class="math inline">\(\theta = \{\beta, \sigma^2\}\)</span>, where <span class="math inline">\(\beta = \{\beta_0, \beta_1, ..., \beta_k\}\)</span>, i.e.&nbsp;a vector of beta parameters, one for each column (variable) in <span class="math inline">\(X\)</span>, the predictor matrix of observations; this is why <span class="math inline">\(beta\)</span> is selected from the first K values in <code>pars</code> where K is the number of columns in <span class="math inline">\(X\)</span>.</p>
<p>The last value in <code>pars</code> is used to derive the proposed <span class="math inline">\(\sigma^2\)</span>. If we call this last value <code>eta</code> (<span class="math inline">\(\eta\)</span>), then we can say <span class="math inline">\(\sigma^2 = e^{\eta}\)</span>. So, whereas <span class="math inline">\(\theta\)</span> is a vector that ‘packs’ <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span> into a single ordered series of values, <code>pars</code> packs <code>eta</code> in place of <span class="math inline">\(\sigma^2\)</span>. This substitution of <code>eta</code> for <span class="math inline">\(\sigma^2\)</span> is done to make it easier for standard parameter fitting algorithms to work, as they tend to operate over the full real number range, rather than just over positive values.</p>
<p>In order to illustrate how the log likelihood function <code>llNormal</code> works in practice, let’s construct a simple toy dataset <span class="math inline">\(D\)</span>, and decompose <span class="math inline">\(D = \{y, X\}\)</span>, the two types of data input that go into the <code>llNormal</code> function.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set a seed so runs are identical</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">7</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># create a main predictor variable vector: -3 to 5 in increments of 1</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> (<span class="sc">-</span><span class="dv">3</span>)<span class="sc">:</span><span class="dv">5</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Record the number of observations in x</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a response variable with variability</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fl">2.5</span> <span class="sc">+</span> <span class="fl">1.4</span> <span class="sc">*</span> x  <span class="sc">+</span> <span class="fu">rnorm</span>(N, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fl">0.5</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># bind x into a two column matrix whose first column is a vector of 1s (for the intercept)</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>, N), x)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Clean up names</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(X) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In the code above we have created <span class="math inline">\(y\)</span>, a vector of nine observed responses; and <span class="math inline">\(X\)</span>, a matrix of predictors with two columns (the number of variables for which <span class="math inline">\(beta\)</span> terms need to be estimated) and nine rows (the number of observations).</p>
<p>Graphically, the relationship between x and y looks as follows:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x=</span>x, <span class="at">y=</span>y) <span class="sc">|&gt;</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span> </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>In this toy example, but almost never in reality, we <em>know</em> the correct parameters for the model. These are <span class="math inline">\({\beta_0 = 2.5, \beta_1 = 1.4}\)</span> and <span class="math inline">\(\sigma^2 = 0.25\)</span>. <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> Soon, we will see how effectively we can use optimisation algorithms to recover these true model parameters. But first, let’s see how the log likelihood varies as a function jointly of different candidate values of <span class="math inline">\(\beta_0\)</span> (the intercept) and <span class="math inline">\(\beta_1\)</span> (the slope parameter), if we already set <span class="math inline">\(\sigma^2\)</span> to <code>0.25</code>.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>candidate_param_values <span class="ot">&lt;-</span> <span class="fu">expand_grid</span>(</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">beta_0 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">by =</span> <span class="fl">0.1</span>),</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">beta_1 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">by =</span> <span class="fl">0.1</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>feed_to_ll <span class="ot">&lt;-</span> <span class="cf">function</span>(b0, b1){</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    pars <span class="ot">&lt;-</span> <span class="fu">c</span>(b0, b1, <span class="fu">log</span>(<span class="fl">0.25</span>))</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">llNormal</span>(pars, y, X)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>candidate_param_values <span class="ot">&lt;-</span> candidate_param_values <span class="sc">|&gt;</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="at">ll =</span> <span class="fu">map2_dbl</span>(beta_0, beta_1, feed_to_ll)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>candidate_param_values <span class="sc">|&gt;</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(beta_0, beta_1, <span class="at">z =</span> ll)) <span class="sc">+</span> </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_contour_filled</span>() <span class="sc">+</span> </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">title =</span> <span class="st">"Log likelihood as a function of possible values of beta_0 and beta_1"</span>,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="at">x =</span> <span class="st">"beta0 (the intercept)"</span>,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="at">y =</span> <span class="st">"beta1 (the slope)"</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Looking at this joint surface of values, we can see a ‘hotspot’ where <span class="math inline">\(\beta_0\)</span> is around 2.5, and <span class="math inline">\(\beta_1\)</span> is around 1.4, just as we should expect. We can check this further by filtering candidate_param_values on the highest observed values of ll.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>candidate_param_values <span class="sc">|&gt;</span> </span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(ll <span class="sc">==</span> <span class="fu">max</span>(ll))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 3
  beta_0 beta_1    ll
   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
1    2.4    1.4  1.41</code></pre>
</div>
</div>
</section>
<section id="optimisation-algorithms-getting-there-faster" class="level2">
<h2 class="anchored" data-anchor-id="optimisation-algorithms-getting-there-faster">Optimisation algorithms: getting there faster</h2>
<p>Previously, we ‘cheated’ a bit when using the log likelihood function, fixing the value for one of the parameters <span class="math inline">\(\sigma^2\)</span> to the value we used when we generated the data, so we could instead look at how the log likelihood surface varied as different combinations of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> were plugged into the formula. <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> values ranging from -5 to 5, and at steps of 0.1, were considered: 101 values of <span class="math inline">\(\beta_0\)</span>, 101 values of <span class="math inline">\(\beta_1\)</span>, and so over 10,000<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> unique <span class="math inline">\(\{\beta_0, \beta_1\}\)</span> combinations were stepped through. This approach is known as grid search, and seldom used in practice (except for illustration purposes) because the number of calculations involved can very easily get out of hand. For example, if we were to use it to explore as many distinct values of <span class="math inline">\(\sigma^2\)</span> as we considered for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the total number of <span class="math inline">\(\{\beta_0, \beta_1, \sigma^2 \}\)</span> combinations we would crawl through would be over 100,000 <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> rather than over 10,000.</p>
<p>One feature we noticed with the likelihood surface over <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> in the previous post is that it appears to look like a hill, with a clearly defined highest point (the region of maximum likelihood) and descent in all directions from this highest point. Where likelihood surfaces have this feature of being single-peaked in this way (known as ‘unimodal’), then a class of algorithms known as ‘hill climbing algorithms’ can be applied to find the top of such peaks in a way that tends to be both quicker (fewer steps) and more precise than the grid search approach used for illustration in the previous post.</p>
</section>
<section id="optim-for-parameter-point-estimation-our-robo-chauffeur" class="level2">
<h2 class="anchored" data-anchor-id="optim-for-parameter-point-estimation-our-robo-chauffeur"><code>optim</code> for parameter point estimation: our Robo-Chauffeur</h2>
<p>Note how the <code>llNormal</code> function takes a single argument, <code>pars</code>, which packages up all the specific candidate parameter values we want to try out. In our previous post, we also had a ‘feeder function’, <code>feed_to_ll</code>, which takes the various <span class="math inline">\(\beta\)</span> candidate values from the grid and packages them into <code>pars</code>. In our previous post, we had to specify the candidate values to try to feed to <code>llNormal</code> packages inside <code>pars</code>.</p>
<p>But we don’t have to do this. We can instead use an algorithm to take candidate parameters, try them out, then make new candidate parameters and try them out, for us. Much as a taxi driver needs to know where to meet a passenger, but doesn’t want the passenger to tell them exactly which route to take, we just need to specify a starting set of values for the parameters to optimise. R’s standard way of doing this is with the <code>optim</code> function. Here’s it in action:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>optim_results <span class="ot">&lt;-</span>  <span class="fu">optim</span>(</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># par contains our initial guesses for the three parameters to estimate</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">par =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), </span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># by default, most optim algorithms prefer to search for a minima (lowest point) rather than maxima </span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (highest point). So, I'm making a function to call which simply inverts the log likelihood by multiplying </span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># what it returns by -1</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">fn =</span> <span class="cf">function</span>(par, y, X) {<span class="sc">-</span><span class="fu">llNormal</span>(par, y, X)}, </span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># in addition to the par vector, our function also needs the observed output (y)</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and the observed predictors (X). These have to be specified as additional arguments.</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> y, <span class="at">X =</span> X</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>optim_results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>$par
[1]  2.460571  1.375421 -1.336209

$value
[1] -1.51397

$counts
function gradient 
     216       NA 

$convergence
[1] 0

$message
NULL</code></pre>
</div>
</div>
<p>The <code>optim</code> function returns a fairly complex output structure, with the following components:</p>
<ul>
<li><p><code>par</code>: the values for the parameters (in our case <span class="math inline">\(\{\beta_0, \beta_1, \eta \}\)</span>) which the optimisation algorithm ended up with.</p></li>
<li><p><code>value</code>: the value returned by the function <code>fn</code> when the optim routine was stopped.</p></li>
<li><p><code>counts</code>: the number of times the function <code>fn</code> was repeatedly called by <code>optim</code> before <code>optim</code> decided it had had enough</p></li>
<li><p><code>convergence</code>: whether the algorithm used by <code>optim</code> completed successfully (i.e.&nbsp;reached what it considers a good set of parameter estimates in <code>par</code>), or not.</p></li>
</ul>
<p>In this case, convergence is <code>0</code>, which (perhaps counterintuitively) indicates a successful completion. <code>counts</code> indicates that optim called the log likelihood function 216 times before stopping, and <code>par</code> indicates values of <span class="math inline">\(\{\beta_0 = 2.46, \beta_1 = 1.38, \eta = -1.34\}\)</span> were arrived at. As <span class="math inline">\(\sigma^2 = e^\eta\)</span>, this means <span class="math inline">\(\theta = \{\beta_0 = 2.46, \beta_1 = 1.38, \sigma^2 = 0.26 \}\)</span>. As a reminder, the ‘true’ values are <span class="math inline">\(\{\beta_0 = 2.50, \beta_1 = 1.40, \sigma^2 = 0.25\}\)</span>.</p>
<p>So, the <code>optim</code> algorithm has arrived at pretty much the correct answers for all three parameters, in 216 calls to the log likelihood function, whereas for the grid search approach in the last post we made over 10,000 calls to the log likelihood function for just <em>two</em> of the three parameters.</p>
<p>Let’s see if we can get more information on exactly what kind of path <code>optim</code> took to get to this set of parameter estimates. We should be able to do this by specifying a value in the <code>trace</code> component in the <code>control</code> argument slot…</p>
<section id="comparisons-with-canned-functions" class="level3">
<h3 class="anchored" data-anchor-id="comparisons-with-canned-functions">Comparisons with ‘canned’ functions</h3>
<p>For comparison let’s see what <code>lm</code> and <code>glm</code> produce.</p>
<p>First <code>lm</code>:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>toy_df <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> x, </span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> y</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>mod_lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> toy_df)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod_lm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = y ~ x, data = toy_df)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.6082 -0.3852 -0.1668  0.2385  1.1092 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***
x            1.37542    0.07504   18.33 3.56e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.5813 on 7 degrees of freedom
Multiple R-squared:  0.9796,    Adjusted R-squared:  0.9767 
F-statistic:   336 on 1 and 7 DF,  p-value: 3.564e-07</code></pre>
</div>
</div>
<p><span class="math inline">\(\{\beta_0 = 2.46, \beta_1 = 1.38\}\)</span>, i.e.&nbsp;the same to 2 decimal places.</p>
<p>And now with <code>glm</code>:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>mod_glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> toy_df, <span class="at">family =</span> <span class="fu">gaussian</span>(<span class="at">link =</span> <span class="st">"identity"</span>))</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod_glm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = y ~ x, family = gaussian(link = "identity"), data = toy_df)

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***
x            1.37542    0.07504   18.33 3.56e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for gaussian family taken to be 0.3378601)

    Null deviance: 115.873  on 8  degrees of freedom
Residual deviance:   2.365  on 7  degrees of freedom
AIC: 19.513

Number of Fisher Scoring iterations: 2</code></pre>
</div>
</div>
<p>Once again, <span class="math inline">\(\{\beta_0 = 2.46, \beta_1 = 1.38\}\)</span></p>
</section>
<section id="discussion" class="level3">
<h3 class="anchored" data-anchor-id="discussion">Discussion</h3>
<p>In the above, we’ve successfully used <code>optim</code>, our Robo-Chauffeur, to arrive very quickly at some good estimates for our parameters of interest, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, which are in effect identical to those produced by the <code>lm</code> and <code>glm</code> functions.</p>
<p>This isn’t a coincidence. What we’ve done the hard way is what the <code>glm</code> function (in particular) largely does ‘under the hood’.</p>
</section>
</section>
<section id="optimal-uncertainty" class="level2">
<h2 class="anchored" data-anchor-id="optimal-uncertainty"><code>optim</code>al uncertainty</h2>
<p>When using <code>optim()</code> above, we managed to get it to return a set of parameter values for our model that it thought was ‘best’, i.e.&nbsp;minimised the loss function specified by the log likelihood. These are known as <strong>point estimates</strong>, and are effectively the coefficients presented by <code>lm</code> or <code>glm</code> or equivalent statistical functions and packages. However <code>optim()</code> just returned these point estimates, without any indication of how uncertain we should be about these point estimates. A standard statistical model summary will tend to also report measures of uncertainty around the point estimates, in the form of standard errors. When these are implicitly combined with a Null hypothesis, namely that the ‘true’ value of a parameter may be zero, the point estimate together with its standard error allows the calculation of z values and p values.</p>
<p>How can we use <code>optim()</code> to return measures of uncertainty, which will allow the standard errors to be estimated as well as the point values?</p>
<p>We’ll start with a weird analogy to get an intuition for how this can be done with <code>optim()</code>.</p>
<section id="barefoot-and-blind-a-weird-analogy-for-a-complicated-idea" class="level3">
<h3 class="anchored" data-anchor-id="barefoot-and-blind-a-weird-analogy-for-a-complicated-idea">Barefoot and Blind: A weird analogy for a complicated idea</h3>
<p>Imagine optim, your hill-finding robo-chauffeur, has taken you to the top of a likelihood surface. Then it leaves you there…</p>
<p>… and you’re blind, and have no shoes. (You also have an uncanny sense of your orientation, whether north-south, east-west, or some other angle.)</p>
<p>So, you know you’re at the top of the hill, but you can’t see what the landscape around you looks like. However, you still want to get a sense of this landscape, and how it varies around the spot you’re standing on.</p>
<p>What do you do?</p>
<p>If you’re playing along with this weird thought experiment, one approach would be to use your feet as depth sensors. You make sure you never stray from where you started, and to always keep one foot planted on this initial spot (which you understand to be the highest point on the landscape). Then you use your other foot to work out how much further down the surface is from the highest point as you venture away from the highest point in different directions.</p>
<p>Say you keep your left foot planted on the highest point, and make sure your right foot is always positioned (say) 10 cm horizontally from your left foot. Initially your two feet are arranged east-west; let’s call this 0 degrees. When you put your right foot down, you notice it needs to travel 2 cm further down to reach terra ferma relative to your left foot.</p>
<p>2cm at 0 degrees. You’ll remember that.</p>
<p>Now you rotate yourself 45 degrees, and repeat the same right foot drop. This time it needs to travel 3cm down relative to your left foot.</p>
<p>3cm at 45 degrees. You remember that too.</p>
<p>Now you rotate another 45 degrees, north-south orientation, place your right foot down; now it falls 5cm down relative to your left foot.</p>
<p>2cm at 0 degrees; 3cm at 45 degrees; 5cm at 90 degrees.</p>
<p>Now with this information, you try to construct the landscape you’re on top of with your mind’s eye, making the assumption that the way it has to have curved from the peak you’re on to lead to the drops you’ve observed is consistent all around you; i.e.&nbsp;that there’s only one hill, you’re on top of it, and it’s smoothly curved in all directions.</p>
</section>
<section id="information-and-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="information-and-uncertainty">Information and uncertainty</h3>
<p>If you could further entertain the idea that your feet are infinitely small, and the gap between feet is also infinitely small (rather than the 10cm above), then you have the intuition behind this scary-looking but very important formula from <span class="citation" data-cites="King98">King (<a href="#ref-King98" role="doc-biblioref">1998</a>)</span> (p.&nbsp;89):</p>
<p><span class="math display">\[
\widehat{V(\hat{\theta})} = - \frac{1}{n}[\frac{\delta^2lnL(\tilde{\theta}|y)}{\delta \tilde{\theta} \delta \tilde{\theta}^{'}}]^{-1}_{\tilde{\theta} = \hat{\theta}}
\]</span></p>
<p>What this is saying, in something closer to humanese, is something like:</p>
<blockquote class="blockquote">
<p>Our best estimate of the amount of uncertainty we have in our estimates is a function of how much the likelihood surface curves at the highest point on the surface. (It also gets less uncertain, the more observations we have).</p>
</blockquote>
<p>Amongst the various bells, whistles and decals in the previous formula is the superscript <span class="math inline">\((.)^{-1}\)</span>. This means <em>invert</em>, which for a single value means <span class="math inline">\(\frac{1}{.}\)</span> but for a matrix means something conceptually the same but technically not.</p>
<p>And what’s being <em>inverted</em> in the last formula? A horrible-looking expression, <span class="math inline">\([\frac{\delta^2lnL(\tilde{\theta}|y)}{\delta \tilde{\theta} \delta \tilde{\theta}^{'}}]_{\tilde{\theta} = \hat{\theta}}\)</span>, that’s basically an answer to the question of <em>how curvy is the log likelihood surface at its peak position?</em></p>
<p>Within <span class="citation" data-cites="King98">King (<a href="#ref-King98" role="doc-biblioref">1998</a>)</span> (p.89, eq. 4.18), this expression (or rather the negative of the term) is defined as <span class="math inline">\(I(\hat{\theta} | y)\)</span>, where <span class="math inline">\(I(.)\)</span> stands for <strong>information</strong>.</p>
<p>So, the algebra are saying</p>
<blockquote class="blockquote">
<p>Uncertainty is inversely related to information</p>
</blockquote>
<p>Or perhaps even more intuitively</p>
<blockquote class="blockquote">
<p>The more information we have, the less uncertain we are</p>
</blockquote>
<p>Of course this makes sense. If you ask someone “How long will this task take?”, and they say “Between one hour and one month”, they likely have less information about how long the task will actually than if they had said “Between two and a half and three hours”. More generally:</p>
<ul>
<li><strong>Shallow gradients</strong> mean <strong>wide uncertainty intervals</strong> mean <strong>low information</strong></li>
<li><strong>Sharp gradients</strong> mean <strong>narrow uncertaintly intervals</strong> mean <strong>high information</strong></li>
</ul>
<p>This is, fundamentally, what the blind and barefoot person in the previous analogy is trying to achieve: by feeling out the local curvature around the highest point, they are trying to work out how much information they have about different pieces of the model. The curvature along any one dimension of the surface (equivalent to the 0 and 90 degree explorations) indicates how much information there is about any single coefficient, and the curvature along the equivalent of a 45 degree plane gives a measure of how associated any two coefficients tend to be.</p>
<p>With these many analogies and equations spinning in our heads, let’s now see how these concepts can be applied in practice.</p>
</section>
<section id="how-to-get-optim-to-return-this-information" class="level3">
<h3 class="anchored" data-anchor-id="how-to-get-optim-to-return-this-information">How to get <code>optim()</code> to return this information</h3>
<p>Having reminded myself of the particular options for <code>optim</code> that are typically used to report parameter uncertainty, let’s run the follows:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>fuller_optim_output <span class="ot">&lt;-</span> <span class="fu">optim</span>(</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">par =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), </span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">fn =</span> llNormal,</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">"BFGS"</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">control =</span> <span class="fu">list</span>(<span class="at">fnscale =</span> <span class="sc">-</span><span class="dv">1</span>),</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">hessian =</span> <span class="cn">TRUE</span>,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> y, </span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">X =</span> X</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>fuller_optim_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>$par
[1]  2.460675  1.375424 -1.336438

$value
[1] 1.51397

$counts
function gradient 
      80       36 

$convergence
[1] 0

$message
NULL

$hessian
              [,1]          [,2]          [,3]
[1,] -3.424917e+01 -3.424917e+01  2.727840e-05
[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05
[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00</code></pre>
</div>
</div>
<p>We have used a slightly different algorithm (‘BFGS’), and a different way of specifying the function to search over (using <code>fnscale = -1</code> to invert the likelihood), but we have the same <code>par</code> estimates as before: <span class="math inline">\(\beta = \{\beta_0 = 2.46, \beta_1 = 1.38\}\)</span>. So the changes we’ve made to the <code>optim</code> arguments haven’t changed what it estimates.</p>
<p>One new argument we’ve set in <code>optim</code> is <code>hessian = TRUE</code>. <a href="https://en.wikipedia.org/wiki/Hessian_fabric">Hessian</a> is a kind of coarse fabric made from vegetable waste, typically woven in a criss-crossing, grid-like pattern. Hessian matrices are matrices of second derivatives, as described in <a href="https://en.wikipedia.org/wiki/Hessian_matrix">the wikipedia article</a>. <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> If you can bear to recall the really complex expression above, for calculating the curvature around a point on a surface, you’ll recall it’s also about second derivatives.</p>
<p>None of this is a coincidence. The <code>hessian</code> component of the optim output above contains what we need.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>hess <span class="ot">&lt;-</span> fuller_optim_output<span class="sc">$</span>hessian</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>hess</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>              [,1]          [,2]          [,3]
[1,] -3.424917e+01 -3.424917e+01  2.727840e-05
[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05
[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00</code></pre>
</div>
</div>
<p>You might notice that the Hessian matrix is square, with as many columns as rows. And, that the number of columns (or rows) is equal to the number of parameters we have estimated, i.e.&nbsp;three in this case.</p>
<p>You might also notice that the values are symmetrical about the diagonal running from the top left to the bottom right.</p>
<p>Again, this is no accident.</p>
<p>Remember that variation is inversely related to information, and that <span class="math inline">\((.)^{-1}\)</span> is the inversion operator on <span class="math inline">\(I(.)\)</span>, the Information Matrix. Well, this Hessian <em>is</em> (pretty much) <span class="math inline">\(I(.)\)</span>. So let’s see what happens when we invert it (using the <code>solve</code> operator):</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>inv_hess <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="sc">-</span>hess)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>inv_hess</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>              [,1]          [,2]          [,3]
[1,]  3.357745e-02 -4.379668e-03  2.309709e-07
[2,] -4.379668e-03  4.379668e-03 -5.397790e-08
[3,]  2.309709e-07 -5.397790e-08  2.222221e-01</code></pre>
</div>
</div>
<p>As with <code>hess</code>, <code>inv_hess</code> is symmetric around the top-left to bottom-right diagonal. For example, the value on row 2 and column 1 is the same as on row 1, column 2.</p>
<p>We’re mainly interested in the first two columns and rows, as these contain the values most comparable with the glm summary reports</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>inv_hess_betas <span class="ot">&lt;-</span> inv_hess[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>inv_hess_betas</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>             [,1]         [,2]
[1,]  0.033577455 -0.004379668
[2,] -0.004379668  0.004379668</code></pre>
</div>
</div>
<p>What the elements of the above matrix provide are estimates of the <em>variances</em> of a single parameter <span class="math inline">\(\beta_j\)</span>, and/or the <em>covariances</em> between any two parameters <span class="math inline">\(\{\beta_0, \beta_1\}\)</span>. In this example:</p>
<p><span class="math display">\[
\begin{bmatrix}
var(\beta_0) &amp; cov(\beta_0, \beta_1) \\
cov(\beta_1, \beta_0) &amp; var(\beta_1)
\end{bmatrix}
\]</span></p>
<p>It’s because the on-diagonal terms are variances of uncertaintly for a single term, that it can be useful to take the square root of these terms to get estimates of the standard errors:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">diag</span>(inv_hess_betas))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.18324152 0.06617906</code></pre>
</div>
</div>
<p>Compare with the <code>Std Err</code> term in the following:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod_glm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = y ~ x, family = gaussian(link = "identity"), data = toy_df)

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***
x            1.37542    0.07504   18.33 3.56e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for gaussian family taken to be 0.3378601)

    Null deviance: 115.873  on 8  degrees of freedom
Residual deviance:   2.365  on 7  degrees of freedom
AIC: 19.513

Number of Fisher Scoring iterations: 2</code></pre>
</div>
</div>
<p>The estimates from the Hessian in optim, of <span class="math inline">\(\{0.18, 0.07\}\)</span>, are not exactly the same as the <span class="math inline">\(\{0.21, 0.08\}\)</span> reported for <code>mod_glm</code>; the methods employed are not identical. But they are hopefully similar enough to demonstrate they provide similar information about similar quantities of uncertainty.</p>
<p>Back in part five, we used this same dataset to show how the log likelihood varies for various, equally spaced, candidate values for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> (having fixed <span class="math inline">\(\eta = \exp({\sigma^2})\)</span> at its true value). This led to the followng map of the landscape<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>candidate_param_values <span class="ot">&lt;-</span> <span class="fu">expand_grid</span>(</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">beta_0 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">15</span>, <span class="dv">15</span>, <span class="at">by =</span> <span class="fl">0.05</span>),</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">beta_1 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">15</span>, <span class="dv">15</span>, <span class="at">by =</span> <span class="fl">0.05</span>)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>feed_to_ll <span class="ot">&lt;-</span> <span class="cf">function</span>(b0, b1){</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    pars <span class="ot">&lt;-</span> <span class="fu">c</span>(b0, b1, <span class="fu">log</span>(<span class="fl">0.25</span>))</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">llNormal</span>(pars, y, X)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>candidate_param_values <span class="ot">&lt;-</span> candidate_param_values <span class="sc">|&gt;</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>        <span class="at">ll =</span> <span class="fu">map2_dbl</span>(beta_0, beta_1, feed_to_ll)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>candidate_param_values <span class="sc">|&gt;</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(beta_0, beta_1, <span class="at">z =</span> ll)) <span class="sc">+</span> </span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_contour_filled</span>() <span class="sc">+</span> </span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>        <span class="at">title =</span> <span class="st">"Log likelihood as a function of possible values of beta_0 and beta_1"</span>,</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>        <span class="at">x =</span> <span class="st">"beta0 (the intercept)"</span>,</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>        <span class="at">y =</span> <span class="st">"beta1 (the slope)"</span></span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Within the above we can see that the log likelihood landscape for these two parameters looks like a bivariate normal distribution, we can also see a bit of a slant in this normal distribution. This implies a <em>correlation</em> between the two candidate values. The direction of the slant is downwards from left to right, implying the correlation is negative.</p>
<p>Firstly let’s check that the correlation between <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> implied by the Hessian <em>is</em> negative. These are the off-diagonal elements, either first row, second column, or second row, first column:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>inv_hess_betas[<span class="dv">1</span>,<span class="dv">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.004379668</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>inv_hess_betas[<span class="dv">2</span>,<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.004379668</code></pre>
</div>
</div>
<p>Yes they are!</p>
<p>As mentioned previously, the likelihood surface produced by the gridsearch method involves a lot of computations, so a lot of steps, and likely a lot of trial and error, if it were to be used to try to find the maximum likelihood value for the parameters. By contrast, the <code>optim()</code> algorithm typically involves far fewer steps, ‘feeling’ its way up the hill until it reaches a point where there’s nowhere higher. <a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> When it then reaches this highest point, it then ‘feels’ the curvature around this point in multiple directions, producing the Hessian. The algorithm doesn’t <em>see</em> the likelihood surface, because it hasn’t travelled along most of it. But the Hessian can be used to <em>infer</em> the likelihood surface, subject to subject (usually) reasonable assumptions.</p>
<p>What are these (usually) reasonable assumptions? Well, that the likelihood surface can be approximated by a multivariate normal distribution, which is a generalisation of the standard Normal distribution over more than one dimensions.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p>We can use the <code>mvrnorm</code> function from the <code>MASS</code> package, alongside the point estimates and Hessian from <code>optim</code>, in order to produce estimates of <span class="math inline">\(\theta = \{ \beta_0, \beta_1, \eta \}\)</span> which represent reasonable uncertainty about the true values of each of these parameters. Algebraically, this can be expressed as something like the following:</p>
<p><span class="math display">\[
\tilde{\theta} \sim Multivariate Normal(\mu = \dot{\theta}, \sigma^2 = \Sigma)
\]</span></p>
<p>Where <span class="math inline">\(\dot{\theta}\)</span> are the point estimates from <code>optim()</code> and <span class="math inline">\(\Sigma\)</span> is the implied variance-covariance matrix recovered from the Hessian.</p>
<p>Let’s create this MVN model and see what kinds of outputs it produces.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>point_estimates <span class="ot">&lt;-</span> fuller_optim_output<span class="sc">$</span>par</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>vcov <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fu">solve</span>(fuller_optim_output<span class="sc">$</span>hessian)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>param_draws <span class="ot">&lt;-</span> MASS<span class="sc">::</span><span class="fu">mvrnorm</span>(</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">n =</span> <span class="dv">10000</span>, </span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">mu =</span> point_estimates, </span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">Sigma =</span> vcov</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(param_draws) <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"beta0"</span>, <span class="st">"beta1"</span>, <span class="st">"eta"</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(param_draws)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>        beta0    beta1         eta
[1,] 2.564978 1.375636 -0.30407255
[2,] 2.440111 1.367774 -1.16815288
[3,] 2.775332 1.338583 -0.05574937
[4,] 2.283011 1.481799 -0.26095101
[5,] 2.695635 1.228565 -1.18369341
[6,] 2.686818 1.483601 -0.44262363</code></pre>
</div>
</div>
<p>We can see that <code>mvrnorm()</code>, with these inputs from <code>optim()</code> produces three columns: one for each parameter being estimated <span class="math inline">\(\{ \beta_0, \beta_1, \eta \}\)</span>. The <code>n</code> argumment indicates the number of draws to take; in this case, 10000. This number of draws makes it easier to see how much variation there is in each of the estimates.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>df_param_draws <span class="ot">&lt;-</span> </span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>param_draws <span class="sc">|&gt;</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as_tibble</span>(</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">rownames =</span> <span class="st">'draw'</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">|&gt;</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">sig2 =</span> <span class="fu">exp</span>(eta)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">|&gt;</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pivot_longer</span>(</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>        <span class="sc">-</span>draw, </span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>        <span class="at">names_to =</span> <span class="st">"param"</span>,</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>        <span class="at">values_to =</span> <span class="st">"value"</span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>    ) </span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>df_param_draws <span class="sc">|&gt;</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value)) <span class="sc">+</span> </span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_density</span>() <span class="sc">+</span> </span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">facet_grid</span>(param <span class="sc">~</span> .) <span class="sc">+</span> </span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_vline</span>(<span class="at">xintercept=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>There are a number of things to note here: firstly, that the average of the <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> values appear close to their known ‘true’ values of 2.5 and 1.4 respectively. Secondly, that whereas the <span class="math inline">\(\eta\)</span> values are normally distributed, the <span class="math inline">\(\sigma^2\)</span> values derived from them are not, and are never below zero; this is the effect of the exponential link between quantities. Thirdly, that the implied values of <span class="math inline">\(\sigma^2\)</span> do appear to be centred around <code>0.25</code>, as they should be as <span class="math inline">\(\sigma\)</span> was set to <code>0.50</code> in the model.</p>
<p>And forthly, that the density around <span class="math inline">\(\beta_1\)</span> is more peaked than around <span class="math inline">\(\beta_0\)</span>. This concords with what we saw previously in the filled contour map: both the horizontal <code>beta0</code> axis and vertical <code>beta1</code> axis are on the same scale, but the oval is broader along the horizontal axis than the vertical axis. This in effect implies that we have more information about the true value of <span class="math inline">\(\beta_1\)</span>, the slope, than about the true value of <span class="math inline">\(\beta_0\)</span>, the intercept.</p>
<p>We can also use these draws to reproduce something similar to, but not identical to, <a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> the previous filled contour map:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># param_draws |&gt;</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="co">#     as_tibble(</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="co">#         rownames = 'draw'</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co">#     ) |&gt;</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co">#     ggplot(aes(x = beta0, y = beta1)) + </span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co">#     geom_point(alpha = 0.1) + </span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co">#     coord_cartesian(xlim = c(-10, 10), ylim = c(-10, 10))</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>param_draws <span class="sc">|&gt;</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as_tibble</span>(</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>        <span class="at">rownames =</span> <span class="st">'draw'</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">|&gt;</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> beta0, <span class="at">y =</span> beta1)) <span class="sc">+</span> </span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_density_2d_filled</span>() <span class="sc">+</span> </span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">coord_equal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Once again, we see the same qualities as the contour map produced by interrogating the likelihood surface exhaustively: the distribution appears bivariate normal; there is a greater range in the distribution along the <code>beta0</code> than the <code>beta1</code> axis; and there is evidence of some negative correlation between the two parameters.</p>
</section>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<p>This post has shown how <code>optim()</code>, which in its vanilla state only returns point estimates, can be configured to also calculater and report the Hessian, a record of instantaneous curvature around the point estimates. Even without a fine-grained and exhausive search throughout the likelihood surface, this measure of curvature can be used to produce similar measures of uncertainty to the more exhausive approach, in a fraction of the number of computations.</p>
<p>More importantly, it can be used to generate draws of plausible combinations of parameter values, something denoted as <span class="math inline">\(\tilde{\theta}\)</span> earlier. This is something especially useful for producing <em>honest quantities of interest</em>, which both tell users of models something they want to know, while also representing how uncertain we are in this knowledge.</p>
</section>
</section>
<section id="quantities-of-interest" class="level2">
<h2 class="anchored" data-anchor-id="quantities-of-interest">Quantities of interest</h2>
<p>We’ll now, finally, show how this knowledge can be applied to do something with statistical models that ought to be done far more often: report on what <span class="citation" data-cites="KinTomWit00">King, Tomz, and Wittenberg (<a href="#ref-KinTomWit00" role="doc-biblioref">2000</a>)</span> calls <strong>quantities of interest</strong>, including <em>predicted values</em>, <em>expected values</em>, and <em>first differences</em>. Quantities of interest <em>are not</em> the direction and statistical significance (P-values) that many users of statistical models convince themselves matter, leading to the kind of mindless stargazing summaries of model outputs <a href="../../pages/intro-to-glms/index.html">described in section one</a>. Instead, they’re the kind of questions that someone, <em>not trained to think that stargazing is satisfactory</em>, might reasonably want answers to. These might include:</p>
<ul>
<li>What is the expected income of someone who completes course X in the five years after graduation? (<em>Expected values</em>)</li>
<li>What is the expected range of incomes of someone who completes course X in the five years after graduation? (<em>Predicted values</em>)</li>
<li>What is the expected difference in incomes between someone who completes course X, compared to course Y, in the five years after graduation? (<em>First Differences</em>)</li>
</ul>
<p>In <a href="../../pages/intro-to-glms/index.html">section one</a>, we showed how to answer some of the questions of this form, for both standard linear regression and logistic regression. We showed that for linear regression such answers tend to come directly from the summary of coefficients, but that for logistic regression such answers tend to be both more ambiguous and dependent on other factors (such as gender of graduate, degree, ethnicity, age and so on), and require more processing in order to produce estimates for.</p>
<p>However, we previously produced only <em>point estimates</em> for these questions, and so in a sense misled the questioner with the apparent certainty of our estimates. We now know, from earlier in this section, that we can use information about parameter uncertainty to produce parameter estimates <span class="math inline">\(\tilde{\theta}\)</span> that <em>do</em> convey parameter uncertainty, and so we can do better than the point estimates alone to answer such questions in way that takes into account such uncertainty, with a range of values rather than a single value.</p>
<section id="method" class="level3">
<h3 class="anchored" data-anchor-id="method">Method</h3>
<p>Let’s make use of our toy dataset one last time, and go through the motions to produce the <span class="math inline">\(\tilde{\theta}\)</span> draws we ended with on the last post:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>llNormal <span class="ot">&lt;-</span> <span class="cf">function</span>(pars, y, X){</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    beta <span class="ot">&lt;-</span> pars[<span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(X)]</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    sigma2 <span class="ot">&lt;-</span> <span class="fu">exp</span>(pars[<span class="fu">ncol</span>(X)<span class="sc">+</span><span class="dv">1</span>])</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    <span class="sc">-</span><span class="dv">1</span><span class="sc">/</span><span class="dv">2</span> <span class="sc">*</span> (<span class="fu">sum</span>(<span class="fu">log</span>(sigma2) <span class="sc">+</span> (y <span class="sc">-</span> (X<span class="sc">%*%</span>beta))<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> sigma2))</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set a seed so runs are identical</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">7</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co"># create a main predictor variable vector: -3 to 5 in increments of 1</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> (<span class="sc">-</span><span class="dv">3</span>)<span class="sc">:</span><span class="dv">5</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Record the number of observations in x</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a response variable with variability</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fl">2.5</span> <span class="sc">+</span> <span class="fl">1.4</span> <span class="sc">*</span> x  <span class="sc">+</span> <span class="fu">rnorm</span>(N, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fl">0.5</span>)</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a><span class="co"># bind x into a two column matrix whose first column is a vector of 1s (for the intercept)</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>, N), x)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Clean up names</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(X) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>fuller_optim_output <span class="ot">&lt;-</span> <span class="fu">optim</span>(</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">par =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), </span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">fn =</span> llNormal,</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">"BFGS"</span>,</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">control =</span> <span class="fu">list</span>(<span class="at">fnscale =</span> <span class="sc">-</span><span class="dv">1</span>),</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">hessian =</span> <span class="cn">TRUE</span>,</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> y, </span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">X =</span> X</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>fuller_optim_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>$par
[1]  2.460675  1.375424 -1.336438

$value
[1] 1.51397

$counts
function gradient 
      80       36 

$convergence
[1] 0

$message
NULL

$hessian
              [,1]          [,2]          [,3]
[1,] -3.424917e+01 -3.424917e+01  2.727840e-05
[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05
[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>hess <span class="ot">&lt;-</span> fuller_optim_output<span class="sc">$</span>hessian</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>inv_hess <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="sc">-</span>hess)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>inv_hess</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>              [,1]          [,2]          [,3]
[1,]  3.357745e-02 -4.379668e-03  2.309709e-07
[2,] -4.379668e-03  4.379668e-03 -5.397790e-08
[3,]  2.309709e-07 -5.397790e-08  2.222221e-01</code></pre>
</div>
</div>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>point_estimates <span class="ot">&lt;-</span> fuller_optim_output<span class="sc">$</span>par</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>vcov <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fu">solve</span>(fuller_optim_output<span class="sc">$</span>hessian)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>param_draws <span class="ot">&lt;-</span> MASS<span class="sc">::</span><span class="fu">mvrnorm</span>(</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">n =</span> <span class="dv">10000</span>, </span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">mu =</span> point_estimates, </span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">Sigma =</span> vcov</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(param_draws) <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"beta0"</span>, <span class="st">"beta1"</span>, <span class="st">"eta"</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let’s now look at our toy data again, and decide on some specific questions to answer:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>toy_df <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>toy_df <span class="sc">|&gt;</span> </span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span> </span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>() </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Within the data itself, we have only supplied x and y values for whole numbers of x between -3 and 5. But we can use the model to produce estimates for non-integer values of x. Let’s try 2.5. For this single value of x, we can produce both <em>predicted values</em> and <em>expected values</em>, by passing the same value of x to each of the plausible estimates of <span class="math inline">\(\theta\)</span> returned by the multivariate normal function above.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>candidate_x <span class="ot">&lt;-</span> <span class="fl">2.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="expected-values" class="level3">
<h3 class="anchored" data-anchor-id="expected-values">Expected values</h3>
<p>Here’s an example of estimating the expected value of y for x = 2.5 using loops and standard algebra:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using standard algebra and loops</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">nrow</span>(param_draws)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>expected_y_simpler <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, N)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N){</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>    expected_y_simpler[i] <span class="ot">&lt;-</span> param_draws[i, <span class="st">"beta0"</span>] <span class="sc">+</span> candidate_x <span class="sc">*</span> param_draws[i, <span class="st">"beta1"</span>]</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(expected_y_simpler)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 6.004068 5.859547 6.121791 5.987509 5.767047 6.395820</code></pre>
</div>
</div>
<p>We can see just from the first few values that each estimate is slightly different. Let’s order the values from lowest to highest, and find the range where 95% of values sit:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>ev_range <span class="ot">&lt;-</span> <span class="fu">quantile</span>(expected_y_simpler,  <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.500</span>, <span class="fl">0.975</span>)) </span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>ev_range</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>    2.5%      50%    97.5% 
5.505104 5.898148 6.291150 </code></pre>
</div>
</div>
<p>The 95% interval is therefore between <code>5.51</code> and <code>6.29</code>, with the median (similar but not quite the point estimate) being <code>5.90</code>. Let’s plot this against the data:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>toy_df <span class="sc">|&gt;</span> </span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span> </span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">annotate</span>(<span class="st">"point"</span>, <span class="at">x =</span> candidate_x, <span class="at">y =</span>  <span class="fu">median</span>(expected_y_simpler), <span class="at">size =</span> <span class="fl">1.2</span>, <span class="at">shape =</span> <span class="dv">2</span>, <span class="at">colour =</span> <span class="st">"blue"</span>) <span class="sc">+</span> </span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">annotate</span>(<span class="st">"segment"</span>, <span class="at">x =</span> candidate_x, <span class="at">xend=</span>candidate_x, <span class="at">y =</span> ev_range[<span class="dv">1</span>], <span class="at">yend =</span> ev_range[<span class="dv">3</span>], <span class="at">colour =</span> <span class="st">"blue"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-29-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The vertical blue line therefore shows the range of estimates for <span class="math inline">\(Y|x=2.5\)</span> that contain 95% of the expected values given the draws of <span class="math inline">\(\beta = \{\beta_0, \beta_1\}\)</span> which we produced from the Multivariate Normal given the point estimates and Hessian from <code>optim()</code>. This is our estimated range for the <em>expected value</em>, not <em>predicted value</em>. What’s the difference?</p>
</section>
<section id="predicted-values" class="level3">
<h3 class="anchored" data-anchor-id="predicted-values">Predicted values</h3>
<p>One clue about the difference between expected value lies in the parameters from <code>optim()</code> we did and did not use: Whereas we have both point estimates and uncertainty estimates for the parameters <span class="math inline">\(\{\beta_0, \beta_1, \sigma^2\}\)</span>,<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> we only made use of the the two <span class="math inline">\(\beta\)</span> parameters when producing this estimate.</p>
<p>Now let’s recall the general model formula, from the start of <span class="citation" data-cites="KinTomWit00">King, Tomz, and Wittenberg (<a href="#ref-KinTomWit00" role="doc-biblioref">2000</a>)</span>, which we repeated for the first few posts in the series:</p>
<p><strong>Stochastic Component</strong></p>
<p><span class="math display">\[
Y_i \sim f(\theta_i, \alpha)
\]</span></p>
<p><strong>Systematic Component</strong></p>
<p><span class="math display">\[
\theta_i = g(X_i, \beta)
\]</span></p>
<p>The manual for Zelig, the (now defunct) R package that used to support analysis using this approach, states that for Normal Linear Regression <a href="https://docs.zeligproject.org/articles/zelig_normal.html#model">these two components are resolved as follows</a>:</p>
<p><strong>Stochastic Component</strong></p>
<p><span class="math display">\[
Y_i \sim Normal(\mu_i, \sigma^2)
\]</span></p>
<p><strong>Systematic Component</strong></p>
<p><span class="math display">\[
\mu_i = x_i \beta
\]</span></p>
<p>The page then goes onto state that the <em>expected value</em>, <span class="math inline">\(E(Y)\)</span>, is :</p>
<p><span class="math display">\[
E(Y) = \mu_i = x_i \beta
\]</span></p>
<p>So, in this case, the expected value is the systematic component only, and does not involve the dispersion parameter in the stochastic component, which for normal linear regression is the <span class="math inline">\(\sigma^2\)</span> term. That’s why we didn’t use estimates of <span class="math inline">\(\sigma^2\)</span> when simulating the expected values.</p>
<p>But why is this? Well, it comes from the expectation operator, <span class="math inline">\(E(.)\)</span>. This operator means something like, <em>return to me the value that would be expected if this experiment were performed an infinite number of times</em>.</p>
<p>There are two types of uncertainty which give rise to variation in the predicted estimate: <em>sampling uncertainty</em>, and <em>stochastic variation</em>. In the expected value condition, this second source of variation falls to zero,<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> leaving only the influence of sampling uncertainty, as in uncertainty about the true value of the <span class="math inline">\(\beta\)</span> parameters, remaining on uncertainty on the predicted outputs.</p>
<p>For <em>predicted values</em>, we therefore need to reintroduce <em>stochastic variation</em> as a source of variation in the range of estimates produced. Each <span class="math inline">\(\eta\)</span> value we have implies a different <span class="math inline">\(\sigma^2\)</span> value in the stochastic part of the equation, which we can then add onto the variation caused by parameter uncertainty alone:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">nrow</span>(param_draws)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>predicted_y_simpler <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, N)</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N){</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>    predicted_y_simpler[i] <span class="ot">&lt;-</span> param_draws[i, <span class="st">"beta0"</span>] <span class="sc">+</span> candidate_x <span class="sc">*</span> param_draws[i, <span class="st">"beta1"</span>] <span class="sc">+</span> </span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>        <span class="fu">rnorm</span>(</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>            <span class="dv">1</span>, <span class="at">mean =</span> <span class="dv">0</span>, </span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>            <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="fu">exp</span>(param_draws[i, <span class="st">"eta"</span>]))</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(predicted_y_simpler)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4.802092 6.706397 7.073450 6.118750 6.757717 7.461254</code></pre>
</div>
</div>
<p>Let’s now get the 95% prediction interval for the predicted values, and compare them with the expected values predicted interval earlier</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>pv_range <span class="ot">&lt;-</span> </span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">quantile</span>(</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>        predicted_y_simpler, </span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.500</span>, <span class="fl">0.975</span>)</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>pv_range</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>    2.5%      50%    97.5% 
4.766300 5.895763 7.055408 </code></pre>
</div>
</div>
<p>So, whereas the median is similar to before, <code>5.90</code>, the 95% interval is now from <code>4.77</code> to <code>7.06</code><a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>. This compares with the <code>5.51</code> to <code>6.29</code> range for the expected values. Let’s now plot this predicted value range just as we did with the expected values:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>toy_df <span class="sc">|&gt;</span> </span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span> </span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">annotate</span>(<span class="st">"point"</span>, <span class="at">x =</span> candidate_x, <span class="at">y =</span>  pv_range[<span class="dv">2</span>], <span class="at">size =</span> <span class="fl">1.2</span>, <span class="at">shape =</span> <span class="dv">2</span>, <span class="at">colour =</span> <span class="st">"blue"</span>) <span class="sc">+</span> </span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">annotate</span>(<span class="st">"segment"</span>, <span class="at">x =</span> candidate_x, <span class="at">xend=</span>candidate_x, <span class="at">y =</span> pv_range[<span class="dv">1</span>], <span class="at">yend =</span> pv_range[<span class="dv">3</span>], <span class="at">colour =</span> <span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-32-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Clearly considerably wider.</p>
</section>
</section>
<section id="log-likelihood-for-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="log-likelihood-for-logistic-regression">Log likelihood for logistic regression</h2>
<p>Previously we derived the log likelihood for Normal (Gaussian) regression and did some cool things with it. Let’s now do the same with logistic regression. We need to start with definition, then calculate log likelihood, then write it as a function in R that <code>optim()</code> can work its magic with.</p>
<p>According to <a href="https://docs.zeligproject.org/articles/zelig_logit.html#model-definition">the relevant section of the Zelig website</a>:</p>
<p><strong>Stochastic component</strong> <span class="math display">\[
Y_i \sim Bernoulli(y_i | \pi_i )
\]</span></p>
<p><span class="math display">\[
Y_i = \pi_i^{y_i}(1 - \pi_i)^{1-y_i}
\]</span></p>
<p>where <span class="math inline">\(\pi_i = P(Y_i = 1)\)</span></p>
<p>And</p>
<p><strong>Systematic Component</strong></p>
<p><span class="math display">\[
\pi_i = \frac{1}{1 + \exp{(-x_i \beta)}}
\]</span></p>
<p>The likelihood is the product of the above for all observations in the dataset <span class="math inline">\(i \in N\)</span></p>
<p><span class="math display">\[
L(.) = \prod{\pi_i^{y_i}(1 - \pi_i)^{1-y_i}}
\]</span></p>
<p>The effect of logging the above<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>:</p>
<p><span class="math display">\[
\log{L(.)} = \sum{[y_i \log{\pi_i} + (1-y_i)\log{(1-y_i)}]}
\]</span></p>
<p>This can now be implemented as a function:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>llogit <span class="ot">&lt;-</span> <span class="cf">function</span>(par, y, X){</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>    xform <span class="ot">&lt;-</span> <span class="cf">function</span>(z) {<span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>z))}</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    p <span class="ot">&lt;-</span> <span class="fu">xform</span>(X<span class="sc">%*%</span>par)</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sum</span>(y <span class="sc">*</span> <span class="fu">log</span>(p) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">-</span>y) <span class="sc">*</span> <span class="fu">log</span>(<span class="dv">1</span> <span class="sc">-</span> p))</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let’s pick an appropriate dataset. How about… picking a <a href="https://allisonhorst.github.io/palmerpenguins/">Palmer Penguin</a>!?</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>palmerpenguins<span class="sc">::</span>penguins</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 344 × 8
   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g
   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;
 1 Adelie  Torgersen           39.1          18.7               181        3750
 2 Adelie  Torgersen           39.5          17.4               186        3800
 3 Adelie  Torgersen           40.3          18                 195        3250
 4 Adelie  Torgersen           NA            NA                  NA          NA
 5 Adelie  Torgersen           36.7          19.3               193        3450
 6 Adelie  Torgersen           39.3          20.6               190        3650
 7 Adelie  Torgersen           38.9          17.8               181        3625
 8 Adelie  Torgersen           39.2          19.6               195        4675
 9 Adelie  Torgersen           34.1          18.1               193        3475
10 Adelie  Torgersen           42            20.2               190        4250
# ℹ 334 more rows
# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;</code></pre>
</div>
</div>
<p>Let’s say we want to predict whether a penguin is of the Chinstrap species</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>palmerpenguins<span class="sc">::</span>penguins <span class="sc">%&gt;%</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(<span class="fu">complete.cases</span>(.)) <span class="sc">|&gt;</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">is_chinstrap =</span> species <span class="sc">==</span> <span class="st">"Chinstrap"</span>) <span class="sc">|&gt;</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> bill_length_mm, <span class="at">y =</span> bill_depth_mm, <span class="at">colour =</span> is_chinstrap, <span class="at">shape =</span> sex)) <span class="sc">+</span> </span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-35-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Neither bill length nor bill depth alone appears to distinguish between chinstrap and other species. But perhaps the interaction (product) of the two terms would do:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>palmerpenguins<span class="sc">::</span>penguins <span class="sc">%&gt;%</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(<span class="fu">complete.cases</span>(.)) <span class="sc">|&gt;</span></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">is_chinstrap =</span> species <span class="sc">==</span> <span class="st">"Chinstrap"</span>) <span class="sc">|&gt;</span></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">bill_size =</span> bill_length_mm <span class="sc">*</span> bill_depth_mm) <span class="sc">|&gt;</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> bill_size, <span class="at">fill =</span> is_chinstrap)) <span class="sc">+</span> </span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">facet_wrap</span>(<span class="sc">~</span>sex) <span class="sc">+</span> </span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_histogram</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-36-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The interaction term isn’t great at separating the two classes, but seems to be better than either length or size alone. So I’ll include it in the model.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> palmerpenguins<span class="sc">::</span>penguins <span class="sc">%&gt;%</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(<span class="fu">complete.cases</span>(.)) <span class="sc">|&gt;</span></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">is_chinstrap =</span> species <span class="sc">==</span> <span class="st">"Chinstrap"</span>) <span class="sc">|&gt;</span></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">bill_size =</span> bill_length_mm <span class="sc">*</span> bill_depth_mm) <span class="sc">|&gt;</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">is_male =</span> <span class="fu">as.numeric</span>(sex <span class="sc">==</span> <span class="st">"male"</span>))</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> df<span class="sc">$</span>is_chinstrap</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, df[,<span class="fu">c</span>(<span class="st">"bill_length_mm"</span>, <span class="st">"bill_depth_mm"</span>, <span class="st">"bill_size"</span>, <span class="st">"is_male"</span>)]) <span class="sc">|&gt;</span></span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a><span class="fu">as.matrix</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>So, including the intercept term, our predictor matrix <span class="math inline">\(X\)</span> contains 5 columns, including the interaction term <code>bill_size</code>. <a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></p>
<p>Let’s try now to use the above in <code>optim()</code></p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>fuller_optim_output <span class="ot">&lt;-</span> <span class="fu">optim</span>(</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">par =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">5</span>), </span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">fn =</span> llogit,</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">"BFGS"</span>,</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">control =</span> <span class="fu">list</span>(<span class="at">fnscale =</span> <span class="sc">-</span><span class="dv">1</span>),</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">hessian =</span> <span class="cn">TRUE</span>,</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> y, </span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">X =</span> X</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>fuller_optim_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>$par
[1] 82.9075239 -2.4368673 -6.4311531  0.1787047 -6.4900678

$value
[1] -33.31473

$counts
function gradient 
     137       45 

$convergence
[1] 0

$message
NULL

$hessian
             [,1]         [,2]          [,3]         [,4]         [,5]
[1,]   -12.103063    -550.0621    -209.30944    -9674.925    -3.700623
[2,]  -550.062097  -25256.3082   -9500.55848  -443670.225  -184.360139
[3,]  -209.309443   -9500.5585   -3650.65107  -168517.417   -68.158844
[4,] -9674.924703 -443670.2251 -168517.41718 -7846293.352 -3464.964868
[5,]    -3.700623    -184.3601     -68.15884    -3464.965    -3.700623</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>hess <span class="ot">&lt;-</span> fuller_optim_output<span class="sc">$</span>hessian</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>inv_hess <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="sc">-</span>hess)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>inv_hess</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>            [,1]         [,2]         [,3]          [,4]         [,5]
[1,] 41.95816335 -0.156192235 -0.309892876 -4.036895e-02  9.329019450
[2,] -0.15619224 -0.005017392 -0.024806420  1.070652e-03 -0.139430425
[3,] -0.30989288 -0.024806420 -0.042869947  2.854565e-03 -0.337480429
[4,] -0.04036895  0.001070652  0.002854565 -7.331214e-05  0.003098092
[5,]  9.32901945 -0.139430425 -0.337480429  3.098092e-03  1.202424836</code></pre>
</div>
</div>
<p>Now let’s compare with <code>glm()</code></p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>mod_glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(is_chinstrap <span class="sc">~</span> bill_length_mm <span class="sc">*</span> bill_depth_mm <span class="sc">+</span>is_male, <span class="at">data =</span> df, </span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="at">family =</span> <span class="fu">binomial</span>())</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod_glm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = is_chinstrap ~ bill_length_mm * bill_depth_mm + 
    is_male, family = binomial(), data = df)

Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                  365.2924    88.3341   4.135 3.54e-05 ***
bill_length_mm                -8.9312     2.0713  -4.312 1.62e-05 ***
bill_depth_mm                -23.6184     5.5003  -4.294 1.75e-05 ***
is_male                      -11.8725     2.6121  -4.545 5.49e-06 ***
bill_length_mm:bill_depth_mm   0.5752     0.1292   4.452 8.53e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 337.113  on 332  degrees of freedom
Residual deviance:  49.746  on 328  degrees of freedom
AIC: 59.746

Number of Fisher Scoring iterations: 9</code></pre>
</div>
</div>
<p>Uh oh! On this occasion it appears one or both approaches have become confused. A five dimensional search space might be too much for the algorithms to cope with, especially with collinearity <a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> between some of the terms. Let’s simplify the task a bit, and just use intercept, bill size, and is_male as covariates. First with the standard package:</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>mod_glm_simpler <span class="ot">&lt;-</span> <span class="fu">glm</span>(is_chinstrap <span class="sc">~</span> bill_size <span class="sc">+</span>is_male,   <span class="at">data =</span> df, </span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="at">family =</span> <span class="fu">binomial</span>())</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod_glm_simpler)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = is_chinstrap ~ bill_size + is_male, family = binomial(), 
    data = df)

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -32.815339   4.325143  -7.587 3.27e-14 ***
bill_size     0.043433   0.005869   7.400 1.36e-13 ***
is_male      -7.038215   1.207740  -5.828 5.62e-09 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 337.11  on 332  degrees of freedom
Residual deviance:  90.60  on 330  degrees of freedom
AIC: 96.6

Number of Fisher Scoring iterations: 7</code></pre>
</div>
</div>
<p>And now with the bespoke function and optim</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, df[,<span class="fu">c</span>(<span class="st">"bill_size"</span>, <span class="st">"is_male"</span>)]) <span class="sc">|&gt;</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="fu">as.matrix</span>()</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>fuller_optim_output <span class="ot">&lt;-</span> <span class="fu">optim</span>(</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">par =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>), </span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">fn =</span> llogit,</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">"BFGS"</span>,</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">control =</span> <span class="fu">list</span>(<span class="at">fnscale =</span> <span class="sc">-</span><span class="dv">1</span>),</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">hessian =</span> <span class="cn">TRUE</span>,</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> y, </span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">X =</span> X</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>fuller_optim_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>$par
[1] -32.60343219   0.04314546  -6.98585077

$value
[1] -45.30114

$counts
function gradient 
      73       18 

$convergence
[1] 0

$message
NULL

$hessian
              [,1]         [,2]         [,3]
[1,]    -13.008605   -10662.078    -5.201308
[2,] -10662.078251 -8846787.584 -4846.390833
[3,]     -5.201308    -4846.391    -5.201308</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>hess <span class="ot">&lt;-</span> fuller_optim_output<span class="sc">$</span>hessian</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>inv_hess <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="sc">-</span>hess)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>inv_hess</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>             [,1]          [,2]         [,3]
[1,] -536.7022079  0.7206703142 -134.7923170
[2,]    0.7206703 -0.0009674672    0.1807806
[3,] -134.7923170  0.1807806218  -33.4602664</code></pre>
</div>
</div>
<p>The estimates from the two approaches are now much closer, even if they aren’t as close to each other as in the earlier examples. Using <code>optim()</code>, we have parameter estimates <span class="math inline">\(\beta = \{\beta_0 = -32.60, \beta_1 = 0.04, \beta_2 = -6.99\}\)</span>, and using <code>glm()</code>, we have estimates <span class="math inline">\(\beta = \{\beta_0 = -32.82, \beta_1 = 0.04, \beta_2 = -7.04 \}\)</span></p>
<p>If we cheat a bit, and give the five dimensional version starting values closer to the estimates from <code>glm()</code>, we can probably get similar estimates too.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, df[,<span class="fu">c</span>(<span class="st">"bill_length_mm"</span>, <span class="st">"bill_depth_mm"</span>, <span class="st">"bill_size"</span>, <span class="st">"is_male"</span>)]) <span class="sc">|&gt;</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="fu">as.matrix</span>()</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>fuller_optim_output <span class="ot">&lt;-</span> <span class="fu">optim</span>(</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">par =</span> <span class="fu">c</span>(<span class="dv">300</span>, <span class="sc">-</span><span class="dv">10</span>, <span class="sc">-</span><span class="dv">29</span>, <span class="fl">0.5</span>, <span class="sc">-</span><span class="dv">10</span>), </span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">fn =</span> llogit,</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">"BFGS"</span>,</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">control =</span> <span class="fu">list</span>(<span class="at">fnscale =</span> <span class="sc">-</span><span class="dv">1</span>),</span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">hessian =</span> <span class="cn">TRUE</span>,</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> y, </span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">X =</span> X</span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>fuller_optim_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>$par
[1] 299.5512512  -7.3684567 -19.3951742   0.4747209  -9.7521255

$value
[1] -25.33208

$counts
function gradient 
     153       22 

$convergence
[1] 0

$message
NULL

$hessian
             [,1]          [,2]          [,3]         [,4]         [,5]
[1,]    -8.378918    -370.41592    -140.86865    -6342.301    -1.800406
[2,]  -370.415921  -16580.87909   -6238.75358  -284403.350   -91.239716
[3,]  -140.868648   -6238.75358   -2387.19776  -107598.410   -33.018551
[4,] -6342.300809 -284403.34960 -107598.40987 -4906697.476 -1685.235507
[5,]    -1.800406     -91.23972     -33.01855    -1685.236    -1.800406</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>hess <span class="ot">&lt;-</span> fuller_optim_output<span class="sc">$</span>hessian</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>inv_hess <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="sc">-</span>hess)</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>inv_hess</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>            [,1]         [,2]        [,3]          [,4]         [,5]
[1,] -59.5448267  2.316365876  5.14842594 -0.1737609491 10.383684649
[2,]   2.3163659 -0.064512887 -0.16844980  0.0044962968 -0.166413655
[3,]   5.1484259 -0.168449797 -0.33888931  0.0106735535 -0.387558164
[4,]  -0.1737609  0.004496297  0.01067355 -0.0002712683  0.004068597
[5,]  10.3836846 -0.166413655 -0.38755816  0.0040685965  1.904433768</code></pre>
</div>
</div>
<p>Well, they are <em>closer</em>, but they aren’t <em>very close</em>. As mentioned, the <code>glm()</code> model produced warnings, and some of the variables are likely to be collinear, so this initial specification may have been especially difficult to fit. Both approaches found an answer, but neither seem happy about it!</p>
<section id="summary-1" class="level3">
<h3 class="anchored" data-anchor-id="summary-1">Summary</h3>
<p>In the exercise above we did for logistic regression what the previous few posts in section two did for standard regression: i.e.&nbsp;we derived the log likelihood, applied it using optim, and compared with results from the <code>glm()</code> package. We saw in this case that fitting models isn’t always straightforward. We were - well, I was - overly ambitious in building and applying an overly parameterised model specification. But we eventually got to similar parameter values using both approaches.</p>
<p>Though this wasn’t as straightforward as I was hoping for, I’m presenting it warts-and-all. In principle, the log-likelihood maximisation approach generalises to a great many model specifications, even if in practice some model structures aren’t as straightforward to fit as others.</p>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-King98" class="csl-entry" role="listitem">
King, Gary. 1998. <em>Unifying Political Methodology: The Likelihood Theory of Statistical Inference</em>. University of Michigan Press. <a href="http://www.jstor.org/stable/10.3998/mpub.23784">http://www.jstor.org/stable/10.3998/mpub.23784</a>.
</div>
<div id="ref-KinTomWit00" class="csl-entry" role="listitem">
King, Gary, Michael Tomz, and Jason Wittenberg. 2000. <span>“Making the Most of Statistical Analyses: Improving Interpretation and Presentation.”</span> <em>American Journal of Political Science</em> 44: 341355. <a href="http://gking.harvard.edu/files/abs/making-abs.shtml">http://gking.harvard.edu/files/abs/making-abs.shtml</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>If <span class="math inline">\(Y_i\)</span> is what the model predicts given observations <span class="math inline">\(X_i\)</span>, and <span class="math inline">\(y_i\)</span> is the outcome observed to have occurred alongside <span class="math inline">\(X_i\)</span>, then we can call <span class="math inline">\(\delta_i = h(y_i, Y_i)\)</span> the difference, or error, between predicted and observed value. The function <span class="math inline">\(h(.,.)\)</span> is typically the squared difference between predicted and observed values, <span class="math inline">\((Y_i - y_i)^2\)</span>, but could also in principle be the absolute difference <span class="math inline">\(|Y_i - y_i|\)</span>. Term-fitting algorithms usually compare not any individual <span class="math inline">\(\delta_i\)</span>, but a sum of these error terms <span class="math inline">\(\delta\)</span>. The aim of the algorithm is to find the set of <span class="math inline">\(\beta\)</span> terms that is least wrong for the whole dataset <span class="math inline">\(D\)</span>, rather than any specific row in the dataset <span class="math inline">\(D_i\)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>As <span class="citation" data-cites="King98">King (<a href="#ref-King98" role="doc-biblioref">1998</a>)</span> (p.&nbsp;59) describes it, “<span class="math inline">\(k(y)\)</span> is an unknown fuction of the data. Whereas traditional probability is a measure of <em>absolute</em> uncertainty … the constant <span class="math inline">\(k(y)\)</span> means that likelihood is only a <em>relative</em> measure of uncertainty”<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Frequentist approaches can thus be considered a kind of ‘improper Bayesian’ approach by considering <span class="math inline">\(k(y)\)</span> in the Likelihood formula as a stand-in for <span class="math inline">\(\frac{P(\tilde{\theta})}{P(y)}\)</span> in Bayes’ Rule. Roughly speaking, it’s because of the improperness of treating the two terms as equivalent, and the relativeness of <span class="math inline">\(k(y)\)</span>, that mean frequentist probability statements can’t be interpreted as Bayesian probability statements. But thinking of the two terms as equivalent can be helpful for spotting the similarity between the two formulae.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>i.e.&nbsp;the square of the <code>sd</code> passed to <code>rnorm()</code> of 0.5<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><span class="math inline">\(101^2 = 10201\)</span><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p><span class="math inline">\(101^3 = 1030301\)</span><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Though I had assumed Hessian matrices are called Hessian matrices because they sort-of resemble the criss-crossing grids of Hessian bags, they’re actually named after <a href="https://en.wikipedia.org/wiki/Otto_Hesse">Otto Hesse</a>, who proposed them.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>I’ve narrowed the space between values slightly, and increased the range of permutations of values to search through, for an even more precise recovery of the likelihood landscape.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>In practice, the algorithm seeks to minimise the value returned by the function, not maximise it, hence the negative being applied through the argument <code>fnscale = -1</code> in the <code>control</code> argument. But the principle is identical.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>This means that, whereas the standard Normal returns a single output, the Multivariate Normal returns a vector of outputs, one for each parameter in <span class="math inline">\(\theta\)</span>, which should also be the length of the diagonal (or alternatively either the number of rows or columns) of <span class="math inline">\(\Sigma\)</span>.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>The values will not be identical because the values for <span class="math inline">\(\eta\)</span>, and so <span class="math inline">\(\sigma^2\)</span>, have not been fixed at the true value in this example.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>Where <span class="math inline">\(\sigma^2\)</span> is from <span class="math inline">\(\eta\)</span> and we defined <span class="math inline">\(e^{\eta} = \sigma^2\)</span>, a transformation which allowed <code>optim()</code> to search over an unbounded rather than bounded real number line<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>It can be easier to see this by using the more conventional way of expressing Normal linear regression: <span class="math inline">\(Y_i = x_i \beta + \epsilon\)</span>, where <span class="math inline">\(\epsilon \sim Normal(0, \sigma^2)\)</span>. The expectation is therefore <span class="math inline">\(E(Y_i) = E( x_i \beta + \epsilon ) = E(x_i \beta) + E(\epsilon)\)</span>. For the first part of this equation, <span class="math inline">\(E(x_i \beta) = x_i \beta\)</span>, because the systematic component is always the same value, no matter how many times a draw is taken from the model. And for the second part, <span class="math inline">\(E(\epsilon) = 0\)</span>, because Normal distributions are symmetrical around their central value over the long term: on average, every large positive value drawn from this distribution will become cancelled out by an equally large negative value, meaning the expected value returned by the distribution is zero. Hence, <span class="math inline">\(E(Y) = x_i \beta\)</span>.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>Because these estimates depend on random variation, these intervals may be slightly different to two decimal places than the values I’m quoting here.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>Thanks to <a href="https://arunaddagatla.medium.com/maximum-likelihood-estimation-in-logistic-regression-f86ff1627b67">this post</a>. My calculus is a bit rusty these days.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>An important point to note is that, though bill_size is derived from other variables, it’s its own variable, and so has another distinct ‘slot’ in the vector of <span class="math inline">\(\beta\)</span> parameters. It’s just another dimension in the search space for optim to search through.<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>This is fancy-speak for when two terms aren’t independent, or both adding unique information. For example, length in mm, length in cm, and length in inches would all be perfectly collinear, so shouldn’t all be included in the model.<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>